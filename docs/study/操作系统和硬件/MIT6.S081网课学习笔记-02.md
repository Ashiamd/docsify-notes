# MIT6.S081网课学习笔记-02

> [MIT6.S081 操作系统工程中文翻译 - 知乎 (zhihu.com)](https://www.zhihu.com/column/c_1294282919087964160) => 知乎大佬视频中文笔记，很全
>
> [6.S081 / Fall 2020 (mit.edu)](https://pdos.csail.mit.edu/6.S081/2020/schedule.html) => 课表+实验超链接等信息

# Lecture13 Sleep & Wakeup

​	该章节大部分时间都会讨论coordination，XV6通过Sleep&Wakeup实现了coordination。后面会讨论lost wake-up的问题。

## 13.1 XV6线程switch的锁限制

> [13.1 线程切换过程中锁的限制 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349778171) <= 图文来自此博文

这里先说结论，**switch中两个锁限制**：

1. switch之前，需要先acquire当前进程锁`p->lock`

2. switch之前，当前进程**必须有且只持有**进程锁`p->lock`。(不能持有其他锁，否则可能导致死锁)

---

​	先回顾线程切换switch。**在XV6中，任何时候调用switch函数都会从一个线程切换到另一个线程，通常是在用户进程的内核线程和调度器线程之间切换。在调用switch函数之前，总是会先获取线程对应的用户进程的锁。所以过程是这样，一个进程先获取自己的锁，然后调用switch函数切换到调度器线程，调度器线程再释放进程锁**。	

​	代码执行顺序大致如下：

1. 一个进程出于某种原因想要进入休眠状态，比如说yield出让CPU或者等待数据，它会先acquire获取自己的锁；
2. 之后进程将自己的状态从RUNNING设置为RUNNABLE；
3. 之后进程调用switch函数，其实是调用sched函数（sched里会调用switch函数）；
4. switch函数将当前的线程切换到调度器线程；
5. 调度器线程之前也调用了switch函数，现在恢复执行会从自己的switch函数返回；
6. 返回之后，调度器线程会release释放刚刚出让了CPU的进程的锁

![img](https://pic3.zhimg.com/80/v2-b295bd6e4562ba5fda1cbd48cd89cc8e_1440w.webp)

​	**上面最需要关注的就是第一步acquire上锁**。<u>如果没有上锁，直接改变进程状态为RUNNABLE，那么其他CPU核的调度器线程很可能正好遍历进程表单发现这个刚转为RUNNABLE的进程，进而出现当前CPU核、其他CPU核都运行这个进程的现象，这会使系统崩溃。所以，在进程切换的最开始，进程先获取自己的锁，并且直到调用switch函数时也不释放锁。而另一个线程，也就是调度器线程会在进程的线程完全停止使用自己的栈之后，再释放进程的锁。释放锁之后，就可以由其他的CPU核再来运行进程的线程，因为这些线程现在已经不在运行了</u>。**简言之，需要保证切换进程的整个过程的原子性，这个过程需要锁保护，直到进程切换完毕后，才释放锁**。

​	另外，XV6的switch有个锁使用的重点限制，**XV6在进程切换过程中，要求当前进程有且仅持有进程锁`p->lock`**(proc进程结构体中的lock锁对象)。<u>其他诸如Sleep的实现也有类似的要求</u>。

​	下面通过反例解释，为什么XV6进程切换过程中，进程必须有且仅持有`p->lock`进程锁。

1. 假设P1进程之前调用了UART或者console等需要与硬件交互的函数，于是持有lock1；
2. P1由于等待硬件反馈，准备主动yield出让CPU资源，于是先acquire了`p->lock`，然后调用switch
3. 线程切换到CPU0的调度线程，然后正常switch流程，切换到P2，释放P1的`p->lock`
4. 切换到P2后，由于P2也想调用UART或者console等与硬件交互的函数，于是申请P1之前占用的lock1，调用acquire函数
5. 由于P1已经占有lock1并且switch之前没有释放锁，而且acquire函数实现中第一步就是关闭中断，所以现在P2在while死循环里无限申请lock1，又无法被中断
6. CPU0死锁，资源被P2一直占用无法让出....

​	*这里之所以P2没机会获取P1占有的锁，是因为P2对于锁的acquire调用在直到锁释放之前都不会返回，而唯一锁能被释放的方式就是进程P1恢复执行并在稍后release锁，但是这一步又还没有发生，因为进程P1通过调用switch函数切换到了P2，而P2又在不停的“旋转”并等待锁被释放。这是一种死锁，它会导致系统停止运行*。

​	虽然上面描述是基于机器上只有一个CPU核，但是你可以通过多个锁在多个CPU核的机器上构建类似的死锁场景。所以，**在XV6中禁止在调用switch时持有除进程自身锁（注，也就是p->lock）以外的其他锁**。

​	将这里描述的对于锁的两个限制条件记住，因为我们后面讨论Sleep&Wakeup如何工作时会再次使用它们。

---

问题：当我们有多个CPU核时，它们能看到同样的锁对象的唯一原因只可能是它们有一个共享的物理内存系统，对吧？

回答：是的。如果你有两个电脑，那么它们不会共享内存，并且我们就不会有这些问题。现在的处理器上，总是有多个CPU核，它们共享了相同的内存系统。

问题：难道定时器中断不会将CPU控制切换回进程P1从而解决死锁的问题吗？

回答：**首先，所有的进程切换过程都发生在内核中，所有的acquire，switch，release都发生在内核代码而不是用户代码**。实际上XV6允许在执行内核代码时触发中断，如果你查看`trap.c`中的代码你可以发现，**如果XV6正在执行内核代码时发生了定时器中断，中断处理程序会调用yield函数并出让CPU**。但是在之前的课程中我们讲过**acquire函数在等待锁之前会关闭中断**，否则的话可能会引起死锁（比如响应中断的时候，中断处理程序又尝试获取某个已被其他进程占用的锁），所以我们不能在等待锁的时候处理中断。**如果你查看XV6中的acquire函数，你可以发现函数中第一件事情就是关闭中断，之后再“自旋”等待锁释放**。你或许会想，为什么不能先“自旋”等待锁释放，再关闭中断？因为这样会有一个短暂的时间段锁被持有了但是中断没有关闭，在这个时间段内的设备的中断处理程序可能会引起死锁。所以不幸的是，当我们在自旋等待锁释放时会关闭中断，进而阻止了定时器中断并且阻止了进程P2将CPU出让回给进程P1。嗯，这是个好问题。

问题：能重复一下XV6进程切换中，死锁是如何避免的吗？

回答：在XV6中，**死锁是通过禁止在线程切换的时候加锁（这里指除了进程锁以外不持有其他锁）来避免的**。**XV6禁止在调用switch函数时，获取除了`p->lock`以外的其他锁**。如果你查看sched函数的代码里面包含了一些检查代码来确保除了`p->lock`以外线程不持有其他锁。所以上面会产生死锁的代码在XV6中是不合法的并被禁止的。

## 13.2 Sleep&WakeUp

> [13.2 Sleep&Wakeup 接口 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349778459) <= 图文出处

​	当你在写一个线程的代码时，有些场景需要等待一些特定的事件，或者不同的线程之间需要交互：

- 假设我们有一个Pipe，并且我正在从Pipe中读数据。但是Pipe当前又没有数据，所以我需要等待一个Pipe非空的事件（即读操作会阻塞到有数据写入Pipe为止）。
- 类似的，假设我在读取磁盘，我会告诉磁盘控制器请读取磁盘上的特定块。这或许要花费较长的时间，尤其当磁碟需要旋转时（通常是毫秒级别），磁盘才能完成读取。而执行读磁盘的进程需要等待读磁盘结束的事件。
- 类似的，一个Unix进程可以调用wait函数。这个会使得调用进程等待任何一个子进程退出。所以这里父进程有意在等待另一个进程产生的事件。

​	以上就是进程需要等待特定事件的一些例子。特定事件可能来自于I/O，也可能来自于另一个进程，并且它描述了某件事情已经发生。<u>Coordination是帮助我们解决这些问题并帮助我们实现这些需求的工具。Coordination是非常基础的工具，就像锁一样，在实现线程代码时它会一直出现</u>。

​	我们怎么能让进程或者线程等待一些特定的事件呢？一种非常直观的方法是通过循环实现busy-wait。假设我们想从一个Pipe读取数据，我们就写一个循环一直等待Pipe的buffer不为空。实际中会有这样的代码。**如果你知道你要等待的事件极有可能在0.1微秒内发生，通过循环等待或许是最好的实现方式**。通常来说在操作设备硬件的代码中会采用这样的等待方式，如果你要求一个硬件完成一个任务，并且你知道硬件总是能非常快的完成任务，这时通过一个类似的循环等待或许是最正确的方式。另一方面，事件可能需要数个毫秒甚至你都不知道事件要多久才能发生，或许要10分钟其他的进程才能向Pipe写入数据，那么我们就不想在这一直循环并且浪费本可以用来完成其他任务的CPU时间。这时我们想要通过类似switch函数调用的方式出让CPU，并在我们关心的事件发生时重新获取CPU。**Coordination就是有关出让CPU，直到等待的事件发生再恢复执行。人们发明了很多不同的Coordination的实现方式，但是与许多Unix风格操作系统一样，XV6使用的是Sleep&Wakeup这种方式**。

​	下面通过教师编写的UART驱动代码，看看sleep&wakeup在程序中的应用：

```c
// uart.c

// transmit buf[].
void uartwrite(char buf[], int n)
{
  acquire(&uart_tx_lock);

  int i = 0;
  while(i < n){
    while(tx_done == 0){
      // UART is busy sending a character.
      // wait for it to interrupt.
      sleep(&tx_chan, &uart_tx_lock);
    }
    WriteReg(THR, buf[i]);
    i += 1;
    tx_done = 0;
  }

  release(&uart_tx_lock);
}

// ...

// handle a uart interrupt, raised because input has
// arrived, or the uart is ready for more output, or
// both. called from trap.c.
void uartintr(void)
{
  acquire(&uart_tx_lock);
  if(ReadReg(LSR) & LSR_TX_IDLE) {
    // UART finished transmitting; wake up any sending thread.
    tx_done = 1;
    wakeup(&tx_chan);
  }
  release(&uart_tx_lock);
  
 	// read and process incoming characters.
  while(1) {
    int c = uargetc();
    // ....
  }
  // ... 
}
```

​	首先是uartwrite函数。当shell需要输出时会调用write系统调用最终走到uartwrite函数中，这个函数会在循环中将buf中的字符一个一个的向UART硬件写入。这是一种经典的设备驱动实现风格，你可以在很多设备驱动中看到类似的代码。UART硬件一次只能接受一个字符的传输，而通常来说会有很多字符需要写到UART硬件。你可以向UART硬件写入一个字符，并等待UART硬件说：好的我完成了传输上一个字符并且准备好了传输下一个字符，之后驱动程序才可以写入下一个字符。因为这里的硬件可能会非常慢，或许每秒只能传输1000个字符，所以我们在两个字符之间的等待时间可能会很长。而1毫秒在现在计算机上是一个非常非常长的时间，它可能包含了数百万条指令时间，所以我们不想通过循环来等待UART完成字符传输，我们想通过一个更好的方式来等待。如大多数操作系统一样，XV6也的确存在更好的等待方式。

​	<u>UART硬件会在完成传输一个字符后，触发一个中断</u>。所以UART驱动中除了uartwrite函数外，还有名为uartintr的中断处理程序。这个中断处理程序会在UART硬件触发中断时由`trap.c`代码调用。<u>中断处理程序会在最开始读取UART对应的memory mapped register，并检查其中表明传输完成的相应的标志位，也就是LSR_TX_IDLE标志位。如果这个标志位为1，代码会将tx_done设置为1，并调用wakeup函数。这个函数会使得uartwrite中的sleep函数恢复执行，并尝试发送一个新的字符</u>。

​	所以这里的机制是，**如果一个线程需要等待某些事件，比如说等待UART硬件愿意接收一个新的字符，线程调用sleep函数并等待一个特定的条件。当特定的条件满足时，代码会调用wakeup函数。这里的sleep函数和wakeup函数是成对出现的**。我们之后会看sleep函数的具体实现，它会做很多事情最后再调用switch函数来出让CPU。

​	**这里有件事情需要注意，sleep和wakeup函数需要通过某种方式链接到一起。也就是说，如果我们调用wakeup函数，我们只想唤醒正在等待刚刚发生的<u>特定事件</u>的线程**。所以，sleep函数和wakeup函数都带有一个叫做sleep channel的参数。我们在调用wakeup的时候，需要传入与调用sleep函数相同的sleep channel。不过sleep和wakeup函数只是接收表示了sleep channel的64bit数值，它们并不关心这个数值代表什么。**当我们调用sleep函数时，我们通过一个sleep channel表明我们等待的特定事件，当调用wakeup时我们希望能传入相同的数值来表明想唤醒哪个线程**。

​	以上就是接口的演示。<u>Sleep&wakeup的一个优点是它们可以很灵活，它们不关心代码正在执行什么操作，你不用告诉sleep函数你在等待什么事件，你也不用告诉wakeup函数发生了什么事件，你只需要匹配好64bit的sleep channel就行</u>。

​	不过，对于sleep函数，有一个有趣的参数，我们需要将一个锁作为第二个参数传入，这背后是一个大的故事，我后面会介绍背后的原因。**总的来说，不太可能设计一个sleep函数并完全忽略需要等待的事件**。所以很难写一个通用的sleep函数，只是睡眠并等待一些特定的事件，并且这也很危险，因为可能会导致lost wakeup，而**几乎所有的Coordination机制都需要处理lost wakeup的问题**。在sleep接口中，我们需要传入一个锁是一种稍微丑陋的实现，我在稍后会再介绍。

---

问题：进程会在写入每个字符时候都被唤醒一次吗？

回答：在这个我出于演示目的而特别改过的UART驱动中，传输每个字符都会有一个中断，所以你是对的，对于buffer中的每个字符，我们都会等待UART可以接收下一个字符，之后写入一个字符，将tx_done设置为0，回到循环的最开始并再次调用sleep函数进行睡眠状态，直到tx_done为1。当UART传输完了这个字符，uartintr函数会将tx_done设置为1，并唤醒uartwrite所在的线程。所以对于每个字符都有调用一次sleep和wakeup，并占用一次循环。<u>UART实际上支持一次传输4或者16个字符，所以一个更有效的驱动会在每一次循环都传输16个字符给UART，并且中断也是每16个字符触发一次</u>。**更高速的设备，例如以太网卡通常会更多个字节触发一次中断**。

## 13.3 Lost Wakeup

> [13.3 Lost wakeup - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349778792) <= 图文出处

​	在解释sleep函数为什么需要一个锁使用作为参数传入之前，我们先来看看假设我们有了一个更简单的不带锁作为参数的sleep函数，会有什么样的结果。这里的结果就是**lost wakeup**。

​	假设sleep只是接收任意的sleep channel作为唯一的参数。它其实不能正常工作，我们称这个sleep实现为broken_sleep。你可以想象一个sleep函数内会将进程的状态设置为SLEEPING，表明当前进程不想再运行，而是正在等待一个特定的事件。**如果你们看过了XV6的实现，你们可以发现sleep函数中还会做很多其他操作。我们需要记录特定的sleep channel值，这样之后的wakeup函数才能发现是当前进程正在等待wakeup对应的事件。最后再调用switch函数出让CPU**。

![img](https://pic4.zhimg.com/80/v2-835acf240d6e83b90f6a1a2e5811d357_1440w.webp)

​	如果sleep函数只做了这些操作，那么很明显sleep函数会出问题，我们至少还应该在这里获取进程的锁。

​	之后是wakeup函数。**我们希望唤醒所有正在等待特定sleep channel的线程。所以wakeup函数中会查询进程表单中的所有进程，如果进程的状态是SLEEPING并且进程对应的channel是当前wakeup的参数，那么将进程的状态设置为RUNNABLE**。

![img](https://pic3.zhimg.com/80/v2-bd4c79c2ea605b3ab9657b84732caef2_1440w.webp)

​	在一些平行宇宙中，sleep&wakeup或许就是这么简单。在我回到XV6代码之前，让我演示一下如何在UART驱动中使用刚刚介绍的sleep和wakeup函数。这基本上是重复前一节的内容，不过这次我们使用刚刚介绍的稍微简单的接口。

​	首先是定义done标志位。之后是定义uartwrite函数。在函数中，对于buffer内的每一个字符，检查done标志位，如果标志位为0，就调用sleep函数并传入`tx_channel`。之后将字符传递给UART并将done设置为0。

![img](https://pic3.zhimg.com/80/v2-201bea1e993a45403e44f9c0b2ad868e_1440w.webp)

​	之后是中断处理函数uartintr。函数中首先将done标志位设置为1，并调用wakeup。

![img](https://pic4.zhimg.com/80/v2-fed51eb1a190b165f7d3daf3c04d2467_1440w.webp)

​	以上就是使用broken_sleep的方式。这里缺失的是锁。这里uartwrite和uartintr两个函数需要使用锁来协调工作。

- 第一个原因是done标志位，**任何时候我们有了共享的数据，我们需要为这个数据加上锁**。
- 另一个原因是两个函数都需要访问UART硬件，通常来说让两个线程并发的访问memory mapped register是错误的行为。

​	所以我们需要在两个函数中加锁来避免对于done标志位和硬件的竞争访问。

​	现在的问题是，我们该在哪个位置加锁？在中断处理程序中较为简单，我们在最开始加锁，在最后解锁。

![img](https://pic4.zhimg.com/80/v2-c9a90f06431f12f6bf8f1ac01efd03f7_1440w.webp)

​	难的是如何在uartwrite函数中加锁。一种可能是，每次发送一个字符的过程中持有锁，所以在每一次遍历buffer的起始和结束位置加锁和解锁。

![img](https://pic2.zhimg.com/80/v2-cae8ebd13c891a8794825d21c526f9c1_1440w.webp)

​	<u>为什么这样肯定不能工作？一个原因是，我们能从while not done的循环退出的唯一可能是中断处理程序将done设置为1。但是如果我们为整个代码段都加锁的话，中断处理程序就不能获取锁了，中断程序会不停“自旋”并等待锁释放。而锁被uartwrite持有，在done设置为1之前不会释放。而done只有在中断处理程序获取锁之后才可能设置为1。所以我们不能在发送每个字符的整个处理流程都加锁</u>。

​	上面加锁方式的问题是，uartwrite在期望中断处理程序执行的同时又持有了锁。而我们唯一期望中断处理程序执行的位置就是sleep函数执行期间，其他的时候uartwrite持有锁是没有问题的。<u>所以另一种实现可能是，在传输字符的最开始获取锁，因为我们需要保护共享变量done，但是在调用sleep函数之前释放锁。这样中断处理程序就有可能运行并且设置done标志位为1。之后在sleep函数返回时，再次获取锁</u>。

![img](https://pic3.zhimg.com/80/v2-eb590f50ec2f4338d563cf9beede167a_1440w.webp)

​	现有的代码中，uartwrite在最开始获取了锁，并在最后释放了锁。

![img](https://pic1.zhimg.com/80/v2-677e5d0dc27de07b1eae00033385f658_1440w.webp)

​	现有的中断处理程序也在最开始获取锁，之后释放锁。

![img](https://pic1.zhimg.com/80/v2-ccbfa309cd4a0545cf8b2949f32a4840_1440w.webp)

​	接下来，我们会探索为什么只接收一个参数的`broken_sleep`在这不能工作。为了让锁能正常工作，我们需要在调用`broken_sleep`函数之前释放`uart_tx_lock`，并在`broken_sleep`返回时重新获取锁。`broken_sleep`内的代码与之前在白板上演示的是一样的。也就是首先将进程状态设置为SLEEPING，并且保存`tx_chan`到进程结构体中，最后调用switch函数。

![img](https://pic1.zhimg.com/80/v2-0a862f68b88b3d810a990c47b638414c_1440w.webp)

​	接下来编译代码并看一下会发生什么。

![img](https://pic1.zhimg.com/80/v2-689c31763a483e6bdcf97846d39ed930_1440w.webp)

​	在XV6启动的时候会打印“init starting”，这里看来输出了一些字符之后就hang住了。如果我输入任意字符，剩下的字符就能输出。这里发生了什么？

​	这里的问题必然与之前修改的代码相关。**在前面的代码中，sleep之前释放了锁，但是在释放锁和`broken_sleep`之间可能会发生中断**。

![img](https://pic3.zhimg.com/80/v2-00c54bd4575cc34d9e95f00470b78b12_1440w.webp)

​	一旦释放了锁，当前CPU的中断会被重新打开。因为这是一个多核机器，所以中断可能发生在任意一个CPU核。在上面代码标记的位置，其他CPU核上正在执行UART的中断处理程序，并且正在acquire函数中等待当前锁释放。所以一旦锁被释放了，另一个CPU核就会获取锁，并发现UART硬件完成了发送上一个字符，之后会设置tx_done为1，最后再调用wakeup函数，并传入tx_chan。**目前为止一切都还好，除了一点：现在写线程还在执行并位于release和broken_sleep之间，也就是写线程还没有进入SLEEPING状态，所以中断处理程序中的wakeup并没有唤醒任何进程，因为还没有任何进程在tx_chan上睡眠**。之后写线程会继续运行，调用broken_sleep，将进程状态设置为SLEEPING，保存sleep channel。但是中断已经发生了，wakeup也已经被调用了。**所以这次的broken_sleep，没有人会唤醒它，因为wakeup已经发生过了。这就是lost wakeup问题**。

---

问题：是不是总是这样，一旦一个wakeup被丢失了，下一次wakeup时，之前缓存的数据会继续输出？

回答：这完全取决于实现细节。在我们的例子中，实际上出于偶然才会出现当我输入某些内容会导致之前的输出继续的现象。**这里背后的原因是，我们的代码中，UART只有一个中断处理程序。不论是有输入，还是完成了一次输出，都会调用到同一个中断处理程序中**。所以当我输入某些内容时，会触发输入中断，之后会调用uartintr函数。然后在中断处理程序中又会判断LSR_TX_IDLE标志位，并再次调用wakeup，所以刚刚的现象完全是偶然。如果出现了lost wakeup问题，并且你足够幸运的话，某些时候它们能自动修复。**如果UART有不同的接收和发送中断处理程序的话，那么就没办法从lost wakeup恢复**。

问题：tx_done标志位的作用是什么？

回答：这是一种简单的在uartintr和uartwrite函数之间通信的方法。tx_done标志位为1表示已经完成了对于前一个字符的传输，并且uartwrite可以传输下一个字符，所以这是用来在中断处理程序和uartwrite之间通信的标志位。

问题：当从sleep函数中唤醒时，不是已经知道是来自UART的中断处理程序调用wakeup的结果吗？这样的话tx_done有些多余。

回答：我想你的问题也可以描述为：**为什么需要通过一个循环while(tx_done == 0)来调用sleep函数？这个问题的答案适用于一个更通用的场景：实际中不太可能将sleep和wakeup精确匹配。并不是说sleep函数返回了，你等待的事件就一定会发生**。举个例子，假设我们有两个进程同时想写UART，它们都在uartwrite函数中。可能发生这种场景，当一个进程写完一个字符之后，会进入SLEEPING状态并释放锁，而另一个进程可以在这时进入到循环并等待UART空闲下来。之后两个进程都进入到SLEEPING状态，当发生中断时UART可以再次接收一个字符，两个进程都会被唤醒，但是只有一个进程应该写入字符，所以我们才需要在sleep外面包一层while循环。**实际上，你可以在XV6中的每一个sleep函数调用都被一个while循环包着。因为事实是，你或许被唤醒了，但是其他人将你等待的事件拿走了，所以你还得继续sleep。这种现象还挺普遍的**。

问题：我们只看到了一个lost wakeup，当我们随便输入一个字符，整个剩下的字符都能输出，为什么没有在输出剩下字符的时候再次发生lost wakeup？

回答：这会发生的。我来敲一下cat README，这会输出数千个字符。可以看到每过几个字符就会hang一次，需要我再次输入某个字符。这个过程我们可以看到很多lost wakeup。<u>之前之所以没有出现，是因为lost wakeup需要中断已经在等待获取锁，并且uartwrite位于release和broken_sleep之间，这需要一定的巧合并不总是会发生</u>。

## 13.4 sleep和wakeup代码实现

> [13.4 如何避免Lost wakeup - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349779082) <= 图文出处

​	现在我们的目标是消灭掉lost wakeup。这可以通过消除下面的窗口时间来实现。

![img](https://pic3.zhimg.com/80/v2-00c54bd4575cc34d9e95f00470b78b12_1440w.webp)

​	首先我们必须要释放uart_tx_lock锁，因为中断需要获取这个锁，但是我们又不能在释放锁和进程将自己标记为SLEEPING之间留有窗口。这样中断处理程序中的wakeup才能看到SLEEPING状态的进程，并将其唤醒，进而我们才可以避免lost wakeup的问题。所以，我们应该消除这里的窗口。

​	为了实现这个目的，我们需要将sleep函数设计的稍微复杂点。**这里的解决方法是，即使sleep函数不需要知道你在等待什么事件，它还是需要你知道你在等待什么数据，并且传入一个用来保护你在等待数据的锁**。sleep函数需要特定的条件才能执行，而sleep自己又不需要知道这个条件是什么。在我们的例子中，sleep函数执行的特定条件是tx_done等于1。虽然sleep不需要知道tx_done，但是它需要知道保护这个条件的锁，也就是这里的uart_tx_lock。在调用sleep的时候，锁还被当前线程持有，之后这个锁被传递给了sleep。

​	**在接口层面，sleep承诺可以原子性的将进程设置成SLEEPING状态，同时释放锁。这样wakeup就不可能看到这样的场景：锁被释放了但是进程还没有进入到SLEEPING状态**。所以sleep这里将释放锁和设置进程为SLEEPING状态这两个行为合并为一个原子操作。

​	所以我们需要有一个锁来保护sleep的条件，并且这个锁需要传递给sleep作为参数。更进一步的是，当调用wakeup时，锁必须被持有。如果程序员想要写出正确的代码，都必须遵守这些规则来使用sleep和wakeup。

​	接下来我们看一下sleep和wakeup如何使用这一小块额外的信息（注，也就是传入给sleep函数的锁）和刚刚提到的规则，来避免lost wakeup。

​	首先我们来看一下proc.c中的wakeup函数。

```c
// proc.c
// Wake up all processes sleeping on chan.
// Must be called without any p->lock.
void wakeup(void *chan)
{
	struct proc *p;
  
  for(p = proc; p < &p[NPROC]; p++) {
		acquire(&p->lock);
    if(p->state == SLEEPING && p->chan == chan) {
      p->state = RUNNABLE;
    }
    release(&p->lock);
  }
}
```

​	**wakeup函数并不十分出人意料。它查看整个进程表单，对于每个进程首先加锁，这点很重要。之后查看进程的状态，如果进程当前是SLEEPING并且进程的channel与wakeup传入的channel相同，将进程的状态设置为RUNNABLE。最后再释放进程的锁**。

​	接下来我们忽略broken_sleep，直接查看带有锁作为参数的sleep函数。

```c
// Atomically release lock and sleep on chan.
// Reacquires lock when awakened.
void sleep(void *chan, struct spinlock *lk)
{
  struct proc *p = myproc();
  
  // Must acquire p->lock in order to
  // change p->state and then call sched.
  // Once we hold p->lock, we can be
  // guaranteed that we won't miss any wakeup
  // (wakeup locks p->lock),
  // so it's okay to release lk.
  if(lk != &p->lock){
		acquire(&p->lock);
    release(lk);
  }
  
  // Go to sleep.
  p->chan = chan;
  p->state = SLEEPING;
  
  sched();
  
  // Tidy up.
  p->chan = 0;
  
  // Reacquire original lock.
  if(lk != &p->lock) {
    release(&p->lock);
    acquire(lk);
  }
}
```

​	我们已经知道了sleep函数需要释放作为第二个参数传入的锁，这样中断处理程序才能获取锁。函数中第一件事情就是释放这个锁。当然在释放锁之后，我们会担心在这个时间点相应的wakeup会被调用并尝试唤醒当前进程，而当前进程还没有进入到SLEEPING状态。**所以我们不能让wakeup在release锁之后执行。为了让它不在release锁之后执行，在release锁之前，sleep会获取即将进入SLEEPING状态的进程的锁**。

​	如果你还记得的话，**wakeup在唤醒一个进程前，需要先获取进程的锁。所以在整个时间uartwrite检查条件之前到sleep函数中调用sched函数之间，这个线程一直持有了保护sleep条件的锁或者p->lock**。让我回到UART的代码并强调一下这一点。

![img](https://pic4.zhimg.com/80/v2-ab00224564b64984ff8b5a846ba99153_1440w.webp)

​	uartwrite在最开始获取了sleep的condition lock，并且一直持有condition lock知道调用sleep函数。所以它首先获取了condition lock，之后检查condition（注，也就是tx_done等于0），之后在持有condition lock的前提下调用了sleep函数。此时wakeup不能做任何事情，wakeup现在甚至都不能被调用直到调用者能持有condition lock。所以现在wakeup必然还没有执行。

​	sleep函数在释放condition lock之前，先获取了进程的锁。**在释放了condition lock之后，wakeup就可以被调用了，但是除非wakeup获取了进程的锁，否则wakeup不能查看进程的状态。所以，在sleep函数中释放了condition lock之后，wakeup也还没有执行**。

​	在持有进程锁的时候，将进程的状态设置为SLEEPING并记录sleep channel，之后再调用sched函数，这个函数中会再调用switch函数（注，详见11.6），此时sleep函数中仍然持有了进程的锁，wakeup仍然不能做任何事情。

​	如果你还记得的话，当我们从当前线程切换走时，调度器线程中会释放前一个进程的锁（注，详见11.8）。所以**在调度器线程释放进程锁之后，wakeup才能终于获取进程的锁，发现它正在SLEEPING状态，并唤醒它。**

​	这里的效果是由之前定义的一些规则确保的，这些规则包括了：

- 调用sleep时需要持有condition lock，这样sleep函数才能知道相应的锁。
- **sleep函数只有在获取到进程的锁`p->lock`之后，才能释放condition lock**。
- wakeup需要同时持有两个锁才能查看进程。（对wakeup本身来说主要是进程锁影响；而条件锁被占用时，uartintr中断处理函数由于抢不到锁，所以都压根执行不到调用wakeup函数的代码位置）

这样的话，我们就不会再丢失任何一个wakeup，也就是说我们修复了lost wakeup的问题。

## 13.5 pipe中sleep&wakeup的使用

> [13.5 Pipe中的sleep 和wakeup - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349779372) <= 图文出处

​	前面我们介绍了在UART的驱动中，如何使用sleep和wakeup才能避免lost wakeup(13.4)。前面这个特定的场景中，sleep等待的condition是发生了中断并且硬件准备好了传输下一个字符。**在一些其他场景，内核代码会调用sleep函数并等待其他的线程完成某些事情**。这些场景从概念上来说与我们介绍之前的场景没有什么区别，但是感觉上还是有些差异。例如，在读写pipe的代码中，查看`pipe.c`中的piperead函数。

![img](https://pic1.zhimg.com/80/v2-cc4b1dabbb9ebb99e6a0f90cb9821768_1440w.webp)

​	这里有很多无关的代码可以忽略。当read系统调用最终调用到piperead函数时，pi->lock会用来保护pipe，这就是sleep函数对应的condition lock。piperead需要等待的condition是pipe中有数据，而这个condition就是pi->nwrite大于pi->nread，也就是写入pipe的字节数大于被读取的字节数。如果这个condition不满足，那么piperead会调用sleep函数，并等待condition发生。同时piperead会将condition lock也就是pi->lock作为参数传递给sleep函数，以确保不会发生lost wakeup。

​	之所以会出现lost wakeup，是因为在一个不同的CPU核上可能有另一个线程刚刚调用了pipewrite。

![img](https://pic4.zhimg.com/80/v2-d20c3df66112406a2b51f20f52f7895b_1440w.webp)

​	pipewrite会向pipe的缓存写数据，并最后在piperead所等待的sleep channel上调用wakeup。而我们想要避免这样的风险：在piperead函数检查发现没有字节可以读取，到piperead函数调用sleep函数之间，另一个CPU调用了pipewrite函数。因为这样的话，另一个CPU会向pipe写入数据并在piperead进程进入SLEEPING之前调用wakeup，进而产生一次lost wakeup。

​	在pipe的代码中，pipewrite和piperead都将sleep包装在一个while循环中。piperead中的循环等待pipe的缓存为非空（pipewrite中的循环等待的是pipe的缓存不为full）。之所以要将sleep包装在一个循环中，是因为可能有多个进程在读取同一个pipe。如果一个进程向pipe中写入了一个字节，这个进程会调用wakeup进而同时唤醒所有在读取同一个pipe的进程。但是因为pipe中只有一个字节并且总是有一个进程能够先被唤醒，哦，这正好提醒了我有关sleep我忘记了一些非常关键的事情。**sleep函数中最后一件事情就是重新获取condition lock。所以调用sleep函数的时候，需要对condition lock上锁**（注，在sleep函数内部会对condition lock解锁），**在sleep函数返回时会重新对condition lock上锁。这样第一个被唤醒的线程会持有condition lock，而其他的线程在重新对condition lock上锁的时候会在锁的acquire函数中等待**。

​	那个幸运的进程（注，这里线程和进程描述的有些乱，但是基本意思是一样的，当说到线程时是指进程唯一的内核线程）会从sleep函数中返回，之后通过检查可以发现pi->nwrite比pi->nread大1，所以进程可以从piperead的循环中退出，并读取一个字节，之后pipe缓存中就没有数据了。之后piperead函数释放锁并返回。接下来，第二个被唤醒的线程，它的sleep函数可以获取condition lock并返回，但是通过检查发现pi->nwrite等于pi->nread（注，因为唯一的字节已经被前一个进程读走了），所以这个线程以及其他所有的等待线程都会重新进入sleep函数。所以这里也可以看出，**几乎所有对于sleep的调用都需要包装在一个循环中，这样从sleep中返回的时候才能够重新检查condition是否还符合**。

​	<u>sleep和wakeup的规则稍微有点复杂。因为你需要向sleep展示你正在等待什么数据，你需要传入锁并遵循一些规则，某些时候这些规则还挺烦人的。另一方面sleep和wakeup又足够灵活，因为它们并不需要理解对应的condition，只是需要有个condition和保护这个condition的锁</u>。

​	除了sleep&wakeup之外，还有一些其他的更高级的Coordination实现方式。例如今天课程的阅读材料中的semaphore(信号量)，它的接口就没有那么复杂，你不用告诉semaphore有关锁的信息。而semaphore的调用者也不需要担心lost wakeup的问题，在semaphore的内部实现中考虑了lost wakeup问题。因为定制了up-down计数器，所以semaphore可以在不向接口泄露数据的同时（注，也就是不需要向接口传递condition lock），处理lost wakeup问题。**semaphore某种程度来说更简单，尽管它也没那么通用，如果你不是在等待一个计数器，semaphore也就没有那么有用了。这也就是为什么我说sleep和wakeup更通用的原因**。

## 13.6 exit系统调用

> [13.6 exit系统调用 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349779579) <= 图文出处

​	接下来，我想讨论一下XV6面临的一个与Sleep&Wakeup相关的挑战，也就是**如何关闭一个进程**。每个进程最终都需要退出，我们需要清除进程的状态，释放栈。**在XV6中，一个进程如果退出的话，我们需要释放用户内存，释放page table，释放trapframe对象，将进程在进程表单中标为REUSABLE，这些都是典型的清理步骤**。当进程退出或者被杀掉时，有许多东西都需要被释放。

​	这里会产生的两大问题：

- 首先我们不能直接单方面的摧毁另一个线程，因为：另一个线程可能正在另一个CPU核上运行，并使用着自己的栈；也可能另一个线程正在内核中持有了锁；也可能另一个线程正在更新一个复杂的内核数据，如果我们直接就把线程杀掉了，我们可能在线程完成更新复杂的内核数据过程中就把线程杀掉了。我们不能让这里的任何一件事情发生。
- 另一个问题是，即使一个线程调用了exit系统调用，并且是自己决定要退出。它仍然持有了运行代码所需要的一些资源，例如它的栈，以及它在进程表单中的位置。当它还在执行代码，它就不能释放正在使用的资源。所以我们需要一种方法让线程能释放最后几个对于运行代码来说关键的资源。

​	记住这两个问题。

​	**XV6有两个函数与关闭线程进程相关。第一个是exit，第二个是kill**。让我们先来看位于`proc.c`中的exit函数。

![img](https://pic4.zhimg.com/80/v2-4f6d6fedfbca60e3049e32e5524824a7_1440w.webp)

​	这就是exit系统调用的内容。**从exit接口的整体来看，在最后它会释放进程的内存和page table，关闭已经打开的文件，同时我们也知道父进程会从wait系统调用中唤醒，所以exit最终会导致父进程被唤醒**。这些都是我们预期可以从exit代码中看到的内容。

​	从上面的代码中，首先exit函数关闭了所有已打开的文件。这里可能会很复杂，因为关闭文件系统中的文件涉及到引用计数，虽然我们还没学到但是这里需要大量的工作。不管怎样，**一个进程调用exit系统调用时，会关闭所有自己拥有的文件**。

​	接下来是类似的处理，进程有一个对于当前目录的记录，这个记录会随着你执行cd指令而改变。在exit过程中也需要将对这个目录的引用释放给文件系统。

​	**<u>如果一个进程要退出，但是它又有自己的子进程，接下来需要设置这些子进程的父进程为init进程。我们接下来会看到，每一个正在exit的进程，都有一个父进程中的对应的wait系统调用。父进程中的wait系统调用会完成进程退出最后的几个步骤。所以如果父进程退出了，那么子进程就不再有父进程，当它们要退出时就没有对应的父进程的wait。所以在exit函数中，会为即将exit进程的子进程重新指定父进程为init进程，也就是PID为1的进程</u>**。

![img](https://pic1.zhimg.com/80/v2-206c1de963a6923d60717bcb889b9890_1440w.webp)

​	之后，我们需要通过调用wakeup函数唤醒当前进程的父进程，当前进程的父进程或许正在等待当前进程退出。

​	接下来，**进程的状态被设置为ZOMBIE**。<u>现在进程还没有完全释放它的资源，所以它还不能被重用。所谓的进程重用是指，我们期望在最后，进程的所有状态都可以被一些其他无关的fork系统调用复用，但是目前我们还没有到那一步</u>。

​	现在我们还没有结束，因为我们还没有释放进程资源。我们在还没有完全释放所有资源的时候，通过调用sched函数进入到调度器线程。

​	<u>**到目前为止，进程的状态是ZOMBIE，并且进程不会再运行，因为调度器只会运行RUNNABLE进程。同时进程资源也并没有完全释放，如果释放了进程的状态应该是UNUSED**。但是可以肯定的是进程不会再运行了，因为它的状态是ZOMBIE。所以调度器线程会决定运行其他的进程</u>。

## 13.7 wait系统调用

> [13.7 wait系统调用 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349779902) <= 图文出处

​	通过Unix的exit和wait系统调用的说明，我们可以知道如果一个进程exit了，并且它的父进程调用了wait系统调用，父进程的wait会返回。wait函数的返回表明当前进程的一个子进程退出了。所以接下来我们看一下wait系统调用的实现。

![img](https://pic2.zhimg.com/80/v2-57c2c7e0d2763f71920a3c7d27856ea1_1440w.webp)

​	**它里面包含了一个大的循环。当一个进程调用了wait系统调用，它会扫描进程表单，找到父进程是自己且状态是ZOMBIE的进程**。<u>**从上一节可以知道，这些进程已经在exit函数中几乎要执行完了。之后由父进程调用的freeproc函数，来完成释放进程资源的最后几个步骤**</u>。我们看一下freeproc的实现。

![img](https://pic3.zhimg.com/80/v2-63b07c0e3d4542f6071fad46d8bb3eb6_1440w.webp)

​	这是关闭一个进程的最后一些步骤。如果由正在退出的进程自己在exit函数中执行这些步骤，将会非常奇怪。**<u>这里释放了trapframe，释放了page table。如果我们需要释放进程内核栈，那么也应该在这里释放。但是因为内核栈的guard page，我们没有必要再释放一次内核栈。不管怎样，当进程还在exit函数中运行时，任何这些资源在exit函数中释放都会很难受，所以这些资源都是由父进程释放的</u>**。

​	wait不仅是为了父进程方便的知道子进程退出，wait实际上也是进程退出的一个重要组成部分。**<u>在Unix中，对于每一个退出的进程，都需要有一个对应的wait系统调用，这就是为什么当一个进程退出时，它的子进程需要变成init进程的子进程</u>**。**<u>init进程的工作就是在一个循环中不停调用wait，因为每个进程都需要对应一个wait，这样它的父进程才能调用freeproc函数，并清理进程的资源</u>**。
​	**当父进程完成了清理进程的所有资源，子进程的状态会被设置成UNUSED。之后，fork系统调用才能重用进程在进程表单的位置**。

​	**这里我想要强调的是，直到子进程exit的最后，它都没有释放所有的资源，因为它还在运行的过程中，所以不能释放这些资源。相应的其他的进程，也就是父进程，通过wait释放了运行子进程代码所需要的资源。这样的设计可以让我们极大的精简exit的实现。**

---

**问题：在exit系统调用中，为什么需要在重新设置父进程之前，先获取当前进程的父进程？**

**回答：这里其实就是在防止一个进程和它的父进程同时退出。通常情况下，一个进程exit，它的父进程正在wait，一切都正常。但是也可能一个进程和它的父进程同时exit。所以当子进程尝试唤醒父进程，并告诉它自己退出了时，父进程也在退出。这些代码我一年前还记得是干嘛的，现在已经记不太清了。它应该是处理这种父进程和子进程同时退出的情况。如果不是这种情况的话，一切都会非常直观，子进程会在后面通过wakeup函数唤醒父进程**。

问题：为什么我们在唤醒父进程之后才将进程的状态设置为ZOMBIE？难道我们不应该在之前就设置吗？

回答：正在退出的进程会先获取自己进程的锁，同时，因为父进程的wait系统调用中也需要获取子进程的锁，所以父进程并不能查看正在执行exit函数的进程的状态。这意味着，正在退出的进程获取自己的锁到它调用sched进入到调度器线程之间（注，因为调度器线程会释放进程的锁），父进程并不能看到这之间代码引起的中间状态。所以这之间的代码顺序并不重要。大部分时候，如果没有持有锁，exit中任何代码顺序都不能工作。因为有了锁，代码的顺序就不再重要，因为父进程也看不到进程状态。

## 13.8 kill系统调用

> [13.8 kill系统调用 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349780234) <= 图文出处

​	最后我想看的是kill系统调用。Unix中的一个进程可以将另一个进程的ID传递给kill系统调用，并让另一个进程停止运行。如果我们不够小心的话，kill一个还在内核执行代码的进程，会有一些我几分钟前介绍过的风险，比如我们想要杀掉的进程的内核线程还在更新一些数据，比如说更新文件系统，创建一个文件。如果这样的话，我们不能就这样杀掉进程，因为这样会使得一些需要多步完成的操作只执行了一部分。所以**<u>kill系统调用不能就直接停止目标进程的运行。实际上，在XV6和其他的Unix系统中，kill系统调用基本上不做任何事情</u>**。

```c
// Kill the process with the given pid.
// The victim won't exit until it tries to return
// to user space (see usertrap()) in trap.c).
int kill(int pid)
{
  struct proc *p;
  
  for(p = proc; p < &proc[NRPOC]); p++){
    acquire(&p->lock);
    if(p->pid == pid){
      p->killed = 1;
      if(p->state == SLEEPING) {
        // Wake process from sleep().
        p->state = RUNNABLE;
      }
      release(&p->lock);
      return 0;
    }
    release(&p->lock);
  }
  return -1;
}
```

​	**它先扫描进程表单，找到目标进程。然后只是将进程的proc结构体中killed标志位设置为1。如果进程正在SLEEPING状态，将其设置为RUNNABLE。这里只是将killed标志位设置为1，并没有停止进程的运行。所以kill系统调用本身还是很温和的**。

​	<u>而目标进程运行到内核代码中能安全停止运行的位置时，会检查自己的killed标志位，如果设置为1，目标进程会自愿的执行exit系统调用</u>。你可以在`trap.c`中看到所有可以安全停止运行的位置。

![img](https://pic4.zhimg.com/80/v2-bd520f482b5e1aab3cb410575e050bcb_1440w.webp)

​	<u>在usertrap函数中，在执行系统调用之前，如果进程已经被kill了，进程会自己调用exit。在这个内核代码位置，代码并没有持有任何锁，也不在执行任何操作的过程中，所以进程通过exit退出是完全安全的</u>。

​	<u>类似的，在usertrap函数的最后，也有类似的代码。在执行完系统调用之后，进程会再次检查自己是否已经被kill了。即使进程是被中断打断，这里的检查也会被执行。例如当一个定时器中断打断了进程的运行，我们可以通过检查发现进程是killed状态，之后进程会调用exit退出</u>。

​	**所以kill系统调用并不是真正的立即停止进程的运行，它更像是这样：如果进程在用户空间，那么下一次它执行系统调用它就会退出，又或者目标进程正在执行用户代码，当时下一次定时器中断或者其他中断触发了，进程才会退出。所以从一个进程调用kill，到另一个进程真正退出，中间可能有很明显的延时**。

​	这里有个很直观问题：如果进程不在用户空间执行，而是正在执行系统调用的过程中，然后它被kill了，我们需要做什么特别的操作吗？之所以会提出这个问题，是因为进程可能正在从console读取即将输入的字符，而你可能要明天才会输入一个字符，所以当你kill一个进程时，最好进程不是等到明天才退出。出于这个原因，**在XV6的很多位置中，如果进程在SLEEPING状态时被kill了，进程会实际的退出**。让我来给你展示这里的机制。
​	首先要看的是kill函数。

![img](https://pic2.zhimg.com/80/v2-a5fdec6490c4897d13c2744d6e9f82d5_1440w.webp)

​	你可以看到**如果目标进程是SLEEPING状态，kill函数会将其状态设置为RUNNABLE**，这意味着，即使进程之前调用了sleep并进入到SLEEPING状态，调度器现在会重新运行进程，并且进程会从sleep中返回。让我们来查看一下这在哪生效的。

![img](https://pic1.zhimg.com/80/v2-cc4b1dabbb9ebb99e6a0f90cb9821768_1440w.webp)

​	在`pipe.c`的piperead函数中，如果一个进程正在sleep状态等待从pipe中读取数据，然后它被kill了。kill函数会将其设置为RUNNABLE，之后进程会从sleep中返回，返回到循环的最开始。pipe中大概率还是没有数据，之后在piperead中，会判断进程是否被kill了（注，`if(pr->killed)`）。<u>如果进程被kill了，那么接下来piperead会返回-1，并且返回到usertrap函数的syscall位置，因为piperead就是一种系统调用的实现</u>。

![img](https://pic4.zhimg.com/80/v2-bd520f482b5e1aab3cb410575e050bcb_1440w.webp)

​	之后在usertrap函数中会检查`p->killed`，并调用exit。

​	**所以对于SLEEPING状态的进程，如果它被kill了，它会被直接唤醒，包装了sleep的循环会检查进程的killed标志位，最后再调用exit**。

​	**同时还有一些情况，如果进程在SLEEPING状态中被kill了并不能直接退出。例如，一个进程正在更新一个文件系统并创建一个文件的过程中，进程不适宜在这个时间点退出，因为我们想要完成文件系统的操作，之后进程才能退出。我会向你展示一个磁盘驱动中的sleep循环，这个循环中就没有检查进程的killed标志位**。

​	下面就是virtio_disk.c文件中的一段代码：

![img](https://pic4.zhimg.com/80/v2-1a46e2d798fee1aa20ab1b012b08bd6f_1440w.webp)

​	**这里一个进程正在等待磁盘的读取结束，这里没有检查进程的killed标志位。因为现在可能正在创建文件的过程中，而这个过程涉及到多次读写磁盘。我们希望完成所有的文件系统操作，完成整个系统调用，之后再检查`p->killed`并退出**。

​	借同学提问init进程是否会退出，下面看看相关的代码。

![img](https://pic2.zhimg.com/80/v2-671536dfc3a6d9f1bd3bf42ee8ae7e85_1440w.webp)

​	如果fork失败了，init进程也会退出。不过，这个问题的真正的答案是，init不会退出。**<u>init进程的目标就是不退出，它就是在一个循环中不停的调用wait。如果init进程退出了，我认为这是一个Fatal级别的错误，然后系统会崩溃</u>**。在exit函数的最开始就会有如下检查

![img](https://pic1.zhimg.com/80/v2-38857b67b63c0490fbf22915ae40c4c0_1440w.webp)

​	如果调用exit的进程是init进程，那么会触发panic。**因为如果没有init进程的话，系统最终还是会停止运行。<u>如果没有init进程的话就没有人会为退出的进程调用wait系统调用，也就没有人完成进程资源的释放工作，我们最终会用光所有的进程，并引起一些其他的错误，所以我们必须要有init进程</u>。所以这个问题的真正答案是init进程不允许退出**。

---

问题：为什么一个进程允许kill另一个进程？这样一个进程不是能杀掉所有其他进程吗？

回答：如果你在MIT的分时复用计算机Athena上这么做的话，他们可能会开除你。在XV6中允许这么做是因为，XV6这是个教学用的操作系统，任何与权限相关的内容在XV6中都不存在。**在Linux或者真正的操作系统中，每个进程都有一个user id或多或少的对应了执行进程的用户，一些系统调用使用进程的user id来检查进程允许做的操作。所以在Linux中会有额外的检查，调用kill的进程必须与被kill的进程有相同的user id，否则的话，kill操作不被允许**。<u>所以，在一个分时复用的计算机上，我们会有多个用户，我们不会想要用户kill其他人的进程，这样一套机制可以防止用户误删别人的进程</u>。

问题：这节课可能没有怎么讲到，但是如果关闭一个操作系统会发生什么？

回答：这个过程非常复杂，并且依赖于你运行的是什么系统。因为文件系统是持久化的，它能在多次重启之间保持数据，我们需要保持文件系统的良好状态，如果我们正在更新文件系统的过程中，例如创建文件，然后我们想关闭操作系统，断电之类的。我们需要一个策略来确保即使我们正在一个复杂的更新文件系统的过程中，我们并不会破坏磁盘上的文件系统数据。文件系统其实就是一个位于磁盘的数据结构。所以这里涉及到了很多的机制来确保如果你关闭操作系统或者因为断电之类，我们可以恢复磁盘上的文件系统。其他的，你是否需要做一些特殊的操作来关闭系统，取决于你正在运行什么进程。如果你正在运行一些重要的服务器，例如数据库服务器，并且许多其他计算机依赖这个数据库并通过网络使用它。那谁知道呢？答案或许是你不能就这么直接关闭操作系统，因为你正在提供一个对于其他计算机来说非常关键的服务。如果你的计算机并没有在做任何事情，那么你可以直接关闭它。<u>或许对于你的问题来说，如果你想关闭一个计算机，确保文件系统是正确的，之后停止执行指令，之后就可以关闭计算机了</u>。

# Lecture14 文件系统File Systems

## 14.1 章节概述

> [14.1 Why Interesting - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351902599) <= 图文出处

​	今天介绍的是文件系统。实际上我们会花三节课的时间来学习文件系统。前两节课基于XV6来做介绍，第三节课基于Linux来做介绍。实际上，这将是有关XV6的最后一个话题，在这周之后我们就讲完了XV6。

​	对于文件系统，你们都知道它并使用过它。它是操作系统中除了shell以外最常见的用户接口。所以我们希望通过这几节课来理解：文件系统的背后究竟是什么原理，文件系统是如何实现的。这些内容还是让人有些小激动，因为你们一直都在使用文件系统。

​	接下来让我列出一些文件系统突出的特性：

- User-friendly names / pathnames。其中一点刚刚有同学提到了，就是对于用户友好的文件名，具体来说就是层级的路径名，这可以帮助用户组织目录中的文件。
- Share files between users / processes。通过将文件命名成方便易记的名字，可以在用户之间和进程之间更简单的共享文件。
- Persistence / durability。相比我们已经看过的XV6其他子系统，这一点或许是最重要的，文件系统提供了持久化。这意味着，我可以关闭一个计算机，过几天再开机而文件仍然在那，我可以继续基于文件工作。这一点与进程和其他资源不一样，这些资源在计算机重启时就会消失，之后你需要重新启动它们，但是文件系统就可以提供持久化。

![img](https://pic2.zhimg.com/80/v2-4c0429ab56d5eb46fbe531ce773eaf79_1440w.webp)

​	你们都使用了文件系统，接下来几节课我们将学习它内部是如何工作的。出于以下原因，文件系统背后的机制还比较有意思：

- 文件系统**对硬件的抽象**较为有用，所以理解文件系统对于硬件的抽象是如何实现的还是有点意思的。
- 除此之外，还有个关键且有趣的地方就是**crash safety**。有可能在文件系统的操作过程中，计算机崩溃了，在重启之后你的文件系统仍然能保持完好，文件系统的数据仍然存在，并且你可以继续使用你的大部分文件。如果文件系统操作过程中计算机崩溃了，然后你重启之后文件系统不存在了或者磁盘上的数据变了，那么崩溃的将会是你。所以crash safety是一个非常重要且经常出现的话题，我们下节课会专门介绍它。
- 之后是一个通用的问题，**如何在磁盘上排布文件系统**。例如目录和文件，它们都需要以某种形式在磁盘上存在，这样当你重启计算机时，所有的数据都能恢复。所以在磁盘上有一些数据结构表示了文件系统的结构和内容。在XV6中，使用的数据结构非常简单，因为XV6是专门为教学目的创建的。真实的文件系统通常会更加复杂。但是它们都是磁盘上保存的数据结构，我们在今天的课程会重点看这部分。
- 最后一个有趣的话题是**性能**。文件系统所在的硬件设备通常都较慢，比如说向一个SSD磁盘写数据将会是毫秒级别的操作，而在一个毫秒内，计算机可以做大量的工作，所以尽量避免写磁盘很重要，我们将在几个地方看到提升性能的代码。<u>比如说，所有的文件系统都有buffer cache或者叫block cache</u>。同时这里会有更多的并发，比如说你正在查找文件路径名，这是一个多次交互的操作，首先要找到文件结构，然后查找一个目录的文件名，之后再去查找下一个目录等等。你会期望当一个进程在做路径名查找时，另一个进程可以并行的运行。这样的并行运行在文件系统中将会是一个大的话题。

​	除此之外，你会对文件系统感兴趣是因为这是接下来两个lab的内容。下一个lab完全关注在文件系统，下下个lab结合了虚拟内存和文件系统。即使是这周的lab，也会尝试让buffer cache可以支持更多的并发。所以这就是为什么文件系统是有趣的。

## 14.2 File system实现概述

> [14.2 File system实现概述 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351902801) <= 图文出处
>
> [Linux下的softlink和hardlink（转） - 沧海一滴 - 博客园 (cnblogs.com)](https://www.cnblogs.com/softidea/p/5447506.html)

​	为了理解文件系统必须提供什么能力，让我们再看一下一些与文件系统相关的基础系统调用。从这些系统调用接口，我们将可以推断出有关文件系统实现的一些细节，这些系统调用我们在之前的课程已经看过了。首先让我们来看一个简单的场景，假设我们创建了文件“x/y”，或者说在目录x中创建了文件y，同时我们需要提供一些标志位，现在我们还不关心标志位所以我会忽略它。

![img](https://pic1.zhimg.com/80/v2-2d1daefa0b8a95fd95ca0d3371a3de14_1440w.webp)

​	上面的系统调用会创建文件，并返回文件描述符给调用者。调用者也就是用户应用程序可以对文件描述符调用write，有关write我们在之前已经看过很多次了，这里我们向文件写入“abc”三个字符。

![img](https://pic2.zhimg.com/80/v2-2e0680c35f5f522c08910e11e8e3a4c5_1440w.webp)

​	从这两个调用已经可以看出一些信息了：

- 首先出现在接口中的路径名是可读的名字，而不是一串数字，它是由用户选择的字符串。
- **write系统调用并没有使用offset作为参数，所以写入到文件的哪个位置是隐式包含在文件系统中，文件系统在某个位置必然保存了文件的offset**。因为如果你再调用write系统调用，新写入的数据会从第4个字节开始。

​	除此之外，还有一些我们之前没有看过的有趣的系统调用。例如XV6和所有的Unix文件系统都支持通过系统调用创建链接，给同一个文件指定多个名字。你可以通过调用link系统调用，为之前创建的文件“x/y”创建另一个名字“x/z”。所以文件系统内部需要以某种方式跟踪指向同一个文件的多个文件名。

![img](https://pic3.zhimg.com/80/v2-4d51936c94becbe695140f2b3da945b2_1440w.webp)

​	<u>我们还可能会在文件打开时，删除或者更新文件的命名空间。例如，用户可以通过unlink系统调用来删除特定的文件名。如果此时相应的文件描述符还是打开的状态，那我们还可以向文件写数据，并且这也能正常工作</u>。

![img](https://pic3.zhimg.com/80/v2-bc28550f5224d9bf80d0e5439f98cd82_1440w.webp)

​	所以，**在文件系统内部，文件描述符必然与某个对象关联，而这个对象不依赖文件名。这样，即使文件名变化了，文件描述符仍然能够指向或者引用相同的文件对象。所以，实际上操作系统内部需要对于文件有内部的表现形式，并且这种表现形式与文件名无关**。

![img](https://pic3.zhimg.com/80/v2-c5d5b27d321f58cb2892642586b8a516_1440w.webp)

​	除此之外，我还想提一点。文件系统的目的是实现上面描述的API，也即是典型的文件系统API。但是，这并不是唯一构建一个存储系统的方式。如果只是在磁盘上存储数据，你可以想出一个完全不同的API。举个例子，数据库也能持久化的存储数据，但是数据库就提供了一个与文件系统完全不一样的API。**所以记住这一点很重要：还存在其他的方式能组织存储系统**。我们这节课关注在文件系统，**<u>文件系统通常由操作系统提供，而数据库如果没有直接访问磁盘的权限的话，通常是在文件系统之上实现的（注，早期数据库通常直接基于磁盘构建自己的文件系统，因为早期操作系统自带的文件系统在性能上较差，且写入不是同步的，进而导致数据库的ACID不能保证。不过现代操作系统自带的文件系统已经足够好，所以现代的数据库大部分构建在操作系统自带的文件系统之上）</u>**。

​	接下来我们看一下文件系统的结构。文件系统究竟维护了什么样的结构来实现前面介绍的API呢？

​	首先，**最重要的可能就是inode，这是代表一个文件的对象，并且它不依赖于文件名**。**实际上，inode是通过自身的编号来进行区分的，这里的编号就是个整数。所以文件系统内部通过一个数字，而不是通过文件路径名引用inode**。

​	同时，**<u>基于之前的讨论，inode必须有一个link count来跟踪指向这个inode的文件名的数量。一个文件（inode）只能在link count为0的时候被删除。实际的过程可能会更加复杂，实际中还有一个openfd count，也就是当前打开了文件的文件描述符计数。一个文件只能在这两个计数器都为0的时候才能被删除</u>**。

![img](https://pic2.zhimg.com/80/v2-4f007818f2d423d467953fcbe1ffece5_1440w.webp)

​	**同时基于之前的讨论，我们也知道write和read都没有针对文件的offset参数，所以<u>文件描述符必然自己悄悄维护了对于文件的offset</u>**。

![img](https://pic2.zhimg.com/80/v2-5067458a6dc9f9edee55ebc6d7fbcddd_1440w.webp)

​	**<u>文件系统中核心的数据结构就是inode和file descriptor。file descriptor主要与用户进程进行交互</u>**。

​	尽管文件系统的API很相近并且内部实现可能非常不一样。但是很多文件系统都有类似的结构。因为文件系统还挺复杂的，所以最好按照分层的方式进行理解。可以这样看：

- 在最底层是磁盘，也就是一些实际保存数据的存储设备，正是这些设备提供了持久化存储。
- <u>在这之上是buffer cache或者说block cache，这些cache可以避免频繁的读写磁盘</u>。这里我们将磁盘中的数据保存在了内存中。
- 为了保证持久性，再往上通常会有一个**logging层**。许多文件系统都有某种形式的logging，我们下节课会讨论这部分内容，所以今天我就跳过它的介绍。
- 在logging层之上，XV6有inode cache，这主要是为了**同步（synchronization）**，我们稍后会介绍。**inode通常小于一个disk block，所以多个inode通常会打包存储在一个disk block中。为了向单个inode提供同步操作，XV6维护了inode cache**。
- 再往上就是inode本身了。它实现了read/write。
- 再往上，就是文件名，和文件描述符操作。

![img](https://pic4.zhimg.com/80/v2-2d3fed05645a6c1445326f37a32b252b_1440w.webp)

​	<u>不同的文件系统组织方式和每一层可能都略有不同，有的时候分层也没有那么严格，即使在XV6中分层也不是很严格，但是从概念上来说这里的结构对于理解文件系统还是有帮助的。实际上所有的文件系统都有组件对应这里不同的分层，例如buffer cache，logging，inode和路径名</u>。

---

问题：link增加了了对于文件的一个引用，unlink减少了一个引用？

回答：是的。我们稍后会介绍更多相关的内容。

问题：能介绍一下soft link和hard link吗？

回答：今天不会讨论这些内容。但是你们将会在下一个File system lab中实现soft link。所以XV6本身实现了hard link，需要你们来实现soft link。

问题：link是对inode做操作，而不是对文件描述符做操作，对吧？

回答：是的，link是对inode做操作，我们接下来介绍这部分内容。

## 14.3 存储设备和磁盘分布(Storage devices & disk layout)

> [14.3 How file system uses disk - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351903071) <= 图文出处

​	接下来，简单介绍下最底层的存储设备。实际中有非常非常多不同类型的存储设备，这些设备的区别在于性能，容量，数据保存的期限等。其中两种最常见，并且你们应该也挺熟悉的是**SSD和HDD**。这两类存储虽然有着不同的性能，但是都在合理的成本上提供了大量的存储空间。SSD通常是0.1到1毫秒的访问时间，而HDD通常是在10毫秒量级完成读写一个disk block。

![img](https://pic1.zhimg.com/80/v2-7789baa228e2511d9b8ac8bd51f7bb84_1440w.webp)

​	这里有些术语有点让人困惑，它们是**扇区(sectors)和块(blocks)**。

- sector通常是**磁盘驱动**可以读写的最小单元，它过去通常是512字节。
- block通常是**操作系统或者文件系统**视角的数据。它由文件系统定义，在XV6中它是1024字节。所以XV6中一个block对应两个sector。**通常来说一个block对应了一个或者多个sector**。

​	*有的时候，人们也将磁盘上的sector称为block。所以这里的术语也不是很精确。*

​	这些存储设备连接到了电脑总线之上，总线也连接了CPU和内存。一个文件系统运行在CPU上，将内部的数据存储在内存，同时也会以读写block的形式存储在SSD或者HDD。这里的接口还是挺简单的，包括了read/write，然后以block编号作为参数。虽然我们这里描述的过于简单了，但是实际的接口大概就是这样。

![img](https://pic4.zhimg.com/80/v2-2fcce3bbd600614aa3d9e319377aec53_1440w.webp)

​	在内部，SSD和HDD工作方式完全不一样，但是对于硬件的抽象屏蔽了这些差异。磁盘驱动通常会使用一些标准的协议，例如PCIE，与磁盘交互。从上向下看磁盘驱动的接口，大部分的磁盘看起来都一样，你可以提供block编号，在驱动中通过写设备的控制寄存器，然后设备就会完成相应的工作。这是从一个文件系统的角度的描述。**尽管不同的存储设备有着非常不一样的属性，从驱动的角度来看，你可以以大致相同的方式对它们进行编程**。

​	有关存储设备我们就说这么多。

---

​	<u>从文件系统的角度来看磁盘还是很直观的。因为对于磁盘就是读写block或者sector，我们可以将磁盘看作是一个巨大的block的数组，数组从0开始，一直增长到磁盘的最后</u>。

![img](https://pic3.zhimg.com/80/v2-17e1217febc0a9abbc31db4dbb3ce4c6_1440w.webp)

​	<u>而文件系统的工作就是将所有的数据结构以一种能够在重启之后重新构建文件系统的方式，存放在磁盘上</u>。虽然有不同的方式，但是XV6使用了一种非常简单，但是还挺常见的布局结构。

通常来说：

- **block0要么没有用，要么被用作boot sector来启动操作系统**。
- **block1通常被称为super block，它描述了文件系统**。它可能包含磁盘上有多少个block共同构成了文件系统这样的信息。我们之后会看到XV6在里面会存更多的信息，你可以通过block1构造出大部分的文件系统信息。
- 在XV6中，log从block2开始，到block32结束。实际上log的大小可能不同，这里在super block中会定义log就是30个block。
- **接下来在block32到block45之间，XV6存储了inode。我之前说过多个inode会打包存在一个block中，一个inode是64字节**。
- 之后是bitmap block，这是我们构建文件系统的默认方法，它只占据一个block。它记录了数据block是否空闲。
- 之后就全是数据block了，数据block存储了文件的内容和目录的内容。

​	**通常来说，bitmap block，inode blocks和log blocks被统称为metadata block。它们虽然不存储实际的数据，但是它们存储了能帮助文件系统完成工作的元数据**。

​	假设inode是64字节，如果你想要读取inode10，那么你应该按照下面的公式去对应的block读取inode。

![img](https://pic2.zhimg.com/80/v2-fc045425f576b62721ea3f7d894b3b55_1440w.webp)

​	所以inode0在block32，inode17会在block33。**只要有inode的编号，我们总是可以找到inode在磁盘上存储的位置**。

---

问题：对于read/write的接口，是不是提供了同步/异步的选项？

回答：你可以认为一个磁盘的驱动与console的驱动是基本一样的。驱动向设备发送一个命令表明开始读或者写，过了一会当设备完成了操作，会产生一个中断表明完成了相应的命令。但是因为磁盘本身比console复杂的多，所以磁盘的驱动也会比我们之前看过的console的驱动复杂的多。不过驱动中的代码结构还是类似的，也有bottom部分和top部分，中断和读写控制寄存器。

**问题：boot block是不是包含了操作系统启动的代码？**

**回答：是的，它里面通常包含了足够启动操作系统的代码。之后再从文件系统中加载操作系统的更多内容。**

问题：XV6是存储在虚拟磁盘上？

回答：在QEMU中，我们实际上走了捷径。QEMU中有个标志位`-kernel`，它指向了内核的镜像文件，QEMU会将这个镜像的内容加载到了物理内存的0x80000000。所以当我们使用QEMU时，我们不需要考虑boot sector。

追问：所以当你运行QEMU时，你就是将程序通过命令行传入，然后直接就运行传入的程序，然后就不需要从虚拟磁盘上读取数据了？

回答：是的。

## 14.4 inode理论

> [14.4 inode - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351903377) <= 图文出处

​	接下来我们看一下磁盘上存储的inode究竟是什么？首先我们前面已经看过了，这是一个64字节的数据结构。

- 通常来说它有**一个type字段，表明inode是文件还是目录**。
- nlink字段，也就是link计数器，用来跟踪究竟有多少文件名指向了当前的inode。
- size字段，表明了文件数据有多少个字节。
- 不同文件系统中的表达方式可能不一样，不过在XV6中接下来是一些block的编号，例如编号0，编号1，等等。<u>XV6的inode中总共有12个block编号。这些被称为direct block number。这12个block编号指向了构成文件的前12个block</u>。举个例子，如果文件只有2个字节，那么只会有一个block编号0，它包含的数字是磁盘上文件前2个字节的block的位置。
- <u>之后还有一个indirect block number，它对应了磁盘上一个block，这个block包含了256个block number，这256个block number包含了文件的数据（256个，是因为block是1024字节，一个block number用4字节表示）</u>。所以inode中block number 0到block number 11都是direct block number，而block number 12保存的indirect block number指向了另一个block。

​	以上基本就是XV6中inode的组成部分。基于上面的内容，XV6中文件最大的长度是多少呢？学生回答268*1024字节。是的，最大文件尺寸对应的是下面的公式。

![img](https://pic2.zhimg.com/80/v2-1fa851768080366b445689500fdc6b75_1440w.webp)

​	可以算出这里就是268KB，这么点大小能存个什么呢？所以这是个很小的文件长度，实际的文件系统，文件最大的长度会大的多得多。那可以做一些什么来让文件系统支持大得多的文件呢？学生回答：可以扩展inode中indirect部分吗？<u>是的，可以用类似page table的方式，构建一个双重indirect block number指向一个block，这个block中再包含了256个indirect block number，每一个又指向了包含256个block number的block。这样的话，最大的文件长度会大得多（注，是256*256*1K）</u>。这里修改了inode的数据结构，你可以使用类似page table的树状结构，也可以按照B树或者其他更复杂的树结构实现。XV6这里极其简单，基本是按照最早的Uinx实现方式来的，不过你可以实现更复杂的结构。实际上，在接下来的File system lab中，你将会实现双重indirect block number来支持更大的文件。

​	接下来，我们想要实现read系统调用。假设我们需要读取文件的第8000个字节，那么你该读取哪个block呢？从inode的数据结构中该如何计算呢？对于8000，我们首先除以1024，也就是block的大小，得到大概是7。这意味着第7个block就包含了第8000个字节。所以直接在inode的direct block number中，就包含了第8000个字节的block。为了找到这个字节在第7个block的哪个位置，我们需要用8000对1024求余数，我猜结果是是832。所以为了读取文件的第8000个字节，文件系统查看inode，先用8000除以1024得到block number，然后再用8000对1024求余读取block中对应的字节。

​	<u>总结一下，inode中的信息完全足够用来实现read/write系统调用，至少可以找到哪个disk block需要用来执行read/write系统调用</u>。

---

![img](https://pic1.zhimg.com/80/v2-456033468eb808130d1ac7d740256550_1440w.webp)

​	接下来我们讨论一下目录（directory）。文件系统的酷炫特性就是**层次化的命名空间（hierarchical namespace）**，你可以在文件系统中保存对用户友好的文件名。**大部分Unix文件系统有趣的点在于，一个目录本质上是一个文件加上一些文件系统能够理解的结构**。<u>在XV6中，这里的结构极其简单。每一个目录包含了directory entries，每一条entry都有固定的格式</u>（每个entry占16字节）：

- 前2个字节包含了目录中文件或者子目录的inode编号
- 接下来的14个字节包含了文件或者子目录名

​	对于实现路径名查找，这里的信息就足够了。假设我们要查找路径名“/y/x”，我们该怎么做呢？

​	从路径名我们知道，应该从root inode开始查找。通<u>常root inode会有固定的inode编号，在XV6中，这个编号是1</u>。我们该如何根据编号找到root inode呢？从前一节我们可以知道，inode从block 32开始，如果是inode1，那么必然在block 32中的64到128字节的位置。所以文件系统可以直接读到root inode的内容。

​	对于路径名查找程序，接下来就是扫描root inode包含的所有block，以找到“y”。<u>该怎么找到root inode所有对应的block呢？根据前一节的内容就是读取所有的direct block number和indirect block number</u>。结果可能是找到了，也可能是没有找到。如果找到了，那么目录y也会有一个inode编号，假设是251，我们可以继续从inode 251查找，先读取inode 251的内容，之后再扫描inode所有对应的block，找到“x”并得到文件x对应的inode编号，最后将其作为路径名查找的结果返回。

![img](https://pic1.zhimg.com/80/v2-29afcdf9fe383ca42e0fba348a9bb6b4_1440w.webp)

​	<u>很明显，这里的结构不是很有效。为了找到一个目录名，你需要线性扫描。实际的文件系统会使用更复杂的数据结构来使得查找更快，当然这又是设计数据结构的问题，而不是设计操作系统的问题。你可以使用你喜欢的数据结构并提升性能。出于简单和更容易解释的目的，XV6使用了这里这种非常简单的数据结构</u>。

---

问题：为什么每个block存储256个block编号？

回答：因为每个编号是4个字节。1024/4 = 256。这又带出了一个问题，如果block编号只是4个字节，磁盘最大能有多大？是的，2的32次方（注，4TB，4字节编号=>32位，2*32寻址，而每个block大小1kb，所以计算得4TB）。有些磁盘比这个数字要大，所以通常人们会使用比32bit更长的数字来表示block编号。在下一个File system lab，你们需要将inode中的一个block number变成双重indirect block number，这个双重indirect block number将会指向一个包含了256个indirect block number的block，其中的每一个indirect block number再指向一个包含了256个block number的block，这样文件就可以大得多。

问题：有没有一些元数据表明当前的inode是目录而不是一个文件？

回答：有的，实际上是在inode中。inode中的type字段表明这是一个目录还是一个文件。如果你对一个类型是文件的inode进行查找，文件系统会返回错误。

## 14.5 File System工作示例

> [14.5 File system工作示例 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351903926) <= 图文出处

​	接下来我们看一下实际中，XV6的文件系统是如何工作的，这部分内容对于下一个lab是有帮助的。

​	首先我会启动XV6，这里有件事情我想指出。启动XV6的过程中，调用了makefs指令，来创建一个文件系统。

![img](https://pic2.zhimg.com/80/v2-aeed959cb6ce8441f792cdd54a2fc5f9_1440w.webp)

​	所以makefs创建了一个全新的磁盘镜像，在这个磁盘镜像中包含了我们在指令中传入的一些文件。makefs为你创建了一个包含这些文件的新的文件系统。

​	XV6总是会打印文件系统的一些信息，所以从指令的下方可以看出有46个meta block，其中包括了：

- boot block
- super block
- 30个log block
- 13个inode block
- 1个bitmap block

​	之后是954个data block。所以这是一个袖珍级的文件系统，总共就包含了1000个block。在File system lab中，你们会去支持更大的文件系统。

​	我还稍微修改了一下XV6，使得任何时候写入block都会打印出block的编号。我们从console的输出可以看出，在XV6启动过程中，会有一些对于文件系统的调用，并写入了block 33，45，32。

​	接下来我们运行一些命令，来看一下特定的命令对哪些block做了写操作，并理解为什么要对这些block写入数据。我们通过echo “hi” > x，来创建一个文件x，并写入字符“hi”。我会将输出拷贝出来，并做分隔以方便我们更好的理解。

![img](https://pic3.zhimg.com/80/v2-cb0419b2a1ded416a9a9dcef49903376_1440w.webp)

​	这里会有几个阶段：

1. 第一阶段是创建文件
2. 第二阶段将“hi”写入文件
3. 第三阶段将“\n”换行符写入到文件

如果你去看echo的代码实现，基本就是这3个阶段。

![img](https://pic2.zhimg.com/80/v2-f7e2392aab10702c806175d18160074d_1440w.webp)

​	上面就是echo的代码，它先检查参数，并将参数写入到文件描述符1，在最后写入一个换行符。

​	让我们一个阶段一个阶段的看echo的执行过程，并理解对于文件系统发生了什么。相比看代码，这里直接看磁盘的分布图更方便：

![img](https://pic3.zhimg.com/80/v2-17e1217febc0a9abbc31db4dbb3ce4c6_1440w.webp)

​	你们觉得的write 33代表了什么？我们正在创建文件，所以我们期望文件系统干什么呢？学生回答：这是在写inode。是的，看起来给我们分配的inode位于block 33。之所以有两个write 33，第一个是为了标记inode将要被使用。在XV6中，我记得是使用inode中的type字段来标识inode是否空闲，这个字段同时也会用来表示inode是一个文件还是一个目录。所以这里将inode的type从空闲改成了文件，并写入磁盘表示这个inode已经被使用了。第二个write 33就是实际的写入inode的内容。inode的内容会包含linkcount为1以及其他内容。

​	write 46是向第一个data block写数据，那么这个data block属于谁呢？学生回答：属于根目录。是的，block 46是根目录的第一个block。为什么它需要被写入数据呢？学生回答：因为我们正在向根目录创建一个新文件。是的，这里我们向根目录增加了一个新的entry，其中包含了文件名x，以及我们刚刚分配的inode编号。

​	接下来的write 32又是什么意思呢？block 32保存的仍然是inode，那么inode中的什么发生了变化使得需要将更新后的inode写入磁盘？是的，根目录的大小变了，因为我们刚刚添加了16个字节的entry来代表文件x的信息。

​	最后又有一次write 33，我在稍后会介绍这次写入的内容，这里我们再次更新了文件x的inode， 尽管我们又还没有写入任何数据。

​	以上就是第一阶段创建文件的过程。第二阶段是向文件写入“hi”。

​	首先是write 45，这是更新bitmap。文件系统首先会扫描bitmap来找到一个还没有使用的data block，未被使用的data block对应bit 0。找到之后，文件系统需要将该bit设置为1，表示对应的data block已经被使用了。所以更新block 45是为了更新bitmap。

​	接下来的两次write 595表明，文件系统挑选了data block 595。所以在文件x的inode中，第一个direct block number是595。因为写入了两个字符，所以write 595被调用了两次。

​	第二阶段最后的write 33是更新文件x对应的inode中的size字段，因为现在文件x中有了两个字符。

​	以上就是磁盘中文件系统的组织结构的核心，希望你们都能理解背后的原理。

----

问题：block 595看起来在磁盘中很靠后了，是因为前面的block已经被系统内核占用了吗？

回答：我们可以看前面makefs指令，makefs存了很多文件在磁盘镜像中，这些都发生在创建文件x之前，所以磁盘中很大一部分已经被这些文件填满了。

问题：第二阶段最后的write 33是否会将block 595与文件x的inode关联起来？

回答：会的。这里的write 33会发生几件事情：首先inode的size字段会更新；第一个direct block number会更新。这两个信息都会通过write 33一次更新到磁盘上的inode中。

## 14.6 XV6创建inode代码演示

> [14.6 XV6创建inode代码展示 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351904145) <= 图文出处

​	接下来我们通过查看XV6中的代码，更进一步的了解文件系统。因为我们前面已经分配了inode，我们先来看一下这是如何发生的。<u>`sysfile.c`中包含了所有与文件系统相关的函数，分配inode发生在`sys_open`函数中，这个函数会负责创建文件</u>。

![img](https://pic1.zhimg.com/v2-96773d813d3f133526d3c87c71468784_r.jpg)

​	在sys_open函数中，会调用create函数。

![img](https://pic3.zhimg.com/80/v2-0c823d5d13aa64ceb7b08d5ecb1ac91e_1440w.webp)

​	create函数中首先会解析路径名并找到最后一个目录，之后会查看文件是否存在，如果存在的话会返回错误。之后就会调用`ialloc(inode allocate)`，这个函数会为文件x分配inode。ialloc函数位于`fs.c`文件中。

![img](https://pic2.zhimg.com/v2-7299d70bb871e7e08e021873aace2579_r.jpg)

​	以上就是ialloc函数，与XV6中的大部分函数一样，它很简单，但是又不是很高效。<u>它会遍历所有可能的inode编号，找到inode所在的block，再看位于block中的inode数据的type字段。如果这是一个空闲的inode，那么将其type字段设置为文件，这会将inode标记为已被分配。函数中的log_write就是我们之前看到在console中有关写block的输出。这里的log_write是我们看到的整个输出的第一个</u>。

​	以上就是第一次写磁盘涉及到的函数调用。这里有个有趣的问题，如果有多个进程同时调用create函数会发生什么？对于一个多核的计算机，进程可能并行运行，两个进程可能同时会调用到ialloc函数，然后进而调用`bread(block read)`函数。所以必须要有一些机制确保这两个进程不会互相影响。

​	让我们看一下位于`bio.c`的buffer cache代码。首先看一下bread函数：

![img](https://pic1.zhimg.com/80/v2-8bdc746f7a62288b6c1c70be5d702970_1440w.webp)

​	bread函数首先会调用bget函数，<u>bget会为我们从buffer cache中找到block的缓存</u>。让我们看一下bget函数：

![img](https://pic3.zhimg.com/80/v2-c95fadc1ddc93af3e204005cd0cd66a6_1440w.webp)

​	这里的代码还有点复杂。我猜你们之前已经看过这里的代码，那么这里的代码在干嘛？学生回答：这里遍历了linked-list，来看看现有的cache是否符合要找的block。是的，我们这里看一下block 33的cache是否存在，如果存在的话，将block对象的引用计数（refcnt）加1，之后再释放bcache锁，因为现在我们已经完成了对于cache的检查并找到了block cache。之后，代码会尝试获取block cache的锁。

​	所以，如果有多个进程同时调用bget的话，其中一个可以获取bcache的锁并扫描buffer cache。此时，其他进程是没有办法修改buffer cache的（注，因为bacche的锁被占住了）。之后，进程会查找block number是否在cache中，如果在的话将block cache的引用计数加1，表明当前进程对block cache有引用，之后再释放bcache的锁。如果有第二个进程也想扫描buffer cache，那么这时它就可以获取bcache的锁。假设第二个进程也要获取block 33的cache，那么它也会对相应的block cache的引用计数加1。最后这两个进程都会尝试对block 33的block cache调用acquiresleep函数。

​	acquiresleep是另一种锁，我们称之为sleep lock，本质上来说它获取block 33 cache的锁。其中一个进程获取锁之后函数返回。在ialloc函数中会扫描block 33中是否有一个空闲的inode。而另一个进程会在acquiresleep中等待第一个进程释放锁。

​	如果buffer cache中有两份block 33的cache将会出现问题。假设一个进程要更新inode19，另一个进程要更新inode20。如果它们都在处理block 33的cache，并且cache有两份，那么第一个进程可能持有一份cache并先将inode19写回到磁盘中，而另一个进程持有另一份cache会将inode20写回到磁盘中，并将inode19的更新覆盖掉。**所以一个block只能在buffer cache中出现一次**。你们在完成File system lab时，必须要维持buffer cache的这个属性。

![img](https://pic4.zhimg.com/80/v2-0abbfb079d312acabed1b241b9dbafff_1440w.webp)

---

问题：当一个block cache的refcnt不为0时，可以更新block cache吗？因为释放锁之后，可能会修改block cache。

回答：这里我想说几点；首先XV6中对bcache做任何修改的话，都必须持有bcache的锁；其次对block 33的cache做任何修改你需要持有block 33的sleep lock。所以在任何时候，`release(&bcache.lock)`之后，`b->refcnt`都大于0。<u>block的cache只会在refcnt为0的时候才会被驱逐，任何时候refcnt大于0都不会驱逐block cache</u>。所以当`b->refcnt`大于0的时候，block cache本身不会被buffer cache修改。这里的第二个锁，也就是block cache的sleep lock，是用来保护block cache的内容的。它确保了任何时候只有一个进程可以读写block cache。

问题：如果多个进程都在使用同一个block的cache，然后有一个进程在修改block，并通过强制向磁盘写数据修改了block的cache，那么其他进程会看到什么结果？

回答：如果第一个进程结束了对block 33的读写操作，它会对block的cache调用`brelse(block cache release)`函数。这个函数会对refcnt减1，并释放sleep lock。这意味着，如果有任何一个其他进程正在等待使用这个block cache，现在它就能获得这个block cache的sleep lock，并发现刚刚做的改动。假设两个进程都需要分配一个新的inode，且新的inode都位于block 33。如果第一个进程分配到了inode18并完成了更新，那么它对于inode18的更新是可见的。另一个进程就只能分配到inode19，因为inode18已经被标记为已使用，任何之后的进程都可以看到第一个进程对它的更新。这正是我们想看到的结果，如果一个进程创建了一个inode或者创建了一个文件，之后的进程执行读就应该看到那个文件。

## 14.7 Sleep Lock和block cache

> [14.7 Sleep Lock - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351904443) <= 图文出处

​	block cache使用的是sleep lock。sleep lock区别于一个常规的spinlock。我们先看来一下sleep lock。

![img](https://pic2.zhimg.com/80/v2-efcaca305c91b49d1f3c61f81bb8dd05_1440w.webp)

​	**首先是acquiresleep函数，它用来获取sleep lock。函数里首先获取了一个普通的spinlock，这是与sleep lock关联在一起的一个锁。之后，如果sleep lock被持有，那么就进入sleep状态，并将自己从当前CPU调度开**。

​	既然sleep lock是基于spinlock实现的，为什么对于block cache，我们使用的是sleep lock而不是spinlock？

​	这里其实有多种原因。**对于spinlock有很多限制，其中之一是加锁时中断必须要关闭**。所以如果使用spinlock的话，当我们对block cache做操作的时候需要持有锁，那么我们就永远也不能从磁盘收到数据。或许另一个CPU核可以收到中断并读到磁盘数据，但是如果我们只有一个CPU核的话，我们就永远也读不到数据了。**出于同样的原因，也不能在持有spinlock的时候进入sleep状态（注，详见13.1）**。所以这里我们使用sleep lock。**<u>sleep lock的优势就是，我们可以在持有锁的时候不关闭中断。我们可以在磁盘操作的过程中持有锁，我们也可以长时间持有锁。当我们在等待sleep lock的时候，我们并没有让CPU一直空转，我们通过sleep将CPU出让出去了</u>**。

​	接下来让我们看一下brelease函数。

![img](https://pic4.zhimg.com/v2-0abbfb079d312acabed1b241b9dbafff_r.jpg)

​	brelease函数中首先释放了sleep lock；之后获取了bcache的锁；之后减少了block cache的引用计数，表明一个进程不再对block cache感兴趣；<u>最后如果引用计数为0，那么它会修改buffer cache的linked-list，将block cache移到linked-list的头部，这样表示这个block cache是最近使用过的block cache</u>。**这一点很重要，当我们在bget函数中不能找到block cache时，我们需要在buffer cache中腾出空间来存放新的block cache，这时会使用LRU（Least Recent Used）算法找出最不常使用的block cache，并撤回它（注，而将刚刚使用过的block cache放在linked-list的头部就可以直接更新linked-list的tail来完成LRU操作）**。<u>为什么这是一个好的策略呢？因为通常系统都遵循temporal locality策略，也就是说如果一个block cache最近被使用过，那么很有可能它很快会再被使用，所以最好不要撤回这样的block cache</u>。

​	以上就是对于block cache代码的介绍。这里有几件事情需要注意：

- **首先在内存中，对于一个block只能有一份缓存。这是block cache必须维护的特性**。
- **其次，这里使用了与之前的spinlock略微不同的sleep lock。与spinlock不同的是，可以在I/O操作的过程中持有sleep lock**。
- 第三，**它采用了LRU作为cache替换策略**。
- 第四，它有两层锁。第一层锁用来保护buffer cache的内部数据；第二层锁也就是sleep lock用来保护单个block的cache。

![img](https://pic4.zhimg.com/v2-a5866bf104800c1351be8f396a21b1df_r.jpg)

---

问题：我有个关于brelease函数的问题，看起来它先释放了block cache的锁，然后再对引用计数refcnt减一，为什么可以这样呢？

回答：这是个好问题。如果我们释放了sleep lock，这时另一个进程正在等待锁，那么refcnt必然大于1，而`b->refcnt --`只是表明当前执行brelease的进程不再关心block cache。如果还有其他进程正在等待锁，那么refcnt必然不等于0，我们也必然不会执行`if(b->refcnt == 0)`中的代码。

## 14.8 小结	

​	最后让我们来总结一下，并把剩下的内容留到下节课。

- 首先，文件系统是一个位于磁盘的数据结构。我们今天的主要时间都用来介绍这个位于磁盘的数据结构的内容。XV6的这个数据结构实现的很简单，但是你可以实现一个更加复杂的数据结构。
- 其次，我们花了一些时间来看block cache的实现，这对于性能来说是至关重要的，因为读写磁盘是代价较高的操作，可能要消耗数百毫秒，而**block cache确保了如果我们最近从磁盘读取了一个block，那么我们将不会再从磁盘读取相同的block**。

![img](https://pic2.zhimg.com/80/v2-b9eac6ff4e31d448479c2c1520de7dd9_1440w.webp)

​	下节课我将会介绍crash safety，这是文件系统设计中非常棒的一部分。我们将会在crash safety讲两节课。下节课我们会看到基于log实现的crash safety机制，下下节课我们会看到Linux的ext3是如何实现的logging，这种方式要快得多。

# Lecture15 故障恢复Crash Recovery

## 15.1 File system crash概述

> [15.1 File system crash概述 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355199492) <= 图文出处

​	今天的课程是有关文件系统中的Crash safety。<u>这里的Crash safety并不是一个通用的解决方案，而是只关注一个特定的问题的解决方案，也就是crash或者电力故障可能会导致在磁盘上的文件系统处于不一致或者不正确状态的问题</u>。当我说不正确的状态时，是指例如一个data block属于两个文件，或者一个inode被分配给了两个不同的文件。

![img](https://pic3.zhimg.com/v2-559277e61123a4d767eae1db423ed9f2_r.jpg)

​	这个问题可能出现的场景可能是这样，当你在运行make指令时，make与文件系统会有频繁的交互，并读写文件，但是在make执行的过程中断电了，可能是你的笔记本电脑没电了，也可能就是停电了，之后电力恢复之后，你重启电脑并运行ls指令，你会期望你的文件系统仍然在一个好的可用的状态。

![img](https://pic3.zhimg.com/v2-d4b784ef8caafbe72f7383a14889c6d2_r.jpg)

​	这里我们关心的crash或者故障包括了：在文件系统操作过程中的电力故障；在文件系统操作过程中的内核panic。包括XV6在内的大部分内核都会panic，panic可能是由内核bug引起，它会突然导致你的系统故障，但是你肯定期望能够在重启之后还能使用文件系统。

​	你可能会反问，怎么就不能使用文件系统了？文件系统不是存储在一个持久化的存储设备上吗？如果电力故障了，存储设备不会受影响，当电脑恢复运行时，存储设备上的block应该还保存着呀。<u>我们将会看到很多文件系统的操作都包含了多个步骤，如果我们在多个步骤的错误位置crash或者电力故障了，存储在磁盘上的文件系统可能会是一种不一致的状态，之后可能会发生一些坏的事情</u>。而这类问题就是我们今天主要关注的问题。这区别于另一类问题，比如说因为电力故障导致你的磁盘着火了，那么什么数据都没有了，这是一个完全不同的问题，并且有着不同的解决方法，这种情况下需要先备份你的文件系统，然后再重新安装文件系统等等。这个问题我们今天不会关心，<u>今天我们关心的是包含了多个步骤的文件系统操作过程中发生的故障</u>。

​	**我们今天会研究对于这类特定问题的解决方法，也就是logging。这是一个最初来自于数据库世界的很流行的解决方案，现在很多文件系统都在使用logging**。之所以它很流行，是因为它是一个很好用的方法。我们将会看到XV6中的logging实现。当然XV6的实现非常简单，几乎是最简单的实现logging的方法，因为我们只是为了演示关键的思想。但是即使是这么基本的logging实现，也包含了一些微妙的问题，我们将会讨论这些问题，这也是为什么文件系统的logging值得学习的原因。我们将会看到，由于XV6实现的较为简单，XV6中的logging存在一个缺点，它的性能并不咋样，尽管logging系统原则上来说可以获得好的性能。所以**在下节课我们将通过学习Linux的ext3文件系统所使用的logging系统，来看一下如何实现一个高性能logging系统**。

​	另外，这是我们最后一节有关XV6的课程。这节课之后，我们将切换到阅读论文。因为这节课讲完了之后，我们就覆盖了操作系统的基本概念，我们可以通过阅读论文看一些更高级的操作系统思想。

​	接下来让我们看一下这节课关注的场景。类似于创建文件，写文件这样的文件系统操作，都包含了多个步骤的写磁盘操作。我们上节课看过了如何创建一个文件，这里多个步骤的顺序是（注，实际步骤会更多，详见14.5）：

- 分配inode，或者在磁盘上将inode标记为已分配
- 之后更新包含了新文件的目录的data block

​	如果在这两个步骤之间，操作系统crash了。这时可能会使得文件系统的属性被破坏。这里的属性是指，每一个磁盘block要么是空闲的，要么是只分配给了一个文件。即使故障出现在磁盘操作的过程中，我们期望这个属性仍然能够保持。如果这个属性被破坏了，那么重启系统之后程序可能会运行出错，比如：

- 操作系统可能又立刻crash了，因为文件系统中的一些数据结构现在可能处于一种文件系统无法处理的状态。
- 或者，更可能的是操作系统没有crash，但是数据丢失了或者读写了错误的数据。

![img](https://pic2.zhimg.com/80/v2-3419e32b9f788732b782ee11ba195245_1440w.webp)

​	我们将会看一些例子来更好的理解出错的场景，但是基本上来说这就是我们需要担心的一些风险。我不知道你们有没有人在日常使用计算机时经历过这些问题，比如说在电力故障之后，你重启电脑或者手机，然后电脑手机就不能用了，这里的一个原因就是文件系统并没有恢复过来。

## 15.2 File system crash示例

> [15.2 File system crash示例 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355199616) <= 图文出处

​	为了更清晰的理解这里的风险，让我们看一些基于XV6的例子，并看一下哪里可能出错。我们在上节课介绍了XV6有一个非常简单的文件系统和磁盘数据的排布方式。

![img](https://pic3.zhimg.com/80/v2-5b173826f0f444bff0eabafd14628dda_1440w.webp)

​	在super block之后就是log block，我们今天主要介绍的就是log block。log block之后是inode block，每个block可能包含了多个inode。之后是bitmap block，它记录了哪个data block是空闲的。最后是data block，这里包含了文件系统的实际数据。

​	在上节课中，我们看了一下在创建文件时，操作系统与磁盘block的交互过程（注，详见14.5）：

![img](https://pic2.zhimg.com/80/v2-bcf0ed4a192bc6e9afc367718a7e3071_1440w.webp)

​	从上面可以看出，创建一个文件涉及到了多个操作：

- 首先是分配inode，因为首先写的是block 33
- 之后inode被初始化，然后又写了一次block 33
- 之后是写block 46，是将文件x的inode编号写入到x所在目录的inode的data block中
- 之后是更新root inode，因为文件x创建在根目录，所以需要更新根目录的inode的size字段，以包含这里新创建的文件x
- 最后再次更新了文件x的inode

​	现在我们想知道，哪里可能出错。假设我们在下面这个位置出现了电力故障或者内核崩溃。

![img](https://pic1.zhimg.com/80/v2-b71ababa5430c3352388fc40bec594ec_1440w.webp)

​	在出现电力故障之后，因为内存数据保存在RAM中，所有的内存数据都丢失了。所有的进程数据，所有的文件描述符，内存中所有的缓存都没有了，因为内存数据不是持久化的。我们唯一剩下的就是磁盘上的数据，因为磁盘的介质是持久化的，所以只有磁盘上的数据能够在电力故障之后存活。基于这些事实，如果我们在上面的位置出现故障，并且没有额外的机制，没有logging，会有多糟糕呢？我们这里会有什么风险？

​	<u>在这个位置，我们先写了block 33表明inode已被使用，之后出现了电力故障，然后计算机又重启了。这时，我们丢失了刚刚分配给文件x的inode。这个inode虽然被标记为已被分配，但是它并没有放到任何目录中，所以也就没有出现在任何目录中，因此我们也就没办法删除这个inode。所以在这个位置发生电力故障会导致我们丢失inode</u>。

​	你或许会认为，我们应该改一改代码，将写block的顺序调整一下，这样就不会丢失inode了。所以我们可以先写block 46来更新目录内容，之后再写block 32来更新目录的size字段，最后再将block 33中的inode标记为已被分配。

![img](https://pic3.zhimg.com/80/v2-60dc4e2289271701f76250b0376f73f2_1440w.webp)

​	这里的效果是一样的，只是顺序略有不同。并且这样我们应该可以避免丢失inode的问题。那么问题来了，这里可以工作吗？我们应该问问自己，如果在下面的位置发生了电力故障会怎样？

![img](https://pic2.zhimg.com/80/v2-09aa7f8f0866cacc76241f206d9f0ff1_1440w.webp)

​	在这个位置，目录被更新了，但是还没有在磁盘上分配inode（有个问题，如果inode没分配的话，write 46的时候写的是啥）。电力故障之后机器重启，文件系统会是一个什么状态？或者说，如果我们读取根目录下的文件x，会发生什么，因为现在在根目录的data block已经有了文件x的记录？

​	<u>是的，我们会读取一个未被分配的inode，因为inode在crash之前还未被标记成被分配。更糟糕的是，如果inode之后被分配给一个不同的文件，这样会导致有两个应该完全不同的文件共享了同一个inode</u>。如果这两个文件分别属于用户1和用户2，那么用户1就可以读到用户2的文件了。所以上面的解决方案也不好。

​	所以调整写磁盘的顺序并不能彻底解决我们的问题，我们只是从一个问题换到了一个新的问题。

​	让我们再看一个例子，这个例子中会向文件x写入“hi”（注，也就是14.5介绍的第二个部分）

![img](https://pic3.zhimg.com/80/v2-8d46e7f06626e50445e7f04e2c08631a_1440w.webp)

​	一旦成功的创建了文件x，之后会调用write系统调用，我们在上节课看到了write系统调用也执行了多个写磁盘的操作。

- 首先会从bitmap block，也就是block 45中，分配data block，通过从bitmap中分配一个bit，来表明一个data block已被分配。
- 上一步分配的data block是block 595，这里将字符“h”写入到block 595。
- 将字符“i”写入到block 595。
- 最后更新文件夹x的inode来更新size字段。

​	这里我们也可以问自己一个问题，我们在下面的位置crash了会怎样？

![img](https://pic2.zhimg.com/80/v2-ab1b838ad14eb54d260b7d1b8fc256ad_1440w.webp)

​	这里我们从bitmap block中分配了一个data block，但是又还没有更新到文件x的inode中。当我们重启之后，磁盘处于一个特殊的状态，这里的风险是什么？是的，我们这里丢失了data block，因为这个data block被分配了，但是却没有出现在任何文件中，因为它还没有被记录在任何inode中。

​	你或许会想，是因为这里的顺序不对才会导致丢失data block的问题。我们应该先写block 33来更新inode来包含data block 595（同样的问题，这个时候data block都还没有分配怎么知道是595），之后才通过写block 45将data block 595标记为已被分配。

![img](https://pic2.zhimg.com/80/v2-020194b69eb69a37f7aaece29aa24f7d_1440w.webp)

​	所以，为了避免丢失data block，我们将写block的顺序改成这样。现在我们考虑一下，如果故障发生在这两个操作中间会怎样？

![img](https://pic4.zhimg.com/80/v2-adb77bccbf45e6e0d2e49e4918a589a7_1440w.webp)

​	<u>这时inode会认为data block 595属于文件x，但是在磁盘上它还被标记为未被分配的。之后如果另一个文件被创建了，block 595可能会被另一个文件所使用。所以现在两个文件都会在自己的inode中记录block 595。如果两个文件属于两个用户，那么两个用户就可以读写彼此的数据了</u>。很明显，我们不想这样，**文件系统应该确保每一个data block要么属于且只属于一个文件，要么是空闲的。所以这里的修改会导致磁盘block在多个文件之间共享的安全问题，这明显是错误的**。

​	所以**<u>这里的问题并不在于操作的顺序，而在于我们这里有多个写磁盘的操作，这些操作必须作为一个原子操作出现在磁盘上</u>**。

## 15.3 File system logging概述

> [15.3 File system logging - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355199735) <= 图文出处

​	我们这节课要讨论的针对文件系统crash之后的问题的解决方案，其实就是logging。这是来自于数据库的一种解决方案。它有一些好的属性：

- 首先，它可以**确保文件系统的系统调用是原子性的**。比如你调用create/write系统调用，这些系统调用的效果是要么完全出现，要么完全不出现，这样就避免了一个系统调用只有部分写磁盘操作出现在磁盘上。
- 其次，它**支持快速恢复(Fast Recovery)**。在重启之后，我们不需要做大量的工作来修复文件系统，只需要非常小的工作量。这里的快速是相比另一个解决方案来说，在另一个解决方案中，你可能需要读取文件系统的所有block，读取inode，bitmap block，并检查文件系统是否还在一个正确的状态，再来修复。而logging可以有快速恢复的属性。
- 最后，**原则上来说，它可以非常的高效**，尽管我们在XV6中看到的实现不是很高效。

​	我们会在下节课看一下，如何构建一个logging系统，并同时具有**原子性**的系统调用，**快速恢复**和**高性能**，而今天，我们只会关注前两点。

![img](https://pic1.zhimg.com/80/v2-dd8a6b6ef5a067a6b6917324282a254c_1440w.webp)

​	logging的基本思想还是很直观的。首先，你将磁盘分割成两个部分，其中一个部分是log，另一个部分是文件系统，文件系统可能会比log大得多。

​	<u>（log write）当需要更新文件系统时，我们并不是更新文件系统本身。假设我们在内存中缓存了bitmap block，也就是block 45。当需要更新bitmap时，我们并不是直接写block 45，而是将数据写入到log中，并记录这个更新应该写入到block 45。对于所有的写 block都会有相同的操作，例如更新inode，也会记录一条写block 33的log</u>。

![img](https://pic1.zhimg.com/80/v2-51e27b2d0085e2ea4b58074bcb1357c8_1440w.webp)

​	**所以基本上，任何一次写操作都是先写入到log，我们并不是直接写入到block所在的位置，而总是先将写操作写入到log中**。

​	<u>（commit op）之后在某个时间，当文件系统的操作结束了，比如说我们前一节看到的4-5个写block操作都结束，并且都存在于log中，我们会commit文件系统的操作。这意味着我们需要在log的某个位置记录属于同一个文件系统的操作的个数，例如5</u>。

​	<u>（install log）当我们在log中存储了所有写block的内容时，如果我们要真正执行这些操作，只需要将block从log分区移到文件系统分区。我们知道第一个操作该写入到block 45，我们会直接将数据从log写到block45，第二个操作该写入到block 33，我们会将它写入到block 33，依次类推</u>。

​	<u>（clean log）一旦完成了，就可以清除log。清除log实际上就是将属于同一个文件系统的操作的个数设置为0</u>。

![img](https://pic2.zhimg.com/v2-56e6b3a8d712898d2b50187ea279483d_r.jpg)

​	**以上就是log的基本工作方式。为什么这样的工作方式是好的呢？假设我们crash并重启了。在重启的时候，文件系统会查看log的commit记录值，如果是0的话，那么什么也不做。如果大于0的话，我们就知道log中存储的block需要被写入到文件系统中，很明显我们在crash的时候并不一定完成了install log，我们可能是在commit之后，clean log之前crash的。所以这个时候我们需要做的就是reinstall（注，也就是将log中的block再次写入到文件系统），再clean log**。

![img](https://pic4.zhimg.com/v2-f376520353cba27b3ff9b4125fdc7a9f_r.jpg)

​	这里的方法之所以能起作用，是因为可以确保当发生crash（并重启之后），我们要么将写操作所有相关的block都在文件系统中更新了，要么没有更新任何一个block，我们永远也不会只写了一部分block。为什么可以确保呢？我们考虑crash的几种可能情况。

- 在第1步和第2步之间crash会发生什么？在重启的时候什么也不会做，就像系统调用从没有发生过一样，也像crash是在文件系统调用之前发生的一样。这完全可以，并且也是可接受的。
- 在第2步和第3步之间crash会发生什么？在这个时间点，所有的log block都落盘了，因为有commit记录，所以完整的文件系统操作必然已经完成了。我们可以将log block写入到文件系统中相应的位置，这样也不会破坏文件系统。所以这种情况就像系统调用正好在crash之前就完成了。
- 在install（第3步）过程中和第4步之前这段时间crash会发生什么？在下次重启的时候，我们会redo log，我们或许会再次将log block中的数据再次拷贝到文件系统。这样也是没问题的，因为log中的数据是固定的，我们就算重复写了文件系统，每次写入的数据也是不变的。重复写入并没有任何坏处，因为我们写入的数据可能本来就在文件系统中，所以多次install log完全没问题。当然在这个时间点，我们不能执行任何文件系统的系统调用。我们应该在重启文件系统之前，在重启或者恢复的过程中完成这里的恢复操作。<u>换句话说，install log是幂等操作（注，idempotence，表示执行多次和执行一次效果一样），你可以执行任意多次，最后的效果都是一样的</u>。

​	Logging的实现方式有很多，我这里展示的只是一种非常简单的方案，这个方案中clean log和install log都被推迟了。**接下来我会运行这种非常简单的实现方式，之后在下节课我们会看到更加复杂的logging协议。不过所有的<u>这些协议都遵循了write ahead rule，也就是说在写入commit记录之前，你需要确保所有的写操作都在log中</u>。在这个范围内，还有大量设计上的灵活性可以用来设计特定的logging协议**。

​	在XV6中，我们会看到数据有两种状态，是在**磁盘**上还是在**内存**中。内存中的数据会在crash或者电力故障之后丢失。

![img](https://pic3.zhimg.com/v2-187f61b6a9f85d2bd54315d8720d3e16_r.jpg)

​	XV6的log结构如往常一样也是极其的简单。我们在最开始有一个header block，也就是我们的commit record，里面包含了：

- 数字n代表有效的log block的数量
- 每个log block的实际对应的block编号

![img](https://pic3.zhimg.com/80/v2-ee5208b8b6171235d2fcfc5106bd659a_1440w.webp)

​	之后就是log的数据，也就是每个block的数据，依次为bn0对应的block的数据，bn1对应的block的数据以此类推。这就是log中的内容，并且log也不包含其他内容。

![img](https://pic1.zhimg.com/v2-662304c20ded58472f9fd8fe88b4d28c_r.jpg)

​	<u>当文件系统在运行时，在内存中也有header block的一份拷贝，拷贝中也包含了n和block编号的数组。这里的block编号数组就是log数据对应的实际block编号，并且相应的block也会缓存在**block cache**中，这个在Lec14有介绍过</u>。与前一节课对应，log中第一个block编号是45，那么在block cache的某个位置，也会有block 45的cache。

![img](https://pic2.zhimg.com/80/v2-cad61f41707f76fac318178f298fc811_1440w.webp)

​	以上就是内存中的文件系统和磁盘上的文件系统的结构。

---

**问题：因为这里的接口只有read/write，但是如果我们做append操作，就不再安全了，对吧？**

**回答：某种程度来说，append是文件系统层面的操作，在这个层面，我们可以使用上面介绍的logging机制确保其原子性（注，append也可以拆解成底层的read/write）**。

问题：当正在commit log的时候crash了会发生什么？比如说你想执行多个写操作，但是只commit了一半。

回答：在上面的第2步，执行commit操作时，你只会在记录了所有的write操作之后，才会执行commit操作。所以在执行commit时，所有的write操作必然都在log中。<u>而commit操作本身也有个有趣的问题，它究竟会发生什么？如我在前面指出的，**commit操作本身只会写一个block**</u>。**<u>文件系统通常可以这么假设，单个block或者单个sector的write是原子操作（注，有关block和sector的区别详见14.3）。这里的意思是，如果你执行写操作，要么整个sector都会被写入，要么sector完全不会被修改。所以sector本身永远也不会被部分写入，并且commit的目标sector总是包含了有效的数据</u>**。<u>而**commit操作本身只是写log的header**，如果它成功了只是在commit header中写入log的长度，例如5，这样我们就知道log的长度为5。这时crash并重启，我们就知道需要重新install 5个block的log。如果commit header没能成功写入磁盘，那这里的数值会是0。我们会认为这一次事务并没有发生过</u>。**这里本质上是write ahead rule，它表示logging系统在所有的写操作都记录在log中之前，不能install log**。

## 15.4 XV6 logging实现之log_write函数

> [15.4 log_write函数 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355199982) <= 图文出处

​	接下来让我们看一些代码来帮助我们理解这里是怎么工作的。前面我提过事务（transaction），也就是我们不应该在所有的写操作完成之前写入commit record。这意味着**文件系统操作必须表明事务的开始和结束**。在XV6中，以创建文件的sys_open为例（在`sysfile.c`文件中）每个文件系统操作，都有`begin_op()`和`end_op()`分别表示事物的开始和结束。

![img](https://pic2.zhimg.com/v2-b4c43c3920748ca6c29a0aa4d3ea7d19_r.jpg)

​	**`begin_op()`表明想要开始一个事务，在最后有`end_op()`表示事务的结束。并且事务中的所有写block操作具备原子性，这意味着这些写block操作要么全写入，要么全不写入**。<u>XV6中的文件系统调用都有这样的结构，最开始是`begin_op()`，之后是实现系统调用的代码，最后是`end_op()`。在end_op中会实现commit操作</u>。

​	**在`begin_op()`和`end_op()`之间，磁盘上或者内存中的数据结构会更新。但是在`end_op()`之前，并不会有实际的改变（注，也就是不会写入到实际的block中）。在`end_op()`时，我们会将数据写入到log中，之后再写入commit record或者log header**。这里有趣的是，当文件系统调用执行写磁盘时会发生什么？

​	让我们看一下`fs.c`中的ialloc函数。

![img](https://pic3.zhimg.com/v2-5454e423440b0a3842975b358104737e_r.jpg)

​	在这个函数中，并没有直接调用bwrite，这里实际调用的是log_write函数。**log_write是由文件系统的logging实现的方法。任何一个文件系统调用的begin_op和end_op之间的写操作总是会走到log_write**。log_write函数位于`log.c`文件。

![img](https://pic4.zhimg.com/80/v2-08cd04000d64fc866f9bf1e2cc7c52bb_1440w.webp)

​	log_write还是很简单直观的，我们已经向block cache中的某个block写入了数据。比如写block 45，我们已经更新了block cache中的block 45。接下来我们需要在内存中记录，在稍后的commit中，要将block 45写入到磁盘的log中。

​	这里的代码先获取log header的锁，之后再更新log header。首先代码会查看block 45是否已经被log记录了。如果是的话，其实不用做任何事情，因为block 45已经会被写入了。<u>这种忽略的行为称为log absorbtion</u>。如果block 45不在需要写入到磁盘中的block列表中，接下来会对n加1，并将block 45记录在列表的最后。之后，这里会通过调用bpin函数将block 45固定在block cache中，我们稍后会介绍为什么要这么做（注，详见15.8）。

​	以上就是log_write的全部工作了。**任何文件系统调用，如果需要更新block或者说更新block cache中的block，都会将block编号加在这个内存数据中（注，也就是log header在内存中的cache），除非编号已经存在**。

----

问题：这是不是意味着，bwrite不能直接使用？

回答：是的，可以这么认为，文件系统中的所有bwrite都需要被log_write替换。

## 15.5 XV6 logging实现之end_op函数

> [15.5 end_op函数 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355200180) <= 图文出处

​	接下来我们看看位于`log.c`中的end_op函数中会发生什么？

![img](https://pic1.zhimg.com/80/v2-18eb2debc44df2ec6afd1204054aa320_1440w.webp)

​	可以看到，即使是这么简单的一个文件系统也有一些微妙的复杂之处，代码的最开始就是一些复杂情况的处理（注，15.8有这部分的解释）。我直接跳到正常且简单情况的代码。在简单情况下，没有其他的文件系统操作正在处理中。这部分代码非常简单直观，首先调用了commit函数。让我们看一下commit函数的实现。

![img](https://pic2.zhimg.com/80/v2-ef653b61dead31206fe2c474aab18389_1440w.webp)

​	commit中有两个操作：

- **首先是write_log。这基本上就是将所有存在于内存中的log header中的block编号对应的block，从block cache写入到磁盘上的log区域中（注，也就是将变化先从内存拷贝到log中）**。
- **write_head会将内存中的log header写入到磁盘中**。

​	我们看一下write_log的实现。

![img](https://pic2.zhimg.com/v2-b2a781e920c3723a262abecee2261ff1_r.jpg)

​	<u>函数中依次遍历log中记录的block，并写入到log中。它首先读出log block，将cache中的block拷贝到log block，最后再将log block写回到磁盘中。这样可以确保需要写入的block都记录在log中。但是在这个位置，我们还没有commit，现在我们只是将block存放在了log中。如果我们在这个位置也就是在write_head之前crash了，那么最终的表现就像是transaction从来没有发生过</u>。

​	**接下来看一下write_head函数，我之前将write_head称为commit point**。

![img](https://pic1.zhimg.com/80/v2-6cba82f425aa51292ef35ef3f6fa4a10_1440w.webp)

​	函数也比较直观，首先读取log的header block。将n拷贝到block中，将所有的block编号拷贝到header的列表中。最后再将header block写回到磁盘。函数中的倒数第2行，bwrite是实际的commit point吗？如果crash发生在这个bwrite之前，会发生什么？这时虽然我们写了log的header block，但是数据并没有落盘。所以crash并重启恢复时，并不会发生任何事情。

​	**那crash发生在bwrite之后会发生什么呢？这时header会写入到磁盘中，当重启恢复相应的文件系统操作会被恢复。在恢复过程的某个时间点，恢复程序可以读到log header并发现比如说有5个log还没有install，恢复程序可以将这5个log拷贝到实际的位置。所以<u>这里的bwrite就是实际的commit point。在commit point之前，transaction并没有发生，在commit point之后，只要恢复程序正确运行，transaction必然可以完成</u>**。

​	回到commit函数，在commit point之后，就会实际应用transaction。这里很直观，就是读取log block再查看header这个block属于文件系统中的哪个block，最后再将log block写入到文件系统相应的位置。让我们看一下install_trans函数，

![img](https://pic2.zhimg.com/80/v2-b0255fb445d9f369b1e315b92bd7a1cd_1440w.webp)

​	<u>这里先读取log block，再读取文件系统对应的block。将数据从log拷贝到文件系统，最后将文件系统block缓存落盘。这里**实际上就是将block数据从log中拷贝到了实际的文件系统block中**</u>。当然，可能在这里代码的某个位置会出现问题，但是这应该也没问题，因为在恢复的时候，我们会从最开始重新执行过。

​	**在commit函数中，install结束之后，会将log header中的n设置为0，再将log header写回到磁盘中。将n设置为0的效果就是清除log**。

​	以上就是commit内容。

---

问题：install_trans函数在写block的时候，先写的缓存。可不可以优化一下直接写磁盘而不写缓存让代码运行的更快一些？

回答：这里的接口是不太好。你可能会想问反正都要写入新数据，为什么要先读出目标block来。这里的代码肯定还有很多优化空间，但是为了看起来简单我们并没有这么做。

## 15.6 File system recovering

> [15.6 File system recovering - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355200687) <= 图文出处

​	接下来我们看一下发生在XV6的启动过程中的文件系统的恢复流程。当系统crash并重启了，在XV6启动过程中做的一件事情就是调用initlog函数。

![img](https://pic3.zhimg.com/80/v2-32f99672fb5abeeae484493761011c0a_1440w.webp)

​	initlog基本上就是调用recover_from_log函数。

![img](https://pic1.zhimg.com/80/v2-67bf59e83e86be49fe9487af5d540cac_1440w.webp)

​	**recover_from_log先调用read_head函数从磁盘中读取header，之后调用install_trans函数。这个函数之前在commit函数中也调用过，它就是读取log header中的n，然后根据n将所有的log block拷贝到文件系统的block中。recover_from_log在最后也会跟之前一样清除log**。

​	<u>这就是恢复的全部流程。如果我们在install_trans函数中又crash了，也不会有问题，因为之后再重启时，XV6会再次调用initlog函数，再调用recover_from_log来重新install log。如果我们在commit之前crash了多次，在最终成功commit时，log可能会install多次</u>。

---

问题：如果一个进程向磁盘写了一些数据，但是在commit之前进程出现了故障，假设故障之后进程退出了，这样会有问题吗？

回答：简单回答是没问题，因此磁盘不会被更新，所以效果就像文件系统操作没有发生过一样。并且进程并不能在故障后恢复，唯一能在故障之后还能保持的是保存在磁盘中的状态。（注，应该是没有理解问题。进程通过write系统调用成功写入的数据，就算在成功落盘之前进程异常退出了，内核还是会写入到磁盘中，前提是内核还在运行。）

## 15.7 XV6的log写磁盘过程

> [15.7 Log写磁盘流程 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355200891) <= 图文出处

​	我已经在bwrite函数中加了一个print语句。bwrite函数是block cache中实际写磁盘的函数，所以我们将会看到实际写磁盘的记录。在上节课（Lec 14）我将print语句放在了log_write中，log_write只能代表文件系统操作的记录，并不能代表实际写磁盘的记录。我们这里会像上节课一样执行`echo "hi" > x`，并看一下实际的写磁盘过程。

![img](https://pic1.zhimg.com/80/v2-96478024d83797a8f01d79fdd727a850_1440w.webp)

​	很明显这里的记录要比只在log_write中记录要长的多。之前的log_write只有11条记录（注，详见14.5）但是可以看到实际上背后有很多个磁盘写操作，让我们来分别看一下这里的写磁盘操作：

- 首先是前3行的bwrite 3，4，5。因为block 3是第一个log data block，所以前3行是在log中记录了3个写操作。这3个写操作都保存在log中，并且会写入到磁盘中的log部分。
- 第4行的bwrite 2。因为block 2是log的起始位置，也就是log header，所以这条是commit记录。
- 第5，6，7行的bwrite 33，46，32。这里实际就是将前3行的log data写入到实际的文件系统的block位置，这里实际是install log。
- 第8行的bwrite 2，是清除log（注，也就是将log header中的n设置为0）。到此为止，完成了实际上的写block 33，46，32这一系列的操作。第一部分是log write，第二部分是install log，每一部分后面还跟着一个更新commit记录（注，也就是commit log和clean log）。

​	所以以上就是XV6中文件系统的logging介绍，即使是这么一个简单的logging系统也有一定的复杂度。<u>这里立刻可以想到的一个问题是，通过观察这些记录，这是一个很有效的实现吗？很明显不是的，因为数据被写了两次。如果我写一个大文件，我需要在磁盘中将这个大文件写两次。所以这必然不是一个高性能的实现，为了实现Crash safety我们将原本的性能降低了一倍。当你们去读ext3论文时，你们应该时刻思考如何避免这里的性能降低一倍的问题</u>。

---

**问题：可以从这里的记录找到一次文件操作的begin_op和end_op位置吗？**

**回答：大概可以知道。我们实际上不知道begin_op的位置，但是所有的文件系统操作都从begin_op开始。更新commit记录必然在end_op中，所以我们可以找到文件系统操作的end_op位置，之后就是begin_op（注，其实这里所有的操作都在end_op中，只需要区分每一次end_op的调用就可以找到begin_op）。**

## 15.8 File System challenges

> [15.8 File system challenges - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355201082) <= 图文出处

​	前面说到XV6的文件系统有一定的复杂性，接下来我将介绍一下三个复杂的地方或者也可以认为是三个挑战。

​	第一个是**cache eviction**。<u>假设transaction还在进行中，我们刚刚更新了block 45，正要更新下一个block，而整个buffer cache都满了并且决定撤回block 45。在buffer cache中撤回block 45意味着我们需要将其写入到磁盘的block 45位置，这里会不会有问题？如果我们这么做了的话，会破坏什么规则吗？是的，如果将block 45写入到磁盘之后发生了crash，就会破坏transaction的原子性。这里也破坏了前面说过的write ahead rule，write ahead rule的含义是，你需要先将所有的block写入到log中，之后才能实际的更新文件系统block</u>。所以**buffer cache不能撤回任何还位于log的block**。

​	<u>前面在介绍log_write函数时，其中调用了一个叫做bpin的函数，这个函数的作用就如它的名字一样，将block固定在buffer cache中。它是通过给block cache增加引用计数来避免cache撤回对应的block</u>。**在之前（注，详见14.6）我们看过，如果引用计数不为0，那么buffer cache是不会撤回block cache的**。<u>相应的在将来的某个时间，所有的数据都写入到了log中，我们可以在cache中unpin block（注，在15.5中的install_trans函数中会有unpin，因为这时block已经写入到了log中）</u>。所以这是第一个复杂的地方，**我们需要pin/unpin buffer cache中的block**。

![img](https://pic4.zhimg.com/v2-9032082fa8101a9d142fb9165ebee227_r.jpg)

​	第二个挑战是，**文件系统操作必须适配log的大小**。在XV6中，总共有30个log block（注，详见14.3）。当然我们可以提升log的尺寸，在真实的文件系统中会有大得多的log空间。但是无所谓啦，不管log多大，文件系统操作必须能放在log空间中。如果一个文件系统操作尝试写入超过30个block，那么意味着部分内容需要直接写到文件系统区域，而这是不被允许的，因为这违背了write ahead rule。所以所有的文件系统操作都必须适配log的大小。

​	为什么XV6的log大小是30？因为30比任何一个文件系统操作涉及的写操作数都大，Robert和我看了一下所有的文件系统操作，发现都远小于30，所以就将XV6的log大小设为30。我们目前看过的一些文件系统操作，例如创建一个文件只包含了写5个block。实际上大部分文件系统操作只会写几个block。你们可以想到什么样的文件系统操作会写很多很多个block吗？是的，写一个大文件。如果我们调用write系统调用并传入1M字节的数据，这对应了写1000个block，这看起来会有很严重的问题，因为这破坏了我们刚刚说的“文件系统操作必须适配log的大小”这条规则。

​	让我们看一下`file.c`文件中的file_write函数。

![img](https://pic2.zhimg.com/80/v2-de768b37f96ba519699c57253f83a2d1_1440w.webp)

​	<u>从这段代码可以看出，如果写入的block数超过了30，那么一个写操作会被分割成多个小一些的写操作。这里整个写操作不是原子的，但是这还好啦，因为write系统调用的语义并不要求所有1000个block都是原子的写入，它只要求我们不要损坏文件系统。所以**XV6会将一个大的写操作分割成多个小的写操作，每一个小的写操作通过独立的transaction写入。这样文件系统本身不会陷入不正确的状态中**</u>。

​	这里还需要注意，**<u>因为block在落盘之前需要在cache中pin住，所以buffer cache的尺寸也要大于log的尺寸</u>**。

![img](https://pic4.zhimg.com/80/v2-4f8fb15ca08d3cc26e5ebe33ac567b8b_1440w.webp)

​	最后一个要讨论的挑战是**并发文件系统调用**。让我先来解释一下这里会有什么问题，再看对应的解决方案。假设我们有一段log，和两个并发的执行的transaction，其中transaction t0在log的前半段记录，transaction t1在log的后半段记录。可能我们用完了log空间，但是任何一个transaction都还没完成。

![img](https://pic3.zhimg.com/80/v2-a362ca89db56ead3e6f5956fc10a1ca6_1440w.webp)

​	现在我们能提交任何一个transaction吗？我们不能，因为这样的话我们就提交了一个部分完成的transaction，这违背了write ahead rule，log本身也没有起到应该的作用。所以**必须要保证多个并发transaction加在一起也适配log的大小**。所以**当我们还没有完成一个文件系统操作时，我们必须在确保可能写入的总的log数小于log区域的大小的前提下，才允许另一个文件系统操作开始**。

​	**XV6通过限制并发文件系统操作的个数来实现这一点**。<u>在begin_op中，我们会检查当前有多少个文件系统操作正在进行。如果有太多正在进行的文件系统操作，我们会通过sleep停止当前文件系统操作的运行，并等待所有其他所有的文件系统操作都执行完并commit之后再唤醒</u>。**这里的其他所有文件系统操作都会一起commit。有的时候这被称为<u>group commit</u>，因为这里将多个操作像一个大的transaction一样提交了，这里的多个操作要么全部发生了，要么全部没有发生**。

![img](https://pic3.zhimg.com/80/v2-b4a1b28c4855780eb23514e657cfdd2e_1440w.webp)

​	最后我们再回到最开始，看一下begin_op。

![img](https://pic1.zhimg.com/v2-8e5d54ff96e9067b3eb556d6a7172d6c_r.jpg)

​	<u>首先，如果log正在commit过程中，那么就等到log提交完成，因为我们不能在install log的过程中写log；其次，如果当前操作是允许并发的操作个数的后一个，那么当前操作可能会超过log区域的大小，我们也需要sleep并等待所有之前的操作结束；最后，如果当前操作可以继续执行，需要将log的outstanding字段加1，最后再退出函数并执行文件系统操作</u>。

​	再次看一下end_op函数。

![img](https://pic3.zhimg.com/v2-d4628474419b78fb753f9fc2e5f93b06_r.jpg)

​	<u>在最开始首先会对log的outstanding字段减1，因为一个transaction正在结束；其次检查commiting状态，当前不可能在commiting状态，所以如果是的话会触发panic；如果当前操作是整个并发操作的最后一个的话（log.understanding == 0），接下来立刻就会执行commit；如果当前操作不是整个并发操作的最后一个的话，我们需要唤醒在begin_op中sleep的操作，让它们检查是不是能运行</u>。

​	（注，这里的understanding有点迷，它表示的是当前正在并发执行的文件系统操作的个数，MAXOPBLOCKS定义了一个操作最大可能涉及的block数量。在begin_op中，只要log空间还足够，就可以一直增加并发执行的文件系统操作。所以XV6是通过设定了MAXOPBLOCKS，再间接的限定支持的并发文件系统操作的个数）

​	所以，即使是XV6中这样一个简单的文件系统，也有一些复杂性和挑战。

​	最后简单小结一下：

​	**这节课讨论的是使用logging来解决crash safety或者说多个步骤的文件系统操作的安全性。这种方式对于安全性来说没有问题，但是性能不咋地**。

----

问题：group commit有必要吗？不能当一个文件系统操作结束的时候就commit掉，然后再commit其他的操作吗？

回答：如果这样的话你需要非常非常小心。因为有一点我没有说得很清楚，我们需要保证write系统调用的顺序。如果一个read看到了一个write，再执行了一次write，那么第二个write必须要发生在第一个write之后。<u>在log中的顺序，本身就反应了write系统调用的顺序，你不能改变log中write系统调用的执行顺序，因为这可能会导致对用户程序可见的奇怪的行为。所以必须以transaction发生的顺序commit它们，而一次性提交所有的操作总是比较安全的，这可以保证文件系统处于一个好的状态</u>。

问题：前面说到cache size至少要跟log size一样大，如果它们一样大的话，并且log pin了30个block，其他操作就不能再进行了，因为buffer中没有额外的空间了。

回答：如果buffer cache中没有空间了，XV6会直接panic。这并不理想，实际上有点恐怖。所以我们在挑选buffer cache size的时候希望用一个不太可能导致这里问题的数字。<u>这里为什么不能直接返回错误，而是要panic？因为很多文件系统操作都是多个步骤的操作，假设我们执行了两个write操作，但是第三个write操作找不到可用的cache空间，那么第三个操作无法完成，**我们不能就直接返回错误，因为我们可能已经更新了一个目录的某个部分，为了保证文件系统的正确性，我们需要撤回之前的更新**。所以如果log pin了30个block，并且buffer cache没有额外的空间了，会直接panic。当然这种情况不太会发生，只有一些极端情况才会发生</u>。

# Lecture16 文件系统性能和快速崩溃恢复(File System Performance and Fast Crash Recovery)
