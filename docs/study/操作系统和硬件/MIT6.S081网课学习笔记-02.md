# MIT6.S081网课学习笔记-02

> [MIT6.S081 操作系统工程中文翻译 - 知乎 (zhihu.com)](https://www.zhihu.com/column/c_1294282919087964160) => 知乎大佬视频中文笔记，很全
>
> [6.S081 / Fall 2020 (mit.edu)](https://pdos.csail.mit.edu/6.S081/2020/schedule.html) => 课表+实验超链接等信息

# Lecture13 Sleep & Wakeup

​	该章节大部分时间都会讨论coordination，XV6通过Sleep&Wakeup实现了coordination。后面会讨论lost wake-up的问题。

## 13.1 XV6线程switch的锁限制

> [13.1 线程切换过程中锁的限制 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349778171) <= 图文来自此博文

这里先说结论，**switch中两个锁限制**：

1. switch之前，需要先acquire当前进程锁`p->lock`

2. switch之前，当前进程**必须有且只持有**进程锁`p->lock`。(不能持有其他锁，否则可能导致死锁)

---

​	先回顾线程切换switch。**在XV6中，任何时候调用switch函数都会从一个线程切换到另一个线程，通常是在用户进程的内核线程和调度器线程之间切换。在调用switch函数之前，总是会先获取线程对应的用户进程的锁。所以过程是这样，一个进程先获取自己的锁，然后调用switch函数切换到调度器线程，调度器线程再释放进程锁**。	

​	代码执行顺序大致如下：

1. 一个进程出于某种原因想要进入休眠状态，比如说yield出让CPU或者等待数据，它会先acquire获取自己的锁；
2. 之后进程将自己的状态从RUNNING设置为RUNNABLE；
3. 之后进程调用switch函数，其实是调用sched函数（sched里会调用switch函数）；
4. switch函数将当前的线程切换到调度器线程；
5. 调度器线程之前也调用了switch函数，现在恢复执行会从自己的switch函数返回；
6. 返回之后，调度器线程会release释放刚刚出让了CPU的进程的锁

![img](https://pic3.zhimg.com/80/v2-b295bd6e4562ba5fda1cbd48cd89cc8e_1440w.webp)

​	**上面最需要关注的就是第一步acquire上锁**。<u>如果没有上锁，直接改变进程状态为RUNNABLE，那么其他CPU核的调度器线程很可能正好遍历进程表单发现这个刚转为RUNNABLE的进程，进而出现当前CPU核、其他CPU核都运行这个进程的现象，这会使系统崩溃。所以，在进程切换的最开始，进程先获取自己的锁，并且直到调用switch函数时也不释放锁。而另一个线程，也就是调度器线程会在进程的线程完全停止使用自己的栈之后，再释放进程的锁。释放锁之后，就可以由其他的CPU核再来运行进程的线程，因为这些线程现在已经不在运行了</u>。**简言之，需要保证切换进程的整个过程的原子性，这个过程需要锁保护，直到进程切换完毕后，才释放锁**。

​	另外，XV6的switch有个锁使用的重点限制，**XV6在进程切换过程中，要求当前进程有且仅持有进程锁`p->lock`**(proc进程结构体中的lock锁对象)。<u>其他诸如Sleep的实现也有类似的要求</u>。

​	下面通过反例解释，**为什么XV6进程切换过程中，进程必须有且仅持有`p->lock`进程锁**。

1. 假设P1进程之前调用了UART或者console等需要与硬件交互的函数，于是持有lock1；
2. P1由于等待硬件反馈，准备主动yield出让CPU资源，于是先acquire了`p->lock`，然后调用switch
3. 线程切换到CPU0的调度线程，然后正常switch流程，切换到P2，释放P1的`p->lock`
4. 切换到P2后，由于P2也想调用UART或者console等与硬件交互的函数，于是申请P1之前占用的lock1，调用acquire函数
5. 由于P1已经占有lock1并且switch之前没有释放锁，而且acquire函数实现中第一步就是关闭中断，所以现在P2在while死循环里无限申请lock1，又无法被中断
6. CPU0死锁，资源被P2一直占用无法让出....

​	*这里之所以P2没机会获取P1占有的锁，是因为P2对于锁的acquire调用在直到锁释放之前都不会返回，而唯一锁能被释放的方式就是进程P1恢复执行并在稍后release锁，但是这一步又还没有发生，因为进程P1通过调用switch函数切换到了P2，而P2又在不停的“旋转”并等待锁被释放。这是一种死锁，它会导致系统停止运行*。

​	虽然上面描述是基于机器上只有一个CPU核，但是你可以通过多个锁在多个CPU核的机器上构建类似的死锁场景。所以，**在XV6中禁止在调用switch时持有除进程自身锁（注，也就是p->lock）以外的其他锁**。

​	将这里描述的对于锁的两个限制条件记住，因为我们后面讨论Sleep&Wakeup如何工作时会再次使用它们。

---

问题：当我们有多个CPU核时，它们能看到同样的锁对象的唯一原因只可能是它们有一个共享的物理内存系统，对吧？

回答：是的。如果你有两个电脑，那么它们不会共享内存，并且我们就不会有这些问题。现在的处理器上，总是有多个CPU核，它们共享了相同的内存系统。

问题：难道定时器中断不会将CPU控制切换回进程P1从而解决死锁的问题吗？

回答：**首先，所有的进程切换过程都发生在内核中，所有的acquire，switch，release都发生在内核代码而不是用户代码**。实际上XV6允许在执行内核代码时触发中断，如果你查看`trap.c`中的代码你可以发现，**如果XV6正在执行内核代码时发生了定时器中断，中断处理程序会调用yield函数并出让CPU**。但是在之前的课程中我们讲过**acquire函数在等待锁之前会关闭中断**，否则的话可能会引起死锁（比如响应中断的时候，中断处理程序又尝试获取某个已被其他进程占用的锁），所以我们不能在等待锁的时候处理中断。**如果你查看XV6中的acquire函数，你可以发现函数中第一件事情就是关闭中断，之后再“自旋”等待锁释放**。你或许会想，为什么不能先“自旋”等待锁释放，再关闭中断？因为这样会有一个短暂的时间段锁被持有了但是中断没有关闭，在这个时间段内的设备的中断处理程序可能会引起死锁。所以不幸的是，当我们在自旋等待锁释放时会关闭中断，进而阻止了定时器中断并且阻止了进程P2将CPU出让回给进程P1。嗯，这是个好问题。

问题：能重复一下XV6进程切换中，死锁是如何避免的吗？

回答：在XV6中，**死锁是通过禁止在线程切换的时候加锁（这里指除了进程锁以外不持有其他锁）来避免的**。**XV6禁止在调用switch函数时，获取除了`p->lock`以外的其他锁**。如果你查看sched函数的代码里面包含了一些检查代码来确保除了`p->lock`以外线程不持有其他锁。所以上面会产生死锁的代码在XV6中是不合法的并被禁止的。

## 13.2 Sleep&WakeUp

> [13.2 Sleep&Wakeup 接口 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349778459) <= 图文出处

​	当你在写一个线程的代码时，有些场景需要等待一些特定的事件，或者不同的线程之间需要交互：

- 假设我们有一个Pipe，并且我正在从Pipe中读数据。但是Pipe当前又没有数据，所以我需要等待一个Pipe非空的事件（即读操作会阻塞到有数据写入Pipe为止）。
- 类似的，假设我在读取磁盘，我会告诉磁盘控制器请读取磁盘上的特定块。这或许要花费较长的时间，尤其当磁碟需要旋转时（通常是毫秒级别），磁盘才能完成读取。而执行读磁盘的进程需要等待读磁盘结束的事件。
- 类似的，一个Unix进程可以调用wait函数。这个会使得调用进程等待任何一个子进程退出。所以这里父进程有意在等待另一个进程产生的事件。

​	以上就是进程需要等待特定事件的一些例子。特定事件可能来自于I/O，也可能来自于另一个进程，并且它描述了某件事情已经发生。<u>Coordination是帮助我们解决这些问题并帮助我们实现这些需求的工具。Coordination是非常基础的工具，就像锁一样，在实现线程代码时它会一直出现</u>。

​	我们怎么能让进程或者线程等待一些特定的事件呢？一种非常直观的方法是通过循环实现busy-wait。假设我们想从一个Pipe读取数据，我们就写一个循环一直等待Pipe的buffer不为空。实际中会有这样的代码。**如果你知道你要等待的事件极有可能在0.1微秒内发生，通过循环等待或许是最好的实现方式**。通常来说在操作设备硬件的代码中会采用这样的等待方式，如果你要求一个硬件完成一个任务，并且你知道硬件总是能非常快的完成任务，这时通过一个类似的循环等待或许是最正确的方式。另一方面，事件可能需要数个毫秒甚至你都不知道事件要多久才能发生，或许要10分钟其他的进程才能向Pipe写入数据，那么我们就不想在这一直循环并且浪费本可以用来完成其他任务的CPU时间。这时我们想要通过类似switch函数调用的方式出让CPU，并在我们关心的事件发生时重新获取CPU。**Coordination就是有关出让CPU，直到等待的事件发生再恢复执行。人们发明了很多不同的Coordination的实现方式，但是与许多Unix风格操作系统一样，XV6使用的是Sleep&Wakeup这种方式**。

​	下面通过教师编写的UART驱动代码，看看sleep&wakeup在程序中的应用：

```c
// uart.c

// transmit buf[].
void uartwrite(char buf[], int n)
{
  acquire(&uart_tx_lock);

  int i = 0;
  while(i < n){
    while(tx_done == 0){
      // UART is busy sending a character.
      // wait for it to interrupt.
      sleep(&tx_chan, &uart_tx_lock);
    }
    WriteReg(THR, buf[i]);
    i += 1;
    tx_done = 0;
  }

  release(&uart_tx_lock);
}

// ...

// handle a uart interrupt, raised because input has
// arrived, or the uart is ready for more output, or
// both. called from trap.c.
void uartintr(void)
{
  acquire(&uart_tx_lock);
  if(ReadReg(LSR) & LSR_TX_IDLE) {
    // UART finished transmitting; wake up any sending thread.
    tx_done = 1;
    wakeup(&tx_chan);
  }
  release(&uart_tx_lock);
  
 	// read and process incoming characters.
  while(1) {
    int c = uargetc();
    // ....
  }
  // ... 
}
```

​	首先是uartwrite函数。当shell需要输出时会调用write系统调用最终走到uartwrite函数中，这个函数会在循环中将buf中的字符一个一个的向UART硬件写入。这是一种经典的设备驱动实现风格，你可以在很多设备驱动中看到类似的代码。UART硬件一次只能接受一个字符的传输，而通常来说会有很多字符需要写到UART硬件。你可以向UART硬件写入一个字符，并等待UART硬件说：好的我完成了传输上一个字符并且准备好了传输下一个字符，之后驱动程序才可以写入下一个字符。因为这里的硬件可能会非常慢，或许每秒只能传输1000个字符，所以我们在两个字符之间的等待时间可能会很长。而1毫秒在现在计算机上是一个非常非常长的时间，它可能包含了数百万条指令时间，所以我们不想通过循环来等待UART完成字符传输，我们想通过一个更好的方式来等待。如大多数操作系统一样，XV6也的确存在更好的等待方式。

​	<u>UART硬件会在完成传输一个字符后，触发一个中断</u>。所以UART驱动中除了uartwrite函数外，还有名为uartintr的中断处理程序。这个中断处理程序会在UART硬件触发中断时由`trap.c`代码调用。<u>中断处理程序会在最开始读取UART对应的memory mapped register，并检查其中表明传输完成的相应的标志位，也就是LSR_TX_IDLE标志位。如果这个标志位为1，代码会将tx_done设置为1，并调用wakeup函数。这个函数会使得uartwrite中的sleep函数恢复执行，并尝试发送一个新的字符</u>。

​	所以这里的机制是，**如果一个线程需要等待某些事件，比如说等待UART硬件愿意接收一个新的字符，线程调用sleep函数并等待一个特定的条件。当特定的条件满足时，代码会调用wakeup函数。这里的sleep函数和wakeup函数是成对出现的**。我们之后会看sleep函数的具体实现，它会做很多事情最后再调用switch函数来出让CPU。

​	**这里有件事情需要注意，sleep和wakeup函数需要通过某种方式链接到一起。也就是说，如果我们调用wakeup函数，我们只想唤醒正在等待刚刚发生的<u>特定事件</u>的线程**。所以，sleep函数和wakeup函数都带有一个叫做sleep channel的参数。我们在调用wakeup的时候，需要传入与调用sleep函数相同的sleep channel。不过sleep和wakeup函数只是接收表示了sleep channel的64bit数值，它们并不关心这个数值代表什么。**当我们调用sleep函数时，我们通过一个sleep channel表明我们等待的特定事件，当调用wakeup时我们希望能传入相同的数值来表明想唤醒哪个线程**。

​	以上就是接口的演示。<u>Sleep&wakeup的一个优点是它们可以很灵活，它们不关心代码正在执行什么操作，你不用告诉sleep函数你在等待什么事件，你也不用告诉wakeup函数发生了什么事件，你只需要匹配好64bit的sleep channel就行</u>。

​	不过，对于sleep函数，有一个有趣的参数，我们需要将一个锁作为第二个参数传入，这背后是一个大的故事，我后面会介绍背后的原因。**总的来说，不太可能设计一个sleep函数并完全忽略需要等待的事件**。所以很难写一个通用的sleep函数，只是睡眠并等待一些特定的事件，并且这也很危险，因为可能会导致lost wakeup，而**几乎所有的Coordination机制都需要处理lost wakeup的问题**。在sleep接口中，我们需要传入一个锁是一种稍微丑陋的实现，我在稍后会再介绍。

---

问题：进程会在写入每个字符时候都被唤醒一次吗？

回答：在这个我出于演示目的而特别改过的UART驱动中，传输每个字符都会有一个中断，所以你是对的，对于buffer中的每个字符，我们都会等待UART可以接收下一个字符，之后写入一个字符，将tx_done设置为0，回到循环的最开始并再次调用sleep函数进行睡眠状态，直到tx_done为1。当UART传输完了这个字符，uartintr函数会将tx_done设置为1，并唤醒uartwrite所在的线程。所以对于每个字符都有调用一次sleep和wakeup，并占用一次循环。<u>UART实际上支持一次传输4或者16个字符，所以一个更有效的驱动会在每一次循环都传输16个字符给UART，并且中断也是每16个字符触发一次</u>。**更高速的设备，例如以太网卡通常会更多个字节触发一次中断**。

## 13.3 Lost Wakeup

> [13.3 Lost wakeup - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349778792) <= 图文出处

​	在解释sleep函数为什么需要一个锁使用作为参数传入之前，我们先来看看假设我们有了一个更简单的不带锁作为参数的sleep函数，会有什么样的结果。这里的结果就是**lost wakeup**。

​	假设sleep只是接收任意的sleep channel作为唯一的参数。它其实不能正常工作，我们称这个sleep实现为broken_sleep。你可以想象一个sleep函数内会将进程的状态设置为SLEEPING，表明当前进程不想再运行，而是正在等待一个特定的事件。**如果你们看过了XV6的实现，你们可以发现sleep函数中还会做很多其他操作。我们需要记录特定的sleep channel值，这样之后的wakeup函数才能发现是当前进程正在等待wakeup对应的事件。最后再调用switch函数出让CPU**。

![img](https://pic4.zhimg.com/80/v2-835acf240d6e83b90f6a1a2e5811d357_1440w.webp)

​	如果sleep函数只做了这些操作，那么很明显sleep函数会出问题，我们至少还应该在这里获取进程的锁。

​	之后是wakeup函数。**我们希望唤醒所有正在等待特定sleep channel的线程。所以wakeup函数中会查询进程表单中的所有进程，如果进程的状态是SLEEPING并且进程对应的channel是当前wakeup的参数，那么将进程的状态设置为RUNNABLE**。

![img](https://pic3.zhimg.com/80/v2-bd4c79c2ea605b3ab9657b84732caef2_1440w.webp)

​	在一些平行宇宙中，sleep&wakeup或许就是这么简单。在我回到XV6代码之前，让我演示一下如何在UART驱动中使用刚刚介绍的sleep和wakeup函数。这基本上是重复前一节的内容，不过这次我们使用刚刚介绍的稍微简单的接口。

​	首先是定义done标志位。之后是定义uartwrite函数。在函数中，对于buffer内的每一个字符，检查done标志位，如果标志位为0，就调用sleep函数并传入`tx_channel`。之后将字符传递给UART并将done设置为0。

![img](https://pic3.zhimg.com/80/v2-201bea1e993a45403e44f9c0b2ad868e_1440w.webp)

​	之后是中断处理函数uartintr。函数中首先将done标志位设置为1，并调用wakeup。

![img](https://pic4.zhimg.com/80/v2-fed51eb1a190b165f7d3daf3c04d2467_1440w.webp)

​	以上就是使用broken_sleep的方式。这里缺失的是锁。这里uartwrite和uartintr两个函数需要使用锁来协调工作。

- 第一个原因是done标志位，**任何时候我们有了共享的数据，我们需要为这个数据加上锁**。
- 另一个原因是两个函数都需要访问UART硬件，通常来说让两个线程并发的访问memory mapped register是错误的行为。

​	所以我们需要在两个函数中加锁来避免对于done标志位和硬件的竞争访问。

​	现在的问题是，我们该在哪个位置加锁？在中断处理程序中较为简单，我们在最开始加锁，在最后解锁。

![img](https://pic4.zhimg.com/80/v2-c9a90f06431f12f6bf8f1ac01efd03f7_1440w.webp)

​	难的是如何在uartwrite函数中加锁。一种可能是，每次发送一个字符的过程中持有锁，所以在每一次遍历buffer的起始和结束位置加锁和解锁。

![img](https://pic2.zhimg.com/80/v2-cae8ebd13c891a8794825d21c526f9c1_1440w.webp)

​	<u>为什么这样肯定不能工作？一个原因是，我们能从while not done的循环退出的唯一可能是中断处理程序将done设置为1。但是如果我们为整个代码段都加锁的话，中断处理程序就不能获取锁了，中断程序会不停“自旋”并等待锁释放。而锁被uartwrite持有，在done设置为1之前不会释放。而done只有在中断处理程序获取锁之后才可能设置为1。所以我们不能在发送每个字符的整个处理流程都加锁</u>。

​	上面加锁方式的问题是，uartwrite在期望中断处理程序执行的同时又持有了锁。而我们唯一期望中断处理程序执行的位置就是sleep函数执行期间，其他的时候uartwrite持有锁是没有问题的。<u>所以另一种实现可能是，在传输字符的最开始获取锁，因为我们需要保护共享变量done，但是在调用sleep函数之前释放锁。这样中断处理程序就有可能运行并且设置done标志位为1。之后在sleep函数返回时，再次获取锁</u>。

![img](https://pic3.zhimg.com/80/v2-eb590f50ec2f4338d563cf9beede167a_1440w.webp)

​	现有的代码中，uartwrite在最开始获取了锁，并在最后释放了锁。

![img](https://pic1.zhimg.com/80/v2-677e5d0dc27de07b1eae00033385f658_1440w.webp)

​	现有的中断处理程序也在最开始获取锁，之后释放锁。

![img](https://pic1.zhimg.com/80/v2-ccbfa309cd4a0545cf8b2949f32a4840_1440w.webp)

​	接下来，我们会探索为什么只接收一个参数的`broken_sleep`在这不能工作。为了让锁能正常工作，我们需要在调用`broken_sleep`函数之前释放`uart_tx_lock`，并在`broken_sleep`返回时重新获取锁。`broken_sleep`内的代码与之前在白板上演示的是一样的。也就是首先将进程状态设置为SLEEPING，并且保存`tx_chan`到进程结构体中，最后调用switch函数。

![img](https://pic1.zhimg.com/80/v2-0a862f68b88b3d810a990c47b638414c_1440w.webp)

​	接下来编译代码并看一下会发生什么。

![img](https://pic1.zhimg.com/80/v2-689c31763a483e6bdcf97846d39ed930_1440w.webp)

​	在XV6启动的时候会打印“init starting”，这里看来输出了一些字符之后就hang住了。如果我输入任意字符，剩下的字符就能输出。这里发生了什么？

​	这里的问题必然与之前修改的代码相关。**在前面的代码中，sleep之前释放了锁，但是在释放锁和`broken_sleep`之间可能会发生中断**。

![img](https://pic3.zhimg.com/80/v2-00c54bd4575cc34d9e95f00470b78b12_1440w.webp)

​	一旦释放了锁，当前CPU的中断会被重新打开。因为这是一个多核机器，所以中断可能发生在任意一个CPU核。在上面代码标记的位置，其他CPU核上正在执行UART的中断处理程序，并且正在acquire函数中等待当前锁释放。所以一旦锁被释放了，另一个CPU核就会获取锁，并发现UART硬件完成了发送上一个字符，之后会设置tx_done为1，最后再调用wakeup函数，并传入tx_chan。**目前为止一切都还好，除了一点：现在写线程还在执行并位于release和broken_sleep之间，也就是写线程还没有进入SLEEPING状态，所以中断处理程序中的wakeup并没有唤醒任何进程，因为还没有任何进程在tx_chan上睡眠**。之后写线程会继续运行，调用broken_sleep，将进程状态设置为SLEEPING，保存sleep channel。但是中断已经发生了，wakeup也已经被调用了。**所以这次的broken_sleep，没有人会唤醒它，因为wakeup已经发生过了。这就是lost wakeup问题**。

---

问题：是不是总是这样，一旦一个wakeup被丢失了，下一次wakeup时，之前缓存的数据会继续输出？

回答：这完全取决于实现细节。在我们的例子中，实际上出于偶然才会出现当我输入某些内容会导致之前的输出继续的现象。**这里背后的原因是，我们的代码中，UART只有一个中断处理程序。不论是有输入，还是完成了一次输出，都会调用到同一个中断处理程序中**。所以当我输入某些内容时，会触发输入中断，之后会调用uartintr函数。然后在中断处理程序中又会判断LSR_TX_IDLE标志位，并再次调用wakeup，所以刚刚的现象完全是偶然。如果出现了lost wakeup问题，并且你足够幸运的话，某些时候它们能自动修复。**如果UART有不同的接收和发送中断处理程序的话，那么就没办法从lost wakeup恢复**。

问题：tx_done标志位的作用是什么？

回答：这是一种简单的在uartintr和uartwrite函数之间通信的方法。tx_done标志位为1表示已经完成了对于前一个字符的传输，并且uartwrite可以传输下一个字符，所以这是用来在中断处理程序和uartwrite之间通信的标志位。

问题：当从sleep函数中唤醒时，不是已经知道是来自UART的中断处理程序调用wakeup的结果吗？这样的话tx_done有些多余。

回答：我想你的问题也可以描述为：**为什么需要通过一个循环while(tx_done == 0)来调用sleep函数？这个问题的答案适用于一个更通用的场景：实际中不太可能将sleep和wakeup精确匹配。并不是说sleep函数返回了，你等待的事件就一定会发生**。举个例子，假设我们有两个进程同时想写UART，它们都在uartwrite函数中。可能发生这种场景，当一个进程写完一个字符之后，会进入SLEEPING状态并释放锁，而另一个进程可以在这时进入到循环并等待UART空闲下来。之后两个进程都进入到SLEEPING状态，当发生中断时UART可以再次接收一个字符，两个进程都会被唤醒，但是只有一个进程应该写入字符，所以我们才需要在sleep外面包一层while循环。**实际上，你可以在XV6中的每一个sleep函数调用都被一个while循环包着。因为事实是，你或许被唤醒了，但是其他人将你等待的事件拿走了，所以你还得继续sleep。这种现象还挺普遍的**。

问题：我们只看到了一个lost wakeup，当我们随便输入一个字符，整个剩下的字符都能输出，为什么没有在输出剩下字符的时候再次发生lost wakeup？

回答：这会发生的。我来敲一下cat README，这会输出数千个字符。可以看到每过几个字符就会hang一次，需要我再次输入某个字符。这个过程我们可以看到很多lost wakeup。<u>之前之所以没有出现，是因为lost wakeup需要中断已经在等待获取锁，并且uartwrite位于release和broken_sleep之间，这需要一定的巧合并不总是会发生</u>。

## 13.4 sleep和wakeup代码实现

> [13.4 如何避免Lost wakeup - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349779082) <= 图文出处

​	现在我们的目标是消灭掉lost wakeup。这可以通过消除下面的窗口时间来实现。

![img](https://pic3.zhimg.com/80/v2-00c54bd4575cc34d9e95f00470b78b12_1440w.webp)

​	首先我们必须要释放uart_tx_lock锁，因为中断需要获取这个锁，但是我们又不能在释放锁和进程将自己标记为SLEEPING之间留有窗口。这样中断处理程序中的wakeup才能看到SLEEPING状态的进程，并将其唤醒，进而我们才可以避免lost wakeup的问题。所以，我们应该消除这里的窗口。

​	为了实现这个目的，我们需要将sleep函数设计的稍微复杂点。**这里的解决方法是，即使sleep函数不需要知道你在等待什么事件，它还是需要你知道你在等待什么数据，并且传入一个用来保护你在等待数据的锁**。sleep函数需要特定的条件才能执行，而sleep自己又不需要知道这个条件是什么。在我们的例子中，sleep函数执行的特定条件是tx_done等于1。虽然sleep不需要知道tx_done，但是它需要知道保护这个条件的锁，也就是这里的uart_tx_lock。在调用sleep的时候，锁还被当前线程持有，之后这个锁被传递给了sleep。

​	**在接口层面，sleep承诺可以原子性的将进程设置成SLEEPING状态，同时释放锁。这样wakeup就不可能看到这样的场景：锁被释放了但是进程还没有进入到SLEEPING状态**。所以sleep这里将释放锁和设置进程为SLEEPING状态这两个行为合并为一个原子操作。

​	所以我们需要有一个锁来保护sleep的条件，并且这个锁需要传递给sleep作为参数。更进一步的是，当调用wakeup时，锁必须被持有。如果程序员想要写出正确的代码，都必须遵守这些规则来使用sleep和wakeup。

​	接下来我们看一下sleep和wakeup如何使用这一小块额外的信息（注，也就是传入给sleep函数的锁）和刚刚提到的规则，来避免lost wakeup。

​	首先我们来看一下proc.c中的wakeup函数。

```c
// proc.c
// Wake up all processes sleeping on chan.
// Must be called without any p->lock.
void wakeup(void *chan)
{
	struct proc *p;
  
  for(p = proc; p < &p[NPROC]; p++) {
		acquire(&p->lock);
    if(p->state == SLEEPING && p->chan == chan) {
      p->state = RUNNABLE;
    }
    release(&p->lock);
  }
}
```

​	**wakeup函数并不十分出人意料。它查看整个进程表单，对于每个进程首先加锁，这点很重要。之后查看进程的状态，如果进程当前是SLEEPING并且进程的channel与wakeup传入的channel相同，将进程的状态设置为RUNNABLE。最后再释放进程的锁**。

​	接下来我们忽略broken_sleep，直接查看带有锁作为参数的sleep函数。

```c
// Atomically release lock and sleep on chan.
// Reacquires lock when awakened.
void sleep(void *chan, struct spinlock *lk)
{
  struct proc *p = myproc();
  
  // Must acquire p->lock in order to
  // change p->state and then call sched.
  // Once we hold p->lock, we can be
  // guaranteed that we won't miss any wakeup
  // (wakeup locks p->lock),
  // so it's okay to release lk.
  if(lk != &p->lock){
		acquire(&p->lock);
    release(lk);
  }
  
  // Go to sleep.
  p->chan = chan;
  p->state = SLEEPING;
  
  sched();
  
  // Tidy up.
  p->chan = 0;
  
  // Reacquire original lock.
  if(lk != &p->lock) {
    release(&p->lock);
    acquire(lk);
  }
}
```

​	我们已经知道了sleep函数需要释放作为第二个参数传入的锁，这样中断处理程序才能获取锁。函数中第一件事情就是释放这个锁。当然在释放锁之后，我们会担心在这个时间点相应的wakeup会被调用并尝试唤醒当前进程，而当前进程还没有进入到SLEEPING状态。**所以我们不能让wakeup在release锁之后执行。为了让它不在release锁之后执行，在release锁之前，sleep会获取即将进入SLEEPING状态的进程的锁**。

​	如果你还记得的话，**wakeup在唤醒一个进程前，需要先获取进程的锁。所以在整个时间uartwrite检查条件之前到sleep函数中调用sched函数之间，这个线程一直持有了保护sleep条件的锁或者p->lock**。让我回到UART的代码并强调一下这一点。

![img](https://pic4.zhimg.com/80/v2-ab00224564b64984ff8b5a846ba99153_1440w.webp)

​	uartwrite在最开始获取了sleep的condition lock，并且一直持有condition lock知道调用sleep函数。所以它首先获取了condition lock，之后检查condition（注，也就是tx_done等于0），之后在持有condition lock的前提下调用了sleep函数。此时wakeup不能做任何事情，wakeup现在甚至都不能被调用直到调用者能持有condition lock。所以现在wakeup必然还没有执行。

​	sleep函数在释放condition lock之前，先获取了进程的锁。**在释放了condition lock之后，wakeup就可以被调用了，但是除非wakeup获取了进程的锁，否则wakeup不能查看进程的状态。所以，在sleep函数中释放了condition lock之后，wakeup也还没有执行**。

​	在持有进程锁的时候，将进程的状态设置为SLEEPING并记录sleep channel，之后再调用sched函数，这个函数中会再调用switch函数（注，详见11.6），此时sleep函数中仍然持有了进程的锁，wakeup仍然不能做任何事情。

​	如果你还记得的话，当我们从当前线程切换走时，调度器线程中会释放前一个进程的锁（注，详见11.8）。所以**在调度器线程释放进程锁之后，wakeup才能终于获取进程的锁，发现它正在SLEEPING状态，并唤醒它。**

​	这里的效果是由之前定义的一些规则确保的，这些规则包括了：

- 调用sleep时需要持有condition lock，这样sleep函数才能知道相应的锁。
- **sleep函数只有在获取到进程的锁`p->lock`之后，才能释放condition lock**。
- wakeup需要同时持有两个锁才能查看进程。（对wakeup本身来说主要是进程锁影响；而条件锁被占用时，uartintr中断处理函数由于抢不到锁，所以都压根执行不到调用wakeup函数的代码位置）

这样的话，我们就不会再丢失任何一个wakeup，也就是说我们修复了lost wakeup的问题。

## 13.5 pipe中sleep&wakeup的使用

> [13.5 Pipe中的sleep 和wakeup - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349779372) <= 图文出处

​	前面我们介绍了在UART的驱动中，如何使用sleep和wakeup才能避免lost wakeup(13.4)。前面这个特定的场景中，sleep等待的condition是发生了中断并且硬件准备好了传输下一个字符。**在一些其他场景，内核代码会调用sleep函数并等待其他的线程完成某些事情**。这些场景从概念上来说与我们介绍之前的场景没有什么区别，但是感觉上还是有些差异。例如，在读写pipe的代码中，查看`pipe.c`中的piperead函数。

![img](https://pic1.zhimg.com/80/v2-cc4b1dabbb9ebb99e6a0f90cb9821768_1440w.webp)

​	这里有很多无关的代码可以忽略。当read系统调用最终调用到piperead函数时，pi->lock会用来保护pipe，这就是sleep函数对应的condition lock。piperead需要等待的condition是pipe中有数据，而这个condition就是pi->nwrite大于pi->nread，也就是写入pipe的字节数大于被读取的字节数。如果这个condition不满足，那么piperead会调用sleep函数，并等待condition发生。同时piperead会将condition lock也就是pi->lock作为参数传递给sleep函数，以确保不会发生lost wakeup。

​	之所以会出现lost wakeup，是因为在一个不同的CPU核上可能有另一个线程刚刚调用了pipewrite。

![img](https://pic4.zhimg.com/80/v2-d20c3df66112406a2b51f20f52f7895b_1440w.webp)

​	pipewrite会向pipe的缓存写数据，并最后在piperead所等待的sleep channel上调用wakeup。而我们想要避免这样的风险：在piperead函数检查发现没有字节可以读取，到piperead函数调用sleep函数之间，另一个CPU调用了pipewrite函数。因为这样的话，另一个CPU会向pipe写入数据并在piperead进程进入SLEEPING之前调用wakeup，进而产生一次lost wakeup。

​	在pipe的代码中，pipewrite和piperead都将sleep包装在一个while循环中。piperead中的循环等待pipe的缓存为非空（pipewrite中的循环等待的是pipe的缓存不为full）。之所以要将sleep包装在一个循环中，是因为可能有多个进程在读取同一个pipe。如果一个进程向pipe中写入了一个字节，这个进程会调用wakeup进而同时唤醒所有在读取同一个pipe的进程。但是因为pipe中只有一个字节并且总是有一个进程能够先被唤醒，哦，这正好提醒了我有关sleep我忘记了一些非常关键的事情。**sleep函数中最后一件事情就是重新获取condition lock。所以调用sleep函数的时候，需要对condition lock上锁**（注，在sleep函数内部会对condition lock解锁），**在sleep函数返回时会重新对condition lock上锁。这样第一个被唤醒的线程会持有condition lock，而其他的线程在重新对condition lock上锁的时候会在锁的acquire函数中等待**。

​	那个幸运的进程（注，这里线程和进程描述的有些乱，但是基本意思是一样的，当说到线程时是指进程唯一的内核线程）会从sleep函数中返回，之后通过检查可以发现pi->nwrite比pi->nread大1，所以进程可以从piperead的循环中退出，并读取一个字节，之后pipe缓存中就没有数据了。之后piperead函数释放锁并返回。接下来，第二个被唤醒的线程，它的sleep函数可以获取condition lock并返回，但是通过检查发现pi->nwrite等于pi->nread（注，因为唯一的字节已经被前一个进程读走了），所以这个线程以及其他所有的等待线程都会重新进入sleep函数。所以这里也可以看出，**几乎所有对于sleep的调用都需要包装在一个循环中，这样从sleep中返回的时候才能够重新检查condition是否还符合**。

​	<u>sleep和wakeup的规则稍微有点复杂。因为你需要向sleep展示你正在等待什么数据，你需要传入锁并遵循一些规则，某些时候这些规则还挺烦人的。另一方面sleep和wakeup又足够灵活，因为它们并不需要理解对应的condition，只是需要有个condition和保护这个condition的锁</u>。

​	除了sleep&wakeup之外，还有一些其他的更高级的Coordination实现方式。例如今天课程的阅读材料中的semaphore(信号量)，它的接口就没有那么复杂，你不用告诉semaphore有关锁的信息。而semaphore的调用者也不需要担心lost wakeup的问题，在semaphore的内部实现中考虑了lost wakeup问题。因为定制了up-down计数器，所以semaphore可以在不向接口泄露数据的同时（注，也就是不需要向接口传递condition lock），处理lost wakeup问题。**semaphore某种程度来说更简单，尽管它也没那么通用，如果你不是在等待一个计数器，semaphore也就没有那么有用了。这也就是为什么我说sleep和wakeup更通用的原因**。

## 13.6 exit系统调用

> [13.6 exit系统调用 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349779579) <= 图文出处

​	接下来，我想讨论一下XV6面临的一个与Sleep&Wakeup相关的挑战，也就是**如何关闭一个进程**。每个进程最终都需要退出，我们需要清除进程的状态，释放栈。**在XV6中，一个进程如果退出的话，我们需要释放用户内存，释放page table，释放trapframe对象，将进程在进程表单中标为REUSABLE，这些都是典型的清理步骤**。当进程退出或者被杀掉时，有许多东西都需要被释放。

​	这里会产生的两大问题：

- 首先我们不能直接单方面的摧毁另一个线程，因为：另一个线程可能正在另一个CPU核上运行，并使用着自己的栈；也可能另一个线程正在内核中持有了锁；也可能另一个线程正在更新一个复杂的内核数据，如果我们直接就把线程杀掉了，我们可能在线程完成更新复杂的内核数据过程中就把线程杀掉了。我们不能让这里的任何一件事情发生。
- 另一个问题是，即使一个线程调用了exit系统调用，并且是自己决定要退出。它仍然持有了运行代码所需要的一些资源，例如它的栈，以及它在进程表单中的位置。当它还在执行代码，它就不能释放正在使用的资源。所以我们需要一种方法让线程能释放最后几个对于运行代码来说关键的资源。

​	记住这两个问题。

​	**XV6有两个函数与关闭线程进程相关。第一个是exit，第二个是kill**。让我们先来看位于`proc.c`中的exit函数。

![img](https://pic4.zhimg.com/80/v2-4f6d6fedfbca60e3049e32e5524824a7_1440w.webp)

​	这就是exit系统调用的内容。**从exit接口的整体来看，在最后它会释放进程的内存和page table，关闭已经打开的文件，同时我们也知道父进程会从wait系统调用中唤醒，所以exit最终会导致父进程被唤醒**。这些都是我们预期可以从exit代码中看到的内容。

​	从上面的代码中，首先exit函数关闭了所有已打开的文件。这里可能会很复杂，因为关闭文件系统中的文件涉及到引用计数，虽然我们还没学到但是这里需要大量的工作。不管怎样，**一个进程调用exit系统调用时，会关闭所有自己拥有的文件**。

​	接下来是类似的处理，进程有一个对于当前目录的记录，这个记录会随着你执行cd指令而改变。在exit过程中也需要将对这个目录的引用释放给文件系统。

​	**<u>如果一个进程要退出，但是它又有自己的子进程，接下来需要设置这些子进程的父进程为init进程。我们接下来会看到，每一个正在exit的进程，都有一个父进程中的对应的wait系统调用。父进程中的wait系统调用会完成进程退出最后的几个步骤。所以如果父进程退出了，那么子进程就不再有父进程，当它们要退出时就没有对应的父进程的wait。所以在exit函数中，会为即将exit进程的子进程重新指定父进程为init进程，也就是PID为1的进程</u>**。

![img](https://pic1.zhimg.com/80/v2-206c1de963a6923d60717bcb889b9890_1440w.webp)

​	之后，我们需要通过调用wakeup函数唤醒当前进程的父进程，当前进程的父进程或许正在等待当前进程退出。

​	接下来，**进程的状态被设置为ZOMBIE**。<u>现在进程还没有完全释放它的资源，所以它还不能被重用。所谓的进程重用是指，我们期望在最后，进程的所有状态都可以被一些其他无关的fork系统调用复用，但是目前我们还没有到那一步</u>。

​	现在我们还没有结束，因为我们还没有释放进程资源。我们在还没有完全释放所有资源的时候，通过调用sched函数进入到调度器线程。

​	<u>**到目前为止，进程的状态是ZOMBIE，并且进程不会再运行，因为调度器只会运行RUNNABLE进程。同时进程资源也并没有完全释放，如果释放了进程的状态应该是UNUSED**。但是可以肯定的是进程不会再运行了，因为它的状态是ZOMBIE。所以调度器线程会决定运行其他的进程</u>。

## 13.7 wait系统调用

> [13.7 wait系统调用 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349779902) <= 图文出处

​	通过Unix的exit和wait系统调用的说明，我们可以知道如果一个进程exit了，并且它的父进程调用了wait系统调用，父进程的wait会返回。wait函数的返回表明当前进程的一个子进程退出了。所以接下来我们看一下wait系统调用的实现。

![img](https://pic2.zhimg.com/80/v2-57c2c7e0d2763f71920a3c7d27856ea1_1440w.webp)

​	**它里面包含了一个大的循环。当一个进程调用了wait系统调用，它会扫描进程表单，找到父进程是自己且状态是ZOMBIE的进程**。<u>**从上一节可以知道，这些进程已经在exit函数中几乎要执行完了。之后由父进程调用的freeproc函数，来完成释放进程资源的最后几个步骤**</u>。我们看一下freeproc的实现。

![img](https://pic3.zhimg.com/80/v2-63b07c0e3d4542f6071fad46d8bb3eb6_1440w.webp)

​	这是关闭一个进程的最后一些步骤。如果由正在退出的进程自己在exit函数中执行这些步骤，将会非常奇怪。**<u>这里释放了trapframe，释放了page table。如果我们需要释放进程内核栈，那么也应该在这里释放。但是因为内核栈的guard page，我们没有必要再释放一次内核栈。不管怎样，当进程还在exit函数中运行时，任何这些资源在exit函数中释放都会很难受，所以这些资源都是由父进程释放的</u>**。

​	wait不仅是为了父进程方便的知道子进程退出，wait实际上也是进程退出的一个重要组成部分。**<u>在Unix中，对于每一个退出的进程，都需要有一个对应的wait系统调用，这就是为什么当一个进程退出时，它的子进程需要变成init进程的子进程</u>**。**<u>init进程的工作就是在一个循环中不停调用wait，因为每个进程都需要对应一个wait，这样它的父进程才能调用freeproc函数，并清理进程的资源</u>**。
​	**当父进程完成了清理进程的所有资源，子进程的状态会被设置成UNUSED。之后，fork系统调用才能重用进程在进程表单的位置**。

​	**这里我想要强调的是，直到子进程exit的最后，它都没有释放所有的资源，因为它还在运行的过程中，所以不能释放这些资源。相应的其他的进程，也就是父进程，通过wait释放了运行子进程代码所需要的资源。这样的设计可以让我们极大的精简exit的实现。**

---

**问题：在exit系统调用中，为什么需要在重新设置父进程之前，先获取当前进程的父进程？**

**回答：这里其实就是在防止一个进程和它的父进程同时退出。通常情况下，一个进程exit，它的父进程正在wait，一切都正常。但是也可能一个进程和它的父进程同时exit。所以当子进程尝试唤醒父进程，并告诉它自己退出了时，父进程也在退出。这些代码我一年前还记得是干嘛的，现在已经记不太清了。它应该是处理这种父进程和子进程同时退出的情况。如果不是这种情况的话，一切都会非常直观，子进程会在后面通过wakeup函数唤醒父进程**。

问题：为什么我们在唤醒父进程之后才将进程的状态设置为ZOMBIE？难道我们不应该在之前就设置吗？

回答：正在退出的进程会先获取自己进程的锁，同时，因为父进程的wait系统调用中也需要获取子进程的锁，所以父进程并不能查看正在执行exit函数的进程的状态。这意味着，正在退出的进程获取自己的锁到它调用sched进入到调度器线程之间（注，因为调度器线程会释放进程的锁），父进程并不能看到这之间代码引起的中间状态。所以这之间的代码顺序并不重要。大部分时候，如果没有持有锁，exit中任何代码顺序都不能工作。因为有了锁，代码的顺序就不再重要，因为父进程也看不到进程状态。

## 13.8 kill系统调用

> [13.8 kill系统调用 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/349780234) <= 图文出处

​	最后我想看的是kill系统调用。Unix中的一个进程可以将另一个进程的ID传递给kill系统调用，并让另一个进程停止运行。如果我们不够小心的话，kill一个还在内核执行代码的进程，会有一些我几分钟前介绍过的风险，比如我们想要杀掉的进程的内核线程还在更新一些数据，比如说更新文件系统，创建一个文件。如果这样的话，我们不能就这样杀掉进程，因为这样会使得一些需要多步完成的操作只执行了一部分。所以**<u>kill系统调用不能就直接停止目标进程的运行。实际上，在XV6和其他的Unix系统中，kill系统调用基本上不做任何事情</u>**。

```c
// Kill the process with the given pid.
// The victim won't exit until it tries to return
// to user space (see usertrap()) in trap.c).
int kill(int pid)
{
  struct proc *p;
  
  for(p = proc; p < &proc[NRPOC]); p++){
    acquire(&p->lock);
    if(p->pid == pid){
      p->killed = 1;
      if(p->state == SLEEPING) {
        // Wake process from sleep().
        p->state = RUNNABLE;
      }
      release(&p->lock);
      return 0;
    }
    release(&p->lock);
  }
  return -1;
}
```

​	**它先扫描进程表单，找到目标进程。然后只是将进程的proc结构体中killed标志位设置为1。如果进程正在SLEEPING状态，将其设置为RUNNABLE。这里只是将killed标志位设置为1，并没有停止进程的运行。所以kill系统调用本身还是很温和的**。

​	<u>而目标进程运行到内核代码中能安全停止运行的位置时，会检查自己的killed标志位，如果设置为1，目标进程会自愿的执行exit系统调用</u>。你可以在`trap.c`中看到所有可以安全停止运行的位置。

![img](https://pic4.zhimg.com/80/v2-bd520f482b5e1aab3cb410575e050bcb_1440w.webp)

​	<u>在usertrap函数中，在执行系统调用之前，如果进程已经被kill了，进程会自己调用exit。在这个内核代码位置，代码并没有持有任何锁，也不在执行任何操作的过程中，所以进程通过exit退出是完全安全的</u>。

​	<u>类似的，在usertrap函数的最后，也有类似的代码。在执行完系统调用之后，进程会再次检查自己是否已经被kill了。即使进程是被中断打断，这里的检查也会被执行。例如当一个定时器中断打断了进程的运行，我们可以通过检查发现进程是killed状态，之后进程会调用exit退出</u>。

​	**所以kill系统调用并不是真正的立即停止进程的运行，它更像是这样：如果进程在用户空间，那么下一次它执行系统调用它就会退出，又或者目标进程正在执行用户代码，当时下一次定时器中断或者其他中断触发了，进程才会退出。所以从一个进程调用kill，到另一个进程真正退出，中间可能有很明显的延时**。

​	这里有个很直观问题：如果进程不在用户空间执行，而是正在执行系统调用的过程中，然后它被kill了，我们需要做什么特别的操作吗？之所以会提出这个问题，是因为进程可能正在从console读取即将输入的字符，而你可能要明天才会输入一个字符，所以当你kill一个进程时，最好进程不是等到明天才退出。出于这个原因，**在XV6的很多位置中，如果进程在SLEEPING状态时被kill了，进程会实际的退出**。让我来给你展示这里的机制。
​	首先要看的是kill函数。

![img](https://pic2.zhimg.com/80/v2-a5fdec6490c4897d13c2744d6e9f82d5_1440w.webp)

​	你可以看到**如果目标进程是SLEEPING状态，kill函数会将其状态设置为RUNNABLE**，这意味着，即使进程之前调用了sleep并进入到SLEEPING状态，调度器现在会重新运行进程，并且进程会从sleep中返回。让我们来查看一下这在哪生效的。

![img](https://pic1.zhimg.com/80/v2-cc4b1dabbb9ebb99e6a0f90cb9821768_1440w.webp)

​	在`pipe.c`的piperead函数中，如果一个进程正在sleep状态等待从pipe中读取数据，然后它被kill了。kill函数会将其设置为RUNNABLE，之后进程会从sleep中返回，返回到循环的最开始。pipe中大概率还是没有数据，之后在piperead中，会判断进程是否被kill了（注，`if(pr->killed)`）。<u>如果进程被kill了，那么接下来piperead会返回-1，并且返回到usertrap函数的syscall位置，因为piperead就是一种系统调用的实现</u>。

![img](https://pic4.zhimg.com/80/v2-bd520f482b5e1aab3cb410575e050bcb_1440w.webp)

​	之后在usertrap函数中会检查`p->killed`，并调用exit。

​	**所以对于SLEEPING状态的进程，如果它被kill了，它会被直接唤醒，包装了sleep的循环会检查进程的killed标志位，最后再调用exit**。

​	**同时还有一些情况，如果进程在SLEEPING状态中被kill了并不能直接退出。例如，一个进程正在更新一个文件系统并创建一个文件的过程中，进程不适宜在这个时间点退出，因为我们想要完成文件系统的操作，之后进程才能退出。我会向你展示一个磁盘驱动中的sleep循环，这个循环中就没有检查进程的killed标志位**。

​	下面就是virtio_disk.c文件中的一段代码：

![img](https://pic4.zhimg.com/80/v2-1a46e2d798fee1aa20ab1b012b08bd6f_1440w.webp)

​	**这里一个进程正在等待磁盘的读取结束，这里没有检查进程的killed标志位。因为现在可能正在创建文件的过程中，而这个过程涉及到多次读写磁盘。我们希望完成所有的文件系统操作，完成整个系统调用，之后再检查`p->killed`并退出**。

​	借同学提问init进程是否会退出，下面看看相关的代码。

![img](https://pic2.zhimg.com/80/v2-671536dfc3a6d9f1bd3bf42ee8ae7e85_1440w.webp)

​	如果fork失败了，init进程也会退出。不过，这个问题的真正的答案是，init不会退出。**<u>init进程的目标就是不退出，它就是在一个循环中不停的调用wait。如果init进程退出了，我认为这是一个Fatal级别的错误，然后系统会崩溃</u>**。在exit函数的最开始就会有如下检查

![img](https://pic1.zhimg.com/80/v2-38857b67b63c0490fbf22915ae40c4c0_1440w.webp)

​	如果调用exit的进程是init进程，那么会触发panic。**因为如果没有init进程的话，系统最终还是会停止运行。<u>如果没有init进程的话就没有人会为退出的进程调用wait系统调用，也就没有人完成进程资源的释放工作，我们最终会用光所有的进程，并引起一些其他的错误，所以我们必须要有init进程</u>。所以这个问题的真正答案是init进程不允许退出**。

---

问题：为什么一个进程允许kill另一个进程？这样一个进程不是能杀掉所有其他进程吗？

回答：如果你在MIT的分时复用计算机Athena上这么做的话，他们可能会开除你。在XV6中允许这么做是因为，XV6这是个教学用的操作系统，任何与权限相关的内容在XV6中都不存在。**在Linux或者真正的操作系统中，每个进程都有一个user id或多或少的对应了执行进程的用户，一些系统调用使用进程的user id来检查进程允许做的操作。所以在Linux中会有额外的检查，调用kill的进程必须与被kill的进程有相同的user id，否则的话，kill操作不被允许**。<u>所以，在一个分时复用的计算机上，我们会有多个用户，我们不会想要用户kill其他人的进程，这样一套机制可以防止用户误删别人的进程</u>。

问题：这节课可能没有怎么讲到，但是如果关闭一个操作系统会发生什么？

回答：这个过程非常复杂，并且依赖于你运行的是什么系统。因为文件系统是持久化的，它能在多次重启之间保持数据，我们需要保持文件系统的良好状态，如果我们正在更新文件系统的过程中，例如创建文件，然后我们想关闭操作系统，断电之类的。我们需要一个策略来确保即使我们正在一个复杂的更新文件系统的过程中，我们并不会破坏磁盘上的文件系统数据。文件系统其实就是一个位于磁盘的数据结构。所以这里涉及到了很多的机制来确保如果你关闭操作系统或者因为断电之类，我们可以恢复磁盘上的文件系统。其他的，你是否需要做一些特殊的操作来关闭系统，取决于你正在运行什么进程。如果你正在运行一些重要的服务器，例如数据库服务器，并且许多其他计算机依赖这个数据库并通过网络使用它。那谁知道呢？答案或许是你不能就这么直接关闭操作系统，因为你正在提供一个对于其他计算机来说非常关键的服务。如果你的计算机并没有在做任何事情，那么你可以直接关闭它。<u>或许对于你的问题来说，如果你想关闭一个计算机，确保文件系统是正确的，之后停止执行指令，之后就可以关闭计算机了</u>。

# Lecture14 文件系统File Systems

## 14.1 章节概述

> [14.1 Why Interesting - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351902599) <= 图文出处

​	今天介绍的是文件系统。实际上我们会花三节课的时间来学习文件系统。前两节课基于XV6来做介绍，第三节课基于Linux来做介绍。实际上，这将是有关XV6的最后一个话题，在这周之后我们就讲完了XV6。

​	对于文件系统，你们都知道它并使用过它。它是操作系统中除了shell以外最常见的用户接口。所以我们希望通过这几节课来理解：文件系统的背后究竟是什么原理，文件系统是如何实现的。这些内容还是让人有些小激动，因为你们一直都在使用文件系统。

​	接下来让我列出一些文件系统突出的特性：

- User-friendly names / pathnames。其中一点刚刚有同学提到了，就是对于用户友好的文件名，具体来说就是层级的路径名，这可以帮助用户组织目录中的文件。
- Share files between users / processes。通过将文件命名成方便易记的名字，可以在用户之间和进程之间更简单的共享文件。
- Persistence / durability。相比我们已经看过的XV6其他子系统，这一点或许是最重要的，文件系统提供了持久化。这意味着，我可以关闭一个计算机，过几天再开机而文件仍然在那，我可以继续基于文件工作。这一点与进程和其他资源不一样，这些资源在计算机重启时就会消失，之后你需要重新启动它们，但是文件系统就可以提供持久化。

![img](https://pic2.zhimg.com/80/v2-4c0429ab56d5eb46fbe531ce773eaf79_1440w.webp)

​	你们都使用了文件系统，接下来几节课我们将学习它内部是如何工作的。出于以下原因，文件系统背后的机制还比较有意思：

- 文件系统**对硬件的抽象**较为有用，所以理解文件系统对于硬件的抽象是如何实现的还是有点意思的。
- 除此之外，还有个关键且有趣的地方就是**crash safety**。有可能在文件系统的操作过程中，计算机崩溃了，在重启之后你的文件系统仍然能保持完好，文件系统的数据仍然存在，并且你可以继续使用你的大部分文件。如果文件系统操作过程中计算机崩溃了，然后你重启之后文件系统不存在了或者磁盘上的数据变了，那么崩溃的将会是你。所以crash safety是一个非常重要且经常出现的话题，我们下节课会专门介绍它。
- 之后是一个通用的问题，**如何在磁盘上排布文件系统**。例如目录和文件，它们都需要以某种形式在磁盘上存在，这样当你重启计算机时，所有的数据都能恢复。所以在磁盘上有一些数据结构表示了文件系统的结构和内容。在XV6中，使用的数据结构非常简单，因为XV6是专门为教学目的创建的。真实的文件系统通常会更加复杂。但是它们都是磁盘上保存的数据结构，我们在今天的课程会重点看这部分。
- 最后一个有趣的话题是**性能**。文件系统所在的硬件设备通常都较慢，比如说向一个SSD磁盘写数据将会是毫秒级别的操作，而在一个毫秒内，计算机可以做大量的工作，所以尽量避免写磁盘很重要，我们将在几个地方看到提升性能的代码。<u>比如说，所有的文件系统都有buffer cache或者叫block cache</u>。同时这里会有更多的并发，比如说你正在查找文件路径名，这是一个多次交互的操作，首先要找到文件结构，然后查找一个目录的文件名，之后再去查找下一个目录等等。你会期望当一个进程在做路径名查找时，另一个进程可以并行的运行。这样的并行运行在文件系统中将会是一个大的话题。

​	除此之外，你会对文件系统感兴趣是因为这是接下来两个lab的内容。下一个lab完全关注在文件系统，下下个lab结合了虚拟内存和文件系统。即使是这周的lab，也会尝试让buffer cache可以支持更多的并发。所以这就是为什么文件系统是有趣的。

## 14.2 File system实现概述

> [14.2 File system实现概述 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351902801) <= 图文出处
>
> [Linux下的softlink和hardlink（转） - 沧海一滴 - 博客园 (cnblogs.com)](https://www.cnblogs.com/softidea/p/5447506.html)

​	为了理解文件系统必须提供什么能力，让我们再看一下一些与文件系统相关的基础系统调用。从这些系统调用接口，我们将可以推断出有关文件系统实现的一些细节，这些系统调用我们在之前的课程已经看过了。首先让我们来看一个简单的场景，假设我们创建了文件“x/y”，或者说在目录x中创建了文件y，同时我们需要提供一些标志位，现在我们还不关心标志位所以我会忽略它。

![img](https://pic1.zhimg.com/80/v2-2d1daefa0b8a95fd95ca0d3371a3de14_1440w.webp)

​	上面的系统调用会创建文件，并返回文件描述符给调用者。调用者也就是用户应用程序可以对文件描述符调用write，有关write我们在之前已经看过很多次了，这里我们向文件写入“abc”三个字符。

![img](https://pic2.zhimg.com/80/v2-2e0680c35f5f522c08910e11e8e3a4c5_1440w.webp)

​	从这两个调用已经可以看出一些信息了：

- 首先出现在接口中的路径名是可读的名字，而不是一串数字，它是由用户选择的字符串。
- **write系统调用并没有使用offset作为参数，所以写入到文件的哪个位置是隐式包含在文件系统中，文件系统在某个位置必然保存了文件的offset**。因为如果你再调用write系统调用，新写入的数据会从第4个字节开始。

​	除此之外，还有一些我们之前没有看过的有趣的系统调用。例如XV6和所有的Unix文件系统都支持通过系统调用创建链接，给同一个文件指定多个名字。你可以通过调用link系统调用，为之前创建的文件“x/y”创建另一个名字“x/z”。所以文件系统内部需要以某种方式跟踪指向同一个文件的多个文件名。

![img](https://pic3.zhimg.com/80/v2-4d51936c94becbe695140f2b3da945b2_1440w.webp)

​	<u>我们还可能会在文件打开时，删除或者更新文件的命名空间。例如，用户可以通过unlink系统调用来删除特定的文件名。如果此时相应的文件描述符还是打开的状态，那我们还可以向文件写数据，并且这也能正常工作</u>。

![img](https://pic3.zhimg.com/80/v2-bc28550f5224d9bf80d0e5439f98cd82_1440w.webp)

​	所以，**在文件系统内部，文件描述符必然与某个对象关联，而这个对象不依赖文件名。这样，即使文件名变化了，文件描述符仍然能够指向或者引用相同的文件对象。所以，实际上操作系统内部需要对于文件有内部的表现形式，并且这种表现形式与文件名无关**。

![img](https://pic3.zhimg.com/80/v2-c5d5b27d321f58cb2892642586b8a516_1440w.webp)

​	除此之外，我还想提一点。文件系统的目的是实现上面描述的API，也即是典型的文件系统API。但是，这并不是唯一构建一个存储系统的方式。如果只是在磁盘上存储数据，你可以想出一个完全不同的API。举个例子，数据库也能持久化的存储数据，但是数据库就提供了一个与文件系统完全不一样的API。**所以记住这一点很重要：还存在其他的方式能组织存储系统**。我们这节课关注在文件系统，**<u>文件系统通常由操作系统提供，而数据库如果没有直接访问磁盘的权限的话，通常是在文件系统之上实现的（注，早期数据库通常直接基于磁盘构建自己的文件系统，因为早期操作系统自带的文件系统在性能上较差，且写入不是同步的，进而导致数据库的ACID不能保证。不过现代操作系统自带的文件系统已经足够好，所以现代的数据库大部分构建在操作系统自带的文件系统之上）</u>**。

​	接下来我们看一下文件系统的结构。文件系统究竟维护了什么样的结构来实现前面介绍的API呢？

​	首先，**最重要的可能就是inode，这是代表一个文件的对象，并且它不依赖于文件名**。**实际上，inode是通过自身的编号来进行区分的，这里的编号就是个整数。所以文件系统内部通过一个数字，而不是通过文件路径名引用inode**。

​	同时，**<u>基于之前的讨论，inode必须有一个link count来跟踪指向这个inode的文件名的数量。一个文件（inode）只能在link count为0的时候被删除。实际的过程可能会更加复杂，实际中还有一个openfd count，也就是当前打开了文件的文件描述符计数。一个文件只能在这两个计数器都为0的时候才能被删除</u>**。

![img](https://pic2.zhimg.com/80/v2-4f007818f2d423d467953fcbe1ffece5_1440w.webp)

​	**同时基于之前的讨论，我们也知道write和read都没有针对文件的offset参数，所以<u>文件描述符必然自己悄悄维护了对于文件的offset</u>**。

![img](https://pic2.zhimg.com/80/v2-5067458a6dc9f9edee55ebc6d7fbcddd_1440w.webp)

​	**<u>文件系统中核心的数据结构就是inode和file descriptor。file descriptor主要与用户进程进行交互</u>**。

​	尽管文件系统的API很相近并且内部实现可能非常不一样。但是很多文件系统都有类似的结构。因为文件系统还挺复杂的，所以最好按照分层的方式进行理解。可以这样看：

- 在最底层是磁盘，也就是一些实际保存数据的存储设备，正是这些设备提供了持久化存储。
- <u>在这之上是buffer cache或者说block cache，这些cache可以避免频繁的读写磁盘</u>。这里我们将磁盘中的数据保存在了内存中。
- 为了保证持久性，再往上通常会有一个**logging层**。许多文件系统都有某种形式的logging，我们下节课会讨论这部分内容，所以今天我就跳过它的介绍。
- 在logging层之上，XV6有inode cache，这主要是为了**同步（synchronization）**，我们稍后会介绍。**inode通常小于一个disk block，所以多个inode通常会打包存储在一个disk block中。为了向单个inode提供同步操作，XV6维护了inode cache**。
- 再往上就是inode本身了。它实现了read/write。
- 再往上，就是文件名，和文件描述符操作。

![img](https://pic4.zhimg.com/80/v2-2d3fed05645a6c1445326f37a32b252b_1440w.webp)

​	<u>不同的文件系统组织方式和每一层可能都略有不同，有的时候分层也没有那么严格，即使在XV6中分层也不是很严格，但是从概念上来说这里的结构对于理解文件系统还是有帮助的。实际上所有的文件系统都有组件对应这里不同的分层，例如buffer cache，logging，inode和路径名</u>。

---

问题：link增加了了对于文件的一个引用，unlink减少了一个引用？

回答：是的。我们稍后会介绍更多相关的内容。

问题：能介绍一下soft link和hard link吗？

回答：今天不会讨论这些内容。但是你们将会在下一个File system lab中实现soft link。所以XV6本身实现了hard link，需要你们来实现soft link。

问题：link是对inode做操作，而不是对文件描述符做操作，对吧？

回答：是的，link是对inode做操作，我们接下来介绍这部分内容。

## 14.3 存储设备和磁盘分布(Storage devices & disk layout)

> [14.3 How file system uses disk - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351903071) <= 图文出处

​	接下来，简单介绍下最底层的存储设备。实际中有非常非常多不同类型的存储设备，这些设备的区别在于性能，容量，数据保存的期限等。其中两种最常见，并且你们应该也挺熟悉的是**SSD和HDD**。这两类存储虽然有着不同的性能，但是都在合理的成本上提供了大量的存储空间。SSD通常是0.1到1毫秒的访问时间，而HDD通常是在10毫秒量级完成读写一个disk block。

![img](https://pic1.zhimg.com/80/v2-7789baa228e2511d9b8ac8bd51f7bb84_1440w.webp)

​	这里有些术语有点让人困惑，它们是**扇区(sectors)和块(blocks)**。

- sector通常是**磁盘驱动**可以读写的最小单元，它过去通常是512字节。
- block通常是**操作系统或者文件系统**视角的数据。它由文件系统定义，在XV6中它是1024字节。所以XV6中一个block对应两个sector。**通常来说一个block对应了一个或者多个sector**。

​	*有的时候，人们也将磁盘上的sector称为block。所以这里的术语也不是很精确。*

​	这些存储设备连接到了电脑总线之上，总线也连接了CPU和内存。一个文件系统运行在CPU上，将内部的数据存储在内存，同时也会以读写block的形式存储在SSD或者HDD。这里的接口还是挺简单的，包括了read/write，然后以block编号作为参数。虽然我们这里描述的过于简单了，但是实际的接口大概就是这样。

![img](https://pic4.zhimg.com/80/v2-2fcce3bbd600614aa3d9e319377aec53_1440w.webp)

​	在内部，SSD和HDD工作方式完全不一样，但是对于硬件的抽象屏蔽了这些差异。磁盘驱动通常会使用一些标准的协议，例如PCIE，与磁盘交互。从上向下看磁盘驱动的接口，大部分的磁盘看起来都一样，你可以提供block编号，在驱动中通过写设备的控制寄存器，然后设备就会完成相应的工作。这是从一个文件系统的角度的描述。**尽管不同的存储设备有着非常不一样的属性，从驱动的角度来看，你可以以大致相同的方式对它们进行编程**。

​	有关存储设备我们就说这么多。

---

​	<u>从文件系统的角度来看磁盘还是很直观的。因为对于磁盘就是读写block或者sector，我们可以将磁盘看作是一个巨大的block的数组，数组从0开始，一直增长到磁盘的最后</u>。

![img](https://pic3.zhimg.com/80/v2-17e1217febc0a9abbc31db4dbb3ce4c6_1440w.webp)

​	<u>而文件系统的工作就是将所有的数据结构以一种能够在重启之后重新构建文件系统的方式，存放在磁盘上</u>。虽然有不同的方式，但是XV6使用了一种非常简单，但是还挺常见的布局结构。

通常来说：

- **block0要么没有用，要么被用作boot sector来启动操作系统**。
- **block1通常被称为super block，它描述了文件系统**。它可能包含磁盘上有多少个block共同构成了文件系统这样的信息。我们之后会看到XV6在里面会存更多的信息，你可以通过block1构造出大部分的文件系统信息。
- 在XV6中，log从block2开始，到block32结束。实际上log的大小可能不同，这里在super block中会定义log就是30个block。
- **接下来在block32到block45之间，XV6存储了inode。我之前说过多个inode会打包存在一个block中，一个inode是64字节**。
- 之后是bitmap block，这是我们构建文件系统的默认方法，它只占据一个block。它记录了数据block是否空闲。
- 之后就全是数据block了，数据block存储了文件的内容和目录的内容。

​	**通常来说，bitmap block，inode blocks和log blocks被统称为metadata block。它们虽然不存储实际的数据，但是它们存储了能帮助文件系统完成工作的元数据**。

​	假设inode是64字节，如果你想要读取inode10，那么你应该按照下面的公式去对应的block读取inode。

![img](https://pic2.zhimg.com/80/v2-fc045425f576b62721ea3f7d894b3b55_1440w.webp)

​	所以inode0在block32，inode17会在block33。**只要有inode的编号，我们总是可以找到inode在磁盘上存储的位置**。

---

问题：对于read/write的接口，是不是提供了同步/异步的选项？

回答：你可以认为一个磁盘的驱动与console的驱动是基本一样的。驱动向设备发送一个命令表明开始读或者写，过了一会当设备完成了操作，会产生一个中断表明完成了相应的命令。但是因为磁盘本身比console复杂的多，所以磁盘的驱动也会比我们之前看过的console的驱动复杂的多。不过驱动中的代码结构还是类似的，也有bottom部分和top部分，中断和读写控制寄存器。

**问题：boot block是不是包含了操作系统启动的代码？**

**回答：是的，它里面通常包含了足够启动操作系统的代码。之后再从文件系统中加载操作系统的更多内容。**

问题：XV6是存储在虚拟磁盘上？

回答：在QEMU中，我们实际上走了捷径。QEMU中有个标志位`-kernel`，它指向了内核的镜像文件，QEMU会将这个镜像的内容加载到了物理内存的0x80000000。所以当我们使用QEMU时，我们不需要考虑boot sector。

追问：所以当你运行QEMU时，你就是将程序通过命令行传入，然后直接就运行传入的程序，然后就不需要从虚拟磁盘上读取数据了？

回答：是的。

## 14.4 inode理论

> [14.4 inode - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351903377) <= 图文出处

​	接下来我们看一下磁盘上存储的inode究竟是什么？首先我们前面已经看过了，这是一个64字节的数据结构。

- 通常来说它有**一个type字段，表明inode是文件还是目录**。
- nlink字段，也就是link计数器，用来跟踪究竟有多少文件名指向了当前的inode。
- size字段，表明了文件数据有多少个字节。
- 不同文件系统中的表达方式可能不一样，不过在XV6中接下来是一些block的编号，例如编号0，编号1，等等。<u>XV6的inode中总共有12个block编号。这些被称为direct block number。这12个block编号指向了构成文件的前12个block</u>。举个例子，如果文件只有2个字节，那么只会有一个block编号0，它包含的数字是磁盘上文件前2个字节的block的位置。
- <u>之后还有一个indirect block number，它对应了磁盘上一个block，这个block包含了256个block number，这256个block number包含了文件的数据（256个，是因为block是1024字节，一个block number用4字节表示）</u>。所以inode中block number 0到block number 11都是direct block number，而block number 12保存的indirect block number指向了另一个block。

​	以上基本就是XV6中inode的组成部分。基于上面的内容，XV6中文件最大的长度是多少呢？学生回答268*1024字节。是的，最大文件尺寸对应的是下面的公式。

![img](https://pic2.zhimg.com/80/v2-1fa851768080366b445689500fdc6b75_1440w.webp)

​	可以算出这里就是268KB，这么点大小能存个什么呢？所以这是个很小的文件长度，实际的文件系统，文件最大的长度会大的多得多。那可以做一些什么来让文件系统支持大得多的文件呢？学生回答：可以扩展inode中indirect部分吗？<u>是的，可以用类似page table的方式，构建一个双重indirect block number指向一个block，这个block中再包含了256个indirect block number，每一个又指向了包含256个block number的block。这样的话，最大的文件长度会大得多（注，是256*256*1K）</u>。这里修改了inode的数据结构，你可以使用类似page table的树状结构，也可以按照B树或者其他更复杂的树结构实现。XV6这里极其简单，基本是按照最早的Uinx实现方式来的，不过你可以实现更复杂的结构。实际上，在接下来的File system lab中，你将会实现双重indirect block number来支持更大的文件。

​	接下来，我们想要实现read系统调用。假设我们需要读取文件的第8000个字节，那么你该读取哪个block呢？从inode的数据结构中该如何计算呢？对于8000，我们首先除以1024，也就是block的大小，得到大概是7。这意味着第7个block就包含了第8000个字节。所以直接在inode的direct block number中，就包含了第8000个字节的block。为了找到这个字节在第7个block的哪个位置，我们需要用8000对1024求余数，我猜结果是是832。所以为了读取文件的第8000个字节，文件系统查看inode，先用8000除以1024得到block number，然后再用8000对1024求余读取block中对应的字节。

​	<u>总结一下，inode中的信息完全足够用来实现read/write系统调用，至少可以找到哪个disk block需要用来执行read/write系统调用</u>。

---

![img](https://pic1.zhimg.com/80/v2-456033468eb808130d1ac7d740256550_1440w.webp)

​	接下来我们讨论一下目录（directory）。文件系统的酷炫特性就是**层次化的命名空间（hierarchical namespace）**，你可以在文件系统中保存对用户友好的文件名。**大部分Unix文件系统有趣的点在于，一个目录本质上是一个文件加上一些文件系统能够理解的结构**。<u>在XV6中，这里的结构极其简单。每一个目录包含了directory entries，每一条entry都有固定的格式</u>（每个entry占16字节）：

- 前2个字节包含了目录中文件或者子目录的inode编号
- 接下来的14个字节包含了文件或者子目录名

​	对于实现路径名查找，这里的信息就足够了。假设我们要查找路径名“/y/x”，我们该怎么做呢？

​	从路径名我们知道，应该从root inode开始查找。通<u>常root inode会有固定的inode编号，在XV6中，这个编号是1</u>。我们该如何根据编号找到root inode呢？从前一节我们可以知道，inode从block 32开始，如果是inode1，那么必然在block 32中的64到128字节的位置。所以文件系统可以直接读到root inode的内容。

​	对于路径名查找程序，接下来就是扫描root inode包含的所有block，以找到“y”。<u>该怎么找到root inode所有对应的block呢？根据前一节的内容就是读取所有的direct block number和indirect block number</u>。结果可能是找到了，也可能是没有找到。如果找到了，那么目录y也会有一个inode编号，假设是251，我们可以继续从inode 251查找，先读取inode 251的内容，之后再扫描inode所有对应的block，找到“x”并得到文件x对应的inode编号，最后将其作为路径名查找的结果返回。

![img](https://pic1.zhimg.com/80/v2-29afcdf9fe383ca42e0fba348a9bb6b4_1440w.webp)

​	<u>很明显，这里的结构不是很有效。为了找到一个目录名，你需要线性扫描。实际的文件系统会使用更复杂的数据结构来使得查找更快，当然这又是设计数据结构的问题，而不是设计操作系统的问题。你可以使用你喜欢的数据结构并提升性能。出于简单和更容易解释的目的，XV6使用了这里这种非常简单的数据结构</u>。

---

问题：为什么每个block存储256个block编号？

回答：因为每个编号是4个字节。1024/4 = 256。这又带出了一个问题，如果block编号只是4个字节，磁盘最大能有多大？是的，2的32次方（注，4TB，4字节编号=>32位，2*32寻址，而每个block大小1kb，所以计算得4TB）。有些磁盘比这个数字要大，所以通常人们会使用比32bit更长的数字来表示block编号。在下一个File system lab，你们需要将inode中的一个block number变成双重indirect block number，这个双重indirect block number将会指向一个包含了256个indirect block number的block，其中的每一个indirect block number再指向一个包含了256个block number的block，这样文件就可以大得多。

问题：有没有一些元数据表明当前的inode是目录而不是一个文件？

回答：有的，实际上是在inode中。inode中的type字段表明这是一个目录还是一个文件。如果你对一个类型是文件的inode进行查找，文件系统会返回错误。

## 14.5 File System工作示例

> [14.5 File system工作示例 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351903926) <= 图文出处

​	接下来我们看一下实际中，XV6的文件系统是如何工作的，这部分内容对于下一个lab是有帮助的。

​	首先我会启动XV6，这里有件事情我想指出。启动XV6的过程中，调用了makefs指令，来创建一个文件系统。

![img](https://pic2.zhimg.com/80/v2-aeed959cb6ce8441f792cdd54a2fc5f9_1440w.webp)

​	所以makefs创建了一个全新的磁盘镜像，在这个磁盘镜像中包含了我们在指令中传入的一些文件。makefs为你创建了一个包含这些文件的新的文件系统。

​	XV6总是会打印文件系统的一些信息，所以从指令的下方可以看出有46个meta block，其中包括了：

- boot block
- super block
- 30个log block
- 13个inode block
- 1个bitmap block

​	之后是954个data block。所以这是一个袖珍级的文件系统，总共就包含了1000个block。在File system lab中，你们会去支持更大的文件系统。

​	我还稍微修改了一下XV6，使得任何时候写入block都会打印出block的编号。我们从console的输出可以看出，在XV6启动过程中，会有一些对于文件系统的调用，并写入了block 33，45，32。

​	接下来我们运行一些命令，来看一下特定的命令对哪些block做了写操作，并理解为什么要对这些block写入数据。我们通过echo “hi” > x，来创建一个文件x，并写入字符“hi”。我会将输出拷贝出来，并做分隔以方便我们更好的理解。

![img](https://pic3.zhimg.com/80/v2-cb0419b2a1ded416a9a9dcef49903376_1440w.webp)

​	这里会有几个阶段：

1. 第一阶段是创建文件
2. 第二阶段将“hi”写入文件
3. 第三阶段将“\n”换行符写入到文件

如果你去看echo的代码实现，基本就是这3个阶段。

![img](https://pic2.zhimg.com/80/v2-f7e2392aab10702c806175d18160074d_1440w.webp)

​	上面就是echo的代码，它先检查参数，并将参数写入到文件描述符1，在最后写入一个换行符。

​	让我们一个阶段一个阶段的看echo的执行过程，并理解对于文件系统发生了什么。相比看代码，这里直接看磁盘的分布图更方便：

![img](https://pic3.zhimg.com/80/v2-17e1217febc0a9abbc31db4dbb3ce4c6_1440w.webp)

​	你们觉得的write 33代表了什么？我们正在创建文件，所以我们期望文件系统干什么呢？学生回答：这是在写inode。是的，看起来给我们分配的inode位于block 33。之所以有两个write 33，第一个是为了标记inode将要被使用。在XV6中，我记得是使用inode中的type字段来标识inode是否空闲，这个字段同时也会用来表示inode是一个文件还是一个目录。所以这里将inode的type从空闲改成了文件，并写入磁盘表示这个inode已经被使用了。第二个write 33就是实际的写入inode的内容。inode的内容会包含linkcount为1以及其他内容。

​	write 46是向第一个data block写数据，那么这个data block属于谁呢？学生回答：属于根目录。是的，block 46是根目录的第一个block。为什么它需要被写入数据呢？学生回答：因为我们正在向根目录创建一个新文件。是的，这里我们向根目录增加了一个新的entry，其中包含了文件名x，以及我们刚刚分配的inode编号。

​	接下来的write 32又是什么意思呢？block 32保存的仍然是inode，那么inode中的什么发生了变化使得需要将更新后的inode写入磁盘？是的，根目录的大小变了，因为我们刚刚添加了16个字节的entry来代表文件x的信息。

​	最后又有一次write 33，我在稍后会介绍这次写入的内容，这里我们再次更新了文件x的inode， 尽管我们又还没有写入任何数据。

​	以上就是第一阶段创建文件的过程。第二阶段是向文件写入“hi”。

​	首先是write 45，这是更新bitmap。文件系统首先会扫描bitmap来找到一个还没有使用的data block，未被使用的data block对应bit 0。找到之后，文件系统需要将该bit设置为1，表示对应的data block已经被使用了。所以更新block 45是为了更新bitmap。

​	接下来的两次write 595表明，文件系统挑选了data block 595。所以在文件x的inode中，第一个direct block number是595。因为写入了两个字符，所以write 595被调用了两次。

​	第二阶段最后的write 33是更新文件x对应的inode中的size字段，因为现在文件x中有了两个字符。

​	以上就是磁盘中文件系统的组织结构的核心，希望你们都能理解背后的原理。

----

问题：block 595看起来在磁盘中很靠后了，是因为前面的block已经被系统内核占用了吗？

回答：我们可以看前面makefs指令，makefs存了很多文件在磁盘镜像中，这些都发生在创建文件x之前，所以磁盘中很大一部分已经被这些文件填满了。

问题：第二阶段最后的write 33是否会将block 595与文件x的inode关联起来？

回答：会的。这里的write 33会发生几件事情：首先inode的size字段会更新；第一个direct block number会更新。这两个信息都会通过write 33一次更新到磁盘上的inode中。

## 14.6 XV6创建inode代码演示

> [14.6 XV6创建inode代码展示 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351904145) <= 图文出处

​	接下来我们通过查看XV6中的代码，更进一步的了解文件系统。因为我们前面已经分配了inode，我们先来看一下这是如何发生的。<u>`sysfile.c`中包含了所有与文件系统相关的函数，分配inode发生在`sys_open`函数中，这个函数会负责创建文件</u>。

![img](https://pic1.zhimg.com/v2-96773d813d3f133526d3c87c71468784_r.jpg)

​	在sys_open函数中，会调用create函数。

![img](https://pic3.zhimg.com/80/v2-0c823d5d13aa64ceb7b08d5ecb1ac91e_1440w.webp)

​	create函数中首先会解析路径名并找到最后一个目录，之后会查看文件是否存在，如果存在的话会返回错误。之后就会调用`ialloc(inode allocate)`，这个函数会为文件x分配inode。ialloc函数位于`fs.c`文件中。

![img](https://pic2.zhimg.com/v2-7299d70bb871e7e08e021873aace2579_r.jpg)

​	以上就是ialloc函数，与XV6中的大部分函数一样，它很简单，但是又不是很高效。<u>它会遍历所有可能的inode编号，找到inode所在的block，再看位于block中的inode数据的type字段。如果这是一个空闲的inode，那么将其type字段设置为文件，这会将inode标记为已被分配。函数中的log_write就是我们之前看到在console中有关写block的输出。这里的log_write是我们看到的整个输出的第一个</u>。

​	以上就是第一次写磁盘涉及到的函数调用。这里有个有趣的问题，如果有多个进程同时调用create函数会发生什么？对于一个多核的计算机，进程可能并行运行，两个进程可能同时会调用到ialloc函数，然后进而调用`bread(block read)`函数。所以必须要有一些机制确保这两个进程不会互相影响。

​	让我们看一下位于`bio.c`的buffer cache代码。首先看一下bread函数：

![img](https://pic1.zhimg.com/80/v2-8bdc746f7a62288b6c1c70be5d702970_1440w.webp)

​	bread函数首先会调用bget函数，<u>bget会为我们从buffer cache中找到block的缓存</u>。让我们看一下bget函数：

![img](https://pic3.zhimg.com/80/v2-c95fadc1ddc93af3e204005cd0cd66a6_1440w.webp)

​	这里的代码还有点复杂。我猜你们之前已经看过这里的代码，那么这里的代码在干嘛？学生回答：这里遍历了linked-list，来看看现有的cache是否符合要找的block。是的，我们这里看一下block 33的cache是否存在，如果存在的话，将block对象的引用计数（refcnt）加1，之后再释放bcache锁，因为现在我们已经完成了对于cache的检查并找到了block cache。之后，代码会尝试获取block cache的锁。

​	所以，如果有多个进程同时调用bget的话，其中一个可以获取bcache的锁并扫描buffer cache。此时，其他进程是没有办法修改buffer cache的（注，因为bacche的锁被占住了）。之后，进程会查找block number是否在cache中，如果在的话将block cache的引用计数加1，表明当前进程对block cache有引用，之后再释放bcache的锁。如果有第二个进程也想扫描buffer cache，那么这时它就可以获取bcache的锁。假设第二个进程也要获取block 33的cache，那么它也会对相应的block cache的引用计数加1。最后这两个进程都会尝试对block 33的block cache调用acquiresleep函数。

​	acquiresleep是另一种锁，我们称之为sleep lock，本质上来说它获取block 33 cache的锁。其中一个进程获取锁之后函数返回。在ialloc函数中会扫描block 33中是否有一个空闲的inode。而另一个进程会在acquiresleep中等待第一个进程释放锁。

​	如果buffer cache中有两份block 33的cache将会出现问题。假设一个进程要更新inode19，另一个进程要更新inode20。如果它们都在处理block 33的cache，并且cache有两份，那么第一个进程可能持有一份cache并先将inode19写回到磁盘中，而另一个进程持有另一份cache会将inode20写回到磁盘中，并将inode19的更新覆盖掉。**所以一个block只能在buffer cache中出现一次**。你们在完成File system lab时，必须要维持buffer cache的这个属性。

![img](https://pic4.zhimg.com/80/v2-0abbfb079d312acabed1b241b9dbafff_1440w.webp)

---

问题：当一个block cache的refcnt不为0时，可以更新block cache吗？因为释放锁之后，可能会修改block cache。

回答：这里我想说几点；首先XV6中对bcache做任何修改的话，都必须持有bcache的锁；其次对block 33的cache做任何修改你需要持有block 33的sleep lock。所以在任何时候，`release(&bcache.lock)`之后，`b->refcnt`都大于0。<u>block的cache只会在refcnt为0的时候才会被驱逐，任何时候refcnt大于0都不会驱逐block cache</u>。所以当`b->refcnt`大于0的时候，block cache本身不会被buffer cache修改。这里的第二个锁，也就是block cache的sleep lock，是用来保护block cache的内容的。它确保了任何时候只有一个进程可以读写block cache。

问题：如果多个进程都在使用同一个block的cache，然后有一个进程在修改block，并通过强制向磁盘写数据修改了block的cache，那么其他进程会看到什么结果？

回答：如果第一个进程结束了对block 33的读写操作，它会对block的cache调用`brelse(block cache release)`函数。这个函数会对refcnt减1，并释放sleep lock。这意味着，如果有任何一个其他进程正在等待使用这个block cache，现在它就能获得这个block cache的sleep lock，并发现刚刚做的改动。假设两个进程都需要分配一个新的inode，且新的inode都位于block 33。如果第一个进程分配到了inode18并完成了更新，那么它对于inode18的更新是可见的。另一个进程就只能分配到inode19，因为inode18已经被标记为已使用，任何之后的进程都可以看到第一个进程对它的更新。这正是我们想看到的结果，如果一个进程创建了一个inode或者创建了一个文件，之后的进程执行读就应该看到那个文件。

## 14.7 Sleep Lock和block cache

> [14.7 Sleep Lock - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351904443) <= 图文出处

​	block cache使用的是sleep lock。sleep lock区别于一个常规的spinlock。我们先看来一下sleep lock。

![img](https://pic2.zhimg.com/80/v2-efcaca305c91b49d1f3c61f81bb8dd05_1440w.webp)

​	**首先是acquiresleep函数，它用来获取sleep lock。函数里首先获取了一个普通的spinlock，这是与sleep lock关联在一起的一个锁。之后，如果sleep lock被持有，那么就进入sleep状态，并将自己从当前CPU调度开**。

​	既然sleep lock是基于spinlock实现的，为什么对于block cache，我们使用的是sleep lock而不是spinlock？

​	这里其实有多种原因。**对于spinlock有很多限制，其中之一是加锁时中断必须要关闭**。所以如果使用spinlock的话，当我们对block cache做操作的时候需要持有锁，那么我们就永远也不能从磁盘收到数据。或许另一个CPU核可以收到中断并读到磁盘数据，但是如果我们只有一个CPU核的话，我们就永远也读不到数据了。**出于同样的原因，也不能在持有spinlock的时候进入sleep状态（注，详见13.1）**。所以这里我们使用sleep lock。**<u>sleep lock的优势就是，我们可以在持有锁的时候不关闭中断。我们可以在磁盘操作的过程中持有锁，我们也可以长时间持有锁。当我们在等待sleep lock的时候，我们并没有让CPU一直空转，我们通过sleep将CPU出让出去了</u>**。

​	接下来让我们看一下brelease函数。

![img](https://pic4.zhimg.com/v2-0abbfb079d312acabed1b241b9dbafff_r.jpg)

​	brelease函数中首先释放了sleep lock；之后获取了bcache的锁；之后减少了block cache的引用计数，表明一个进程不再对block cache感兴趣；<u>最后如果引用计数为0，那么它会修改buffer cache的linked-list，将block cache移到linked-list的头部，这样表示这个block cache是最近使用过的block cache</u>。**这一点很重要，当我们在bget函数中不能找到block cache时，我们需要在buffer cache中腾出空间来存放新的block cache，这时会使用LRU（Least Recent Used）算法找出最不常使用的block cache，并撤回它（注，而将刚刚使用过的block cache放在linked-list的头部就可以直接更新linked-list的tail来完成LRU操作）**。<u>为什么这是一个好的策略呢？因为通常系统都遵循temporal locality策略，也就是说如果一个block cache最近被使用过，那么很有可能它很快会再被使用，所以最好不要撤回这样的block cache</u>。

​	以上就是对于block cache代码的介绍。这里有几件事情需要注意：

- **首先在内存中，对于一个block只能有一份缓存。这是block cache必须维护的特性**。
- **其次，这里使用了与之前的spinlock略微不同的sleep lock。与spinlock不同的是，可以在I/O操作的过程中持有sleep lock**。
- 第三，**它采用了LRU作为cache替换策略**。
- 第四，它有两层锁。第一层锁用来保护buffer cache的内部数据；第二层锁也就是sleep lock用来保护单个block的cache。

![img](https://pic4.zhimg.com/v2-a5866bf104800c1351be8f396a21b1df_r.jpg)

---

问题：我有个关于brelease函数的问题，看起来它先释放了block cache的锁，然后再对引用计数refcnt减一，为什么可以这样呢？

回答：这是个好问题。如果我们释放了sleep lock，这时另一个进程正在等待锁，那么refcnt必然大于1，而`b->refcnt --`只是表明当前执行brelease的进程不再关心block cache。如果还有其他进程正在等待锁，那么refcnt必然不等于0，我们也必然不会执行`if(b->refcnt == 0)`中的代码。

## 14.8 小结	

​	最后让我们来总结一下，并把剩下的内容留到下节课。

- 首先，文件系统是一个位于磁盘的数据结构。我们今天的主要时间都用来介绍这个位于磁盘的数据结构的内容。XV6的这个数据结构实现的很简单，但是你可以实现一个更加复杂的数据结构。
- 其次，我们花了一些时间来看block cache的实现，这对于性能来说是至关重要的，因为读写磁盘是代价较高的操作，可能要消耗数百毫秒，而**block cache确保了如果我们最近从磁盘读取了一个block，那么我们将不会再从磁盘读取相同的block**。

![img](https://pic2.zhimg.com/80/v2-b9eac6ff4e31d448479c2c1520de7dd9_1440w.webp)

​	下节课我将会介绍crash safety，这是文件系统设计中非常棒的一部分。我们将会在crash safety讲两节课。下节课我们会看到基于log实现的crash safety机制，下下节课我们会看到Linux的ext3是如何实现的logging，这种方式要快得多。

# Lecture15 故障恢复Crash Recovery

## 15.1 File system crash概述

> [15.1 File system crash概述 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355199492) <= 图文出处

​	今天的课程是有关文件系统中的Crash safety。<u>这里的Crash safety并不是一个通用的解决方案，而是只关注一个特定的问题的解决方案，也就是crash或者电力故障可能会导致在磁盘上的文件系统处于不一致或者不正确状态的问题</u>。当我说不正确的状态时，是指例如一个data block属于两个文件，或者一个inode被分配给了两个不同的文件。

![img](https://pic3.zhimg.com/v2-559277e61123a4d767eae1db423ed9f2_r.jpg)

​	这个问题可能出现的场景可能是这样，当你在运行make指令时，make与文件系统会有频繁的交互，并读写文件，但是在make执行的过程中断电了，可能是你的笔记本电脑没电了，也可能就是停电了，之后电力恢复之后，你重启电脑并运行ls指令，你会期望你的文件系统仍然在一个好的可用的状态。

![img](https://pic3.zhimg.com/v2-d4b784ef8caafbe72f7383a14889c6d2_r.jpg)

​	这里我们关心的crash或者故障包括了：在文件系统操作过程中的电力故障；在文件系统操作过程中的内核panic。包括XV6在内的大部分内核都会panic，panic可能是由内核bug引起，它会突然导致你的系统故障，但是你肯定期望能够在重启之后还能使用文件系统。

​	你可能会反问，怎么就不能使用文件系统了？文件系统不是存储在一个持久化的存储设备上吗？如果电力故障了，存储设备不会受影响，当电脑恢复运行时，存储设备上的block应该还保存着呀。<u>我们将会看到很多文件系统的操作都包含了多个步骤，如果我们在多个步骤的错误位置crash或者电力故障了，存储在磁盘上的文件系统可能会是一种不一致的状态，之后可能会发生一些坏的事情</u>。而这类问题就是我们今天主要关注的问题。这区别于另一类问题，比如说因为电力故障导致你的磁盘着火了，那么什么数据都没有了，这是一个完全不同的问题，并且有着不同的解决方法，这种情况下需要先备份你的文件系统，然后再重新安装文件系统等等。这个问题我们今天不会关心，<u>今天我们关心的是包含了多个步骤的文件系统操作过程中发生的故障</u>。

​	**我们今天会研究对于这类特定问题的解决方法，也就是logging。这是一个最初来自于数据库世界的很流行的解决方案，现在很多文件系统都在使用logging**。之所以它很流行，是因为它是一个很好用的方法。我们将会看到XV6中的logging实现。当然XV6的实现非常简单，几乎是最简单的实现logging的方法，因为我们只是为了演示关键的思想。但是即使是这么基本的logging实现，也包含了一些微妙的问题，我们将会讨论这些问题，这也是为什么文件系统的logging值得学习的原因。我们将会看到，由于XV6实现的较为简单，XV6中的logging存在一个缺点，它的性能并不咋样，尽管logging系统原则上来说可以获得好的性能。所以**在下节课我们将通过学习Linux的ext3文件系统所使用的logging系统，来看一下如何实现一个高性能logging系统**。

​	另外，这是我们最后一节有关XV6的课程。这节课之后，我们将切换到阅读论文。因为这节课讲完了之后，我们就覆盖了操作系统的基本概念，我们可以通过阅读论文看一些更高级的操作系统思想。

​	接下来让我们看一下这节课关注的场景。类似于创建文件，写文件这样的文件系统操作，都包含了多个步骤的写磁盘操作。我们上节课看过了如何创建一个文件，这里多个步骤的顺序是（注，实际步骤会更多，详见14.5）：

- 分配inode，或者在磁盘上将inode标记为已分配
- 之后更新包含了新文件的目录的data block

​	如果在这两个步骤之间，操作系统crash了。这时可能会使得文件系统的属性被破坏。这里的属性是指，每一个磁盘block要么是空闲的，要么是只分配给了一个文件。即使故障出现在磁盘操作的过程中，我们期望这个属性仍然能够保持。如果这个属性被破坏了，那么重启系统之后程序可能会运行出错，比如：

- 操作系统可能又立刻crash了，因为文件系统中的一些数据结构现在可能处于一种文件系统无法处理的状态。
- 或者，更可能的是操作系统没有crash，但是数据丢失了或者读写了错误的数据。

![img](https://pic2.zhimg.com/80/v2-3419e32b9f788732b782ee11ba195245_1440w.webp)

​	我们将会看一些例子来更好的理解出错的场景，但是基本上来说这就是我们需要担心的一些风险。我不知道你们有没有人在日常使用计算机时经历过这些问题，比如说在电力故障之后，你重启电脑或者手机，然后电脑手机就不能用了，这里的一个原因就是文件系统并没有恢复过来。

## 15.2 File system crash示例

> [15.2 File system crash示例 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355199616) <= 图文出处

​	为了更清晰的理解这里的风险，让我们看一些基于XV6的例子，并看一下哪里可能出错。我们在上节课介绍了XV6有一个非常简单的文件系统和磁盘数据的排布方式。

![img](https://pic3.zhimg.com/80/v2-5b173826f0f444bff0eabafd14628dda_1440w.webp)

​	在super block之后就是log block，我们今天主要介绍的就是log block。log block之后是inode block，每个block可能包含了多个inode。之后是bitmap block，它记录了哪个data block是空闲的。最后是data block，这里包含了文件系统的实际数据。

​	在上节课中，我们看了一下在创建文件时，操作系统与磁盘block的交互过程（注，详见14.5）：

![img](https://pic2.zhimg.com/80/v2-bcf0ed4a192bc6e9afc367718a7e3071_1440w.webp)

​	从上面可以看出，创建一个文件涉及到了多个操作：

- 首先是分配inode，因为首先写的是block 33
- 之后inode被初始化，然后又写了一次block 33
- 之后是写block 46，是将文件x的inode编号写入到x所在目录的inode的data block中
- 之后是更新root inode，因为文件x创建在根目录，所以需要更新根目录的inode的size字段，以包含这里新创建的文件x
- 最后再次更新了文件x的inode

​	现在我们想知道，哪里可能出错。假设我们在下面这个位置出现了电力故障或者内核崩溃。

![img](https://pic1.zhimg.com/80/v2-b71ababa5430c3352388fc40bec594ec_1440w.webp)

​	在出现电力故障之后，因为内存数据保存在RAM中，所有的内存数据都丢失了。所有的进程数据，所有的文件描述符，内存中所有的缓存都没有了，因为内存数据不是持久化的。我们唯一剩下的就是磁盘上的数据，因为磁盘的介质是持久化的，所以只有磁盘上的数据能够在电力故障之后存活。基于这些事实，如果我们在上面的位置出现故障，并且没有额外的机制，没有logging，会有多糟糕呢？我们这里会有什么风险？

​	<u>在这个位置，我们先写了block 33表明inode已被使用，之后出现了电力故障，然后计算机又重启了。这时，我们丢失了刚刚分配给文件x的inode。这个inode虽然被标记为已被分配，但是它并没有放到任何目录中，所以也就没有出现在任何目录中，因此我们也就没办法删除这个inode。所以在这个位置发生电力故障会导致我们丢失inode</u>。

​	你或许会认为，我们应该改一改代码，将写block的顺序调整一下，这样就不会丢失inode了。所以我们可以先写block 46来更新目录内容，之后再写block 32来更新目录的size字段，最后再将block 33中的inode标记为已被分配。

![img](https://pic3.zhimg.com/80/v2-60dc4e2289271701f76250b0376f73f2_1440w.webp)

​	这里的效果是一样的，只是顺序略有不同。并且这样我们应该可以避免丢失inode的问题。那么问题来了，这里可以工作吗？我们应该问问自己，如果在下面的位置发生了电力故障会怎样？

![img](https://pic2.zhimg.com/80/v2-09aa7f8f0866cacc76241f206d9f0ff1_1440w.webp)

​	在这个位置，目录被更新了，但是还没有在磁盘上分配inode（有个问题，如果inode没分配的话，write 46的时候写的是啥）。电力故障之后机器重启，文件系统会是一个什么状态？或者说，如果我们读取根目录下的文件x，会发生什么，因为现在在根目录的data block已经有了文件x的记录？

​	<u>是的，我们会读取一个未被分配的inode，因为inode在crash之前还未被标记成被分配。更糟糕的是，如果inode之后被分配给一个不同的文件，这样会导致有两个应该完全不同的文件共享了同一个inode</u>。如果这两个文件分别属于用户1和用户2，那么用户1就可以读到用户2的文件了。所以上面的解决方案也不好。

​	所以调整写磁盘的顺序并不能彻底解决我们的问题，我们只是从一个问题换到了一个新的问题。

​	让我们再看一个例子，这个例子中会向文件x写入“hi”（注，也就是14.5介绍的第二个部分）

![img](https://pic3.zhimg.com/80/v2-8d46e7f06626e50445e7f04e2c08631a_1440w.webp)

​	一旦成功的创建了文件x，之后会调用write系统调用，我们在上节课看到了write系统调用也执行了多个写磁盘的操作。

- 首先会从bitmap block，也就是block 45中，分配data block，通过从bitmap中分配一个bit，来表明一个data block已被分配。
- 上一步分配的data block是block 595，这里将字符“h”写入到block 595。
- 将字符“i”写入到block 595。
- 最后更新文件夹x的inode来更新size字段。

​	这里我们也可以问自己一个问题，我们在下面的位置crash了会怎样？

![img](https://pic2.zhimg.com/80/v2-ab1b838ad14eb54d260b7d1b8fc256ad_1440w.webp)

​	这里我们从bitmap block中分配了一个data block，但是又还没有更新到文件x的inode中。当我们重启之后，磁盘处于一个特殊的状态，这里的风险是什么？是的，我们这里丢失了data block，因为这个data block被分配了，但是却没有出现在任何文件中，因为它还没有被记录在任何inode中。

​	你或许会想，是因为这里的顺序不对才会导致丢失data block的问题。我们应该先写block 33来更新inode来包含data block 595（同样的问题，这个时候data block都还没有分配怎么知道是595），之后才通过写block 45将data block 595标记为已被分配。

![img](https://pic2.zhimg.com/80/v2-020194b69eb69a37f7aaece29aa24f7d_1440w.webp)

​	所以，为了避免丢失data block，我们将写block的顺序改成这样。现在我们考虑一下，如果故障发生在这两个操作中间会怎样？

![img](https://pic4.zhimg.com/80/v2-adb77bccbf45e6e0d2e49e4918a589a7_1440w.webp)

​	<u>这时inode会认为data block 595属于文件x，但是在磁盘上它还被标记为未被分配的。之后如果另一个文件被创建了，block 595可能会被另一个文件所使用。所以现在两个文件都会在自己的inode中记录block 595。如果两个文件属于两个用户，那么两个用户就可以读写彼此的数据了</u>。很明显，我们不想这样，**文件系统应该确保每一个data block要么属于且只属于一个文件，要么是空闲的。所以这里的修改会导致磁盘block在多个文件之间共享的安全问题，这明显是错误的**。

​	所以**<u>这里的问题并不在于操作的顺序，而在于我们这里有多个写磁盘的操作，这些操作必须作为一个原子操作出现在磁盘上</u>**。

## 15.3 File system logging概述

> [15.3 File system logging - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355199735) <= 图文出处

​	我们这节课要讨论的针对文件系统crash之后的问题的解决方案，其实就是logging。这是来自于数据库的一种解决方案。它有一些好的属性：

- 首先，它可以**确保文件系统的系统调用是原子性的**。比如你调用create/write系统调用，这些系统调用的效果是要么完全出现，要么完全不出现，这样就避免了一个系统调用只有部分写磁盘操作出现在磁盘上。
- 其次，它**支持快速恢复(Fast Recovery)**。在重启之后，我们不需要做大量的工作来修复文件系统，只需要非常小的工作量。这里的快速是相比另一个解决方案来说，在另一个解决方案中，你可能需要读取文件系统的所有block，读取inode，bitmap block，并检查文件系统是否还在一个正确的状态，再来修复。而logging可以有快速恢复的属性。
- 最后，**原则上来说，它可以非常的高效**，尽管我们在XV6中看到的实现不是很高效。

​	我们会在下节课看一下，如何构建一个logging系统，并同时具有**原子性**的系统调用，**快速恢复**和**高性能**，而今天，我们只会关注前两点。

![img](https://pic1.zhimg.com/80/v2-dd8a6b6ef5a067a6b6917324282a254c_1440w.webp)

​	logging的基本思想还是很直观的。首先，你将磁盘分割成两个部分，其中一个部分是log，另一个部分是文件系统，文件系统可能会比log大得多。

​	<u>（log write）当需要更新文件系统时，我们并不是更新文件系统本身。假设我们在内存中缓存了bitmap block，也就是block 45。当需要更新bitmap时，我们并不是直接写block 45，而是将数据写入到log中，并记录这个更新应该写入到block 45。对于所有的写 block都会有相同的操作，例如更新inode，也会记录一条写block 33的log</u>。

![img](https://pic1.zhimg.com/80/v2-51e27b2d0085e2ea4b58074bcb1357c8_1440w.webp)

​	**所以基本上，任何一次写操作都是先写入到log，我们并不是直接写入到block所在的位置，而总是先将写操作写入到log中**。

​	<u>（commit op）之后在某个时间，当文件系统的操作结束了，比如说我们前一节看到的4-5个写block操作都结束，并且都存在于log中，我们会commit文件系统的操作。这意味着我们需要在log的某个位置记录属于同一个文件系统的操作的个数，例如5</u>。

​	<u>（install log）当我们在log中存储了所有写block的内容时，如果我们要真正执行这些操作，只需要将block从log分区移到文件系统分区。我们知道第一个操作该写入到block 45，我们会直接将数据从log写到block45，第二个操作该写入到block 33，我们会将它写入到block 33，依次类推</u>。

​	<u>（clean log）一旦完成了，就可以清除log。清除log实际上就是将属于同一个文件系统的操作的个数设置为0</u>。

![img](https://pic2.zhimg.com/v2-56e6b3a8d712898d2b50187ea279483d_r.jpg)

​	**以上就是log的基本工作方式。为什么这样的工作方式是好的呢？假设我们crash并重启了。在重启的时候，文件系统会查看log的commit记录值，如果是0的话，那么什么也不做。如果大于0的话，我们就知道log中存储的block需要被写入到文件系统中，很明显我们在crash的时候并不一定完成了install log，我们可能是在commit之后，clean log之前crash的。所以这个时候我们需要做的就是reinstall（注，也就是将log中的block再次写入到文件系统），再clean log**。

![img](https://pic4.zhimg.com/v2-f376520353cba27b3ff9b4125fdc7a9f_r.jpg)

​	这里的方法之所以能起作用，是因为可以确保当发生crash（并重启之后），我们要么将写操作所有相关的block都在文件系统中更新了，要么没有更新任何一个block，我们永远也不会只写了一部分block。为什么可以确保呢？我们考虑crash的几种可能情况。

- 在第1步和第2步之间crash会发生什么？在重启的时候什么也不会做，就像系统调用从没有发生过一样，也像crash是在文件系统调用之前发生的一样。这完全可以，并且也是可接受的。
- 在第2步和第3步之间crash会发生什么？在这个时间点，所有的log block都落盘了，因为有commit记录，所以完整的文件系统操作必然已经完成了。我们可以将log block写入到文件系统中相应的位置，这样也不会破坏文件系统。所以这种情况就像系统调用正好在crash之前就完成了。
- 在install（第3步）过程中和第4步之前这段时间crash会发生什么？在下次重启的时候，我们会redo log，我们或许会再次将log block中的数据再次拷贝到文件系统。这样也是没问题的，因为log中的数据是固定的，我们就算重复写了文件系统，每次写入的数据也是不变的。重复写入并没有任何坏处，因为我们写入的数据可能本来就在文件系统中，所以多次install log完全没问题。当然在这个时间点，我们不能执行任何文件系统的系统调用。我们应该在重启文件系统之前，在重启或者恢复的过程中完成这里的恢复操作。<u>换句话说，install log是幂等操作（注，idempotence，表示执行多次和执行一次效果一样），你可以执行任意多次，最后的效果都是一样的</u>。

​	Logging的实现方式有很多，我这里展示的只是一种非常简单的方案，这个方案中clean log和install log都被推迟了。**接下来我会运行这种非常简单的实现方式，之后在下节课我们会看到更加复杂的logging协议。不过所有的<u>这些协议都遵循了write ahead rule，也就是说在写入commit记录之前，你需要确保所有的写操作都在log中</u>。在这个范围内，还有大量设计上的灵活性可以用来设计特定的logging协议**。

​	在XV6中，我们会看到数据有两种状态，是在**磁盘**上还是在**内存**中。内存中的数据会在crash或者电力故障之后丢失。

![img](https://pic3.zhimg.com/v2-187f61b6a9f85d2bd54315d8720d3e16_r.jpg)

​	XV6的log结构如往常一样也是极其的简单。我们在最开始有一个header block，也就是我们的commit record，里面包含了：

- 数字n代表有效的log block的数量
- 每个log block的实际对应的block编号

![img](https://pic3.zhimg.com/80/v2-ee5208b8b6171235d2fcfc5106bd659a_1440w.webp)

​	之后就是log的数据，也就是每个block的数据，依次为bn0对应的block的数据，bn1对应的block的数据以此类推。这就是log中的内容，并且log也不包含其他内容。

![img](https://pic1.zhimg.com/v2-662304c20ded58472f9fd8fe88b4d28c_r.jpg)

​	<u>当文件系统在运行时，在内存中也有header block的一份拷贝，拷贝中也包含了n和block编号的数组。这里的block编号数组就是log数据对应的实际block编号，并且相应的block也会缓存在**block cache**中，这个在Lec14有介绍过</u>。与前一节课对应，log中第一个block编号是45，那么在block cache的某个位置，也会有block 45的cache。

![img](https://pic2.zhimg.com/80/v2-cad61f41707f76fac318178f298fc811_1440w.webp)

​	以上就是内存中的文件系统和磁盘上的文件系统的结构。

---

**问题：因为这里的接口只有read/write，但是如果我们做append操作，就不再安全了，对吧？**

**回答：某种程度来说，append是文件系统层面的操作，在这个层面，我们可以使用上面介绍的logging机制确保其原子性（注，append也可以拆解成底层的read/write）**。

问题：当正在commit log的时候crash了会发生什么？比如说你想执行多个写操作，但是只commit了一半。

回答：在上面的第2步，执行commit操作时，你只会在记录了所有的write操作之后，才会执行commit操作。所以在执行commit时，所有的write操作必然都在log中。<u>而commit操作本身也有个有趣的问题，它究竟会发生什么？如我在前面指出的，**commit操作本身只会写一个block**</u>。**<u>文件系统通常可以这么假设，单个block或者单个sector的write是原子操作（注，有关block和sector的区别详见14.3）。这里的意思是，如果你执行写操作，要么整个sector都会被写入，要么sector完全不会被修改。所以sector本身永远也不会被部分写入，并且commit的目标sector总是包含了有效的数据</u>**。<u>而**commit操作本身只是写log的header**，如果它成功了只是在commit header中写入log的长度，例如5，这样我们就知道log的长度为5。这时crash并重启，我们就知道需要重新install 5个block的log。如果commit header没能成功写入磁盘，那这里的数值会是0。我们会认为这一次事务并没有发生过</u>。**这里本质上是write ahead rule，它表示logging系统在所有的写操作都记录在log中之前，不能install log**。

## 15.4 XV6 logging实现之log_write函数

> [15.4 log_write函数 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355199982) <= 图文出处

​	接下来让我们看一些代码来帮助我们理解这里是怎么工作的。前面我提过事务（transaction），也就是我们不应该在所有的写操作完成之前写入commit record。这意味着**文件系统操作必须表明事务的开始和结束**。在XV6中，以创建文件的sys_open为例（在`sysfile.c`文件中）每个文件系统操作，都有`begin_op()`和`end_op()`分别表示事物的开始和结束。

![img](https://pic2.zhimg.com/v2-b4c43c3920748ca6c29a0aa4d3ea7d19_r.jpg)

​	**`begin_op()`表明想要开始一个事务，在最后有`end_op()`表示事务的结束。并且事务中的所有写block操作具备原子性，这意味着这些写block操作要么全写入，要么全不写入**。<u>XV6中的文件系统调用都有这样的结构，最开始是`begin_op()`，之后是实现系统调用的代码，最后是`end_op()`。在end_op中会实现commit操作</u>。

​	**在`begin_op()`和`end_op()`之间，磁盘上或者内存中的数据结构会更新。但是在`end_op()`之前，并不会有实际的改变（注，也就是不会写入到实际的block中）。在`end_op()`时，我们会将数据写入到log中，之后再写入commit record或者log header**。这里有趣的是，当文件系统调用执行写磁盘时会发生什么？

​	让我们看一下`fs.c`中的ialloc函数。

![img](https://pic3.zhimg.com/v2-5454e423440b0a3842975b358104737e_r.jpg)

​	在这个函数中，并没有直接调用bwrite，这里实际调用的是log_write函数。**log_write是由文件系统的logging实现的方法。任何一个文件系统调用的begin_op和end_op之间的写操作总是会走到log_write**。log_write函数位于`log.c`文件。

![img](https://pic4.zhimg.com/80/v2-08cd04000d64fc866f9bf1e2cc7c52bb_1440w.webp)

​	log_write还是很简单直观的，我们已经向block cache中的某个block写入了数据。比如写block 45，我们已经更新了block cache中的block 45。接下来我们需要在内存中记录，在稍后的commit中，要将block 45写入到磁盘的log中。

​	这里的代码先获取log header的锁，之后再更新log header。首先代码会查看block 45是否已经被log记录了。如果是的话，其实不用做任何事情，因为block 45已经会被写入了。<u>这种忽略的行为称为log absorbtion</u>。如果block 45不在需要写入到磁盘中的block列表中，接下来会对n加1，并将block 45记录在列表的最后。之后，这里会通过调用bpin函数将block 45固定在block cache中，我们稍后会介绍为什么要这么做（注，详见15.8）。

​	以上就是log_write的全部工作了。**任何文件系统调用，如果需要更新block或者说更新block cache中的block，都会将block编号加在这个内存数据中（注，也就是log header在内存中的cache），除非编号已经存在**。

----

问题：这是不是意味着，bwrite不能直接使用？

回答：是的，可以这么认为，文件系统中的所有bwrite都需要被log_write替换。

## 15.5 XV6 logging实现之end_op函数

> [15.5 end_op函数 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355200180) <= 图文出处

​	接下来我们看看位于`log.c`中的end_op函数中会发生什么？

![img](https://pic1.zhimg.com/80/v2-18eb2debc44df2ec6afd1204054aa320_1440w.webp)

​	可以看到，即使是这么简单的一个文件系统也有一些微妙的复杂之处，代码的最开始就是一些复杂情况的处理（注，15.8有这部分的解释）。我直接跳到正常且简单情况的代码。在简单情况下，没有其他的文件系统操作正在处理中。这部分代码非常简单直观，首先调用了commit函数。让我们看一下commit函数的实现。

![img](https://pic2.zhimg.com/80/v2-ef653b61dead31206fe2c474aab18389_1440w.webp)

​	commit中有两个操作：

- **首先是write_log。这基本上就是将所有存在于内存中的log header中的block编号对应的block，从block cache写入到磁盘上的log区域中（注，也就是将变化先从内存拷贝到log中）**。
- **write_head会将内存中的log header写入到磁盘中**。

​	我们看一下write_log的实现。

![img](https://pic2.zhimg.com/v2-b2a781e920c3723a262abecee2261ff1_r.jpg)

​	<u>函数中依次遍历log中记录的block，并写入到log中。它首先读出log block，将cache中的block拷贝到log block，最后再将log block写回到磁盘中。这样可以确保需要写入的block都记录在log中。但是在这个位置，我们还没有commit，现在我们只是将block存放在了log中。如果我们在这个位置也就是在write_head之前crash了，那么最终的表现就像是transaction从来没有发生过</u>。

​	**接下来看一下write_head函数，我之前将write_head称为commit point**。

![img](https://pic1.zhimg.com/80/v2-6cba82f425aa51292ef35ef3f6fa4a10_1440w.webp)

​	函数也比较直观，首先读取log的header block。将n拷贝到block中，将所有的block编号拷贝到header的列表中。最后再将header block写回到磁盘。函数中的倒数第2行，bwrite是实际的commit point吗？如果crash发生在这个bwrite之前，会发生什么？这时虽然我们写了log的header block，但是数据并没有落盘。所以crash并重启恢复时，并不会发生任何事情。

​	**那crash发生在bwrite之后会发生什么呢？这时header会写入到磁盘中，当重启恢复相应的文件系统操作会被恢复。在恢复过程的某个时间点，恢复程序可以读到log header并发现比如说有5个log还没有install，恢复程序可以将这5个log拷贝到实际的位置。所以<u>这里的bwrite就是实际的commit point。在commit point之前，transaction并没有发生，在commit point之后，只要恢复程序正确运行，transaction必然可以完成</u>**。

​	回到commit函数，在commit point之后，就会实际应用transaction。这里很直观，就是读取log block再查看header这个block属于文件系统中的哪个block，最后再将log block写入到文件系统相应的位置。让我们看一下install_trans函数，

![img](https://pic2.zhimg.com/80/v2-b0255fb445d9f369b1e315b92bd7a1cd_1440w.webp)

​	<u>这里先读取log block，再读取文件系统对应的block。将数据从log拷贝到文件系统，最后将文件系统block缓存落盘。这里**实际上就是将block数据从log中拷贝到了实际的文件系统block中**</u>。当然，可能在这里代码的某个位置会出现问题，但是这应该也没问题，因为在恢复的时候，我们会从最开始重新执行过。

​	**在commit函数中，install结束之后，会将log header中的n设置为0，再将log header写回到磁盘中。将n设置为0的效果就是清除log**。

​	以上就是commit内容。

---

问题：install_trans函数在写block的时候，先写的缓存。可不可以优化一下直接写磁盘而不写缓存让代码运行的更快一些？

回答：这里的接口是不太好。你可能会想问反正都要写入新数据，为什么要先读出目标block来。这里的代码肯定还有很多优化空间，但是为了看起来简单我们并没有这么做。

## 15.6 File system recovering

> [15.6 File system recovering - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355200687) <= 图文出处

​	接下来我们看一下发生在XV6的启动过程中的文件系统的恢复流程。当系统crash并重启了，在XV6启动过程中做的一件事情就是调用initlog函数。

![img](https://pic3.zhimg.com/80/v2-32f99672fb5abeeae484493761011c0a_1440w.webp)

​	initlog基本上就是调用recover_from_log函数。

![img](https://pic1.zhimg.com/80/v2-67bf59e83e86be49fe9487af5d540cac_1440w.webp)

​	**recover_from_log先调用read_head函数从磁盘中读取header，之后调用install_trans函数。这个函数之前在commit函数中也调用过，它就是读取log header中的n，然后根据n将所有的log block拷贝到文件系统的block中。recover_from_log在最后也会跟之前一样清除log**。

​	<u>这就是恢复的全部流程。如果我们在install_trans函数中又crash了，也不会有问题，因为之后再重启时，XV6会再次调用initlog函数，再调用recover_from_log来重新install log。如果我们在commit之前crash了多次，在最终成功commit时，log可能会install多次</u>。

---

问题：如果一个进程向磁盘写了一些数据，但是在commit之前进程出现了故障，假设故障之后进程退出了，这样会有问题吗？

回答：简单回答是没问题，因此磁盘不会被更新，所以效果就像文件系统操作没有发生过一样。并且进程并不能在故障后恢复，唯一能在故障之后还能保持的是保存在磁盘中的状态。（注，应该是没有理解问题。进程通过write系统调用成功写入的数据，就算在成功落盘之前进程异常退出了，内核还是会写入到磁盘中，前提是内核还在运行。）

## 15.7 XV6的log写磁盘过程

> [15.7 Log写磁盘流程 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355200891) <= 图文出处

​	我已经在bwrite函数中加了一个print语句。bwrite函数是block cache中实际写磁盘的函数，所以我们将会看到实际写磁盘的记录。在上节课（Lec 14）我将print语句放在了log_write中，log_write只能代表文件系统操作的记录，并不能代表实际写磁盘的记录。我们这里会像上节课一样执行`echo "hi" > x`，并看一下实际的写磁盘过程。

![img](https://pic1.zhimg.com/80/v2-96478024d83797a8f01d79fdd727a850_1440w.webp)

​	很明显这里的记录要比只在log_write中记录要长的多。之前的log_write只有11条记录（注，详见14.5）但是可以看到实际上背后有很多个磁盘写操作，让我们来分别看一下这里的写磁盘操作：

- 首先是前3行的bwrite 3，4，5。因为block 3是第一个log data block，所以前3行是在log中记录了3个写操作。这3个写操作都保存在log中，并且会写入到磁盘中的log部分。
- 第4行的bwrite 2。因为block 2是log的起始位置，也就是log header，所以这条是commit记录。
- 第5，6，7行的bwrite 33，46，32。这里实际就是将前3行的log data写入到实际的文件系统的block位置，这里实际是install log。
- 第8行的bwrite 2，是清除log（注，也就是将log header中的n设置为0）。到此为止，完成了实际上的写block 33，46，32这一系列的操作。第一部分是log write，第二部分是install log，每一部分后面还跟着一个更新commit记录（注，也就是commit log和clean log）。

​	所以以上就是XV6中文件系统的logging介绍，即使是这么一个简单的logging系统也有一定的复杂度。<u>这里立刻可以想到的一个问题是，通过观察这些记录，这是一个很有效的实现吗？很明显不是的，因为数据被写了两次。如果我写一个大文件，我需要在磁盘中将这个大文件写两次。所以这必然不是一个高性能的实现，为了实现Crash safety我们将原本的性能降低了一倍。当你们去读ext3论文时，你们应该时刻思考如何避免这里的性能降低一倍的问题</u>。

---

**问题：可以从这里的记录找到一次文件操作的begin_op和end_op位置吗？**

**回答：大概可以知道。我们实际上不知道begin_op的位置，但是所有的文件系统操作都从begin_op开始。更新commit记录必然在end_op中，所以我们可以找到文件系统操作的end_op位置，之后就是begin_op（注，其实这里所有的操作都在end_op中，只需要区分每一次end_op的调用就可以找到begin_op）。**

## 15.8 File System challenges

> [15.8 File system challenges - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/355201082) <= 图文出处

​	前面说到XV6的文件系统有一定的复杂性，接下来我将介绍一下三个复杂的地方或者也可以认为是三个挑战。

​	第一个是**cache eviction**。<u>假设transaction还在进行中，我们刚刚更新了block 45，正要更新下一个block，而整个buffer cache都满了并且决定撤回block 45。在buffer cache中撤回block 45意味着我们需要将其写入到磁盘的block 45位置，这里会不会有问题？如果我们这么做了的话，会破坏什么规则吗？是的，如果将block 45写入到磁盘之后发生了crash，就会破坏transaction的原子性。这里也破坏了前面说过的write ahead rule，write ahead rule的含义是，你需要先将所有的block写入到log中，之后才能实际的更新文件系统block</u>。所以**buffer cache不能撤回任何还位于log的block**。

​	<u>前面在介绍log_write函数时，其中调用了一个叫做bpin的函数，这个函数的作用就如它的名字一样，将block固定在buffer cache中。它是通过给block cache增加引用计数来避免cache撤回对应的block</u>。**在之前（注，详见14.6）我们看过，如果引用计数不为0，那么buffer cache是不会撤回block cache的**。<u>相应的在将来的某个时间，所有的数据都写入到了log中，我们可以在cache中unpin block（注，在15.5中的install_trans函数中会有unpin，因为这时block已经写入到了log中）</u>。所以这是第一个复杂的地方，**我们需要pin/unpin buffer cache中的block**。

![img](https://pic4.zhimg.com/v2-9032082fa8101a9d142fb9165ebee227_r.jpg)

​	第二个挑战是，**文件系统操作必须适配log的大小**。在XV6中，总共有30个log block（注，详见14.3）。当然我们可以提升log的尺寸，在真实的文件系统中会有大得多的log空间。但是无所谓啦，不管log多大，文件系统操作必须能放在log空间中。如果一个文件系统操作尝试写入超过30个block，那么意味着部分内容需要直接写到文件系统区域，而这是不被允许的，因为这违背了write ahead rule。所以所有的文件系统操作都必须适配log的大小。

​	为什么XV6的log大小是30？因为30比任何一个文件系统操作涉及的写操作数都大，Robert和我看了一下所有的文件系统操作，发现都远小于30，所以就将XV6的log大小设为30。我们目前看过的一些文件系统操作，例如创建一个文件只包含了写5个block。实际上大部分文件系统操作只会写几个block。你们可以想到什么样的文件系统操作会写很多很多个block吗？是的，写一个大文件。如果我们调用write系统调用并传入1M字节的数据，这对应了写1000个block，这看起来会有很严重的问题，因为这破坏了我们刚刚说的“文件系统操作必须适配log的大小”这条规则。

​	让我们看一下`file.c`文件中的file_write函数。

![img](https://pic2.zhimg.com/80/v2-de768b37f96ba519699c57253f83a2d1_1440w.webp)

​	<u>从这段代码可以看出，如果写入的block数超过了30，那么一个写操作会被分割成多个小一些的写操作。这里整个写操作不是原子的，但是这还好啦，因为write系统调用的语义并不要求所有1000个block都是原子的写入，它只要求我们不要损坏文件系统。所以**XV6会将一个大的写操作分割成多个小的写操作，每一个小的写操作通过独立的transaction写入。这样文件系统本身不会陷入不正确的状态中**</u>。

​	这里还需要注意，**<u>因为block在落盘之前需要在cache中pin住，所以buffer cache的尺寸也要大于log的尺寸</u>**。

![img](https://pic4.zhimg.com/80/v2-4f8fb15ca08d3cc26e5ebe33ac567b8b_1440w.webp)

​	最后一个要讨论的挑战是**并发文件系统调用**。让我先来解释一下这里会有什么问题，再看对应的解决方案。假设我们有一段log，和两个并发的执行的transaction，其中transaction t0在log的前半段记录，transaction t1在log的后半段记录。可能我们用完了log空间，但是任何一个transaction都还没完成。

![img](https://pic3.zhimg.com/80/v2-a362ca89db56ead3e6f5956fc10a1ca6_1440w.webp)

​	现在我们能提交任何一个transaction吗？我们不能，因为这样的话我们就提交了一个部分完成的transaction，这违背了write ahead rule，log本身也没有起到应该的作用。所以**必须要保证多个并发transaction加在一起也适配log的大小**。所以**当我们还没有完成一个文件系统操作时，我们必须在确保可能写入的总的log数小于log区域的大小的前提下，才允许另一个文件系统操作开始**。

​	**XV6通过限制并发文件系统操作的个数来实现这一点**。<u>在begin_op中，我们会检查当前有多少个文件系统操作正在进行。如果有太多正在进行的文件系统操作，我们会通过sleep停止当前文件系统操作的运行，并等待所有其他所有的文件系统操作都执行完并commit之后再唤醒</u>。**这里的其他所有文件系统操作都会一起commit。有的时候这被称为<u>group commit</u>，因为这里将多个操作像一个大的transaction一样提交了，这里的多个操作要么全部发生了，要么全部没有发生**。

![img](https://pic3.zhimg.com/80/v2-b4a1b28c4855780eb23514e657cfdd2e_1440w.webp)

​	最后我们再回到最开始，看一下begin_op。

![img](https://pic1.zhimg.com/v2-8e5d54ff96e9067b3eb556d6a7172d6c_r.jpg)

​	<u>首先，如果log正在commit过程中，那么就等到log提交完成，因为我们不能在install log的过程中写log；其次，如果当前操作是允许并发的操作个数的后一个，那么当前操作可能会超过log区域的大小，我们也需要sleep并等待所有之前的操作结束；最后，如果当前操作可以继续执行，需要将log的outstanding字段加1，最后再退出函数并执行文件系统操作</u>。

​	再次看一下end_op函数。

![img](https://pic3.zhimg.com/v2-d4628474419b78fb753f9fc2e5f93b06_r.jpg)

​	<u>在最开始首先会对log的outstanding字段减1，因为一个transaction正在结束；其次检查commiting状态，当前不可能在commiting状态，所以如果是的话会触发panic；如果当前操作是整个并发操作的最后一个的话（log.understanding == 0），接下来立刻就会执行commit；如果当前操作不是整个并发操作的最后一个的话，我们需要唤醒在begin_op中sleep的操作，让它们检查是不是能运行</u>。

​	（注，这里的understanding有点迷，它表示的是当前正在并发执行的文件系统操作的个数，MAXOPBLOCKS定义了一个操作最大可能涉及的block数量。在begin_op中，只要log空间还足够，就可以一直增加并发执行的文件系统操作。所以XV6是通过设定了MAXOPBLOCKS，再间接的限定支持的并发文件系统操作的个数）

​	所以，即使是XV6中这样一个简单的文件系统，也有一些复杂性和挑战。

​	最后简单小结一下：

​	**这节课讨论的是使用logging来解决crash safety或者说多个步骤的文件系统操作的安全性。这种方式对于安全性来说没有问题，但是性能不咋地**。

----

问题：group commit有必要吗？不能当一个文件系统操作结束的时候就commit掉，然后再commit其他的操作吗？

回答：如果这样的话你需要非常非常小心。因为有一点我没有说得很清楚，我们需要保证write系统调用的顺序。如果一个read看到了一个write，再执行了一次write，那么第二个write必须要发生在第一个write之后。<u>在log中的顺序，本身就反应了write系统调用的顺序，你不能改变log中write系统调用的执行顺序，因为这可能会导致对用户程序可见的奇怪的行为。所以必须以transaction发生的顺序commit它们，而一次性提交所有的操作总是比较安全的，这可以保证文件系统处于一个好的状态</u>。

问题：前面说到cache size至少要跟log size一样大，如果它们一样大的话，并且log pin了30个block，其他操作就不能再进行了，因为buffer中没有额外的空间了。

回答：如果buffer cache中没有空间了，XV6会直接panic。这并不理想，实际上有点恐怖。所以我们在挑选buffer cache size的时候希望用一个不太可能导致这里问题的数字。<u>这里为什么不能直接返回错误，而是要panic？因为很多文件系统操作都是多个步骤的操作，假设我们执行了两个write操作，但是第三个write操作找不到可用的cache空间，那么第三个操作无法完成，**我们不能就直接返回错误，因为我们可能已经更新了一个目录的某个部分，为了保证文件系统的正确性，我们需要撤回之前的更新**。所以如果log pin了30个block，并且buffer cache没有额外的空间了，会直接panic。当然这种情况不太会发生，只有一些极端情况才会发生</u>。

# Lecture16 文件系统性能和快速崩溃恢复(File System Performance and Fast Crash Recovery)

## 16.1 为什么需要logging

> [16.1 Why logging - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/358793935) <= 图文出处

​	今天这节课也是讲解文件系统的logging，这节课讲的是Linux中的广泛使用的ext3文件系统所使用的logging系统，同时我们也会讨论在高性能文件系统中添加log需要面对的一些问题。首先我会花几分钟来回顾一下，为什么我们要学习logging。

- 之所以我们认为logging很重要，是因为这是一个巨大的成功和重要的思想，几乎可以认为logging是一种魔法，它可以用在任何一个已知的存储系统的故障恢复流程中，它在很多地方都与你想存储的任何数据相关。所以你们可以在大量的存储场景中看到log，比如说数据库，文件系统，甚至一些需要在crash之后恢复的定制化的系统中。
- 你们也可以发现log作为从故障中恢复的机制，在分布式系统中也有大量的应用。因为log是一种可以用来表示所有在crash之前发生事情的数据结构，如果能够理解log，那么就可以更容易的从crash中恢复过来。
- 除此之外，当你尝试构建高性能logging系统时，log本身也有大量有意思的地方。

![img](https://pic4.zhimg.com/80/v2-c2b77039be74320f83be6b785f091edb_1440w.webp)

​	这里有一个术语，当我们谈到log时，与今天阅读的[论文](https://pdos.csail.mit.edu/6.828/2020/readings/journal-design.pdf)所用到的journal是同一件事情，它们是同义词。除此之外，今天的论文是有关向ext2增加journal，并得到ext3文件系统（注，所以可以认为ext3文件系统就是ext2加上了logging系统）。

​	接下来我将从这几个方面来讨论ext3文件系统：某种程度上将其与XV6进行对比；解释ext3是如何修复XV6的logging存在的性能问题；解释ext3在故障恢复时语义上的一些变化。

## 16.2 XV6 File system logging回顾

> [16.2 XV6 File system logging回顾 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/358794117) <= 图文出处

​	首先来回顾一下XV6的logging系统。我们有一个磁盘用来存储XV6的文件系统，你可以认为磁盘分为了两个部分：

+ 首先是文件系统目录的树结构，以root目录为根节点，往下可能有其他的目录，我们可以认为目录结构就是一个树状的数据结构。假设root目录下有两个子目录D1和D2，D1目录下有两个文件F1和F2，每个文件又包含了一些block。除此之外，还有一些其他并非是树状结构的数据，比如bitmap表明了每一个data block是空闲的还是已经被分配了。<u>inode，目录内容，bitmap block，我们将会称之为metadata block（注，Frans和Robert在这里可能有些概念不统一，对于Frans来说，目录内容应该也属于文件内容，目录是一种特殊的文件，详见14.3；而对于Robert来说，目录内容是metadata。），另一类就是持有了文件内容的block，或者叫data block</u>。

![img](https://pic2.zhimg.com/80/v2-cbe312078a3d38316be4340c0b0daed1_1440w.webp)

+ 除了文件系统之外，XV6在磁盘最开始的位置还有一段log。XV6的log相对来说比较简单，它有header block，之后是一些包含了有变更的文件系统block，这里可以是metadata block也可以是data block。header block会记录之后的每一个log block应该属于文件系统中哪个block，假设第一个log block属于block 17，第二个属于block 29。

![img](https://pic2.zhimg.com/80/v2-f8d70edbc62401194a70acdaba7d0995_1440w.webp)

​	在计算机上，我们会有一些用户程序调用write/create系统调用来修改文件系统。<u>在内核中存在block cache，最初write请求会被发到block cache。block cache就是磁盘中block在内存中的拷贝，所以最初对于文件block或者inode的更新走到了block cache</u>。

![img](https://pic1.zhimg.com/80/v2-ff73655d60017858eee56f4522638fc0_1440w.webp)

​	在write系统调用的最后，这些更新都被拷贝到了log中，之后我们会更新header block的计数来表明当前的transaction已经结束了。在文件系统的代码中，任何修改了文件系统的系统调用函数中，某个位置会有begin_op，表明马上就要进行一系列对于文件系统的更新了，不过在完成所有的更新之前，不要执行任何一个更新。在begin_op之后是一系列的read/write操作。最后是end_op，用来告诉文件系统现在已经完成了所有write操作。**所以在begin_op和end_op之间，所有的write block操作只会走到block cache中。当系统调用走到了end_op函数，文件系统会将修改过的block cache拷贝到log中**。

![img](https://pic4.zhimg.com/80/v2-87561b4875fd11f842f80dd75f4b67d7_1440w.webp)

​	在拷贝完成之后，文件系统会将修改过的block数量，通过一个磁盘写操作写入到log的header block，这次写入被称为**commit point**。<u>在commit point之前，如果发生了crash，在重启时，整个transaction的所有写磁盘操作最后都不会应用。在commit point之后，即使立即发生了crash，重启时恢复软件会发现在log header中记录的修改过的block数量不为0，接下来就会将log header中记录的所有block，从log区域写入到文件系统区域</u>。

​	**这里实际上使得系统调用中位于begin_op和end_op之间的所有写操作在面对crash时具备原子性**。<u>也就是说，要么文件系统在crash之前更新了log的header block，这样所有的写操作都能生效；要么crash发生在文件系统更新log的header block之前，这样没有一个写操作能生效</u>。

​	在crash并重启时，必须有一些恢复软件能读取log的header block，并判断里面是否记录了未被应用的block编号，如果有的话，需要写（也有可能是重写）log block到文件系统中对应的位置；如果没有的话，恢复软件什么也不用做。

​	这里有几个超级重要的点，不仅针对XV6，对于大部分logging系统都适用：

- **包括XV6在内的所有logging系统，都需要遵守write ahead rule**。这里的意思是，<u>任何时候如果一堆写操作需要具备原子性，系统需要先将所有的写操作记录在log中，之后才能将这些写操作应用到文件系统的实际位置</u>。也就是说，我们需要预先在log中定义好所有需要具备原子性的更新，之后才能应用这些更新。**write ahead rule是logging能实现故障恢复的基础。write ahead rule使得一系列的更新在面对crash时具备了原子性**。
- 另一点是，**XV6对于不同的系统调用复用的是同一段log空间，但是直到log中所有的写操作被更新到文件系统之前，我们都不能释放或者重用log。我将这个规则称为freeing rule，它表明我们不能覆盖或者重用log空间，直到保存了transaction所有更新的这段log，都已经反应在了文件系统中**。

​	所以在XV6中，end_op做了大量的工作，首先是将所有的block记录在log中，之后是更新log header。在没有crash的正常情况，文件系统需要再次将所有的block写入到磁盘的文件系统中。磁盘中的文件系统更新完成之后，XV6文件系统还需要删除header block记录的变更了的block数量，以表明transaction已经完成了，之后就可以重用log空间。

​	在向log写入任何新内容之前，删除header block中记录的block数量也很重要。因为你不会想要在header block中记录的还是前一个transaction的信息，而log中记录的又是一个新的transaction的数据。可以假设新的transaction对应的是与之前不同的block编号的数据，这样的话，在crash重启时，log中的数据会被写入到之前记录的旧的block编号位置。所以我们必须要先清除header block。

​	<u>freeing rule的意思就是，在从log中删除一个transaction之前，我们必须将所有log中的所有block都写到文件系统中</u>。

![img](https://pic2.zhimg.com/80/v2-3f105b47a44bda9ec3abadd1e592b9bd_1440w.webp)

​	这些规则使得，就算一个文件系统更新可能会复杂且包含多个写操作，但是每次更新都是原子的，在crash并重启之后，要么所有的写操作都生效，要么没有写操作能生效。

​	要介绍Linux的logging方案，就需要了解XV6的logging有什么问题？为什么Linux不使用与XV6完全一样的logging方案？这里的回答简单来说就是XV6的logging太慢了。

​	**XV6中的任何一个例如create/write的系统调用，需要在整个transaction完成之后才能返回。所以在创建文件的系统调用返回到用户空间之前，它需要完成所有end_op包含的内容**，这包括了：

- 将所有更新了的block写入到log
- 更新header block
- 将log中的所有block写回到文件系统分区中
- 清除header block

​	之后才能从系统调用中返回。在任何一个文件系统调用的commit过程中，不仅是占据了大量的时间，而且其他系统调用也不能对文件系统有任何的更新。所以这里的系统调用实际上是一次一个的发生，而每个系统调用需要许多个写磁盘的操作。这里每个系统调用需要等待它包含的所有写磁盘结束，对应的技术术语被称为**synchronize**。**XV6的系统调用对于写磁盘操作来说是同步的（synchronized），所以它非常非常的慢**。在使用机械硬盘时，它出奇的慢，因为每个写磁盘都需要花费10毫秒，而每个系统调用又包含了多个写磁盘操作。所以XV6每秒只能完成几个更改文件系统的系统调用。如果我们在SSD上运行XV6会快一些，但是离真正的高效还差得远。

​	**另一件需要注意的更具体的事情是，在XV6的logging方案中，每个block都被写了两次。第一次写入到了log，第二次才写入到实际的位置。虽然这么做有它的原因，但是ext3可以一定程度上修复这个问题**。

## 16.3 Ext3 File System log fotmat

> [16.3 ext3 file system log format - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/358794236) <= 图文出处
>
> [Linux 中 ext、ext2、ext3、ext4 文件系统介绍与区别_早知晓的博客-CSDN博客_ext2 ext4](https://blog.csdn.net/piupiu78/article/details/118691195)
>
> [Ext4_百度百科 (baidu.com)](https://baike.baidu.com/item/Ext4/1858450?fr=aladdin)
>
> [xfs_百度百科 (baidu.com)](https://baike.baidu.com/item/xfs/8713636?fr=aladdin)

​	ext3文件系统就是基于今天要阅读的[论文](https://pdos.csail.mit.edu/6.828/2020/readings/journal-design.pdf)，再加上几年的开发得到的，并且ext3也曾经广泛的应用过。ext3是针对之前一种的文件系统（ext2）logging方案的修改，所以ext3就是在几乎不改变之前的ext2文件系统的前提下，在其上增加一层logging系统。所以某种程度来说，logging是一个容易升级的模块。

​	e<u>xt3的数据结构与XV6是类似的。在内存中，存在block cache，这是一种write-back cache（注，区别于write-through cache，指的是cache稍后才会同步到真正的后端）</u>。block cache中缓存了一些block，其中的一些是干净的数据，因为它们与磁盘上的数据是一致的；其他一些是脏数据，因为从磁盘读出来之后被修改过；有一些被固定在cache中，基于前面介绍的write-ahead rule和freeing rule，不被允许写回到磁盘中。

![img](https://pic4.zhimg.com/80/v2-3397ec2773f46729b050a8f1fdb90b3f_1440w.webp)

​	除此之外，**ext3还维护了一些transaction信息。它可以维护多个在不同阶段的transaction的信息**。

​	**每个transaction的信息**包含有：

- 一个序列号
- 一系列该transaction修改的block编号。<u>这些block编号指向的是在cache中的block，因为任何修改最初都是在cache中完成</u>。
- 以及一系列的**handle**，<u>handle对应了系统调用，并且这些系统调用是transaction的一部分，会读写cache中的block</u>

![img](https://pic3.zhimg.com/80/v2-414d954c8d57a6a0b5646b093c739826_1440w.webp)

​	在磁盘上，与XV6一样：

- 会有一个文件系统树，包含了inode，目录，文件等等
- 会有bitmap block来表明每个data block是被分配的还是空闲的
- 在磁盘的一个指定区域，会保存log

![img](https://pic1.zhimg.com/80/v2-cbc2409faa5fef8f39d25f5e685464a8_1440w.webp)

​	目前为止，这与XV6非常相似。**主要的区别在于ext3可以同时跟踪多个在不同执行阶段的transaction**。

​	接下来我们详细看一下ext3的log中有什么，这与XV6中的log有点不一样。**在log的最开始，是super block。这是log的super block，而不是文件系统的super block。<u>log的super block包含了log中第一个有效的transaction的起始位置(block编号)和序列号</u>**。起始位置就是磁盘上log分区的block编号，序列号就是前面提到的每个transaction都有的序列号。**log是磁盘上一段固定大小的连续的block**。<u>log中，除了super block以外的block存储了transaction。每个transaction在log中包含了</u>：

- 一个descriptor block，其中包含了log数据对应的实际block编号，这与XV6中的header block很像。
- 之后是针对每一个block编号的更新数据。
- <u>最后当一个transaction完成并commit了，会有一个commit block</u>

![img](https://pic1.zhimg.com/80/v2-4d487c26de906e92162b63a0e0aa6570_1440w.webp)

​	**因为log中可能有多个transaction，commit block之后可能会跟着下一个transaction的descriptor block，data block和commit block**。所以log可能会很长并包含多个transaction。我们可以认为super block中的起始位置和序列号属于最早的，排名最靠前的，并且是有效的transaction。

![img](https://pic3.zhimg.com/80/v2-569b15b8f56f1c667d973e331e282a9e_1440w.webp)

​	这里有一些细节对于后面的内容很重要。**在crash之后的恢复过程会扫描log，<u>为了将descriptor block和commit block与data block区分开，descriptor block和commit block会以一个32bit的魔法数字(magic number)作为起始</u>。这个魔法数字不太可能出现在数据中，并且可以帮助恢复软件区分不同的block**。

![img](https://pic4.zhimg.com/80/v2-80e7150861344dfa0060f77e6357cda3_1440w.webp)

​	记住这里的log的结构，它对于后面的内容也很重要（上面表示log也是一个环形的）。

---

**问题：有没有可能使用一个descriptor block管理两个transaction？是不是只能一个transaction结束了才能开始下一个transaction？**

回答：Log中会有多个transaction，但是的确**一个时间只有一个正在进行的transaction**。上面的图片没能很好的说明这一点，当前正在进行的transaction对应的是正在执行写操作的系统调用。所以当前正在进行的transaction只存在于内存中，对应的系统调用只会更新cache中的block，也就是内存中的文件系统block。**当ext3决定结束当前正在进行的transaction，它会做两件事情：首先开始一个新的transaction，这将会是下一个transaction；其次将刚刚完成的transaction写入到磁盘中，这可能要花一点时间**。所以完整的故事是，磁盘上的log分区有一系列旧的transaction，这些transaction已经commit了，除此之外，还有一个位于内存的正在进行的transaction。<u>在磁盘上的transaction，只能以log记录的形式存在，并且还没有写到对应的文件系统block中</u>。**logging系统在后台会从最早的transaction开始，将transaction中的data block写入到对应的文件系统中。当整个transaction的data block都写完了，之后logging系统才能释放并重用log中的空间**。所以**<u>log其实是个循环的数据结构，如果用到了log的最后，logging系统会从log的最开始位置重新使用。</u>**

## 16.4 Ext3如何提升性能

> [16.4 ext3如何提升性能 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/358794358) <= 图文出处
>
> [fsync_百度百科 (baidu.com)](https://baike.baidu.com/item/fsync/5008353?fr=aladdin) fsync函数同步内存中所有已修改的文件数据到储存设备。

​	ext3通过3种方式提升了性能：

- 首先，它提供了**异步的（asynchronous）系统调用**，也就是说系统调用在写入到磁盘之前就返回了，系统调用只会更新缓存在内存中的block，并不用等待写磁盘操作。不过它可能会等待读磁盘。
- 第二，它提供了**批量执行（batching）**的能力，可以将多个系统调用打包成一个transaction。
- 最后，它提供了**并发（concurrency）**。

![img](https://pic1.zhimg.com/v2-55605140d06bc6b8d5570ad86859b95c_r.jpg)

​	这些基本上就是ext3有的，而XV6没有的特性。接下来我将一一介绍这里的特性。

​	首先是**异步的系统调用。这表示系统调用修改完位于缓存中的block之后就返回，并不会触发写磁盘**。所以这里明显的优势就是系统调用能够快速的返回。同时它也使得I/O可以并行的运行，也就是说应用程序可以调用一些文件系统的系统调用，但是应用程序可以很快从系统调用中返回并继续运算，与此同时文件系统在后台会并行的完成之前的系统调用所要求的写磁盘操作。这被称为**I/O concurrency**。<u>如果没有异步系统调用，很难获得I/O concurrency，或者说很难同时进行磁盘操作和应用程序运算，因为同步系统调用中，应用程序总是要等待磁盘操作结束才能从系统调用中返回</u>。

​	**另一个异步系统调用带来的好处是，它使得大量的批量执行变得容易**。

​	**异步系统调用的缺点是系统调用的返回并不能表示系统调用应该完成的工作实际完成了**。举个例子，如果你创建了一个文件并写了一些数据然后关闭文件并在console向用户输出done，最后你把电脑的电给断了。尽管所有的系统调用都完成了，程序也输出了done，但是在你重启之后，你的数据并不一定存在。这意味着，在异步系统调用的世界里，如果应用程序关心可能发生的crash，那么应用程序代码应该更加的小心。这在XV6并不是什么大事，因为如果XV6中的write返回了，那么数据就在磁盘上，crash之后也还在。而ext3中，如果write返回了，你完全不能确定crash之后数据还在不在。所以一些应用程序的代码应该仔细编写，例如对于数据库，对于文本编辑器，我如果写了一个文件，我不想在我写文件过程断电然后再重启之后看到的是垃圾文件或者不完整的文件，我想看到的要么是旧的文件，要么是新的文件。

​	**所以文件系统对于这类应用程序也提供了一些工具以确保在crash之后可以有预期的结果。这里的工具是一个系统调用，叫做fsync，所有的UNIX都有这个系统调用**。<u>这个系统调用接收一个文件描述符作为参数，它会告诉文件系统去完成所有的与该文件相关的写磁盘操作，在所有的数据都确认写入到磁盘之后，fsync才会返回</u>。所以如果你查看数据库，文本编辑器或者一些非常关心文件数据的应用程序的源代码，你将会看到精心放置的对于fsync的调用。fsync可以帮助解决异步系统调用的问题。对于大部分程序，例如编译器，如果crash了编译器的输出丢失了其实没什么，所以许多程序并不会调用fsync，并且乐于获得异步系统调用带来的高性能。

![img](https://pic4.zhimg.com/80/v2-90c5694b6cc941ea7cba89ba06d4ddcf_1440w.webp)

​	以上就是异步系统调用，下一个ext3使用的技术是**批量执行（batching）**。**<u>在任何时候，ext3只会有一个open transaction</u>**。ext3中的一个transaction可以包含多个不同的系统调用。所以ext3是这么工作的：它首先会宣告要开始一个新的transaction，接下来的几秒所有的系统调用都是这个大的transaction的一部分。我认为默认情况下，ext3每5秒钟都会创建一个新的transaction，所以每个transaction都会包含5秒钟内的系统调用，这些系统调用都打包在一个transaction中。在5秒钟结束的时候，ext3会commit这个包含了可能有数百个更新的大transaction。

​	为什么这是个好的方案呢？

- 首先它在多个系统调用之间分摊了transaction带来的固有的损耗。固有的损耗包括写transaction的descriptor block和commit block；在一个机械硬盘中需要查找log的位置并等待磁碟旋转，这些都是成本很高的操作，<u>现在只需要对一批系统调用执行一次，而不用对每个系统调用执行一次这些操作，所以batching可以降低这些损耗带来的影响</u>。
- 另外，**它可以更容易触发write absorption。经常会有这样的情况，你有一堆系统调用最终在反复更新相同的一组磁盘block**。举个例子，如果我创建了一些文件，我需要分配一些inode，inode或许都很小只有64个字节，一个block包含了很多个inode，所以同时创建一堆文件只会影响几个block的数据。类似的，如果我向一个文件写一堆数据，我需要申请大量的data block，我需要修改表示block空闲状态的bitmap block中的很多个bit位，如果我分配到的是相邻的data block，它们对应的bit会在同一个bitmap block中，所以我可能只是修改一个block的很多个bit位。所以一堆系统调用可能会反复更新一组相同的磁盘block。**通过batching，多次更新同一组block会先快速的在内存的block cache中完成，之后在transaction结束时，一次性的写入磁盘的log中。这被称为write absorption，相比一个类似于XV6的同步文件系统，它可以极大的减少写磁盘的总时间**。
- 最后就是**disk scheduling**。假设我们要向磁盘写1000个block，不论是在机械硬盘还是SSD（机械硬盘效果会更好），<u>一次性的向磁盘的连续位置写入1000个block，要比分1000次每次写一个不同位置的磁盘block快得多。我们写log就是向磁盘的连续位置写block。通过向磁盘提交大批量的写操作，可以更加的高效</u>。这里我们不仅通过向log中连续位置写入大量block来获得更高的效率，甚至当我们向文件系统分区写入包含在一个大的transaction中的多个更新时，如果我们能将大量的写请求同时发送到驱动，即使它们位于磁盘的不同位置，我们也使得磁盘可以调度这些写请求，并以特定的顺序执行这些写请求，这也很有效。**在一个机械硬盘上，如果一次发送大量需要更新block的写请求，驱动可以对这些写请求根据轨道号排序。甚至在一个固态硬盘中，通过一次发送给硬盘大量的更新操作也可以稍微提升性能**。**<u>所以，只有发送给驱动大量的写操作，才有可能获得disk scheduling</u>**。这是batching带来的另一个好处。

![img](https://pic2.zhimg.com/80/v2-856f29af4527f1a31a9f12d800ddb141_1440w.webp)

​	ext3使用的最后一个技术就是concurrency，相比XV6这里包含了两种concurrency。

- **首先ext3允许多个系统调用同时执行，所以我们可以有并行执行的多个不同的系统调用**。<u>在ext3决定关闭并commit当前的transaction之前，系统调用不必等待其他的系统调用完成，它可以直接修改作为transaction一部分的block</u>。许多个系统调用都可以并行的执行，并向当前transaction增加block，这在一个多核计算机上尤其重要，因为我们不会想要其他的CPU核在等待锁。在XV6中，如果当前的transaction还没有完成，新的系统调用不能继续执行。而**在ext3中，大多数时候多个系统调用都可以更改当前正在进行的transaction**。

- **另一种ext3提供的并发是，可以有多个不同状态的transaction同时存在。所以<u>尽管只有一个open transaction可以接收系统调用，但是其他之前的transaction可以并行的写磁盘</u>**。这里可以并行存在的不同transaction状态包括了：

  + 首先是一个open transaction

  - 若干个正在commit到log的transaction，我们并不需要等待这些transaction结束。当之前的transaction还没有commit并还在写log的过程中，新的系统调用仍然可以在当前的open transaction中进行。

  - 若干个正在从cache中向文件系统block写数据的transaction

  - 若干个正在被释放的transaction，这个并不占用太多的工作


​	**<u>通常来说会有位于不同阶段的多个transaction，新的系统调用不必等待旧的transaction提交到log或者写入到文件系统</u>。对比之下，XV6中新的系统调用就需要等待前一个transaction完全完成**。

![img](https://pic1.zhimg.com/80/v2-a06b2b9124339b151c03d39314f14940_1440w.webp)

​	<u>concurrency之所以能帮助提升性能，是因为它可以帮助我们并行的运行系统调用，我们可以得到多核的并行能力。如果我们可以在运行应用程序和系统调用的同时，来写磁盘，我们可以得到I/O concurrency，也就是同时运行CPU和磁盘I/O。这些都能帮助我们更有效，更精细的使用硬件资源</u>。

---

问题：有关batching，XV6不是也支持多个系统调用同时执行start_op和end_op，然后再一起commit吗？

回答：是的，XV6具备有限能力的batching（有限制最多同时只能有几个，因为log block数量有限，不足以容纳太多transaction同时存在）。

问题：fsync是不是有时也被称为flush，因为我之前经常听到这个单词？

回答：是的，一个合理的解释fsync的工作的方式是，它flush了所有文件相关的写磁盘操作到了磁盘中，之后再返回，所以flush也是针对这个场景的一个合理的单词。

**问题：如果一个block cache正在被更新，而这个block又正在被写入到磁盘的过程中，会怎样呢？**

回答：这的确会是一个问题，这里有个潜在的困难点，因为transaction写入到log中的内容只能包含由该transaction中的系统调用所做的更新，而不能包含在该transaction之后的系统调用的更新。因为如果这么做了的话，那么可能log中会只包含系统调用的部分更新，而我们需要确保transaction包含系统调用的所有更新。所以我们不能承担transaction包含任何在该transaction之后的更新的风险。**<u>ext3是这样解决这个问题的，当它决定结束当前的open transaction时，它会在内存中拷贝所有相关的block，之后transaction的commit是基于这些block的拷贝进行的</u>。所以transaction会有属于自己的block的拷贝。为了保证这里的效率，<u>操作系统会使用copy-on-write来避免不必要的拷贝</u>，这样只有当对应的block在后面的transaction中被更新了，它在内存中才会实际被拷贝。**

## 16.5 Ext3文件系统调用格式

> [16.5 ext3文件系统调用格式 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/358794495) <= 图文出处

​	接下来我们大概过一下Linux中的文件系统调用，并介绍抽象上每个系统调用的结构。

​	在Linux的文件系统中，我们需要每个系统调用都表明写操作的开始和结束。**实际上在任何transaction系统中，都需要明确的表示开始和结束，这样之间的所有内容都是原子的**。所以系统调用中会调用start函数。<u>ext3需要知道当前正在进行的系统调用个数，所以每个系统调用在调用了start函数之后，会得到一个handle句柄，它某种程度上唯一识别了当前系统调用</u>。**当前系统调用的所有写操作都是通过这个handle来识别跟踪的**（注，handle是ext3 transaction中的一部分数据，详见16.3）。

![img](https://pic4.zhimg.com/v2-903840c1fad4c218f68c09dc28d7855b_r.jpg)

​	之后系统调用需要读写block，它可以通过get获取block在buffer中的缓存，同时告诉handle这个block需要被读或者被写。如果你需要更改多个block，类似的操作可能会执行多次。之后是修改位于缓存中的block。

![img](https://pic2.zhimg.com/80/v2-4bc511324408afa665e0af53a31bc0b9_1440w.webp)

​	<u>当这个系统调用结束时，它会调用stop函数，并将handle作为参数传入</u>。

![img](https://pic3.zhimg.com/80/v2-3c6579a9386d8be33d014a80c585531a_1440w.webp)

​	除非transaction中所有已经开始的系统调用都完成了，否则transaction是不能commit的。<u>因为可能有多个transaction，文件系统需要有种方式能够记住系统调用属于哪个transaction，这样当系统调用结束时，文件系统就知道这是哪个transaction正在等待的系统调用，所以句柄handle需要作为参数传递给stop函数</u>。

​	<u>因为每个transaction都有一堆block与之关联，修改这些block就是transaction的一部分内容，所以我们将handle作为参数传递给get函数是为了告诉logging系统，这个block是handle对应的transaction的一部分</u>。

​	**stop函数并不会导致transaction的commit，它只是告诉logging系统，当前的transaction少了一个正在进行的系统调用**。

​	<u>**transaction只能在所有已经开始了的系统调用都执行了stop之后才能commit。所以transaction需要记住所有已经开始了的handle，这样才能在系统调用结束的时候(即执行了stop之后)做好记录**</u>。

## 16.6 Ext3 transaction commit步骤

> [16.6 ext3 transaction commit步骤 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/358794637) <= 图文出处

​	基于上面的系统调用的结构，接下来我将介绍commit transaction完整的步骤。每隔5秒，文件系统都会commit当前的open transaction，下面是commit transaction涉及到的步骤：

1. 首先需要**阻止新的系统调用**。<u>当我们正在commit一个transaction时，我们不会想要有新增的系统调用，我们只会想要包含已经开始了的系统调用，所以我们需要阻止新的系统调用。这实际上会损害性能，因为在这段时间内系统调用需要等待并且不能执行</u>。
2. 第二，需要等待包含在transaction中的已经开始了的系统调用们结束。所以我们需要等待transaction中未完成的系统调用完成，这样transaction能够反映所有的写操作。
3. <u>一旦transaction中的所有系统调用都完成了，也就是完成了更新cache中的数据，那么就可以开始一个新的transaction，并且让在第一步中等待的系统调用继续执行</u>。所以现在需要**为后续的系统调用开始一个新的transaction**。
4. 还记得ext3中的log包含了descriptor，data和commit block吗？现在我们知道了transaction中包含的所有的系统调用所修改的block，因为系统调用在调用get函数时都将handle作为参数传入，表明了block对应哪个transaction。<u>接下来我们可以更新descriptor block，其中包含了所有在transaction中被修改了的block编号</u>。
5. 我们还需要**将被修改了的block，从缓存中写入到磁盘的log中**。<u>之前有同学问过，新的transaction可能会修改相同的block，所以在这个阶段，我们写入到磁盘log中的是transaction结束时，对于相关block cache的拷贝(COW写时复制)</u>。所以**这一阶段是将实际的block写入到log中**。
6. 接下来，我们需要等待前两步中的写log结束。
7. 之后我们可以**写入commit block**。
8. 接下来我们需要等待写commit block结束。结束之后，从技术上来说，**当前transaction已经到达了commit point**，也就是说<u>transaction中的写操作可以保证在面对crash并重启时还是可见的。如果crash发生在写commit block之前，那么transaction中的写操作在crash并重启时会丢失</u>。
9. 接下来我们可以**将transaction包含的block写入到文件系统中的实际位置**。
10. **在第9步中的所有写操作完成之后，我们才能重用transaction对应的那部分log空间**。

![img](https://pic4.zhimg.com/80/v2-a375ab2911ee99ae8ab596508996f873_1440w.webp)

​	在一个非常繁忙的系统中，log的头指针一直追着尾指针在跑（注，也就是说一直没有新的log空间）。在当前最早的transaction的所有步骤都完成之前，或许不能开始commit一个新的transaction，因为我们需要重复利用最早的transaction对应的log空间。不过<u>人们通常会将log设置的足够大，让这种情况就不太可能发生</u>。

![img](https://pic1.zhimg.com/80/v2-b699017a43305027a3352d5144f35c34_1440w.webp)

​	有关如何重用log空间，这里有个小细节。**在log的最开始有一个super block，所以在任何时候log都是由一个super block和一些transaction组成**。假设T4是最新的transaction，之前是T1，T2，T3。

​	<u>我们是否能重用一段log空间，取决于相应的transaction，例如T2，是否已经commit并且写入到文件系统的实际位置中，这样在crash并重启时就不需要重新执行这段transaction了。同时也取决于T2之前的的所有transaction是否已经被释放了。所有的这些条件都满足时，我们就可以释放并重用T2对应的log空间</u>。

---

问题：你刚刚说没有进程会等待这些步骤完成，那么这些步骤是在哪里完成的呢？

回答：这些是**在后台的内核线程完成的**。

问题：我有个有关重用log空间的问题，假设我们使用了一段特定的log空间，并且这段log空间占据了是刚刚释放出来的所有log空间，但是还不够，那么文件系统会等待另一部分的log空间释放出来吗，还是会做点别的？

回答：是的，会等待。让我画张图来确保我回答的是正确的问题。我们可以认为log是磁盘中的一段线性空间，假设现存的transaction中最早的是T7，之后是T8，T9，我们想要将T10放在T9之后的空闲区域。我们或许要等待T7将所有的block写入到文件系统对应的位置，这样我们才能释放T7对应的空间。这意味着T10中的步骤需要暂停以等待T7释放出来。这是你的问题吗？

![img](https://pic1.zhimg.com/80/v2-3fbf771f5deba5d2a8d04bdd71eae398_1440w.webp)

追问：是的，所以可能是这样，我先写入T10的block到现有的log空闲区域，但是如果最后log足够大并且我们用光了空闲区域，我们就需要等待T7的空间被释放出来，是吗？

回答：<u>是的，如果需要写入的数据足够多，并且log迅速的用光了。我们甚至都不能在释放log空间之前开始新的系统调用。如果你们关注细节的话，这里会有一些潜在的死锁。首先系统调用需要预声明需要多少个block，这样logging系统才知道对于该transaction需要多少log空间，因为我们不会在没有足够空间来commit transaction时，开始一个新的transaction</u>（注，难道不能将不能写入到磁盘log中的transaction先缓存在内存中吗？虽然这样可能会导致堆积）

追问：如果新的transaction需要的空间走到了T8，那么现在就需要等待T7，T8结束，这是怎么工作的呢？

回答：图中的T7，T8，T9其中的系统调用都完成了，并且都已经在commit到log中了。在上面的图中，我们会直接开始T10，新的系统调用会写入到transaction T10，最终当T10需要commit到log中，并且它大到需用用到T8的空间时，它需要等待T7，T8结束。文件系统会记录每个transaction的大小，这样文件系统就知道要等待多少个之前的transaction结束。所以这里还有不少的记录工作，这样文件系统才能理解所有旧的transaction的状态。

## 16.7 Ext3 File System恢复过程

> [16.7 ext3 file system恢复过程 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/358794749) <= 图文出处

​	为了简化重启时恢复软件的工作，<u>当决定释放某段log空间时，文件系统会更新super block中的指针将其指向当前最早的transaction的起始位置</u>。

![img](https://pic4.zhimg.com/v2-10c3eccea7fdeba4683ebc5cecea931f_r.jpg)

​	之后如果crash并重启，恢复软件会读取super block，并找到log的起始位置。所以如果crash了，内存中的所有数据都会消失，例如文件系统中记录的哪些block被写入到了磁盘中这些信息都会丢失，所以可以假设这时内存中没有可用的数据，唯一可用的数据存在于磁盘中。当然我们这里的讨论都是基于磁盘还是完好的，所以你可以认为只是一次电力故障，系统突然停止了运行过程，在电力恢复时，断电那一瞬间磁盘中的数据还存在。我们并没有考虑磁盘被损坏或者被摧毁的情况。

​	crash或许会打断任何在进行中的transaction，或许transaction正在commit，或许transaction正在向文件系统写block。让我重新画一个例子，我们在log中有一个super block，之后是transaction T6，T7，T8，在T8之后是一个已近被释放了log空间的T5，假设T8已经用了T5的一部分空间。并且现在super block指向的是T6的起始位置，因为T6是最早的transaction。

![img](https://pic3.zhimg.com/v2-c58a7fa5fb605b0108028e82ce351ab6_r.jpg)

​	现在crash并重启，恢复软件读取super block就可以知道log的起始位置，之后恢复软件会在log中一直扫描并尝试找到log的结束位置，现在我们需要有一种方式确定log的结束位置。我们知道**每个transaction包含了一个descriptor block，里面记录了该transaction中包含了多少个data block**，假设descriptor block记录了17个block，那么恢复软件会扫描17个data block，最后是commit block。这样可以一直扫描到T8。

![img](https://pic4.zhimg.com/80/v2-27d7f21cefe71b0812c9812ed7f417d3_1440w.webp)

​	在扫描T8时有两种可能，一种可能是T8完成了commit，并且包含了commit block。这时恢复软件并不知道T8就是最后一个transaction，所以它会接着看T8的commit block的下一个block，来看看这是不是一个有效的descriptor block。我们知道这不是一个descriptor block，而是一个包含在T5内的随机block。现在的问题是恢复软件如何可靠的区分出来呢？是的，**每个descriptor和commit block都以某个魔法数字(magic number)作为起始，这是一个32bit的数字**。所以如果扫描完了T8，下一个block以魔法数字作为起始，那么恢复软件就会认为这是一个descriptor block。（注，也有可能T5正好完美的跟在T8后面，也就是说T8的commit block之后就是T5的descriptor block，同时T5的commit block也存在，所以这里必然还需要一些其他的机制，博主猜是用到了transaction的序列号）

​	但是，现在我们看到的block可能是包含了任意数据的data block，所以它可能是文件中的一个data block并且也是以魔法数字作为起始。所以这里的最后一个细节是，**logging系统需要能区分一个以魔法数字作为起始的descriptor block和一个以魔法数字作为起始的data block**。你可以想到各种方法来实现这种区分，<u>**ext3是这样做的，当它向log写一个block时，如果这个block既不是descriptor block也不是commit block，但是又以魔法数字作为起始，文件系统会以0替换前32bit，并在transaction的descriptor block中为该data block设置一个bit。这个bit表示，对应的data block本来是以魔法数字作为起始，但是现在我们将其替换成了0。而恢复软件会检查这个bit位，在将block写回到文件系统之前，会用魔法数字替换0**</u>。

​	<u>**因此，在log中，除了descriptor和commit block，不会有其他的block以这32bit的魔法数字作为起始。所以我们不会有模棱两可的判断，如果一个commit block之后的block以魔法数字作为起始，那么它必然是一个descriptor block**</u>。

​	**所以恢复软件会从super block指向的位置(super block会记录第一个还未释放的transaction的位置-block编号，以及transaciton序号)开始一直扫描**，直到：

- 某个commit block之后的一个block并不是descriptor block
- 或者某个commit block之后是descriptor block，但是根据descriptor block找到的并不是一个commit block(即某个decriptor block往后找没有对应的完整的commit block，这个transaction相当于不成立/损坏了，不能恢复)

​	这时，恢复软件会停止扫描，并认为**最后一个有效的commit block是log的结束位置**。或许在最后一个commit block之后会跟一个并没有commit完成的transaction（注，上面的第二种情况），但是**<u>恢复软件会忽略未完成的transaction，因为这个transaction并没有包含所有的写操作，所以它并不能原子性的恢复</u>**。<u>之后恢复软件会回到log的最开始位置，并将每个log block写入到文件系统的实际位置，直到走到最后一个有效的commit block。之后才是启动剩下的操作系统，并且运行普通的程序</u>。**<u>在恢复完成之前，是不能运行任何程序的，因为这个时候文件系统并不是有效的</u>**。

---

问题：XV6相比这里的log机制，缺少了什么呢？

回答：XV6主要缺失的是在log中包含多个transaction的能力，在XV6的log中最多只会有一个transaction，所以在XV6中缺少了并发的能力。比如说当我在执行transaction T7的系统调用时，ext3可以同时向磁盘提交T6，而这在XV6中这是不可能的，因为log只保存了一个transaction。所以我们必须先完成一个transaction的所有工作，之后才能开始下一个transaction。所以XV6是简单且正确的，但是缺少了并发的能力。

问题：但是在XV6我还是可以有多个transaction，只是说不能异步的执行它们，对吗？

回答：这里其实有点模糊，XV6实际上允许在一个transaction中包含多个系统调用（注，详见15.8），所以XV6有一些并发和batching的能力，但是当XV6决定要commit一个transaction时，在完全完成这个transaction之前，是不能执行任何新的系统调用的。因为直到前一个transaction完全完成，并没有log空间来存放新的系统调用。<u>所以XV6要么是在运行一些系统调用，要么是在commit transaction，但是它不能同时干这两件事情，而ext3可以同时干这两件事情</u>。

## 16.8 Ext3新事务系统调用阻塞直到旧事务所有系统调用完成

> [16.8 为什么新transaction需要等前一个transaction中系统调用执行完成 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/358794836) <= 图文出处

​	以上就是ext3中相对来说直观的部分。实际上还有一些棘手的细节我想讨论一下。之前我提到过，**<u>ext3中存在一个open transaction，但是当ext3决定要关闭该transaction时，它需要等待该transaction中的所有系统调用都结束，之后才能开始新的transaction</u>**。假设我们现在有transaction T1，其中包含了多个系统调用。

![img](https://pic2.zhimg.com/80/v2-f2c9ce5e210550296b9244528ac281f9_1440w.webp)

​	**如果我们想要关闭T1，我们需要停止接收新的系统调用，因为我们想要等待现有的系统调用结束，这样才能commit transaction。所以直到这些系统调用都结束了，在ext3中不能允许开始任何新的系统调用。所以只有在T1中的系统调用完成之后，才能开始在接下来的transaction T2中接收系统调用**。在这之间有一段时间，新的系统调用是被拦截的，这降低了性能，因为我们本来应该执行系统调用的但是又不被允许。

![img](https://pic2.zhimg.com/80/v2-15d53cf047aa6f13c8e89c8cda7c1ea9_1440w.webp)

​	这里的问题是，直到T1中所有的系统调用都结束之前，ext3为什么不让T2中的系统调用开始执行呢？让我们来看一下没有这个限制条件可能会带来的错误的场景。我们假设T1只包含了一个系统调用，这是一个create系统调用用来创建文件x。在create系统调用结束之前，文件系统决定开始一个新的transaction T2用来接收create之后的所有系统调用。我们假设T2在T1结束之前就开始了，T2对另一个文件y调用了unlink系统调用。unlink会释放与y关联的inode。

![img](https://pic3.zhimg.com/80/v2-5220cc6bd448dac519897fdd18a4906e_1440w.webp)

​	假设在下面的时间点T2将inode标记为空闲的，create会为x分配inode，或许它在之后的一个时间点分配了inode。

![img](https://pic3.zhimg.com/80/v2-44a818b659a6980623e6141509b622d2_1440w.webp)

​	<u>因为create在unlink释放inode之后分配的inode，它可能会重用同一个inode，所以x可能会获得y的inode</u>，假设是inode 17。目前为止没有问题，因为unlink本来就是释放inode。当T1中的create结束之后，我们会关闭T1，在最后我们会将T1的所有更新都写入到磁盘的log中。之后unlink还要花点时间才能结束，但是在它结束之前计算机crash了。

![img](https://pic3.zhimg.com/80/v2-edcf7a2f6530b893af47d428c8899fbe_1440w.webp)

​	<u>在重启并运行恢复软件时，可以发现T1已经commit了，而T2没有。所以恢复软件会完全忽略T2，这意味着T2中的unlink就跟没有发生过一样，恢复软件不会执行T2中的unlink，也就不会删除文件y。所以crash并重启之后y文件仍然存在，并还在使用inode 17。然而T1又完成了，x文件使用的也是inode 17，所以现在我们错误的有了两个文件都使用了相同的inode，这意味着它们共享了文件内容，向一个文件写数据会神奇的出现在另一个文件中。这完全是错误的，因为我们本来想的是删除y，并为x分配一个空闲的inode，而不是一个已经在使用中的inode。这里可以这么想，T2中的unlink修改了一个block，最终这个修改过的block被前一个transaction所使用。T2中修改的信息，被T1所使用了，这意味着我们丢失了T2的原子性</u>。因为T2的目标是unlink的效果要么是全发生，要么是完全不发生。但是刚刚的例子中，因为T1使用了T2中释放的inode，这意味着T2中部分修改已经生效了，但是其他的修改随着crash又丢失了。

​	或许你可以想到一些修复这里问题的方法，或许T1可以发现inode是由后一个transaction释放的而不去使用它。而**<u>ext3采用了一个非常简单的方法，在前一个transaction中所有系统调用都结束之前，它不允许任何新的系统调用执行</u>。所以transaction T1也就不可能看到之后的transaction包含的更新。因为直到T1 commit了，整个unlink都不被允许执行**。

​	以上是众多ext3需要处理的小细节之一，因为为了支持并发，ext3需要处理各种各样的特殊细节，但是我们没有时间讨论所有的细节。

---

**问题：当你关闭一个open transaction时，具体会发生什么呢？会对当前的缓存做一个快照吗？**

**回答：会的，当我们关闭一个transaction，文件系统会拷贝被transaction中的系统调用所修改的所有block，之后transaction才会commit这些block。后面的transaction会在真正的block cache上运行。当将block都commit到log之后，对于block cache的拷贝就可以丢弃了。**

## 16.9 Ext3 小结

> [16.9 ext3总结 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/358795026) <= 图文出处

最后我希望同学们记住的有关logging和ext3的是：

- **log是为了保证多个步骤的写磁盘操作具备原子性。在发生crash时，要么这些写操作都发生，要么都不发生。这是logging的主要作用**。
- **logging的正确性由write ahead rule来保证**。你们将会在故障恢复相关的业务中经常看到write ahead rule或者write ahead log（WAL）。<u>write ahead rule的意思是，你必须在做任何实际修改之前，将所有的更新commit到log中。在稍后的恢复过程中完全依赖write ahead rule。对于文件系统来说，logging的意义在于简单的快速恢复。log中可能包含了数百个block，你可以在一秒中之内重新执行这数百个block，不管你的文件系统有多大，之后又能正常使用了</u>。
- 最后有关ext3的一个细节点是，它使用了**批量执行**和**并发**来获得可观的性能提升，不过同时也带来了可观的复杂性的提升。

----

问题：你刚刚说有一个文件系统线程会做这里所有的工作，那么只能有一个这样的线程，否则的话就会有不同步的问题了，对吗？

回答：或许真的只有一个线程，我其实不知道有多少个线程，但是1是个不错的数字，因为**logging的正确性取决于旧的transaction要在新的transaction之前提交**。但是逻辑上来说又没有必要只有一个线程，你可以想象不同的transaction使用不同的线程来提交（注，只要锁加的合适多个线程应该也是没问题的）。

问题：**当你在讨论crash的时候，你有一个图是T8正在使用之前释放的T5的空间，如果T8在crash的时候还没有commit，并且T5的commit block正好在T8的descriptor block所指定的位置，这样会不会不正确的表明T8已经被commit了**（注，这时T8有一个假的commit block）？

回答：让我尝试画出这里的场景。首先我们有一个古老的transaction T5，因为log的循环特性，在顺序上T8位于T5之前。因为T5已经被释放了，T8正在蚕食T5的log空间。假设T8没有完成commit，但是如果完成commit的话，T8的commit block会写到T5的commit block位置。T8并没有能写入commit block，T8前面所有的block都写入了，但是最后跟的是T5的commit block。这里的答案是，**descriptor block和commit block都有transaction的序列号，所以T8的descriptor block里面的序列号是8，但是T5的commit block里面的序列号是5，所以两者不能匹配**。

![img](https://pic3.zhimg.com/80/v2-1bf5e57796f78dff800f5e89da96ca56_1440w.webp)

**问题：我们可以在transaction T8开始的时候就知道它的大小吗？**

回答：这是个复杂的问题。当T8作为活跃的transaction开始时，系统调用会写入数据，这时文件系统并不知道T8有多大。**当文件系统开始commit T8时，是知道T8有多大的，因为文件系统只会在T8中所有的系统调用都结束之后才commit它，而在那个时间点，文件系统知道所有的写操作，所以就知道T8究竟有多大**。<u>除此之外，descriptor block里面包含了所有block的实际编号，所以当写入transaction的第一个block，也就是descriptor block时，logging系统知道T8会包含多少个block</u>。

问题：为什么不在descriptor block里面记录commit信息？虽然这样可能不太好，因为要回到之前的一个位置去更新之前的一个block。

回答：所以这里的提议是，与其要一个专门的commit block，可以让descriptor block来实现commit block的功能。XV6与这个提议非常像，我认为可以这么做，至少在ext3中这么做了不会牺牲性能。你需要像XV6一样来组织这里的结构，也就是<u>需要在descriptor block包含某个数据表明这是一个已经提交过的transaction</u>。这样做的话，可以节省一个commit block的空间，但是不能节省整个时间。Linux文件系统的后续版本实现了你的提议，**ext4做了以下工作来更有效的写commit block。ext4会同时写入所有的data block和commit block，它并不是等待所有的data block写完了之后才写的commit block**。<u>但是这里有个问题，磁盘可以无序的执行写操作，所以磁盘可能会先写commit block之后再写data block。如果中间有了crash，那么我们有了commit block，但是却没有全部的data block</u>。**ext4通过在commit block中增加<u>校验和(checksum)</u>来避免这种问题。所以commit block写入之后发生了crash，如果data block没有全写入那么<u>校验和</u>不能得出正确的结果，恢复软件可以据此判断出错了**。ext4可以通过这种方式在机械硬盘上写入一批block而避免磁碟旋转，进而提升磁盘性能。

**问题：log中的data block是怎么写入到文件系统中的？**

回答：这个问题有多个答案。对于data block，ext3有三种模式，但是我只记得两个，journaled data和ordered data（注，第三种是writeback）。当你在配置ext3文件系统时，你需要告诉Linux你想要哪种模式。<u>如果你想要的是**journaled data**，文件内容就是写入到log中，如果你向一个文件写数据，这会导致inode更新，log中会包含文件数据和更新了的inode，也就是说任何更新了的block都会记录在log中。这种方法非常慢，因为数据需要先写在log中，再写到文件系统中</u>。所以journaled data很直观，但是很慢。<u>**ordered data**是最流行的模式，它不会将文件数据写入到log中，只会将metadata block，例如inode，目录block，写入到log中，文件的内容会直接写到文件系统的实际位置中</u>。所以这种模式要快得多，因为你不用将文件内容写两次。但是它也会导致更多的复杂性，因为你不能随时写入文件内容。假设你执行一个写操作导致一个新的block被分配给一个文件，并将包含了新分配block编号的inode写入到log中并commit，在实际写入文件内容至刚刚分配的data block之前发生crash。在稍后的恢复流程中，你将会看到包含了新分配的block编号的inode，但是对应data block里面的内容却属于之前使用了这个data block的旧的文件。如果你运行的是一个类似Athena的多用户系统，那么可能就是一个用户拥有一个文件，其中的内容又属于另一个用户已经删除的文件，如果我们不是非常小心的处理写入数据和inode的顺序就会有这样的问题。**ext3的ordered data通过先写入文件内容到磁盘中，再commit修改了的inode来解决这里的问题**。<u>如果你是个应用程序，你写了一个文件并导致一个新的文件系统data block被分配出来，文件系统会将新的文件内容写到新分配的data block中，之后才会commit transaction，进而导致inode更新并包含新分配的data block编号</u>。如果在写文件数据和更新inode之间发生了crash，你也看不到其他人的旧的数据，因为这时就算有了更新了的data block，但是也没关系，因为现在不仅inode没有更新，连bitmap block也没更新，相应的data block还处于空闲状态，并且可以分配给其他的程序，你并不会因此丢失block。这里的效果就是我们写了一个data block但是最终并没有被任何文件所使用。

# Lecture17 应用的虚拟内存(Virtual Memory for Application)

## 17.1 虚拟内存和原语概述(VM and Primitives)

> [17.1 应用程序使用虚拟内存所需要的特性 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/362373721) <= 图片出处，大部分文字出处（原博文有些描述我感觉不太对）
>
> [Shared Virtual Memory（SVM）介绍-lvyilong316-ChinaUnix博客](http://blog.chinaunix.net/uid-28541347-id-5854016.html) <= 大致介绍，还不错
>
> 共享虚拟内存（SVM）技术最初是为了解决在GPU场景下，设备（GPU）和host（CPU）之间共享内存的。目的是在设备GPU和CPU之间可以直接传递指针（地址），为了上设备可以直接使用进程空间的地址，简化编程模型。我们知道通常host测采用的地址是主机的虚拟地址（VA），而设备侧通常使用的是物理地址（PA）或IOVA。......
>
> [DPDK介绍_growing_up_的博客-CSDN博客_dpdk](https://blog.csdn.net/growing_up_/article/details/124323725) <= 额外拓展知识，网卡数据处理相关，建议阅读。前面章节有提过网卡这种设备，如果用中断机制，容易一直抢占CPU资源，这文章里就提到了轮询的方式，避免频繁中断处理(以及随之的频繁switch)，也提到了既支持中断也支持轮询的方案。
>
> [DMA（直接存储器访问）_百度百科 (baidu.com) ](https://baike.baidu.com/item/DMA/2385376?fr=aladdin)
>
> DMA(Direct Memory Access，直接[存储器](https://baike.baidu.com/item/存储器/1583185?fromModule=lemma_inlink)访问) 是所有现代[电脑](https://baike.baidu.com/item/电脑/124859?fromModule=lemma_inlink)的重要特色，它允许不同速度的硬件装置来沟通，而不需要依赖于[ CPU ](https://baike.baidu.com/item/ CPU /120556?fromModule=lemma_inlink)的大量中断负载。否则，CPU 需要从来源把每一片段的资料复制到[暂存器](https://baike.baidu.com/item/暂存器/4308343?fromModule=lemma_inlink)，然后把它们再次写回到新的地方。在这个时间中，CPU 对于其他的工作来说就无法使用。

​	今天的话题是用户应用程序使用的虚拟内存，它主要是受这篇1991年的[论文](https://pdos.csail.mit.edu/6.828/2020/readings/appel-li.pdf)的启发。

​	首先，你们已经知道了，操作系统内核以非常灵活的方式使用了虚拟内存Page Table。你们已经通过Lazy Allocation Lab，Copy on Write Lab，以及XV6中的各种内存实现了解到了这一点。而今天论文中的核心观点是，用户应用程序也应该从灵活的虚拟内存中获得收益，也就是说用户应用程序也可以使用虚拟内存。用户应用程序本身就是运行在虚拟内存之上，<u>我们这里说的**虚拟内存**是指：在User Mode下应用程序想要使用与内核相同的机制，能够接收并响应Page Fault；以及能够修改PTE的Protection位（注，Protection位是PTE中表明对当前Page的保护，对应了4.3中的Writeable和Readable位）、Privileged level（page table的特权级别）等等</u>。今天的论文，通过查看6-7种不同的应用程序，来说明用户应用程序使用虚拟内存的必要性。这些应用程序包括了：

- Garbage Collector
- Data Compression Application
- Shared Virtual Memory（SVM，共享虚拟内存）

![img](https://pic1.zhimg.com/80/v2-414c55b414eb6bf2b834ac276d52da64_1440w.webp)

​	你可以发现这都是一些非常不同的应用程序，并且它们都需要依赖一些**虚拟内存原语(virtual memory primitives)**来正常工作。所以第一个问题是，上面的应用程序需要什么primitives来支持？所以我们先来讨论一下应用程序需要的**原语(primitives)**大概是什么样子的？

- Trap：它使得发生在内核中的Page Fault可以传播到用户空间，然后在用户空间的handler可以处理相应的Page Fault，之后再以正常的方式返回到内核并恢复指令的执行。这个原语(primitives)是必须的，否则你不能对Page Fault作出任何响应。
- Prot1：它会降低了内存Page的可访问性/访达性(accessability)。accessability的意思是指内存Page的读写权限。内存Page的accessability有不同的降低方式，例如，将一个可以读写的Page变成只读的，或者将一个只读的Page变成完全没任何访问权限。
- ProtN：除了对于单个内存Page生效的Prot1，还有作用于多个Page的ProtN原语。ProtN基本上等效于调用N次Prot1，那为什么还需要有ProtN？在使用Prot1时，你需要修改PTE的bit位，并且在Prot1的结束时，通常需要刷新TLB(flush TLB)（注，详见4.4 Translation Lookaside Buffer），而flush TLB比较费时。如果能对所有需要修改的内存Page集中清理一次TLB，就可以将成本分摊。所以ProtN等效于修改PTE的bit位N次，再加上仅一次flush TLB。每次Prot1执行，都需要1次页表切换page table switch，1次修改PTE的bit位，再加上1次 flush TLB；而ProtN主要可以减少耗时操作flush TLB的次数，进而提升性能。
- Unprot：它增加了内存Page的accessability，例如将本来只读的Page变成可读可写的。
- Dirty：找出脏页的原语(find out which page is dirty)
- Map2：允许一个应用程序将一段特定的内存地址空间映射两次(map a particular range twice in the same address page)，并且这两次映射拥有不同的accessability（注，也就是一段物理内存对应两份虚拟内存，并且两份虚拟内存有不同的accessability）。

![img](https://pic1.zhimg.com/80/v2-59ce89db0f22fb5306893c9ab5781e18_1440w.webp)

​	XV6在用户程序中支持以上的primitives吗？除了有类似于trap及其相关的alarm hander之外，XV6不支持以上任何一个的原语。XV6只有一个最小化的Unix接口，并不支持高级的**虚拟内存原语(virtual memory primitives)**。尽管在XV6的内核中包含了所有的可用的虚拟内存的机制，但是并没有以系统调用的形式将它们暴露给用户空间。**论文的观点是，任何一个优秀的操作系统都应该提供这些primitives 或 system calls，以便应用程序使用**。

## 17.2 Unix的虚拟内存原语举例

> [17.2 支持应用程序使用虚拟内存的系统调用 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/362374011) <= 图片出处，大部分文字出处（个人做了部分调整）

​	所以自然的，这就引出了另一个问题，当今的Unix系统可用的功能范围该包括哪些？如果你查看现在的Unix系统，例如Linux，你会发现，或许并不与论文中描述的完全一样，但是这些原语(primitives)都存在对应的实现。在论文那个年代（1991年），某些操作系统只包含了部分原语(primitives)实现，但是如今这些原语(primitives)都已经在现代操作系统中得到广泛支持了。接下来看看如今的Unix是如何实现这些原语(primitives)的。

​	第一个先介绍Unix的mmap系统调用，常见且重要。**mmap能够指定对象(通常指文件/设备)映射到调用者的地址空间，映射文件/设备时需要传递文件描述符作为参数**。详细参数手册参考[man page](https://man7.org/linux/man-pages/man2/mmap.2.html)

```c
void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);
```

![img](https://pic4.zhimg.com/80/v2-19821324b896f3b9608ddb4b39fc845b_1440w.webp)

- 第一个参数是一个你想映射到的特定地址，如果传入null表示不指定特定地址，这样的话内核会选择一个地址来完成映射，并从系统调用返回。
- 第二个参数是想要映射的地址段长度len。
- 第三个参数是Protection bit，例如读写R|W。
- 第四个参数我们会跳过不做讨论，它的值可以是MAP_PRIVATE。它指定了如果你更新了传入的对象，会发生什么。（注，第四个参数是flags，MAP_PRIVATE是其中一个值，在mmap文件的场景下，MAP_PRIVATE表明更新文件不会写入磁盘，只会更新在内存中的拷贝，详见[man page](https://man7.org/linux/man-pages/man2/mmap.2.html)）。
- 第五个参数是传入的对象，在上面的例子中就是文件描述符。
- 第六个参数是offset。

​	通过上面的系统调用，可以将文件描述符指向的文件内容，从起始位置加上offset的地方开始，映射到特定的内存地址（如果指定了的话），并且连续映射len长度。这使得你可以实现Memory Mapped File，你可以将文件的内容带到内存地址空间，进而只需要方便的通过普通的指针操作，而不用调用read/write系统调用，就可以从磁盘读写文件内容。这是一个方便的接口，可以用来操纵存储在文件中的数据结构。

​	*实际上，你们将会在下个lab实现基于文件的mmap，下个lab结合了XV6的文件系统和虚拟内存，进而实现mmap。*

​	**mmap还可以用作他途，除了可以映射文件之外，还可以用来<u>映射匿名的内存（Anonymous Memory）</u>。这是sbrk（注，详见8.2）的替代方案，你<u>可以向内核申请物理内存，然后映射到特定的虚拟内存地址</u>**。

​	mmap是实现应用程序虚拟内存的核心系统调用之一，我们稍后会将它与之前提到的原语(primitive)关联起来。

---

​	除此之外，还需要有一些系统调用来支持论文中讨论到的原语(primitive)。

![img](https://pic3.zhimg.com/80/v2-607bb9cb39f51ce18073abff1ef6d56a_1440w.webp)

​	**mprotect系统调用（详见[man page](https://man7.org/linux/man-pages/man2/mprotect.2.html)）。当你将某个对象映射到了虚拟内存地址空间，你可以修改对应虚拟内存的权限，这样你就可以以特定的权限保护对象的一部分，或者整个对象**。

​	上图对于mprotect的调用将权限设置成只读（R），这时，对于addr到addr+len这段地址，load指令还能执行，但是store指令将会变成Page Fault。类似的，<u>如果你想要将一段地址空间变成完成不可访问的，那么可以在mprotect中的权限参数传入None，那么任何对于addr到addr+len这段地址的访问，都会生成Page Fault</u>。

---

​	对应mmap还有一个**系统调用munmap，它使得你可以移除一个地址或者一段内存地址的映射关系**。如果你好奇这里是怎么工作的，你应该查看这些系统调用的[man page](https://man7.org/linux/man-pages/man2/)。

![img](https://pic1.zhimg.com/80/v2-488a520c9379c962073ae0be3041ff9c_1440w.webp)

---

​	最后一个**系统调用sigaction，它本质上是用来处理信号(signal)**。<u>它使得应用程序可以设置好一旦特定的signal发生了，就调用特定的函数。可以给它传入函数f作为特定signal的handler</u>。

​	在Page Fault的场景下，生成的signal是segfault。你或许之前在用户代码中看过了segfault，通常来说当发生segfault时，应用程序会停止运行并crash。但是<u>如果应用程序为segfault signal设置了handler，发生segfault时，应用程序不会停止，相应的handler会被内核调用，然后应用程序可以在handler中响应segfault</u>。

​	当内核发现Page Fault时，或许会通过修复Page Table来使得应用程序还能继续执行。与内核响应Page Fault的方式类似，在这里的handler中或许会调用**mprotect**来修改内存的权限来避免segfault，这样应用程序的指令就可以恢复运行。

![img](https://pic1.zhimg.com/80/v2-449aeec251e13548016ce75d1191d824_1440w.webp)

​	与sigaction类似的有sigalarm（注，sigalarm不是一个标准的Unix接口，应该是这门课程相关课程中需要学生自己实现的一个系统调用），在sigalarm中可以设置每隔一段时间就调用的handler。sigaction也可以实现这个功能，只是它更加的通用，因为它可以响应不同类型的signal。

---

​	如果你回想前面提到过的虚拟内存特性，我们可以将它们对应到这一节描述的Unix接口中来。

- trap对应的是sigaction系统调用
- Prot1，ProtN和Unprot可以使用mprotect系统调用来实现。**mprotect足够的灵活，你可以用它来修改一个Page的权限，也可以用它来修改多个Page的权限**。当修改多个Page的权限时，可以获得只清除一次TLB的好处。
- 查看Page的Dirty位要稍微复杂点，并没有一个直接的系统调用实现这个特性，不过你可以使用一些技巧完成它，我稍后会介绍它。
- Map2也没有一个系统调用能直接对应它，通过多次调用mmap，你可以实现Map2原语。

​	或许并不完全受这篇论文所驱动，但是内核开发人员已经在操作系统中为现在的应用程序提供了这些原语。接下来，我将在框架层面简单介绍一下这些原语是如何实现的，之后再看看应用程序是如何使用这些原语。

---

问题：看起来mprotect暗示了你可以为单独的地址添加不同的权限，然而在XV6中，我们只能为整个Page设置相同的权限，这里是有区别吗？

回答：不，这里并没有区别，它们**都在Page粒度工作**。如果你好奇的话，有一个单独的系统调用可以查看Page的大小。

## 17.3 VMA和VM原语的应用

> [17.3 虚拟内存系统如何支持用户应用程序 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/362374163) <= 图文出处
>
> [VMA（虚拟内存空间）_百度百科 (baidu.com)](https://baike.baidu.com/item/VMA/9839255?fr=aladdin)
>
> 从user process 角度来说明的话，VMA 是 user process 里一段 virtual address space 区块；virtual address space 是连续的记忆体空间，当然VMA也会是连续的空间。VMA 对 Linux 的主要好处是，可以使记忆体的使用更有效率，并且更容易管理 user process address space。
>
> [深入Linux内核（内存篇）—用户内存空间之VMA_迷途小生的博客-CSDN博客_vma模式表任意一个字符的是()](https://blog.csdn.net/liyuewuwunaile/article/details/107272123) <= 推荐阅读，包括Linux中VMA的数据结构介绍。
>
> 内核管理用户空间的数据结构是`struct vm_area_struct`，简称VMA。
>
> [Linux的进程地址空间一 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/66794639)
>
> [Linux的进程地址空间二 - VMA - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/67936075) <= vma图文介绍，挺不错

​	在现代的Unix系统中，地址空间是由硬件Page Table来体现的，在Page Table中包含了地址翻译。但是通常来说，地址空间还包含了一些操作系统的数据结构，这些数据结构与任何硬件设计都无关，它们被称为**Virtual Memory Areas（VMAs）**。

​	**VMA会记录一些有关连续虚拟内存地址段的信息。在一个地址空间中，可能包含了多个section，每一个section都由一个连续的地址段构成，对于每个section，都有一个VMA对象**。<u>连续地址段中的所有Page都有相同的权限，并且都对应同一个对象VMA</u>（例如一个进程的代码是一个section，数据是另一个section，它们对应不同的VMA，VMA还可以表示属于进程的映射关系，例如下面提到的Memory Mapped File）。

![img](https://pic4.zhimg.com/80/v2-a0e10e1b19b16d8b176df8b54612189f_1440w.webp)

​	举个例子，**如果进程有一个Memory Mapped File，那么对于这段地址，会有一个VMA与之对应，VMA中会包含文件的权限，以及文件本身的信息，例如文件描述符，文件的offset等**。

​	*在接下来的mmap lab中，你们将会实现一个非常简单版本的VMA，并用它来实现针对文件的mmap系统调用。你可以在VMA中记录mmap系统调用参数中的文件描述符和offset*。

​	第二个部分我们了解的就不多了，它或许值得仔细看一下，也就是**User level trap**是如何实现的？我们假设一个PTE被标记成invalid或者只读，而你想要向它写入数据。这时，CPU会跳转到kernel中的固定程序地址，比如XV6中的trampoline代码（注，详见6.2）。kernel会保存应用程序的状态，在XV6中是保存到trapframe。之后再向虚拟内存系统查询，现在该做什么呢？虚拟内存系统或许会做点什么，例如在lazy lab和copy-on-write lab中，trap handler会查看Page Table数据结构。而在我们的例子中会查看VMA，并查看需要做什么。举个例子，如果是segfault，并且应用程序设置了一个handler来处理它，那么：

- segfault事件会被传播到用户空间
- 并且通过一个到用户空间的upcall在用户空间运行handler
- 在handler中或许会调用mprotect来修改PTE的访问权限
- 之后handler返回到内核代码
- 最后，内核再恢复之前被中断的进程。

![img](https://pic2.zhimg.com/80/v2-3eda94eef42b63e360a84169e2904ed1_1440w.webp)

​	当内核恢复了中断的进程时，如果handler修复了用户程序的地址空间，那么程序指令可以继续正确的运行，如果哪里出错了，那么会通过trap再次回到内核，因为硬件还是不能翻译特定的虚拟内存地址。

---

**问题：当我们允许用户针对Page Fault来运行handler代码时，这不会引入安全漏洞吗？**

回答：这是个很好的问题。会有安全问题吗？你们怎么想的？这会破坏User/kernel或者不同进程之间的隔离性吗？或者从另一个角度来说，你的问题是sigalarm会破坏隔离性吗？**当我们执行upcall的时候，upcall会走到设置了handler的用户空间进程中，所以handler与设置了它的应用程序运行在相同的context，相同的Page Table中。所以handler唯一能做的事情就是影响那个应用程序，并不能影响其他的应用程序，因为它不能访问其他应用程序的Page Table，或者切换到其他应用程序的Page Table**。所以这里还好。当然，如果handler没有返回，或者做了一些坏事，最终内核还是会杀掉进程。所以唯一可能出错的地方就是进程伤害了自己，但是它不能伤害任何其他进程。

## 17.4 VM原语使用-示例

> [17.4 构建大的缓存表 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/362374664) <= 图文出处
>
> [信号（LINUX信号机制）_百度百科 (baidu.com)](https://baike.baidu.com/item/信号/7927794?fr=aladdin)
>
> 内核处理一个进程收到的软中断信号是在该进程的上下文中，因此，进程必须处于运行状态。前面介绍概念的时候讲过，处理信号有三种类型：进程接收到信号后退出；进程忽略该信号；进程收到信号后执行用户设定用系统调用signal的函数。当进程接收到一个它忽略的信号时，进程丢弃该信号，就象没有收到该信号似 的继续运行。**如果进程收到一个要捕捉的信号，那么进程从内核态返回用户态时执行用户定义的函数。而且执行用户定义的函数的方法很巧妙，内核是在用户栈上创建一个新的层，该层中将返回地址的值设置成用户定义的处理函数的地址，这样进程从内核返回弹出栈顶时就返回到用户定义的函数处，从函数返回再弹出栈顶时， 才返回原先进入内核的地方。这样做的原因是<u>用户定义的处理函数不能且不允许在内核态下执行（如果用户定义的函数在内核态下运行的话，用户就可以获得任何权限）</u>**。

​	接下来，我将通过介绍几个例子来看一下如何使用之前介绍的内容。我会从一个非常简单的例子开始，之后我们会看一下Garbage Collector，因为很多同学都问了Garbage Collector的问题，所以GC是一个可以深入探讨的好话题。

​	首先我想讨论的是一个非常简单的应用，它甚至都没有在论文中提到，但它却是展示这节课的内容非常酷的一个方法。这个应用里是构建一个大的缓存表，什么是缓存表？它是用来记录一些运算结果的表单。举个例子，你可以这么想，下面是我们的表单，它从0开始到n。表单记录的是一些费时的函数运算的结果，函数的参数就是0到n之间的数字。

![img](https://pic4.zhimg.com/80/v2-7415100ec80026aa6d9eb758ad8f4c53_1440w.webp)

​	如果这个表单在最开始的时候就预计算好了，那么当你想知道f(i)的结果是什么时，你需要做的就是查看表单的i槽位，并获取f(i)的值。这样你可以将一个费时的函数运算转变成快速的表单查找，所以这里一个酷的技巧就是预先将费时的运算结果保存下来。如果相同的计算需要运行很多很多次，那么预计算或许是一个聪明的方案。

![img](https://pic4.zhimg.com/80/v2-371d7cdb78ca0ed4bd74590b18693757_1440w.webp)

​	这里的挑战是，表单可能会很大，或许会大过物理内存，这里可以使用论文提到的虚拟内存原语来解决这个挑战。

​	<u>首先，你需要分配一个大的虚拟地址段，但是并不分配任何物理内存到这个虚拟地址段</u>。这里只是从地址空间获取了很大一段地址，并说我将要使用地址空间的这部分来保存表单。

​	但是现在表单中并没有内容，表单只是一段内存地址。如果你现在查找表单的i槽位，会导致Page Fault。所以这里的计划是，在发生Page Fault时，先针对对应的虚拟内存地址分配物理内存Page，之后计算f(i)，并将结果存储于tb[i]，也就是表单的第i个槽位，最后再恢复程序的运行。

​	这种方式的优势是，如果你需要再次计算f(i)，你不需要在进行任何费时的计算，只需要进行表单查找。即使接下来你要查找表单的i+1槽位，因为一个内存Page可能可以包含多个表单项，这时也不用通过Page Fault来分配物理内存Page。

![img](https://pic4.zhimg.com/80/v2-7fd2a41fafdc13eb10cc534bbf6b7e3b_1440w.webp)

​	不过如果你一直这么做的话，因为表单足够大，你最终还是会消耗掉所有的物理内存。所以Page Fault Handler需要在消耗完所有的内存时，回收一些已经使用过的物理内存Page。<u>当然，你需要修改已经被回收了的物理内存对应的PTE的权限，这样在将来使用对应地址段时，就可以获得Page Fault。所以你需要使用Prot1或者ProtN来减少这些Page的accessbility</u>。

![img](https://pic1.zhimg.com/v2-952363547cfd5c475bb12be9bdfeefb4_r.jpg)

​	为了更具体的描述这里的应用，我这里有个小的实现，我们可以看一看这里是如何使用现有的Unix原语。

![img](https://pic4.zhimg.com/v2-e6d34f72b695b27f30bbb7ed096a7bdb_r.jpg)

​	在main函数中，首先调用setup_sqrt_region函数，它会从地址空间分配地址段，但是又不实际分配物理Page。之后调用test_sqrt_region。

![img](https://pic1.zhimg.com/v2-ce8871c7a16c241789a31cf0197e44e8_r.jpg)

​	在test_sqrt_region中，会以随机数来遍历表单，并通过实际运算对应的平方根值，来检查表单中相应位置值是不是保存了正确的平方根值。在test_sqrt_region运行的过程中，会产生Page Fault，因为现在还并没有分配任何物理内存Page。

​	**应用程序该如何收到Page Fault呢？在setup_sqrt_region函数中有一段代码，通过将handle_sigsegv函数注册到了SIGSEGV事件。这样当segfault或者Page Fault发生时，内核会调用handle_sigsegv函数**。

![img](https://pic3.zhimg.com/80/v2-0f0aa90f08d54a22776560d60194962a_1440w.webp)

​	handle_sigsegv函数与你们之前看过很多很多次的trap代码非常相似。

- 它首先会获取触发Page Fault的地址
- 之后调用mmap对这个虚拟内存地址分配一个物理内存Page（注，这里是mmap映射匿名内存）。这里的虚拟内存地址就是我们想要在表单中用来保存数据的地址
- 然后我们为这个Page中所有的表单项都计算对应的平方根值，之后就完事了

![img](https://pic2.zhimg.com/80/v2-8f1148e571c76451dbef1b69a00b9915_1440w.webp)

​	这个应用程序有点极端，它在运行的时候只会使用一个物理内存Page，所以不论上一次使用的Page是什么，在handle_sigsegv的最后都会通过munmap释放它。所以我们有一个巨大的表单，但是它只对应一个物理内存Page。

​	接下来我将运行一下这个应用程序。test_sqrt_region会随机查看表单的内容，所以可以假设这会触发很多Page Fault，但是可以看出表单中的所有内容都能通过检查。

![img](https://pic4.zhimg.com/80/v2-53e952575b5e4d04e74b834165088de7_1440w.webp)

​	所以，尽管这里有一个巨大的表单用来保存平方根，但是实际在物理内存中只有一个内存Page。这是一个简单的例子，它展示了用户应用程序使用之前提到的虚拟内存原语之后可以做的一些酷的事情。

---

**问题：在分配物理内存Page时，我们需要让操作系统映射到地址空间的特定地址，否则的话可能会映射到任意地址，是吧？**

**回答：操作系统会告知是哪个地址，并且这里可能是任意的地址。**

问题：能再讲一下为什么一个物理内存Page就可以工作吗？我觉得这像是lazy allocation，但是区别又是什么呢？

回答：<u>当我们刚刚完成设置时，我们一个内存Page都没有，setup_sqrt_region分配了一个地址段，但是又立即通过munmap将与这个地址段关联的内存释放了。所以在启动的最开始对于表单并没有一个物理内存Page与之关联</u>。之后，当我们得到了一个Page Fault，这意味着整个表单对应的地址中至少有一个Page没有被映射，虽然实际上我们一个Page都没有映射。现在我们得到了一个Page Fault，我们只需要映射一个Page，在这个Page中，我们会存入i，i+1。。。的平方根（注，因为一个Page大小4096字节，一个double8个字节，所以一个Page可以保存512个表单项）。因为这是第一个Page Fault，之前并没有映射了内存Page，所以不需要做任何事情。之后，程序继续运行并且查找了表单中的更多项，如果查找一个没有位于已分配Page上的表单项时，会得到另一个Page Fault。这时，在handle_sigsegv会分配第二个内存Page，并为这个Page计算平方根的值。之后会munmap记录在last_page_base中的内存。<u>当然，在实际中我们永远也不会这么做，在实际中至少会保留一些内存Page，这里只是以一种极端的方式展示，你可以只通过内存中的一个Page来表示一个巨大的表单。所以在handle_sigsegv中，会释放上一次映射的内存Page</u>。之后程序继续运行，所以在任何一个时间，只有一个物理内存Page被使用了。很明显，你们在实际中不会这么做，这里更多的是展示前面提到原语的能力。

## 17.5 Baker's Real-Time Copying Garbage Collector

> [17.5 Baker's Real-Time Copying Garbage Collector - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/362374806) <= 图文出处
>
> [GC 增量式垃圾回收_衣舞晨风的博客-CSDN博客_c# 增量式gc](https://blog.csdn.net/jiankunking/article/details/85011570)
>
> [Copying GC (Part one) - Léon_The_Pro - 博客园 (cnblogs.com)](https://www.cnblogs.com/Leon-The-Professional/p/9991589.html) <= 图文并茂，特别推荐阅读，可以补充这小节知识点的一些细节。
>
> 该算法把某个空间里的活动对象复制到其他空间，把原空间里的所有对象进行回收。再此我们**把复制活动对象的空间称为From空间，将粘贴活动对象的空间称为To空间**。

​	接下来我会讨论另一个例子，也就是Garbage Collector（注，后面将Garbage Collector和Garbage Collection都简称为GC），并且我也收到了很多有关Garbage Collector的问题。

​	GC是指编程语言替程序员完成内存释放，这样程序员就不用像在C语言中一样调用free来释放内存。对于拥有GC的编程语言，程序员只需要调用类似malloc的函数来申请内存，但是又不需要担心释放内存的过程。GC会决定内存是否还在使用，如果内存并没有被使用，那么GC会释放内存。GC是一个很好的特性，有哪些编程语言带有GC呢？Java，Python，Golang，几乎除了C和Rust，其他所有的编程语言都带有GC。

​	你可以想象，GC有很大的设计空间。这节课讨论的论文并没有说什么样的GC是最好的，它只是展示了GC可以利用用户空间虚拟内存原语。论文中讨论了一种特定的GC，这是一种**copying GC**。什么是copying GC？假设你有一段内存作为heap，应用程序从其中申请内存。你将这段内存分为两个空间，其中一个是from空间，另一个是to空间。当程序刚刚启动的时候，所有的内存都是空闲的，应用程序会从from空间申请内存。假设我们申请了一个类似树的数据结构。树的根节点中包含了一个指针指向另一个对象，这个对象和根节点又都包含了一个指针指向第三个对象，这里构成了一个循环。

![img](https://pic1.zhimg.com/v2-9a031b80e09b0ed00e66f4e22123dd30_r.jpg)

​	或许应用程序在内存中还有其他对象，但是没有别的指针指向这些对象，所以所有仍然在使用的对象都可以从根节点访问到。在某个时间，或许因为之前申请了大量的内存，已经没有内存空间给新对象了，也就是说整个from空间都被使用了。

​	<u>Copying GC的基本思想是将仍然在使用的对象拷贝到to空间去，具体的流程是从根节点开始拷贝</u>。每一个应用程序都会在一系列的寄存器或者位于stack上的变量中保存所有对象的根节点指针，通常来说会存在多个根节点，但是为了说明的简单，我们假设只有一个根节点。拷贝的流程会从根节点开始向下跟踪，所以最开始将根节点拷贝到了to空间，但是现在根节点中的指针还是指向着之前的对象。

![img](https://pic3.zhimg.com/80/v2-bc4b407ae8ce973e9ff6208a00831722_1440w.webp)

​	之后，GC会扫描根节点对象。因为程序的运行时知道对象的类型是什么，当然也就知道对象中的指针。接下来GC会将根节点对象中指针指向的对象也拷贝到to空间，很明显这些也是还在使用中的对象。当一个对象被拷贝到to空间时，根节点中的指针会被更新到指向拷贝到了to空间的对象。

![img](https://pic1.zhimg.com/80/v2-20e48bcd673740b50fb2eb43b1616c74_1440w.webp)

​	<u>在之后的过程中，我们需要记住这个对象已经被拷贝过了。所以，我们还会存储一些额外的信息来记住相应的对象已经保存在了to空间，这里会在from空间保留一个forwarding指针。这里将对象从from空间拷贝到to空间的过程称为forward</u>。

![img](https://pic3.zhimg.com/80/v2-cd96b62ef6250105130e30a40208d2a2_1440w.webp)

​	接下来还剩下一个对象，我们将这个对象从from空间拷贝到to空间，这个对象还包含一个指针指向第二个对象。

![img](https://pic4.zhimg.com/80/v2-8d5eae3fc297ec972f46fdfd68696dcf_1440w.webp)

​	但是通过查看指针可以看到这个对象已经被拷贝了，并且我们已经知道了这个对象被拷贝到的地址（注，也就是之前在from空间留下的forwarding指针）。所以我们可以直接更新第三个对象的指针到正确的地址。

![img](https://pic1.zhimg.com/80/v2-d44a4d0b658881f3ab753c6f2551421c_1440w.webp)

​	现在与根节点相关的对象都从from空间移到了to空间，并且所有的指针都被正确的更新了，所以现在我们就完成了GC，from空间的所有对象都可以被丢弃，并且from空间现在变成了空闲区域。

![img](https://pic2.zhimg.com/80/v2-8a34fb28eb77d9967fbf7948c0ac7319_1440w.webp)

​	以上就是copying GC的基本思路。论文中讨论的是一种更为复杂的GC算法，它被称为Baker算法，这是一种很老的算法。它的一个优势是它是**实时**的，这意味着它是一种**incremental GC**（注，incremental GC是指GC并不是一次做完，而是分批分步骤完成)。在Baker算法中，我们还是有from和to两个空间。假设其中还是包含了上面介绍的几个对象。

![img](https://pic3.zhimg.com/80/v2-cb51dc596bf111f44e8e1bf075534f4e_1440w.webp)

​	<u>这里的基本思想是，GC的过程没有必要停止程序的运行并将所有的对象都从from空间拷贝到to空间，然后再恢复程序的运行。GC开始之后，唯一必要的事情，就是将根节点拷贝到to空间</u>。所以现在根节点被拷贝了，但是根节点内的指针还是指向位于from空间的对象。根节点只是被拷贝了并没有被扫描，其中的指针还没有被更新。

![img](https://pic3.zhimg.com/80/v2-b6873c3a65d348062a0a8cb87f7e202e_1440w.webp)

​	<u>如果应用程序调用了new来申请内存，那就再扫描几个对象，并将这些对象从from空间forward到to空间。这很好，因为现在我们将拷贝heap中还在使用的所有对象的过程，拆分成了渐进的步骤。每一次调用new都使得整个拷贝过程向前进一步</u>。

![img](https://pic1.zhimg.com/80/v2-eb3f5eec0353d17f455253ad52d1a0c4_1440w.webp)

​	当然应用程序也会使用这里对象所指向的指针。举个例子，现在当根节点需要读出其指向的一个对象时，这个对象仍然在from空间。这是危险的，因为我们不应该跟踪from空间的指针（注，换言之GC时的指针跟踪都应该只在同一个空间中完成）。<u>所以每次获取一个指针指向的对象时（dereference），你需要检查对象是否在在from空间，如果是的话，将其从from空间forward到to空间。所以应用程序允许使用指针，但是编译器需要对每个指针的访问都包上一层检查，这样我们就可以保证在to空间的任何指针指向的是位于to空间的对象。我们需要确保这一点，因为在最后当GC完成了所有对象的跟踪之后，我们会清空from部分并重用这部分内存</u>。

![img](https://pic4.zhimg.com/80/v2-1270d6b9651b7c41ea4bd89eaae3a277_1440w.webp)

​	论文对于这里的方案提出了两个问题：

- 第一个是每次dereference都需要有以上的额外步骤，每次dereference不再是对于内存地址的单个load或者store指令，而是多个load或者store指令，这增加了应用程序的开销。
- 第二个问题是并不能容易并行运行GC。如果程序运行在多核CPU的机器上，并且你拥有大量的空闲CPU，我们本来可以将GC运行在后台来遍历对象的图关系，并渐进的拷贝对象。但是如果应用程序也在操作对象，那么这里可能会有抢占。应用程序或许在运行dereference检查并拷贝一个对象，而同时GC也在拷贝这个对象。如果我们不够小心的话，我们可能会将对象拷贝两遍，并且最后指针指向的不是正确的位置。所以这里<u>存在GC和应用程序race condition的可能</u>。

## 17.6 使用虚拟内存原语的GC

> [17.6 使用虚拟内存特性的GC - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/362374971) <= 图文出处

​	论文中介绍，如果拥有了前面提到的虚拟内存原语，你可以使用虚拟内存来减少指针检查的损耗，并且以几乎零成本的代价来并行运行GC。<u>这里的基本思想是将heap内存中from和to空间，再做一次划分，每一个部分包含scanned，unscanned两个区域。在程序启动，或者刚刚完成了from和to空间的切换时，整个空间都是unscanned，因为其中还没有任何对象</u>。

​	之后的过程与前面描述的相同，在开始GC时，我们将根节点对象拷贝到to空间，但是根节点中的指针还是指向了位于from空间的对象。现在unscanned区域包括了所有的对象（注，现在只有根节点），我们会将unscanned区域的权限设置为None。这意味着，当开始GC之后，应用程序第一次使用根节点，它会得到Page Fault，因为这部分内存的权限为None。

![img](https://pic3.zhimg.com/80/v2-b192c220116cbfe8af4905dd0ce16cda_1440w.webp)

​	<u>在Page Fault Handler中，GC需要扫描位于内存Page中所有的对象，然后将这些对象所指向的其他对象从from空间forward到to空间。所以，在GC最开始的时候，我们将根节点拷贝过来了；之后在Page Fault Handler中通过扫描，将根节点指向的对象也都拷贝过来了</u>。在我们的例子中根节点指向的只有两个对象，这两个对象会被拷贝到unscanned区域中，而根节点会被标记成scanned。在我们扫描完一个内存Page中的对象时，我们可以通过Unprot（注，详见17.1）恢复对应内存Page的权限。

![img](https://pic3.zhimg.com/80/v2-2e6626ce9f52f791830c55e24b4e6636_1440w.webp)

​	之后，应用程序就可以访问特定的对象，因为我们将对象中的指针转换成了可以安全暴露给应用程序的指针（注，因为这些指针现在指向了位于to空间的对象），所以应用程序可以访问这些指针。<u>当然这些指针对应的对象中还没有被扫描。如果dereference这些指针，我们会再次得到Page Fault，之后我们会继续扫描</u>。

​	这种方案的好处是，它仍然是**递增的GC**，因为每次只需要做一小部分GC的工作。除此之外，它还有额外的优势：现在不需要对指针做额外的检查了（注，也就是不需要查看指针是不是指向from空间，如果是的话，将其forward到to空间）。<u>或者说指针检查还在，只是现在通过虚拟内存相关的硬件来完成了</u>。

![img](https://pic3.zhimg.com/80/v2-8498c0f04aae704581d23c815dfaf132_1440w.webp)

​	论文中提到使用虚拟内存的另一个好处是，它**简化了GC的并发**。GC现在可以遍历未被扫描的内存Page，并且一次扫描一个Page，同时可以确保应用程序不能访问这个内存Page，因为对于应用程序来说，未被扫描的内存Page权限为None。虚拟内存硬件引入了这种显式的同步机制，或者说对于抢占的保护。

​	**现在只有GC可以访问未被扫描的内存Page，而应用程序不能访问。所以这里提供了自动的并发，应用程序可以运行并完成它的工作，GC也可以完成自己的工作，它们不会互相得罪，因为一旦应用程序访问了一个未被扫描的Page，它就会得到一个Page Fault。而GC也永远不会访问扫描过的Page，所以也永远不会干扰到应用程序。所以这里以近乎零成本获取到了并发性**。

​	但是实际上有个麻烦的问题。回到我们之前那张图，我们在heap中有from空间，to空间。在to空间中又分为了unscanned和scanned区域，<u>对于应用程序来说，unscanned区域中的Page权限为None</u>。

![img](https://pic1.zhimg.com/80/v2-fe96ab810c92aa738c33818211915a28_1440w.webp)

​	**这就引出了另一个问题，GC怎么能访问这个区域的内存Page？因为对于应用程序来说，这些Page是inaccessible**。

​	**这里的技巧是使用map2（注，详见17.1）。这里我们会将同一个物理内存映射两次，第一次是我们之前介绍的方式，也就是为应用程序进行映射，第二次专门为GC映射**。在GC的视角中，我们仍然有from和to空间。在to空间的unscanned区域中，Page具有读写权限。所以GC可以遍历这些内存Page，读取内容并forward必要的对象。**这里使用了map2将物理内存映射到应用程序地址空间中两次的能力，其中<u>每次映射都有不同的权限</u>，这样这里的场景才能工作**。

![img](https://pic4.zhimg.com/80/v2-59d1aa971b0ac1dc95fe635a75c3210f_1440w.webp)

---

问题：刚刚说到在Handler里面会扫描一个Page中的所有对象，但是对象怎么跟内存Page对应起来呢？

回答：在最开始的时候，to空间是没有任何对象的。当需要forward的时候，我刚刚描述的是拷贝一个对象，但是实际上拷贝的是一个内存Page中的N个对象，这样它们可以填满整个Page。所以现在我们在to空间中，有N个对象位于一个Page中，并且它们都没有被扫描。之后某个时间，Page Fault Handler会被调用，GC会遍历这个内存Page上的N个对象，并检查它们的指针。对于这些指针，GC会将对应的对象拷贝到to空间的unscanned区域中。之后，当应用程序使用了这些未被扫描的对象，它会再次得到Page Fault，进而再扫描这些对象，以此类推。

**问题：在完成了GC之后，会切换from和to空间吗？**

**回答：最开始我们使用的是from空间，当用完了的时候，你会将对象拷贝到to空间，一旦完成了扫描，from空间也被完全清空了，你可以切换两个空间的名字。现在会使用to空间来完成内存分配。直到它也满了，你会再次切换。**

**问题：GC和应用程序是不是有不同的Page Table？**

**回答：不，它们拥有相同的Page Table。它们只是将物理内存映射到了地址空间的两个位置，也就是Page Table的两个位置。在一个位置，PTE被标记成invalid，在另一个位置，PTE被标记成可读写的。**

## 17.7 使用虚拟内存原语的GC代码示例

> [17.7 使用虚拟内存特性的GC代码展示 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/362375116) <= 图文出处

​	为了更清晰的说明上一节的内容，我这里有个针对论文中方法的简单实现，我可以肯定它包含了一些bug，因为我并没有认真的测试它。

​	首先，应用程序使用的API包括了new和readptr。

![img](https://pic1.zhimg.com/80/v2-b5386dfef005ceaaf3d36d54303b6f84_1440w.webp)

​	readptr会检查指针是否位于from空间，如果是的话，那么它指向的对象需要被拷贝。当然，当我们使用虚拟内存时，这里的readptr成本会比较低，它会直接返回参数。在这个简单的例子中，我有一个循环链表，并且有两个根节点，其中一个指向链表的头节点，另一个指向链表的尾节点。

​	应用程序线程的工作是循环1000次，每次创建list，再检查list。

![img](https://pic3.zhimg.com/80/v2-a775d21ce203992ca454219221a9599a_1440w.webp)

​	所以它会产生大量的垃圾，因为每次make_clist完成之后，再次make_clist，上一个list就成为垃圾了。所以GC必然会有一些工作要做。

​	<u>make_clist的代码有点丑，主要是因为每个指针都需要被readptr检查包围。通常这里的检查代码是由编译器生成的。但是我这里并没有一个针对带GC的编程语言的编译器，所以我只能模仿一个编译器可能生成的内容</u>。

![img](https://pic2.zhimg.com/80/v2-50bd32732cf512f421cdbeb5c45a4e5d_1440w.webp)

​	make_clist会构建一个LISTSZ大小的链表，分配新的元素，并将新元素加到链表的起始位置，之后更新链表尾指针指向链表新的起始位置，这样就能构成一个循环链表。

​	这里更有趣的部分是，GC部分怎么实现。首先让我们看看如果没有虚拟内存会怎样。我们只需要查看两个API：new和readptr。

![img](https://pic2.zhimg.com/80/v2-f5a8a29747e8dfaf2379f2d84f9ca0b5_1440w.webp)

​	以上就是new的实现，先不考虑这里的mutex，因为这是为基于虚拟内存的实现提供的。先假设我们不需要扫描，也不需要collect。接下来会检查是否有足够的空间，如果有足够的空间，我们就将指针地址增加一些，以分配内存空间给新的对象，最后返回。

​	如果没有足够的空间，我们需要调用flip，也就是运行GC。

![img](https://pic4.zhimg.com/80/v2-cea20eee4ad89dde77f16089e4f8882b_1440w.webp)

​	flip首先会切换from和to指针，之后将这个应用程序的两个根节点从from空间forward到to空间。接下来我们看一下forward函数。

![img](https://pic3.zhimg.com/80/v2-6f787aa21ac64dab4d08f7bedb7d1c4a_1440w.webp)

​	<u>这个函数会forward指针o指向的对象，首先检查指针o是不是在from空间，如果是的话，并且之前没有被拷贝过，那么就将它拷贝到to空间。如果之前拷贝过，那么就可以用to空间的指针代替对象指针，并将其返回</u>。

![img](https://pic2.zhimg.com/80/v2-3ff22cd55d70cc8c6fc0eda68f217855_1440w.webp)

​	对于readptr，如果我们没有使用虚拟内存。会对指针p做forward操作，forward操作的意思是如果对象在from空间，那么就将其拷贝到to空间，所以这里会有耗时的检查。

​	接下来我们看一下这里如何使用虚拟内存。

![img](https://pic4.zhimg.com/80/v2-f0e86d1cefbe0ed550620db78ee20f33_1440w.webp)

​	首先是设置内存，通过[shm_open](https://man7.org/linux/man-pages/man3/shm_open.3.html)创建一个Share-memory object，shm_open是一个Linux/Uinx系统调用。<u>Share-memory object表现的像是一个文件，但是它并不是一个文件，它位于内存，并没有磁盘文件与之对应，如果你愿意的话，可以认为它是一个位于内存的文件系统</u>。

​	之后我们裁剪这个Shared-memory object到from和to空间的大小。

​	**之后我们通过mmap先将其映射一次，以供mutator也就是实际的应用程序使用。然后再映射一次，以供GC使用。这里shm_open，ftruncate，和两次mmap，等效于map2**。

​	回过去看之前的代码，

![img](https://pic2.zhimg.com/80/v2-3ff22cd55d70cc8c6fc0eda68f217855_1440w.webp)

​	使用了虚拟内存之后，readptr将不做任何事情，直接将参数返回。<u>当然，如果我们使用这里的指针，并且指针对应的对象位于unscanned区域，我们会得到Page Fault</u>。

![img](https://pic3.zhimg.com/80/v2-312db59454c9f135c9077a34191561c2_1440w.webp)

​	**在Page Fault hanlder中，GC会运行scan函数。但是scan函数是以GC对应的PTE来运行的，所以它能工作。而同时，应用程序或者mutator不能访问这些Page，如果访问了的话，这会产生Page Fault。一旦scan执行完成，handler中会将Page设置成对应用程序可访问的（注，也就是调用mprotect）**。

​	<u>在flip函数中，完成from和to空间的切换时，如果使用了虚拟内存，我们会通过mprotect将整个to空间对应用程序标记成不可访问的。之后GC将root_head和root_last移到to空间中，这样应用程序就不能访问这两个对象，任何时候应用程序需要访问这两个对象，都会导致一个Page Fault。在Page Fault handler中，GC可以将其他对象从from空间拷贝到to空间，然后再Unprot对应的Page</u>。

![img](https://pic4.zhimg.com/80/v2-cea20eee4ad89dde77f16089e4f8882b_1440w.webp)	

​	**在Page Fault handler中，先scan内存Page，再将内存Page标记成对应用程序可访问的这个顺序是至关重要的。因为如果你先将内存Page标记成应用程序可访问的，然后再扫描它，如果有多个应用程序线程，那么应用程序可能会查看到unscanned区域的对象。当然我们要禁止这一点（注，因为为了避免抢占，unscanned区域只能GC访问），所以这里的代码是先扫描，再增加内存的访问权限，这样应用程序就可以安全的访问这些内存Page**。

## 17.8 小结

> [17.7 使用虚拟内存特性的GC代码展示 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/362375116) <= 图文出处
>
> [Linux下的Meltdown攻击实践（含代码） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/391325673)

​	接下来，我总结一下这节课的内容。有一个问题，你应该在这里使用虚拟内存吗？或者说这里的这些技巧值得吗？<u>许多的GC并没有使用虚拟内存，而是通过编译器生成的代码来完成GC，并且还有各种其他的技巧来减少性能损耗。所以GC的大部分场景都可以通过一些额外的指令来完成</u>。这对于一个编译器，程序运行时，或者编程语言来说，并不是一个太糟糕的选择，因为编译器就可以完成这些操作。但是如果没有程序运行时或者编译器，那么这个过程就会很痛苦。所以**对于一些完全没有编译器参与的应用程序，例如checkpointing，shared-virtual memory，它们的确需要这里提到的虚拟内存特性。实际中，足够多的应用程序开发人员发现了这些特性的价值，所以今天的操作系统都支持了这些虚拟内存特性**。

![img](https://pic4.zhimg.com/80/v2-69961d9c370aa4048d410ec61844bc7f_1440w.webp)

​	很多人问了这个问题，从91年（论文发表的年份）至今，虚拟内存系统发生了什么改变？其中一个改变是，大部分的Unix系统都支持了这些虚拟内存特性了，并且从91年至今有许多变化。或许很难想象，但是在虚拟内存系统中有持续的开发，所以如果你查看Linux的git log，你可以发现在内核的各个方面都有持续的开发，其中包括了对虚拟内存系统的持续开发。在过去有一些重大的改变，比如说：

- 现在的Page Table是5级的，这样可以处理非常大的地址
- 可以通过地址空间标识符来处理TLB flush
- 大概一年前，一种叫做KPTI（kernel page table isolation）的功能被引入，它是针对Meltdown attack的功能

​	虚拟内存系统绝对不是一个静态的系统，几乎Linux内核的所有方向都不是静态的。几乎每两个月在内核的不同方向都会有大量的更新。所以每个子系统时不时的就会被重写。

![img](https://pic3.zhimg.com/80/v2-3dfac035dfd3472d42c4655bf978ce52_1440w.webp)

---

问题：VMA中的连续地址是什么意思？

回答：这里是指连续的虚拟内存地址，比如说一个VMA表示1000-2000这段地址。如果你有另一段地址，2100-2200，那么它会有属于自己的VMA。所以每个VMA覆盖了一段连续的地址，中间不会有中断。你们将会在mmap lab中看到这样的设计是更加的合理的。你们可以认为对于每个mmap系统调用，如果地址没有重叠的话，都会有一个VMA。

问题：GC什么时候会停止，什么时候又会再开始？我认为GC可以一直运行，如果它是并发的。

回答：是的，基于虚拟内存的解决方案一个酷的地方在于，GC可以一直运行。它可以在没有unscanned对象时停止。

问题：但是你需要遍历所有在from空间的对象，你怎么知道已经遍历了所有的对象呢？

回答：你会从根节点开始扫描整个对象的图，然后拷贝到to空间。在某个时间点，你不再添加新的对象了，因为所有的对象已经被拷贝过了。当你不再添加新的对象，你的unscanned区域就不再增长，如果它不再增长，那么你就遍历了所有的对象（注，可以想象一个普通的DFS或者BFS过程）。

# Lecture18 内核分类(OS Organization)

## 18.1 宏内核(Monolithic kernel)

> [18.1 Monolithic kernel - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/363843024) <= 图文出处
>
> [一文搞懂宏内核、微内核以及混合内核 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/394560786)

​	首先，让我说明一下操作系统的传统实现方式以及应该具备的功能。我个人将Linux，Unix，XV6称为用传统方式实现的操作系统。另一个形容这些操作系统的词是**monolithic**。monolithic的意思是指操作系统内核是一个完成了各种事情的大的程序。实际上，这也反应了人们觉得内核应该具备什么样的功能。类似于Linux的典型操作系统内核提供了功能强大的抽象。它们选择提供例如文件系统这样一个极其复杂的组件，并且将文件，目录，文件描述符作为文件系统的接口，而不是直接将磁盘硬件作为接口暴露给应用程序。<u>monolithic kernel通常拥有例如文件系统这样强大的抽象概念，这比提供一些简单的抽象有着巨大的优势</u>。

- 其中一个好处是，这些**高度抽象的接口**通常是**可移植**的，你可以在各种各样的存储上实现文件和目录，你可以使用文件和目录而不用担心它们是运行在什么牌子的磁盘，什么类型的存储之上，或许是SSD，或许是HDD，或许是NFS，但是因为文件系统接口是高度抽象的，所以它们都拥有相同的接口。所以这里的一个好处是可以获取可移植性。你可以在不修改应用程序的前提下，将其运行在各种各样的硬件之上。
- 另一个例子是，Linux/Unix提供**地址空间的抽象**而不是直接访问MMU硬件的权限。这不仅可以提供可移植性，并且也可以向应用程序**隐藏复杂性**。所以操作系统具备强大抽象的另一个好处是，它们可以向应用程序隐藏复杂性。举个例子，XV6提供的文件描述符非常简单，你只需要对文件描述符调用read/write就可以，但是在XV6内核中是非常复杂的代码来实现读写磁盘上的文件系统。这对于程序员是极好的，但是内核却因此变得又大又复杂。
- 这里的强大的抽象还可以**帮助管理共享资源**。例如我们将内存管理委托给了内核，内核会跟踪哪些内存是空闲的。类似的，内核还会跟踪磁盘的哪个部分是空闲的，磁盘的哪个部分正在被使用，这样应用程序就不用考虑这些问题，所以这可以帮助简化应用程序。同时也可以**提供健壮性和安全性**，因为如果允许应用程序决定磁盘的某个位置是否是空闲的，那么应用程序或许可以使用一个已经被其他应用程序使用的磁盘位置。所以，内核管理硬件资源可以提供资源共享能力和安全性。但是同样的，这也使得内核变得更大。内核提供的这些诱人的抽象能力，使得内核包含了很多的复杂性，进而导致内核很大且复杂。

![img](https://pic1.zhimg.com/80/v2-c72df6ea8082e99b98cbfd6f9aff2434_1440w.webp)

- 有关monolithic kernel的另一个好处是，因为所有这些功能都在一个程序里面，所有的内核子系统，例如文件系统，内存分配，调度器，虚拟内存系统都是集成在一个巨大的程序中的一个部分，这意味着<u>它们可以访问彼此的数据结构，进而使得依赖多个子系统的工具更容易实现</u>。举个例子，exec系统调用依赖文件系统，因为它要从磁盘中读取二进制文件并加载到内存中，同时它也依赖内存分配和虚拟内存系统，因为它需要设置好新的进程的地址空间，但是它的实现是相对简单的。在XV6或者Linux中做到这些完全没问题，因为这些操作系统已经在内核程序中包含了文件系统和虚拟内存系统。但是如果严格分隔了文件系统和虚拟内存系统，那么实现类似exec的系统调用将会难得多。在一个monolithic操作系统中，因为本身就是一个大的程序，实现起来会容易的多。
- 另一个使得类似于XV6或者Linux的操作系统中可以更简单实现软件的原因是，**内核的所有代码都以完整的硬件权限在运行**。举个例子，整个XV6都运行在Supervisor mode，这意味着你可以读写任意内存地址，并且所有的内核代码都以最大的权限在运行。<u>Linux操作系统也是这样</u>。

​	所以，monolithic kernel这种设计策略对于内核开发人员来说非常方便，并且也更容易构建更强大的抽象，进而对于应用程序开发人员来说也更容易。然而，对于传统的monolithic kernel，也有一些缺点。这也是之所以会出现其他内核架构，比如说微内核的原因。所以这里的问题是，为什么不在所有的场合使用monolithic kernel呢？

+ 第一个原因是它们大且复杂。取决于你怎么计算，Linux总是有数十万到数百万行代码。<u>Linux的一部分可以查看Linux的另一个部分的数据，的确使得编程更加容易，但是同样也使得内部代码有大量的交互和依赖</u>。有的时候查看并弄明白Linux代码会有点挑战。任何时候你有了一个大的程序，尤其它们还具有复杂的结构，你都会有Bug，操作系统内核也不例外。在这些年，内核有过各种各样的Bug，包括了安全性相关的Bug。所以这是一个令人烦恼的关系图，如果你使用了大的内核，你不可避免的会遇到Bug和安全漏洞。

![img](https://pic1.zhimg.com/80/v2-4d9c6c5ca1d91ac20ef8c4896b5fde68_1440w.webp)

+ <u>另一个人们不喜欢monolithic kernel的原因是，随着时间的推移，它们倾向于发展成拥有所有的功能</u>。Linux应用在各种场合中，从移动电话到桌面工作站，从笔记本电脑到平板电脑，从服务器到路由器。Linux可以支持这么多设备是极好的，但是这也使得Linux非常的通用，所以Linux支持了很多很多不同的东西。而任何一个应用程序，例如我的web server程序不太会需要用到Linux中非常复杂的声卡支持。所以，Linux中包含了大量的内容使得它很通用，这很好，但是另一方面，通用就意味着慢。**对于各种不同的场景都能支持，或许就不能对某些特定场景进行优化**。当你尝试快速运行一些程序时，如果程序只做一两件事情是极好的，因为这样你就可以专注在优化一两个代码路径上。<u>但是如果你的程序想要做上千件事情，优化会更加难</u>。Linux并不一定慢，但是你或许会想，它真的在所有场景下都达到了最快速度吗？如果你去看Linux或者XV6中的任何模块，你或许会想它们真的有必要去做所有的事情吗？例如，你从一个进程向一个Pipe写一个字节传输到另一个进程，即使在XV6这样一个简单的内核中，都有大量的指令需要被执行。这里有buffering，locking，或许在Pipe的读写中有sleep/wakeup，或许有线程调度导致context switching，对于从一个进程移动一个字节到另一个进程来说，这里有大量的内容或许并不是必须的。

![img](https://pic1.zhimg.com/80/v2-b8bd436afed04893f2891dc4a59cd5f8_1440w.webp)

+ <u>对于monolithic kernel来说，另一个潜在的问题是，因为它们是如此之大，它们会削弱一些复杂的抽象能力。在内核中会有大量的设计考虑，应用程序需要遵守这些设计并与之共存。反之，在一个理想世界中，应用程序或许可以做更多的决定</u>。举个例子，在Unix中，你可以wait子进程，比如说你fork出来的子进程，但是你不能wait其他进程（注，详见13.7），或许你会想要wait孙子进程或者一个不相关的进程，但是这是不可能的。或许你会想要更改其他进程的地址空间，比如说替其它受你控制的进程调用mmap，但是这也不可能。mmap只能修改你自己的地址空间，但是不能修改其他进程的地址空间。<u>或许你是个数据库，你在磁盘上有B树索引，你或许知道很多快速展开B树的方法，但是当你读写文件系统中的文件时，文件系统并不知道你正在读写一个B树，以及如何更快的在磁盘上展开B树。所以如果你是个数据库的话，你或许很高兴文件系统可以任你摆布，但是文件系统并不会按照你想要的方式工作</u>。以上就是内核中需要考虑的设计。

![img](https://pic2.zhimg.com/80/v2-f6ba7546758de40c51e3058cc5c7c9b5_1440w.webp)

+ monolithic kernel的最后一个问题是，**可扩展性(Extensibility)**。应用程序或许想要实时更改内核，比如说向内核下载代码并更改内核的工作方式，这样数据库或许就可以更改数据在磁盘上的分布方式。至少在10年前，monolithic kernel没有任何功能可以支持这里的Extensibility，你只能使用内核提供的能力。

![img](https://pic3.zhimg.com/80/v2-bb55a65a79658eb0ba9872411a5175f2_1440w.webp)

​	以上就是人们认为的monolithic kernel的问题。这些问题使得人们去思考操作系统的其他架构。这里有很多不同的想法，其中一些想法非常的激进，我们今天只会讨论其中的一种，也是现在非常流行的一种，这就是微内核，Micro kernel。

## 18.2 微内核(Micro kernel)

> [18.2 Micro kernel - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/363843275) <= 图文出处
>
> [微内核_百度百科 (baidu.com)](https://baike.baidu.com/item/微内核/3856137?fr=aladdin)
>
> 微内核（Micro kernel）是提供操作系统核心功能的内核的精简版本，它设计成在很小的[内存空间](https://baike.baidu.com/item/内存空间/22010756?fromModule=lemma_inlink)内增加移植性，提供[模块化设计](https://baike.baidu.com/item/模块化设计/3431616?fromModule=lemma_inlink)，以使用户安装不同的接口，如DOS、Workplace OS、Workplace UNIX等。IBM、Microsoft、[开放软件基金会](https://baike.baidu.com/item/开放软件基金会/1223731?fromModule=lemma_inlink)(OSF)和UNIX系统实验室(USL)、鸿蒙OS等新操作系统都采用了这一研究成果的优点。
>
> 微内核（英文中常译作&micro;-kernel或者micro kernel）。是一种能够提供必要服务的[操作系统内核](https://baike.baidu.com/item/操作系统内核?fromModule=lemma_inlink)；其中这些必要的服务包括任务，线程，交互[进程通信](https://baike.baidu.com/item/进程通信?fromModule=lemma_inlink)（IPC，Inter－Process Communication）以及[内存管理](https://baike.baidu.com/item/内存管理?fromModule=lemma_inlink)等等。所有服务（包括[设备驱动](https://baike.baidu.com/item/设备驱动?fromModule=lemma_inlink)）在用户模式下运行，而处理这些服务同处理其他的任何一个程序一样。因为每个服务只是在自己的[地址空间](https://baike.baidu.com/item/地址空间?fromModule=lemma_inlink)运行。所以这些服务之间彼此之间都受到了保护。
>
> 微核的目标是将[系统服务](https://baike.baidu.com/item/系统服务?fromModule=lemma_inlink)的实现和系统的基本操作规则分离开来。例如，进程的输入/输出锁定服务可以由运行在微核之外的一个服务组件来提供。这些非常模块化的用户态服务器用于完成操作系统中比较高级的操作，这样的设计使内核中最内核的部分的设计更简单。一个服务组件的失效并不会导致整个系统的崩溃，内核需要做的，仅仅是重新启动这个组件，而不必影响其它的部分
>
> [ipc（进程间通信）_百度百科 (baidu.com)](https://baike.baidu.com/item/ipc/91622?fr=aladdin)
>
> [进程间通信_百度百科 (baidu.com)](https://baike.baidu.com/item/进程间通信/1235923?fromModule=lemma_inlink)

​	有关微内核的很多思想可以回溯到计算机的发展历史中。微内核从1980年代中后期开始就是一个非常热门的研究课题，它是指一种通用的方法或者概念，它并不特指任何特定的产品。有很多人遵循微内核的设计思想并构建了操作系统，但是这些项目中的每一个具体的操作系统都与另一个非常不一样。

​	微内核的核心就是实现了**IPC（Inter-Process Communication）**以及线程和任务Task的tiny kernel。**所以微内核只提供了进程抽象和通过IPC进程间通信的方式，除此之外别无他物**。<u>任何你想要做的事情，例如文件系统，你都会通过一个**用户空间**进程来实现，完全不会在内核中实现</u>。

![img](https://pic4.zhimg.com/v2-1f53133dc6a7c46fb3fda775ad204b3b_r.jpg)

​	画个图来展示一下，整个计算机还是分为两层，下面是kernel，上面是用户空间。在用户空间或许还是会有各种各样常见的程序，例如VI，CC，桌面系统。<u>除此之外，在用户空间还会有文件系统以及知道如何与磁盘交互的磁盘驱动，或许我们还会有一个知道如何进行TCP通信的网络协议栈，或许还有一个可以实现酷炫虚拟内存技巧的虚拟内存系统</u>。

![img](https://pic2.zhimg.com/v2-fa514355a7e829793fd472f00fdcaaed_r.jpg)

​	当文本编辑器VI需要读取一个文件时，它需要与文件系统进行交互，所以它通过**IPC**会发送一条消息到文件系统进程。文件系统进程中包含了所有的文件系统代码，它知道文件，目录的信息。文件系统进程需要与磁盘交互，所以它会发送另一个IPC到磁盘驱动程序。磁盘驱动程序再与磁盘硬件进行交互，之后磁盘驱动会返回一个磁盘块给文件系统。之后文件系统再将VI请求的数据通过IPC返回给VI。

![img](https://pic2.zhimg.com/v2-05470eb527176691dc308dfdca6affd9_r.jpg)

​	**这里需要注意的关键信息是，在内核中唯一需要做的是支持进程/任务/线程，以及支持IPC来作为消息的传递途径，除此之外，内核不用做任何事情**。**<u>内核中没有任何文件系统，没有任何设备驱动，没有网络协议栈，所有这些东西以普通用户进程在运行</u>**。所以这提供给你一种非常小的内核，以及相对少的代码去优化，你可以优化IPC，除此之外也没有别的东西了。上图就是我们在这节课剩下的时间要讨论的内容。

​	在今天仍然有使用微内核的场景。实际上，今天[论文](https://pdos.csail.mit.edu/6.828/2020/readings/microkernel.pdf)要讨论的<u>L4微内核</u>就有很多很多的运行实例，它用在很多手机中用来控制手机的射频，<u>在最近的iphone中的一个旁路处理器中，也使用了L4微内核来隐藏加密密钥</u>。所以**<u>在一些微型嵌入式系统中，微内核能够胜出，比如说在一些专门为某种功能设计的计算机中，你需要使用某种操作系统，而你又不需要Linux带来的复杂性</u>**。

​	**微内核中的用户进程通过IPC通信，这在很多操作系统都存在**。例如我现在运行的macOS，它就是一个普通的monolithic kernel，它也很好的支持用户进程通过IPC进行通信。所以**用户进程通过内核内的IPC相互通信，这是一个成功的思想并且被广泛采用**。

​	以上就是微内核的基本架构，我接下来会讨论一下这个架构的优势。

## 18.3 微内核的期望和挑战

> [18.3 Why micro kernel? - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/363843644) <= 图文出处
>
> [开源微内核seL4_刘秋杉的博客-CSDN博客_sel4内核](https://blog.csdn.net/BlueCloudMatrix/article/details/46772171?utm_source=blogxgwz9)
>
> 作为微内核，seL4为应用程序提供少量的服务，如创建和管理虚拟内存地址空间的抽象，线程和进程间通信IPC。

​	人们构建微内核的动机是什么？

​	其中一个动机你可能不常看到，这就是审美（注，原话是sense of aesthetic）。我认为很多人都觉得像Linux内核这样大的复杂的程序并不十分优雅。我们肯定可以构建一些小得多且专注的多的设计，而不是这样一个巨大的拥有各种随机特性的集合体。所以从审美角度来说，我们必然可以做的比大的内核更好。

​	其他还有一些更具体且可量化的动机。

- 更小的内核或许会更加的安全。只有几行代码的话，Bug也不会太多，其他人利用Bug来破坏安全性的可能也就更小。
- 在特殊场景下，你需要证明一个操作系统是正确的，没有Bug，并且只做了它应该做的事情。**现实中至少有一种经过验证是安全的微内核系统：seL4。这是L4微内核的多个衍生项目之一**。人们知道怎么去验证中小型程序，但是不知道怎么验证巨大的程序。微内核通常都很小，这是它能够被证明是安全的一个关键因素。
- 小内核的另一个优势是，少量代码的程序比巨大的程序更容易被优化。
- 小内核可能会运行的更快，你不用为一些用不上的很多功能付出代价。相比monolithic kernel，微内核几乎不会做任何事情，所以你不用为你不使用的很多功能付出代价。
- <u>使用小内核的另一个原因是，小内核或许自带了少得多的设计限制，进而使得应用程序的设计限制也更少</u>。这样给应用程序提供了**更多的灵活性**，使得应用程序可以自己做出设计决定。

![img](https://pic1.zhimg.com/80/v2-c2706c4a8784779df3c38c8c33cbc0c0_1440w.webp)

​	**以上都不是微内核必须要支持的特性，只是说人们期望通过使用微内核可以得到的特性**。

- **另一个微内核吸引人的原因是，有很多我们习惯了位于内核的功能和函数，现在都运行在用户空间**。这种将内核拆分，并在用户空间的不同部分运行，比如说在用户空间运行文件系统服务，可以使得代码更模块化。
- 用户空间代码通常会比内核更容易被修改，调整和替换，所以它更容易被定制化。
- 将操作系统放在用户空间，或许可以使得它更加的健壮。如果内核出错了，通常你需要panic并重启，因为如果内核有Bug，并且会随机更改数据，那就不能信任内核了。然而，如果你将内核运行成一些用户空间的服务，其中一个出现故障，比如说除以0，索引了一个野指针，或许只有这一个服务会崩溃，操作系统的剩余部分还是完好的，这样你可以只重启那一个服务。所以，**将操作系统的功能移到用户进程可以使得系统更加健壮。这对于驱动来说尤其明显，<u>内核中大部分Bug都在硬件驱动中，如果我们能将设备驱动从内核中移出的话，那么内核中可能会有少的多的Bug和Crash</u>**。
- 最后一个优势是，你**<u>可以在微内核上模拟或者运行多个操作系统</u>**。所以尽管微内核几乎不做任何事情，你还是可以在它之上运行一个Unix系统之类的，或许还可以在同一个机器上运行超过一个操作系统。<u>今天的论文主要就是描述如何在微内核之上将Linux作为服务来运行</u>。

![img](https://pic4.zhimg.com/80/v2-b2d57a0f29b01d039fbf14af86a27473_1440w.webp)

​	以上就是人们在微内核这条道路上期望得到的一些好处。

​	当然，这里也有一些挑战。

- 如果你想要设计属于你自己的微内核，其中一个挑战是你会想要你的微内核的系统调用接口尽可能的简单，因为使用微内核的出发点就是内核很小。那么什么才是有用的系统调用的最小集？这一点并不十分明确。所以这里**我们要确定最少的系统调用API，你需要这些系统调用API尽可能的少，但是你又需要基于这些API构建一些非常复杂的功能，因为即使内核没有做太多工作，你最终还是要运行程序**。或许你想要在微内核之上运行Unix，你需要能执行类似fork，mmap的工作。所以底层的系统调用在简单的同时，需要能够足够强大以支持人们需要做的各种事情，比如说exec，fork，copy-on-write fork，mmap file。
- 但是**内核又完全不知道文件和文件系统(因为变成用户空间的应用程序实现了，不归内核管理)。所以现在系统调用需要支持exec，而内核又不知道文件。微内核或许会非常简单，但是我们仍然需要开发一些用户空间服务来实现操作系统的其他部分**。
- 最后，**<u>微内核的设计需要进程间通过IPC有大量的通信。所以有很大的需求使得IPC能够足够的快</u>**。我们会好奇，IPC可以足够的快来使得微内核足够有竞争力吗？
- 有关性能，不仅与IPC的速度相关。通常来说，monolithic kernel可以获得更好的性能，是因为它里面的文件系统代码和与虚拟内存代码可以直接交互，它们开心的位于一个巨大的程序中。<u>但是如果你需要将这些模块都拆分开成为不同的服务，那么在集成的时候就有更少的机会可以优化，这或许会影响性能</u>。

![img](https://pic4.zhimg.com/v2-fd36c085262e947a36965da25e7804ab_r.jpg)

​	以上就是许多微内核项目要面对的用户期望和潜在的挑战。

## 18.4 L4 micro kernel

> [18.4 L4 micro kernel - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/363843813) <= 图文出处

​	今天要讨论的[论文](https://pdos.csail.mit.edu/6.828/2020/readings/microkernel.pdf)，有许多有关L4微内核的内容。这是今天论文作者开发和使用的一种微内核。L4必然不是最早的微内核，但是从1980年代开始，它是最早一批可以工作的微内核之一，并且它非常能展现微内核是如何工作的。在许多年里面它一直都有活跃的开发和演进。如果你查看Wikipedia，L4有15-20个变种，有一些从1980年代开始开发的项目现在还存在。接下来我将从我的理解向你们解释L4在今天的论文发表的时候是如何工作的。

​	<u>首先，L4是微内核，它只有7个系统调用，虽然其中有一些稍微有点复杂，但是它还是**只有7个系统调用**。然而现在的Linux，我上次数了下有大概350个系统调用。甚至XV6这个极其简单的内核，也有21个系统调用。从这个指标来看，L4更加简单</u>。

​	其次，**L4并不大，论文发表的时候，它只有13000行代码，这并不多**。XV6的代码更少，我认为XV6内核只有6000-7000行代码，所以作为内核XV6非常的简单。L4也没有复杂太多，它只有Linux代码的几十分之一，所以它非常的小。

![img](https://pic1.zhimg.com/v2-9c9dfd14a1e68d278a98bfaf47dc17cc_r.jpg)

​	第三，它**只包含几个非常基础的抽象**。

​	**它在内部有一个叫做Task或者地址空间的概念，这或多或少的对应了Uinx内的进程概念**。<u>Task包含了一些内存，地址从0开始，并且可以像进程一样执行指令。区别于XV6的是，每个Task可以有多个线程，L4会调度每个Task内的多个线程的执行。这样设计的原因是，可以非常方便的用线程来作为组织程序结构的工具</u>。我不知道在论文发表的时候，L4是否支持了多处理器，或许它包含了在多个处理器上运行同一个程序的能力。所以**L4内核知道Task，知道线程，也知道地址空间，这样你就可以告诉L4如何映射地址空间内的内存Page**。
​	**另一个L4知道的事情是IPC。每一个线程都有一个标识符，其中一个线程可以说，我想要向拥有这个标识符的另一个线程发送几个字节**。

​	**<u>这里的Task，线程，地址空间，IPC是L4唯一有的抽象</u>**。

![img](https://pic1.zhimg.com/80/v2-cea46d23f61b2aba018e15ff5c43e114_1440w.webp)

​	我不确定是否能列出所有的系统调用，这里涉及到的系统调用有：

- Threadcreate系统调用，你提供一个地址空间ID并要求创建一个新的线程。如果地址空间或者Task不存在，系统调用会创建一个新的Task。<u>所以这个系统调用即可以创建线程，又可以创建Task</u>。
- Send/Recv IPC系统调用。
- <u>Mapping系统调用可以映射内存Page到当前Task或者其他Task的地址空间中</u>。你可以要求L4来改变当前Task的地址空间和Page Table，**如果你有足够的权限，你也可以要求L4改变其他Task的地址空间**。**<u>这实际上是通过IPC完成的，你会发送一个特殊的IPC消息到目标线程，内核可以识别这个IPC消息，并会修改目标线程的地址空间</u>**。如果你创建一个新的线程，新线程最开始没有任何内存。所以如果你想创建一个线程，你先调用Threadcreate系统调用来创建新的线程，新的Task和地址空间。然后你创建一个特殊 IPC，将你自己内存中的一部分，其中包含了指令和数据，映射到新的Task的地址空间中。之后你再发送一个特殊的Start IPC消息到这个新的Task，其中包含了你期望新的Task开始执行程序的程序计数器和Stack Pointer。之后新的Task会在你设置好的内存中，从你要求的程序计数器位置开始执行。
- 虽然我不知道具体是怎么实现的，但是<u>Privileged Task可以将硬件控制寄存器映射到自己的地址空间中</u>。所以**L4并不知道例如磁盘或者网卡的设备信息，但是实现了设备驱动的用户空间软件可以直接访问设备硬件**。
- <u>**你可以设置L4将任何一个设备的中断转换成IPC消息**。这样，运行设备驱动的Task不仅可以读写了设备，并且也可以设置L4将特定设备的中断通过IPC消息发送给自己</u>。
- 最后，**一个Task可以设置L4内核通知自己有关另一个Task的Page Fault**。所以如果<u>一个Task发生了Page Fault，L4会将Page Fault转换成一个IPC消息，并发送给另一个指定的Pager Task。每一个Task都有个与之关联的Pager Task用来处理自己相关的Page Fault。这就是关联到Page Fault的方法，通过它可以实现类似copy-on-write fork或者lazy allocation</u>。

![img](https://pic3.zhimg.com/80/v2-59c0ac79b36eeec3b5978d2bf3bf48aa_1440w.webp)

​	<u>**以上就是内核的内容，L4里面不包含其他的功能，没有文件系统，没有fork/exec系统调用，除了这里非常简单的IPC之外，没有其他例如pipe的通信机制，没有设备驱动，没有网络的支持等等**</u>。任何其他你想要的功能，你需要以用户空间进程的方式提供。

​	**L4能提供的一件事情是完成线程间切换。L4会完成线程调度和context switch，来让多个线程共用一个CPU**。它实现的方式你会觉得非常熟悉，L4会为每个Task保存寄存器，当它执行一个线程时，它会跳到用户空间，切换到那个线程对应Task的Page Table，之后那个线程会在用户空间执行一会。之后或许会有一个定时器中断，定时器是L4知道的一个设备，定时器中断会使代码执行返回到L4内核，L4会保存线程的用户寄存器，然后在一个类似于XV6的线程调度循环中，选择一个Task来运行。通过将这个Task之前保存的寄存器恢复出来，切换Page Table，就可以跳转到Task中再运行一会，直到再发生另一个定时中断，或者当前Task出让了CPU。所以我认为L4或许还有一个yield系统调用。在这种情况下Task可以等待接收一个IPC消息，这时代码会跳转回L4内核，L4内核会保存寄存器，并切换到一个新的Task。所以L4中有关线程切换的部分你们会非常熟悉。

​	**我之前提到过这个概念，Pager。如果一个进程触发了Page Fault，通过trap走到了内核，内核会将Page Fault转换成IPC消息并发送到指定的Pager Task，并告诉Pager Task是哪个线程的哪个地址触发了Page Fault**。在Pager Task中，如果它实现了lazy allocation，那么它会负责从L4分配一些内存，向触发Page Fault的Task发送一个特殊的IPC，来恢复程序的运行。所以Pager Task实现了XV6或者Linux在Page Fault Handler中实现的所有功能。如果你想的话，你可以在Pager Task中实现copy-on-write fork或者memory mapped files，Pager Task可以实现基于Page Fault的各种技巧。

​	这是类似L4的微内核相比传统的内核，对于用户程序要灵活的多的众多例子之一。如果Linux并没有copy-on-write fork，并且你想要有这个功能，你不可能在不修改内核的前提下完成这个功能。Linux中没有办法写一些可移植的用户空间代码来实现copy-on-write fork。这样描述可能并不完全正确，但是一定要这么做的话会很复杂。然而，在L4里面，这就相对简单了。<u>L4就好像是完全设计成让你去写用户空间代码来获取Page Fault，并实现copy-on-write fork。所有这些都可以在用户空间完成，而不用弄乱内核</u>。

​	你们可以看到，**这里的微内核设计非常依赖IPC，因为如果你想与你的文件系统交互，文件系统想要与设备驱动交互，你都需要来回发送IPC消息。对于每个系统调用，每个Page Fault，每个设备中断，都会有反复的IPC消息。所以IPC系统需要非常快**。

----

问题：能说明一下Task和线程之间的区别吗？

回答：可以。一个Task就像XV6的一个进程一样，它有一些内存，一个地址空间，你可以在其中运行用户代码。如果你在XV6中有一个进程，它只能包含一个线程。但是在现代的操作系统和L4中，在一个进程，一个地址空间中，可以有多个线程。如果你有多个CPU核，那么多个CPU核可以同时运行一个Task。每个线程在Task的地址空间中都有一个设置好的Stack，这意味着你可以写一个程序，并通过并行运行在多个CPU核上得到性能的提升，其中的每个线程都运行在不同的CPU核上。

## 18.5 微内核IPC的发展和改进

> [18.5 Improving IPC by Kernel Design - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/363843983) <= 图文出处
>
> [零复制_百度百科 (baidu.com)](https://baike.baidu.com/item/零复制/22742547?fr=aladdin)
>
> **零复制**（英语：**Zero-copy**；也译**零拷贝**）技术是指[计算机](https://baike.baidu.com/item/计算机/140338?fromModule=lemma_inlink)执行操作时，[CPU](https://baike.baidu.com/item/CPU?fromModule=lemma_inlink)不需要先将数据从某处[内存](https://baike.baidu.com/item/内存?fromModule=lemma_inlink)复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。

​	接下来我们讨论微内核里面一个非常重要的问题：IPC的速度。<u>首先让我展示一个非常简单，但是也非常慢的设计。这个设计基于Unix Pipe。我之所以介绍这种方法，是因为一些早期的微内核以一种类似的方式实现的IPC，而这种方式实际上很慢</u>。

​	假设我们有两个进程，P1和P2，P1想要给P2发送消息。这里该怎么工作呢？一种可能是使用send系统调用，传入你想将消息发送到的线程的ID，以及你想发送消息的指针。这个系统调用会跳到内核中，假设我们是基于XV6的pipe来实现，那么这里会有一个缓存。或许P2正在做一些其他的事情，并没有准备好处理P1的消息，所以消息会被先送到内核的缓存中。所以当你调用send系统调用，它会将你的消息追加到一个缓存中等待P2来接收它。在实际中，几乎很少情况你会只想要发送一个消息，你几乎总是想要能再得到一个回复。所以P1在调用完send系统调用之后，会立即调用recv来获取回复。但是现在让我们先假设我们发送的就是单向的IPC消息，send会将你的消息追加到位于内核的缓存中，我们需要从用户空间将消息逐字节的拷贝到内核的缓存中。之后再返回，这样P1可以做一些其他的事情，或许是做好准备去接受回复消息。

![img](https://pic4.zhimg.com/80/v2-587d1df87db241c9461a14b0eed5ce5f_1440w.webp)

​	过了一会，P2可以接收消息了，它会调用recv系统调用，这个系统调用会返回发送消息线程的ID，并将消息从内核拷贝到P2的内存中。所以这里会从内核缓存中取出最前的消息，并拷贝到P2的内存中，之后再返回。

![img](https://pic2.zhimg.com/80/v2-88e8e75671e90352516f3d3eedc6f9d9_1440w.webp)

​	这种方式被称为**异步传输(asynchronous)**，因为P1发完消息之后，只是向缓存队列中追加了一条消息，并没有做任何等待就返回了。同时这样的系统这也被称作是**buffered system**，因为<u>内核在发送消息时将每条消息都拷贝到了内部的缓存中，之后当接收消息时，又从buffer中将消息拷贝到了目标线程。所以这种方法是异步buffered</u>。

![img](https://pic3.zhimg.com/80/v2-e93d57eb5fd342bca3059fd0a6549c42_1440w.webp)

​	如果P1要完成一次完整的消息发送和接收，那么可以假设有两个buffer，一个用来发送消息，一个用来接收消息。P1会先调用send，send返回之后。之后P1会立即调用recv，recv会等待接收消息的buffer出现数据，所以P1会出让CPU。在一个单CPU的系统中，只有当P1出让了CPU，P2才可以运行。论文中的讨论是基于单CPU系统，所以P1先执行，之后P1不再执行，出让CPU并等待回复消息。这时，P2才会被调度，之后P2调用recv，拷贝消息。之后P2自己再调用send将回复消息追加到buffer，之后P2的send系统调用返回。假设在某个时间，或许因为定时器中断触发导致P2出让CPU，这时P1可以恢复运行，内核发现在接收消息buffer有了一条消息，会返回到用户空间的P1进程。

![img](https://pic3.zhimg.com/v2-5a68449a22d5f3decd5ef6e3df6d5196_r.jpg)

​	这意味着在这个慢的设计中，为了让消息能够发送和回复，将要包含：

- 4个系统调用，两个send，两个recv
- 对应8次用户空间内核空间之间的切换，而每一次切换明显都会很慢
- 在recv的时候，需要通过sleep来等待数据出现
- 并且需要至少一次线程调度和context switching来从P1切换到P2

​	**每一次用户空间和内核空间之间的切换和context switching都很费时，因为每次切换，都需要切换Page Table，进而清空TLB，也就是虚拟内存的查找缓存，这些操作很费时**。所以这是一种非常慢的实现方式，它包含了大量的用户空间和内核空间之间的切换、消息的拷贝、缓存的分配等等。实际中，对于这里的场景：发送一个消息并期待收到回复，你可以抛开这种方法并获得简单的多的设计，L4就是采用了后者。

​	有关简单的设计在一篇著名的论文中有提到，论文是[Improving IPC by Kernel Design](https://www.cse.unsw.edu.au/~cs9242/19/papers/Liedtke_93.pdf)，这篇论文在今天要讨论的论文前几年发布。相比上面的慢设计，它有几点不同：

- 其中一点是，它是**同步的(Synchronized)**。所以这里不会丢下消息并等待另一个进程去获取消息，这里的send会等待消息被接收，并且recv会等待回复消息被发送。如果我是进程P1，我想要发送消息，我会调用send。send并不会拷贝我的消息到内核的缓存中，P1的send会等待P2调用recv。P2要么已经在内核中等待接收消息，要么P1的send就要等P2下一次调用recv。<u>当P1和P2都到达了内核中，也就是P1因为调用send进入内核，P2因为调用recv进入内核，这时才会发生一些事情</u>。**这种方式快的一个原因是，如果P2已经在recv中，P1在内核中执行send可以直接跳回到P2的用户空间，从P2的角度来看，就像是从recv中返回一样，这样就不需要context switching或者线程调度**。相比保存寄存器，出让CPU，通过线程调度找到一个新的进程来运行，这是一种快得多的方式。<u>P1的send知道有一个正在等待的recv，它会立即跳转到P2，就像P2从自己的recv系统调用返回一样。这种方式也被称为**unbuffered**。它不需要buffer一部分原因是因为它是同步的</u>。

![img](https://pic1.zhimg.com/v2-05538b9d1f5d2c0de39e261c63eeac64_r.jpg)

- **<u>当send和recv都在内核中时，内核可以直接将消息从用户空间P1拷贝到用户空间P2，而不用先拷贝到内核中，再从内核中拷出来</u>。因为现在消息收发的两端都在等待另一端系统调用，这意味着它们消息收发两端的指针都是确定的。recv会指定它想要消息被投递的位置，所以在这个时间点，我们知道两端的数据内存地址，内核可以直接拷贝消息，而不是需要先拷贝到内核**。
- <u>**如果消息超级小，比如说只有几十个字节，它可以在寄存器中传递，而不需要拷贝，你可以称之为Zero Copy**</u>。前面说过，发送方只会在P2进入到recv时继续执行，之后发送方P1会直接跳转到P2进程中。<u>从P1进入到内核的过程中保存P1的用户寄存器，这意味着，如果P1要发送的消息很短，它可以将消息存放到特定的寄存器中。当内核返回到P2进程的用户空间时，会恢复保存了的寄存器，这意味着当内核从recv系统调用返回时，特定寄存器的内容就是消息的内容，因此完全不需要从内存拷贝到内存，也不需要移动数据，消息就存放在寄存器中，可以非常快的访问到</u>。当然，这只对短的消息生效。

![img](https://pic1.zhimg.com/80/v2-86051a8e0bdd4b237959e12146c98680_1440w.webp)

+ **对于非常长的消息，L4可以在一个IPC消息中携带一个Page映射，所以对于巨大的消息，比如说从一个文件读取数据，你可以发送一个物理内存Page，这个Page会被再次映射到目标Task地址空间，这里也没有拷贝**。这里提供的是共享Page的权限。所以短的消息很快，非常长的消息也非常快。<u>对于长的消息，你需要调整目的Task的Page Table，但是这仍然比拷贝快的多</u>。

![img](https://pic4.zhimg.com/v2-80bc924899d853b3c89a1dc81d42208b_r.jpg)

+ 最后一个L4使用的技巧是，如果它发现这是个RPC，有request和response，并且有非常标准的系统调用包括了send和recv，你或许会结合这两个系统调用，以减少用户态和内核态的切换。所以对于RPC这种特别的场景，同时也是人们使用IPC的一个常见场景，有一个call系统调用，它基本上结合了send和recv，区别是这里不会像两个独立的系统调用一样，先返回到用户空间，再次进入到内核空间。在消息的接收端，会有一个sendrecv系统调用将回复发出，之后等待来自任何人的request消息。<u>这里基本是发送一个回复再加上等待接收下一个request，这样可以减少一半的内核态和用户态切换</u>。

![img](https://pic3.zhimg.com/80/v2-ced1b782f3e7a3a7343db6589fb4c0aa_1440w.webp)

​	**实际中，所有的这些优化，对于短的RPC请求这样一个典型的场景，可以导致20倍速度的提升**。这是论文中给出的对比之前慢设计提升的性能倍数。这个数字很了不起。Improving IPC by Kernel Design这篇论文是由今天这篇论文的同一个作者在前几年发表的，因为现在IPC可以变得非常的快，它使得人们可以更加认同微内核。

![img](https://pic1.zhimg.com/80/v2-1b126fee6819ad9a226b519e99a6e04c_1440w.webp)

---

**问题：当使用这些系统调用时，进程是什么时候发送和接收消息的？**

**回答：对于包含request和response的RPC，进程使用call和sendrecv这一对系统调用，而不是send和recv。对于call，你会传入两个参数，你想要发送的消息，以及你要存放回复消息的位置，这个系统调用在内核中会结合发送和接收两个功能。你可以认为这是一种hack，因为IPC使用的是如此频繁，它值得一些hack来使得它变得更快**。

问题：在上面的图中，P2会调用recv系统调用，P2怎么知道应该去调用这个系统调用？

回答：在RPC的世界中，我们有client会发送request到server，server会做一些事情并返回。因为P2是一个server，我们会假设P2会一直在一个while循环中，它随时准备从任何client接收消息，做一些数据处理工作，比如在数据库中查找数据，之后再发送回复，然后再回到循环的最开始再等待接收消息。所以我们期望P2将所有时间都花费在等待从任何一个客户端接收消息上。前面讨论的设计需要依赖P2进程在暂停运行时，一直位于内核的recv系统调用中，并等待下一个request。这样，下一个request才可以直接从这个系统调用返回，这种快速路径在这里的设计中超级有效率。

问题：这里提到从P1返回到P2，为了能返回到P1，需要P2发送response吗？

回答：是的，我们期望P2发送一个response，发送response与发送request是同一个代码路径，只是方向相反（之前是P1到P2现在是P2到P1），所以当P2发送一个response，这会导致返回到P1。**P1实际调用的是call系统调用，通过从call系统调用返回到P1，会将P2的response送到P1**。这里与你们以为的通常的设置略有不同，通常情况下，你从P1通过系统调用进入到内核，在内核中执行系统调用然后再返回，所有的工作都在P1这边，这也是pipe的read/write的工作方式。<u>在这里，P1进入到内核，但是却返回到了P2。所以这里有点奇怪，但是却非常的快</u>。

## 18.6 在L4微内核上运行Linux

> [18.6 Run Linux on top of L4 micro kernel - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/363844502) <= 图文出处

​	前一节对于IPC的优化使得人们开始认真考虑使用微内核替代monolithic kernel。然而，这里仍然有个问题，即使IPC很快了，操作系统的剩余部分从哪里去获取？现在的微内核大概只有一个完整操作系统的百分之几，我们该怎么处理操作系统剩下的部分？这个问题通常会在一些有着相对较少资源的学校研究项目中被问到，我们需要从某个地方获取到所有这些用户空间服务。

​	实际上在一些特殊的应用场合，以上的问题并不是问题，比如说我们运行的一些设备的控制器，例如车里的点火控制器，只运行了几千行代码，它并且不需要一个文件系统，这样我们就只需要很少的用户空间内容，微内核也特别适合这种应用程序。但是微内核项目发起时，人们非常有雄心壮志，人们想的是完全替换操作系统，人们希望可以构建一些运行在工作站，服务器等各种地方的微内核操作系统，并取代大的monolithic kernel。对于这种场景，你需要一个传统操作系统所需要的所有内容。

​	**一种可能是，重新以微内核的方式，以大量的进程实现所有的内容。实际上有项目在这么做，但是这涉及到大量的工作**。具体的说，比如我想要使用笔记本电脑，我的电脑必须要有emacs和我最喜欢的C编译器，否则我肯定不会用你的操作系统。这意味着，<u>微内核要想获得使用，它必须支持现有的应用程序，它必须兼容或者提供相同的系统调用或者更高层的服务接口，它必须能够完全兼容一些现有的操作系统，例如Unix，Linux，这样人们才愿意切换到微内核</u>。所以这些微内核项目都面临一个具体的问题，它们怎么兼容一些为Linux，Windows写的应用程序？**对于论文中提到的项目，也就是L4，对标的是Linux。与其写一些完全属于自己的新的用户空间服务，并模仿Linux，论文中决定采用一种容易的多的方法，其实许多项目也都采用了这种方法，也就是简单的将一个现有的monolithic kernel运行在微内核之上，而不是重新实现一些新的东西**。这就是今天论文要介绍的内容。

​	**在今天[论文](https://pdos.csail.mit.edu/6.828/2020/readings/microkernel.pdf)的讨论中，L4微内核位于底部，但是同时，一个完整的Linux作为一个巨大的服务运行在用户空间进程中**。听起来有点奇怪，一般的kernel都是运行在硬件之上，而现在Linux kernel是一个用户空间进程。

![img](https://pic3.zhimg.com/80/v2-8eb39a0f8db61b01debb77002394ffa6_1440w.webp)

​	<u>实际上，如你在QEMU上运行XV6时所见，内核也是运行在用户空间</u>。**Linux kernel不过就是一个程序，对其做一些修改它就可以运行在用户空间，所以现在Linux需要被修改**。论文中提到需要对Linux的底层做一些修改，例如Linux中期望能直接修改Page Table的内容，读写CPU寄存器。<u>Linux中一部分需要被修改以将它们改成调用L4微内核的系统调用，或者发送IPC，而不是直接访问硬件</u>。但是Linux的大部分内容都可以不做修改而直接运行。所以按照这种方式，作为Linux的一部分，现在得到了文件系统，网络支持，各种设备驱动等等，而不需要自己实现这些。

![img](https://pic1.zhimg.com/80/v2-2fc9e49041e6fb9f2d89c637b02f34d8_1440w.webp)

​	**这里的实现方式是将Linux内核作为一个L4 Task运行，每一个Linux进程又作为一个独立的L4 Task运行**。所以当你登录到Linux中时，你要它运行一个Shell或者terminal，它会在用户空间创建一个L4 Task来运行这个Linux程序。<u>所以现在有一个Task运行Linux，以及N个Task来运行每一个你在Linux中启动的进程</u>。

![img](https://pic4.zhimg.com/80/v2-aafeedd7524f25330ffe407ff422f25b_1440w.webp)

​	**这里Linux不会直接修改进程的Page Table，而是会向L4发送正确的IPC让L4来修改进程的Page Table**。

​	<u>这里有很多小的改动，其中一个有意思的地方是，当VI想要执行一个系统调用时，VI并不知道它运行在L4之上，在上面的方案中，所有的程序都以为它们运行在Linux中</u>。<u>当VI要执行系统调用时，L4并不支持，因为VI要执行的是Linux系统调用而不是L4系统调用。所以对于Linux进程，会有一个小的库与之关联，这个库会将类似于fork，exec，pipe，read，write的系统调用，转换成发送到Linux kernel Task的IPC消息，并等待Linux kernel Task的返回，然后再返回到进程中。从VI的角度看起来好像就是从系统调用返回了</u>。所以这些小的库会将系统调用转成发送到Linux kernel Task的IPC消息。这意味着，如果Linux kernel Task没有做其他事情的话，它会在一个recv系统调用中等待接收从任何一个进程发来的下一个系统调用请求IPC。

![img](https://pic3.zhimg.com/80/v2-56281725cff092a9d4045df8b8e8b3b6_1440w.webp)

​	这导致了这里的Linux和普通的Linux明显不同的工作方式。**在普通的Linux中，就像XV6一样，会有一个内核线程对应每一个用户空间进程。当用户空间进程调用系统调用时，内核会为这个系统调用运行一个内核线程。并且，<u>在普通的Linux中，如果内核在内核线程之间切换，这基本上意味着从一个用户进程切换到另一个用户进程</u>**。<u>所以这里Linux kernel的内核线程以及当Linux完成工作之后要运行的用户进程之间有一对一的关系</u>。

​	**在这里架构中，这种一对一的关系断了，这里的Linux kernel运行在一个L4线程中**。<u>然而，就像XV6一样，这个线程会使用与XV6中的context switching非常相似的技术，在与每个用户进程对应的内核线程之间切换。**不过这些内核线程完全是在Linux中实现的，与L4线程毫无关系，唯一的L4线程就是运行了Linux kernel的控制线程**</u>。

![img](https://pic3.zhimg.com/80/v2-3fe68eae18fc62610962813ce594355a_1440w.webp)

​	但是**哪个用户进程可以运行，是由L4决定的**。所以在这里的设置中，Linux kernel或许在内核线程中执行来自VI的系统调用，同时，L4又使得Shell在用户空间运行了。这在XV6或者Linux极不可能发生，在这两个系统中，活跃的内核线程和用户进程有直接的对应关系，而L4会运行它喜欢的任何Task。**因为Linux kernel中的内核线程都是私有的实现，Linux可以同时执行不同阶段的多个系统调用，或许一个进程在它的内核线程中在等待磁盘，这时Linux可以运行另一个进程的内核线程来处理另一个进程的系统调用**。

​	**你或许会想知道为什么不直接使用L4线程来实现Linux内的内核线程，或者说Linux为什么要实现自己内部的内核线程，而不是使用L4线程**，答案是，

- 在论文发表时，还没有用到多核CPU硬件，他们使用的是单核CPU硬件。所以在内核中同时运行多个内核线程并没有性能优势，因为只有一个CPU核，所以第二个线程不能执行，由于硬件的限制，一次只能执行一个线程。
- 另一个或许是更强大的原因是，在论文发表时，他们使用的Linux版本并不支持将Linux kernel运行在多个CPU核上。所以他们使用的是旧版本的单核Linux，一次只能期望在内核中使用一个CPU，它并没有类似于XV6的spinlock，可以使得它能正确的在内核中使用多核。所以在Linux内核中使用多个L4线程并没有性能优势。如果一定要使用的话，在没有性能优势的前提下，又需要加入spinlock和其他的内容来支持并发。所以论文中没有在Linux内核使用L4线程。

​	这种架构的一个缺点是，在普通原生的Linux中，存在大量复杂的线程调度机制，例如在不同进程上增加优先级，确保调度公平性等等。Linux可以在你的笔记本上运行这些机制，因为Linux控制了哪些进程可以运行在哪些CPU核上。但是在这里的架构中，Linux完全控制不了哪些进程可以运行，因为现在是L4而不是Linux在完成调度，这些进程都是被L4所调度。所以<u>这里的架构失去了Linux的调度能力，这是这种架构的缺点，我相信L4的后续版本有一些方法能够让Linux通知L4调度器，来给某个进程更高优先级等等</u>。

## 18.7 L4 Linux性能分析

> [18.7 L4 Linux性能分析 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/363844710) <= 图文出处
>
> [混合内核_百度百科 (baidu.com)](https://baike.baidu.com/item/混合内核/4239577?fr=aladdin)
>
> 混合内核实质上是[微内核](https://baike.baidu.com/item/微内核?fromModule=lemma_inlink)，只不过它让一些微核结构运行在[用户空间](https://baike.baidu.com/item/用户空间?fromModule=lemma_inlink)的代码运行在[内核空间](https://baike.baidu.com/item/内核空间?fromModule=lemma_inlink)，这样让内核的运行效率更高些。这是一种妥协做法，设计者参考了[微内核](https://baike.baidu.com/item/微内核?fromModule=lemma_inlink)结构的系统运行速度不佳的理论。
>
> [简单分析一下windows到底是什么内核架构 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/380490096)

​	你应该问自己：通过[论文](https://pdos.csail.mit.edu/6.828/2020/readings/microkernel.pdf)可以学到有关微内核的什么内容呢？

​	对于我们来说，论文中有很多有趣的有关微内核是如何运行，有关Linux是如何运行的小的知识点，以及你该如何设计这么一个系统。但是**论文并没有回答这个问题：微内核是不是一个好的设计？论文只是讨论了微内核是否有足够的性能以值得使用**。

​	论文之所以讨论这个内容的原因是，在论文发表的前5-10年，有一场著名的测试针对一种更早的叫做MACH的微内核。它也运行了与L4类似的结构，但是内部的设计完全不一样。通过测试发现，当按照前一节的架构运行时，MACH明显慢于普通的Unix。这里有很多原因，比如IPC系统并没有如你期望的一样被优化，这样会有更多的用户空间和内核空间的转换，cache-miss等等。有很多原因使得MACH很慢。但是很多人并不关心原因，只是看到了这个测试结果，发现MACH慢于原生的操作系统，并坚信微内核是无可救药的低效，几乎不可能足够快且足够有竞争力。很多人相信应该都使用monolithic kernel。

​	**今天的论文像是对于这种观点的一个反驳，论文中的观点是，你可以构建类似上一节的架构，如果你花费足够的精力去优化性能，你可以获取与原生操作系统相比差不多的性能。因此，你不能只是因为性能就忽视微内核。今天的论文要说明的是，你可以因为其他原因不喜欢微内核，但是你不能使用性能作为拒绝微内核的原因**。

​	达成这一点的一个重要部分是，IPC被优化的快得多了，相应的技术在18.5中提到过。

![img](https://pic3.zhimg.com/80/v2-80b0cc8fa93b472a099f16db4f670d3e_1440w.webp)

​	论文的表二做了性能对比，运行在硬件上的原生Linux执行一个简单的系统调用getpid花费1.7us，对于上一节的实现，需要发送一个IPC request，并获取一个IPC response，以实现getpid系统调用，这需要花费接近4us，这是原生Linux的两倍多。主要是因为这里有两倍的工作，这里涉及到两次用户空间到内核空间的切换，而不是一个简单的系统调用。这也说明L4已经将这种基于IPC的系统调用的代价降到了最低，也就是2倍于一个原生Linux的系统调用。因此，它可以做的大概与你期望的一样好。

​	当然这里的系统调用仍然只有原生Linux一半的速度。现在还不清楚这是否是一个灾难，还是说并没有问题。如果你执行大量的系统调用或许就是个问题；如果你执行了相对较少的系统调用，或者系统调用本身就有很多工作，或者你的系统调用比getpid要复杂的多，这又或许不是个问题。论文中通过使用AIM做的测试结果，给出了答案。测试结果在论文的图8。



![img](https://pic3.zhimg.com/80/v2-99585dbc4b80c4491a7d37c8f655a512_1440w.webp)



​	AIM会执行各种系统调用，例如read/write文件，创建进程等等。**从图8可以看出，在AIM设置的一个更完整的应用中，基于L4的Linux之比原生Linux慢几个百分点**。因此，理想情况下你可以期望你想要运行在计算机上行的应用，如果在L4+Linux上运行可以与运行在原生操作系统上一样快。因为可以以近似原生Linux的速度运行，所以你们现在应该认真对待微内核。图8是一个非常不错的结果，有点超出预期。

​	让时间快进20年，**<u>如果之前所说，现在人们实际上在一些嵌入式系统中使用L4，尤其在智能手机里有很多L4实例在运行，它们与Unix并没有兼容性</u>**。在一些更通用的场景下，像是工作站和服务器， 微内核从来没有真正的流行过，并不是因为这里的设计有什么问题，只是为了能够吸引一些软件，微内核需要做的更好，这样人们才会有动力切换到微内核。对于人们来说很难决定微内核是否足够好，这样才值得让他们经历从现在正在运行的Linux或者其他系统迁移到微内核的所需要的各种麻烦事。所以，**微内核从来没有真正流行过，因为它们并没有明显的更好**。

​	另一方面来看，微内核的很多思想都有持久的影响。

- **人们实现了更加灵活和有趣的方法来在微内核上使用虚拟内存。这些复杂得多的接口导致了mmap这样的系统调用合并到了例如Linux的主流操作系统中**。
- **<u>论文中将一个操作系统作为一个用户程序运行另一个操作系统之上，今天以另一种方式非常流行的存在：在Virtual Machine Monitor上运行虚拟机。这种方式在各种场景，例如云主机上，都有使用</u>**。
- **<u>为了让内核能够具有一个用户空间服务一样的可扩展性，在Linux中演进成了可加载的内核模块，这使得你可以在线修改Linux的工作方式</u>**。
- 当然，这里基于IPC的Client-Server支持，也在macOS有所体现，macOS中也有好用的IPC。

----

问题：论文4.3 Dual-Space Mistake能介绍一下吗？

回答：这里稍微有点复杂。这里的一部分背景是，**论文发表时的Linux，甚至直到最近，当你运行在x86上，且运行在用户空间时，使用的Page Table同时会有用户空间的内存Page，以及所有的内核内存Page**。<u>所以当你执行系统调用，并跳转到内核中，内核已经映射到了Page Table中，因此不需要切换Page Table。所以当你执行一个系统调用时，代价要小得多，因为这里没有切换Page Table</u>。如果你回想我们之前介绍的内容，trampoline代码会切换Page Table（注，也就是更新SATP寄存器，详见6.5）。这是个代价很高的操 作，因为这会涉及到**清除TLB**。所以**出于性能的考虑，Linux将内核和用户进程映射到同一个Page Table，进而导致更快的系统调用**。论文中期望的是，当用户空间进程向Linux发送一个系统调用，并且Linux的内核线程在处理系统调用，Page Table也包含发送系统调用的进程的所有虚拟内存映射，这会使得作为系统调用参数传入的虚拟内存地址查找更加的简单。但是为什么这里不能很好工作？**首先，L4并不知道这里的任何具体实现，在L4的眼里这就是两个进程。当你从一个进程发送IPC到另一个进程，L4只是会切换Page Table**。<u>由于现在Linux的系统调用是基于L4实现的，没有办法在系统调用的过程中保持Page Table，因为L4在两个进程间切换时总是会切换Page Table。所以这里不能得到系统调用时不切换Page Table带来的性能优势</u>。我认为这里希望得到可以在内核中直接使用用户空间的虚拟内存地址的便利，但是这意味着在Linux内核中需要知道是在执行哪个进程的系统调用，并使用那个进程的Page Table。当然**L4并不知道这里的细节，它只是给每个进程关联了一个Page Table。所以L4只会给Linux关联一个Page Table，Linux并没有办法在处理不同进程的系统调用时使用不同的Page Table**。<u>为了解决这个问题，论文中为每个进程都做了共享内存拷贝，每一个共享内存拷贝都有内核的所有内存，所以都有相同的数据结构。因为每个进程都有一个kernel task与之关联，因此可以使得L4切换到合适的Page Table同时包含了进程和内核的内存</u>。我认为这里可以工作，但是忘记了这里是否会很慢之类的，因为这里有大量的任务。这是个复杂的故事，我不知道解释清楚了没有。

问题：看起来一些任务更适合在内核中，但是内核的方案中，要么所有东西都在内核要么都不在。所以要么你有一个monolithic kernel可以完成所有的事情，要么有个micro kernel什么也不做。我认为虚拟内存、文件系统和一些其他的事情在内核中做会非常的有效。不能有些系统具备有一些功能，然后你又可以选择用不用这些功能吗？

回答：所有你说的都有非常有道理。实际上有很多微内核相关的项目都构建了各种hybrid内核，MACH有一些不同的版本，其中一些就是**hybrid内核**，这些内核的核心是包括了IPC的微内核，同时在内核中又是一个完整的Unix系统，比如MACH 2.5就是这样一个hybrid内核，其中一些东西是按照微内核的方式构建，一些东西又是按照Unix方式构建。现代的macOS也以与你描述类似的方式构建，macOS本身是个完整的操作系统，包含了文件系统，同时也很好的支持了IPC和其他用来构建微内核的东西。Google Fuchsia也实现了一些这里的想法。
