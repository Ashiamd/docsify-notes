# Kafka权威指南-学习笔记02

# 7. 构建数据管道

​	Kafka 为数据管道带来的主要价值在于，它可以作为数据管道各个数据段之间的大型缓冲区， 有效地解耦管道数据的生产者和消费者。Kafka 的解耦能力以及在安全和效率方面的可靠性，使它成为构建数据管道的最佳选择。

> **数据集成的场景**
>
> 有些组织把Kafka 看成是数据管道的一个端点，他们会想“我怎么才能把数据从Kafka 移到ElasticSearch 里”。这么想是理所当然的一一特别是当你需要的数据在到达ElasticSearch 之前还停留在Kafka 里的时候，其实我们也是这么想的。不过我们要讨论的是如何在更大的场景里使用Kafka ，这些场景至少包含两个端点（可能会更多） ，而且这些端点都不是Kafka 。**对于那些面临数据集成问题的人来说，我们建议他们从大局考虑问题，而不只是把注意力集中在少量的端点上。过度聚焦在短期问题上，只会增加后期维护的复杂性，付出更高的成本。**

​	本章将讨论在构建**数据管道**时需要考虑的几个常见问题。这些问题并非Kafka 独有， 它们都是与数据集成相关的一般性问题。我们将解释为什么可以使用Kafka 进行数据集成，以及它是如何解决这些问题的。我们将讨论Connect API 与普通的客户端API (Producer 和Consumer）之间的区别，以及这些客户端API 分别适合在什么情况下使用。然后我们会介绍Connect。Connect 的完整手册不在本章的讨论范围之内，不过我们会举几个例子来帮助你入门，而且会告诉你可以从哪里了解到更多关于Connect 的信息。最后介绍其他数据集成系统，以及如何将它们与Kafka 集成起来。

## 7.1 构建数据管道时需要考虑的问题

### 7.1.1 及时性

​	有些系统希望每天一次性地接收大量数据，而有些则希望在数据生成几毫秒之内就能拿到它们。大部分数据管道介于这两者之间。一个好的数据集成系统能够很好地支持数据管道的各种及时性需求，而且在业务需求发生变更时，具有不同及时性需求的数据表之间可以方便地进行迁移。<u>Kafka 作为一个基于流的数据平台，提供了可靠且可伸缩的数据存储，可以支持几近实时的数据管道和基于小时的批处理。生产者可以频繁地向Kafka 写入数据，也可以按需写入：消费者可以在数据到达的第一时间读取它们，也可以每隔一段时间读取一次积压的数据</u>。

​	Kafka 在这里扮演了一个大型缓冲区的角色，降低了生产者和消费者之间的时间敏感度。实时的生产者和基于批处理的消费者可以同时存在，也可以任意组合。实现回压策略也因此变得更加容易， Kafka 本身就使用了回压策略（必要时可以延后向生产者发送确认），消费速率完全取决于消费者自己。

### * 7.1.2 可靠性

​	我们要避免单点故障，并能够自动从各种故障中快速恢复。数据通过数据管道到达业务系统，哪怕出现几秒钟的故障，也会造成灾难性的影响，对于那些要求毫秒级的及时性系统来说尤为如此。数据传递保证是可靠性的另一个重要因素。<u>有些系统允许数据丢失，不过在大多数情况下，它们要求**至少一次传递**。也就是说，源系统的每一个事件都必须到达目的地，不过有时候需要进行重试，而重试可能造成重复传递。有些系统甚至要求**仅一次传递**一一源系统的每一个事件都必须到达目的地，不允许丢失，也不允许重复</u>。

​	我们已经在第6章深入讨论了Kafka 的可用性和可靠性保证。**Kafka 本身就支持“至少一次传递”**，<u>如果再结合具有事务模型或唯一键特性的外部存储系统， Kafka 也能实现“仅一次传递”</u>。因为大部分的端点都是数据存储系统，它们提供了“仅一次传递”的原语支持，所以基于Kafka 的数据管道也能实现“仅一次传递”。

​	<u>值得一提的是， Connect APl 为集成外部系统提供了处理偏移量的APl ，连接器因此可以构建仅一次传递的端到端数据管道。实际上，很多开源的连接器都支持仅一次传递</u>。

### * 7.1.3 高吞吐量和动态吞吐量

​	为了满足现代数据系统的要求，数据管道需要支持非常高的吞吐量。<u>更重要的是，在某些情况下，数据管道还需要能够应对突发的吞吐量增长</u>。

​	由于我们将Kafka 作为生产者和消费者之间的缓冲区，消费者的吞吐量和生产者的吞吐量就不会耦合在一起了。我们也不再需要实现复杂的回压机制，如果生产者的吞吐量超过了消费者的吞吐量，可以把数据积压在Kafka 里，等待消费者追赶上来。通过增加额外的消费者或生产者可以实现Kafka 的伸缩，因此我们可以在数据管道的任何一边进行动态的伸缩，以便满足持续变化的需求。

​	因为Kafka 是一个高吞吐量的分布式系统， 一个适当规模的集群每秒钟可以处理数百兆的数据，所以根本无需担心数据管道无住满足伸缩性需求。另外， <u>Connect API 不仅支持伸缩，而且擅长并行处理任务。稍后，我们将会介绍数据源和数据池（Data Sink）如何在多个线程间拆分任务，最大限度地利用CPU资源，哪怕是运行在单台机器上</u>。

​	Kafka 支持多种类型的压缩，**在增长吞吐量时， Kafka 用户和管理员可以通过压缩来调整网络和存储资源的使用**。

### 7.1.4 数据格式

> [s3（S3 Simple Storage Service 简单存储服务）_百度百科 (baidu.com)](https://baike.baidu.com/item/s3/1409766?fr=aladdin)

​	数据管道需要协调各种数据格式和数据类型，这是数据管道的一个非常重要的因素。数据类型取决于不同的数据库和数据存储系统。你可能会通过Avro 将XML 或关系型数据加载到Kafka 里，然后将它们转成JSON 写入ElasticSearch ，或者转成Parquet 写入HDFS ，或者转成csv 写入S3 。

​	Kafka 和Connect API 与数据格式无关。我们已经在之前的章节介绍过，生产者和消费者可以使用各种序列化器来表示任意格式的数据。Connect API 有自己的内存对象模型，包括数据类型和schema 。不过，可以使用一些可插拔的转换器将这些对象保存成任意的格式，也就是说，不管数据是什么格式的，都不会限制我们使用连接器。

​	<u>很多数据源和数据池都有schema ，我们从数据源读取schema ，把它们保存起来，井用它们验证数据格式的兼容性，甚至用它们更新数据池的schema。从MySQL 到Hive的数据管道就是一个很好的例子。如果有人在MySQL 里增加了一个字段，那么在加载数据时，数据管道可以保证Hive 里也添加了相应的字段</u>。

​	另外，<u>数据池连接器将Kafka 的数据写入外部系统，因此需要负责处理数据格式</u>。有些连接器把数据格式的处理做成可插拔的，比如HDFS 的连接器就支持Avro 和Parquet 。

​	**通用的数据集成框架不仅要支持各种不同的数据类型，而且要处理好不同数据源和数据池之间的行为差异**。例如，在关系型数据库向Syslog 发起抓取数据请求时， Syslog 会将数据推送给它们，而HDFS 只支持追加写入模式，只能向HDFS 写入新数据，而对于其他很多系统来说，既可以追加数据， 也可以更新已有的数据。

### * 7.1.5 转换

​	**数据转换比其他需求更具争议性。数据管道的构建可以分为两大阵营，即ETL 和ELT** 。ETL 表示**提取一转换一加载**（ Extract-Transform-Load ），也就是说，当数据流经数据管道时，数据管道会负责处理它们。这种方式为我们节省了时间和存储空间，因为不需要经过保存数据、修改数据、再保存数据这样的过程。不过，这种好处也要视情况而定。有时候，这种方式会给我们带来实实在在的好处，但也有可能给数据管道造成不适当的计算和存储负担。<u>这种方式有一个明显不足，就是数据的转换会给数据管道下游的应用造成一些限制，特别是当下游的应用希望对数据进行进一步处理的时候。假设有人在MongoDB 和MySQL 之间建立了数据管道，井且过滤掉了一些事件记录，或者移除了一些字段，那么下游应用从MySQL 中访问到的数据是不完整的。如果它们想要访问被移除的字段， 只能重新构建管道，井重新处理历史数据（如果可能的话）</u>。

​	ELT 表示**提取－加载－转换**（Extract-Load-Transform）。<u>在这种模式下，数据管道只做少量的转换（主要是数据类型转换），确保到达数据地的数据尽可能地与数据源保持一致。**这种情况也被称为高保真（high fidelity）数据管道或数据湖（data lake）架构**</u>。目标系统收集“原始数据”，并负责处理它们。这种方式为目标系统的用户提供了最大的灵活性，因为它们可以访问到完整的数据。在这些系统里诊断问题也变得更加容易，因为数据被集中在同一个系统里进行处理，而不是分散在数据管道和其他应用里。<u>这种方式的不足在于，数据的转换占用了目标系统太多的CPU 和存储资源。有时候，目标系统造价高昂，如果有可能，人们希望能够将计算任务移出这些系统</u>。

### * 7.1.6 安全性

> [sasl_百度百科 (baidu.com)](https://baike.baidu.com/item/sasl/5292142?fr=aladdin)

​	安全性是人们一直关心的问题。对于数据管道的安全性来说，人们主要关心如下几个方面。

+ <u>我们能否保证流经数据管道的数据是经过加密的</u>？这是跨数据中心数据管道通常需要考虑的一个主要方面。
+ 谁能够修改数据管道？

+ 如果数据管道需要从一个不受信任的位置读取或写入数据，是否有适当的<u>认证机制</u>？

​	**Kafka 支持加密传输数据，从数据源到Kafka ，再从Kafka 到数据池**。

​	**它还支持认证（通过SASL 来实现）和授权**，所以你可以确信，如果一个主题包含了敏感信息，在不经授权的情况下，数据是不会流到不安全的系统里的。

​	**Kafka 还提供了审计日志用于跟踪访问记录**。通过编写额外的代码，还可能跟踪到每个事件的来源和事件的修改者，从而在每个记录之间建立起整体的联系。

### 7.1.7 故障处理能力

​	我们不能总是假设数据是完美的，而要事先做好应对故障的准备。能否总是把缺损的数据挡在数据管道之外？能否恢复无法解析的记录？能否修复（或许可以手动进行）并重新处理缺损的数据？如果在若干天之后才发现原先看起来正常的数据其实是缺损数据，该怎么办？

​	因为Kafka 会长时间地保留数据，所以我们可以在适当的时候回过头来重新处理出错的数据。

### * 7.1.8 耦合性和灵活性

​	数据管道最重要的作用之一是解耦数据源和数据池。它们在很多情况下可能发生耦合。

+ 临时数据管道

  有些公司为每一对应用程序建立单独的数据管道。例如， 他们使用Logstash向ElasticSearch 导入日志，使用Flume向HDFS 导入日志，使用GoldenGate 将Oracle 的数据导到HDFS ，使用Informatica 将MySQL 的数据或XML 导到Oracle ，等等。他们将数据管道与特定的端点耦合起来，井创建了大量的集成点，需要额外的部署、维护和监控。当有新的系统加入肘，他们需要构建额外的数据管道，从而增加了采用新技术的成本，同时遏制了创新。

+ 元数据丢失

  如果数据管道没有保留schema元数据，而且不允许schema 发生变更，那么最终会导致生产者和消费者之间发生紧密的耦合。没有了schema ，生产者和消费者需要额外的信息来解析数据。假设数据从Oracle 流向HDFS ，如果DBA 在Oracle 里添加了一个字段，而且没有保留schema 信息，也不允许修改schema ，那么从HDFS 读取数据时可能会发生错误，因此需要双方的开发人员同时升级应用程序才能解决这个问题。不管是哪一种情况，它们的解决方案都不具备灵活性。如果数据管道允许schema 发生变更，应用程序各方就可以修改自己的代码，无需担心对整个系统造成破坏。

+ 末端处理

  我们在讨论数据转换时就已提到，数据管道难免要做一些数据处理。在不同的系统之间移动数据肯定会碰到不同的数据格式和不同的应用场景。不过，如果数据管道过多地处理数据，那么就会给下游的系统造成一些限制。在构建数据管道时所做的设计决定都会对下游的系统造成束缚，比如应该保留哪些字段或应该如何聚合数据，等等。如果下游的系统有新的需求，那么数据管道就要作出相应的变更，这种方式不仅不灵活，而且低效、不安全。**<u>更为灵活的方式是尽量保留原始数据的完整性，让下游的应用自己决定如何处理和聚合数据</u>**。

## 7.2 如何在Connect API 和客户端API 之间作出选择

​	在向Kafka 写入数据或从Kafka 读取数据时，要么使用传统的生产者和消费者客户端，就像第3章和第4章所描述的那样，要么使用后面即将介绍的Connect API 和连接器。在具体介绍Connect API 之前，我们不妨先问自己一个问题：“什么时候适合用哪一个？”

​	我们知道， Kafka 客户端是要被内嵌到应用程序里的，应用程序使用它们向Kafka 写入数据或从Kafka 读取数据。如果你是开发人员，你会使用Kafka 客户端将应用程序连接到Kafka ，井修改应用程序的代码，将数据推送到Kafka 或者从Kafka 读取数据。

​	如果要将Kafka连接到数据存储系统，可以使用Connect ，因为这些系统不是你开发的，你无法或者也不想修改它们的代码。**Connect 可以用于从外部数据存储系统读取数据， 或者将数据推送到外部存储系统**。如果数据存储系统提供了相应的连接器，那么非开发人员就可以通过配置连接器的方式来使用Connect。

​	如果你要连接的数据存储系统没有相应的连接器，那么可以考虑使用客户端API 或Connect API 开发一个应用程序。**我们建议首选Connect ，因为它提供了一些开箱即用的特性，比如配置管理、偏移量存储、井行处理、错误处理，而且支持多种数据类型和标准的REST管理API** 。开发一个连接Kafka 和外部数据存储系统的小应用程序看起来很简单，但其实还有很多细节需要处理，比如数据类型和配置选项，这些无疑加大了开发的复杂性一一<u>Connect 处理了大部分细节，让你可以专注于数据的传输</u>。

## 7.3 Kafka Connect

​	Connect是Kafka 的一部分，它为在Kafka和外部数据存储系统之间移动数据提供了一种可靠且可伸缩的方式。它为连接器插件提供了一组API 和一个运行时——Connect 负责运行这些插件， 它们则负责移动数据。Connect 以worker进程集群的方式运行，我们基于worker进程安装连接器插件，然后使用REST API 来管理和配置connector，这些worker进程都是长时间持续运行的作业。连接器启动额外的task ，有效地利用工作节点的资源，以并行的方式移动大量的数据。数据源的连接器负责从源系统读取数据，井把数据对象提供给worker进程。数据地的连接器负责从worker进程获取数据，井把它们写入目标系统。Connect通过connector在Kafka里存储不同格式的数据。Kafka 支持JSON ，而且Confluent Schema Registry 提供了Avro 转换器。开发人员可以选择数据的存储格式，这些完全独立于他们所使用的连接器。

​	本章的内容无也完全覆盖Connect的所有细节和各种连接器，这些内容可以单独写成一本书。不过，我们会提供Connect的概览，还会介绍如何使用它，井提供一些额外的参考资料。

### 7.3.1 运行Connect

​	Connect随着Kafka一起发布，所以无需单独安装。<u>如果你打算在生产环境使用Connect来移动大量的数据，或者打算运行多个连接器，那么最好把Connect 部署在独立于broker 的服务器上</u>。在所有的机器上安装Kafka，并在部分服务器上启动broker，然后在其他服务器上启动Connect 。

​	启动Connect 进程与启动broker差不多， 在调用脚本时传入一个属性文件即可。

```shell
bin/connect-distributed.sh config/connect-distributed.properties
```

​	Connect 进程有以下几个重要的配置参数。

+ `bootstrap.servers` ：该参数列出了将要与Connect 协同工作的broker 服务器，连接器将会向这些broker 写入数据或者从它们那里读取数据。你不需要指定集群所有的broker，不过建议至少指定3 个。
+ `group.id`：<u>具有相同group id的worker属于Ω同一个Connect集群</u>。集群的连接器和它们的任务可以运行在任意一个worker 上。
+ `key.convert`和`value.converter`：Connect 可以处理存储在Kafka 里的不同格式的数据。这两个参数分别指定了消息的键和值所使用的转换器。默认使用Kafka 提供的JSONConverter ，当然也可以配置成Confluent Schema Registry 提供的AvroConverter 。

​	有些转换器还包含了特定的配置参数。例如，通过将key.converter.schema.enable设置成 true 或者 false 来指定 JSON 消息是否可以包含 schema。值转换器也有类似的配置，不过它的参数名是 value.converter.schema.enable 。Avro 消息也包含了 schema，不过需要通过 key.converter.schema.registry.url 和 value.converter.schema.registry.url 来指定 Schema Registry 的位置。

​	我们一般通过Connect的REST API来配置和监控rest.host.name和rest.port连接器。你可以为 REST API 指定特定的端口。

​	在启动worker集群之后，可以通过REST API来验证它们是否运行正常：

```
curl http://localhost:8083/
```

![img](https://www.icode9.com/i/ll/?i=20201216221034596.png)

​	这个REST URI 应该要返回当前Connect 的版本号。我运行的是Kafka 0.10.1.0（预发行）快照版本。我们还可以检查已经安装好的连接器插件：

```
curl http://localhost:8083/connector-plugins
```

![img](https://www.icode9.com/i/ll/?i=20201216221109405.png)

> **单机模式**
>
> 要注意，Connect也支持单机模式。单机模式与分布式模式类似， 只是在启动时使用 bin/connect-standalone.sh代替bin/connect-distributed.sh，也可以通过命令行传入连接器的配置文件，这样就不需要使用REST API了。
>
> 在单机模式下，所有的连接器和任务都运行在单独的worker进程上。单机模式使用起来更简单，特别是在开发和诊断问题的时候，或者是在需要让连接器和任务运行在某台特定机器上的时候（比如Syslog连接器会监听某个端口，所以你需要知道它运行在哪台机器上）

### 7.3.2 连接器示例一一文件数据源和文件数据池

...

​	在删除连接器之后，如果查看Connect 的日志，你会发现其他的连接器会重启它们的任务。这是为了在worker 进程间平衡剩余的任务，确保删除连接器之后可以保持负载的均衡。

### 7.3.3 连接器示例一一从MySQL 到ElasticSearch

​	将一个MySQL 的表数据导入到一个Kafka 主题上，再将它们加载到ElasticSearch 里，然后对它们的内容进行索引。

​	...

> 构建自己的连接器
>
> 任何人都可以基于公开的Connector API 创建自己的连接器。事实上，人们创建了各种连接器，然后把它们发布到连接器中心（Connector Hub），并告诉我们怎么使用它们。如果你在连接器中心找不到可以适配你要集成的数据存储系统的连接器，可以开发自己的连接器。你也可以把自己的连接器贡献给社区，让更多的人知道和使用它们。

### 7.3.4 深入理解Connect

​	要理解Connect 的工作原理，需要先知道3 个基本概念，以及它们之间是如何进行交互的。我们已经在之前的示例里慎示了如何运行worker进程集群以及如何启动和关闭**连接器**。不过我们并没有深入解释**转化器**是如何处理数据的一一转换器把MySQL 的数据行转成JSON 记录，然后由连接器将它们写入Kafka。

​	现在让我们深入理解每一个组件，以及它们之间是如何进行交互的。

1. 连接器和任务

   连接器插件实现了Connector API，API 包含了两部分内容。

   **连接器**

   连接器负责以下3 件事情。

   + 决定需要运行多少个任务。
   +  按照任务来拆分数据复制。
   + 从worker 进程获取任务配置并将其传递下去。例如， JDBC 连接器会连接到数据库，统计需要复制的数据表，井确定需要执行多少个任务，然后在配置参数`max.tasks`和实际数据量之间选择数值较小的那个作为任务数。在确定了任务数之后，连接器会为每个任务生成一个配置，配置里包含了连接器的配置项（比如connection.url）和该任务需要复制的数据表。`taskConfigs()`方法也返回一个映射列表，这些映射包含了任务的相关配置。worker 进程负责启动和配置任务，每个任务只复制配置项里指定的数据表。如果通过REST API 启动连接器，有可能会启动任意节点上的连接器，那么连接器的任务就会在该节点上执行。

   **任务**

   任务负责将数据移入或移出Kafka。任务在初始化时会得到由worker 进程分配的一个上下文： 源系统上下文（ Source Context ）包含了一个对象，可以将源系统记录的偏移量保存在上下文里（ 例如，文件连接器的偏移量就是文件里的字节位置， JDBC 连接器的偏移量可以是数据表的主键ID ） 。目标系统连接器的上下文提供了一些方法，连接器可以用它们操作从Kafka 接收到的数据，比如进行数据清理、错误重试，或者将偏移量保存到外部系统以便实现仅一次传递。任务在完成初始化之后，就开始按照连接器指定的配置（包含在一个Properties 对象里）启动工作。源系统任务对外部系统进行轮询，并返回一些记录， worker 进程将这些记录发送到Kafka 。数据地任务通过worker 进程接收来自Kafka 的记录，井将它们写入外部系统。

2. worker进程

   worker 进程是连接器和任务的“容器”。它们负责处理HTTP请求，这些请求用于定义连接器和连接器的配置。它们还负责保存连接器的配置、启动连接器和连接器任务，并把配置信息传递给任务。<u>如果一个worker 进程停止工作或者发生崩溃，集群里的其他worker进程会感知到（ Kafka 的消费者协议提供了心跳检测机制），井将崩溃进程的连接器和任务重新分配给其他进程</u>。如果有新的进程加入集群，其他进程也会感知到，并将自己的连接器和任务分配给新的进程，确保工作负载的均衡。进程还负责提交偏移量，如果任务抛出异常，可以基于这些偏移量进行重试。

   <u>为了更好地理解worker 进程，我们可以将其与连接器和任务进行简单的比较。连接器和任务负责“数据的移动”， 而worker 进程负责REST API 、配置管理、可靠性、高可用性、伸缩性和负载均衡</u>。

   <u>这种关注点分离是Connect API 给我们带来的最大好处，而这种好处是普通客户端API 所不具备的。有经验的开发人员都知道，编写代码从Kafka 读取数据并将其插入数据库只需要一到两天的时间，但是如果要处理好配置、异常、REST API 、监控、部署、伸缩、失效等问题，可能需要几个月。如果你使用连接器来实现数据复制，连接器插件会为你处理掉一大堆复杂的问题</u>。

3. 转化器和Connect的数据模型

   数据模型和转化器是Connect API 需要讨论的最后一部分内容。Connect 提供了一组数据API——它们包含了数据对象和用于描述数据的schema。例如， JDBC 连接器从数据库读取了一个字段，并基于这个字段的数据类型创建了一个Connect Schema 对象。然后使用这些Schema 对象创建一个包含了所有数据库字段的Struct一一我们保存了每一个字段的名字和它们的值。<u>源连接器所做的事情都很相似一一从源系统读取事件，并为每个事件生成schema 和值（值就是数据对象本身） 。目标连接器正好相反，它们获取schema 和值，井使用schema 来解析值，然后写入到目标系统</u>。

   源连接器只负责基于Data API 生成数据对象，那么worker 进程是如何将这些数据对象保存到Kafka 的？这个时候，转换器就派上用场了。用户在配置worker 进程（或连接器）时可以选择使用合适的转化器，用于将数据保存到Kafka 。目前可用的转化器有Avro 、JSON和String。JSON 转化器可以在转换结果里附带上schema ，当然也可以不使用schema ，这个是可配的。Kafka 系统因此可以支持结构化的数据和半结构化的数据。连接器通过DataAPI 将数据返回给worker 进程， worker 进程使用指定的转化器将数据转换成Avro 对象、JSON 对象或者字符串，然后将它们写入Kafka 。

   对于目标连接器来说，过程刚好相反一一在从Kafka 读取数据时， worker 进程使用指定的转换器将各种格式（ Avro 、JSON 或String ）的数据转换成Data API 格式的对象，然后将它们传给目标连接器，目标连接器再将它们插入到目标系统。

   Connect API 因此可以支持多种类型的数据，数据类型与连接器的实现是相互独立的一一只要有可用的转换器，连接器和数据类型可以自由组合。

4. 偏移量管理

   worker 进程的REST API 提供了部署和配置管理服务，除此之外， worker 进程还提供了偏移量管理服务。连接器只要知道哪些数据是已经被处理过的，就可以通过Kafka 提供的API 来维护偏移量。

   源连接器返回给worker 进程的记录里包含了一个逻辑分区和一个逻辑偏移量。它们并非Kafka 的分区和偏移量，而是源系统的分区和偏移量。例如，对于文件源来说，分区可以是一个文件，偏移量可以是文件里的一个行号或者字符号：而对于JDBC 源来说，分区可以是一个数据表，偏移量可以是一条记录的主键。<u>在设计一个据连接器时，要着重考虑如何对源系统的数据进行分区以及如何跟踪偏移量，这将影响连接器的井行能力，也决定了连接器是否能够实现至少一次传递或者仅一次传递</u>。

   源连接器返回的记录里包含了源系统的分区和偏移量， worker 进程将这些记录发送给Kafka 。如果Kafka 确认记录保存成功， worker 进程就把偏移量保存下来。偏移量的存储机制是可插拔的， 一般会使用Kafka 主题来保存。如果连接器发生崩溃井重启，它可以从最近的偏移量继续处理数据。

   目标连接器的处理过程恰好相反，不过也很相似。它们从Kafka 上读取包含了主题、分区和偏移量信息的记录，然后调用连接器的put() 方法，该方法会将记录保存到目标系统里。如果保存成功，连接器会通过消费者客户端将偏移量提交到Kafka 上。

   框架提供的偏移量跟踪机制简化了连接器的开发工作，并在使用多个连接器时保证了一定程度的行为一致性。

## 7.4 Connect之外的选择

### 7.4.1 用于其他数据存储的摄入框架

​	虽然我们很想说Kafka 是至高无上的明星，但肯定会有人不同意这种说法。有些人将Hadoop 或ElasticSearch 作为他们数据架构的基础，这些系统都有自己的数据摄入工具。Hadoop 使用了Flume，ElasticSearch 使用了Logstash 或Fluentd 。如果架构里包含了Kafka ，并且需要连接大量的源系统和目标系统，那么建议使用Connect API 作为摄入工具。如果构建的系统是以Hadoop 或ElasticSearch 为中心的， Kafka 只是数据的来源之一，那么使用Flume 或Logstash 会更合适。

### 7.4.2 基于图形界面的ETL工具

​	从保守的Informatica 到一些开源的替代方案，比如Talend 和Pentaho ， 或者更新的Apache NiFi 和StreamSets一一这些ETL 解决方案都支持将Kafka 作为数据源和数据地。如果你已经使用了这些系统，比如Pentaho ， 那么就可能不会为了Kafka 而在系统里增加另一种集成工具。如果你已经习惯了基于图形界面的ETL 数据管道解决方案，那就继续使用它们。不过， 这些系统有一些不足的地方，那就是它们的工作流比较复杂，如果你只是希望从Kafka 里获取数据或者将数据写入Kafka ，那么它们就显得有点笨重。我们在本章的开头部分已经说过，**在进行数据集成时，应该将注意力集中在消息的传输上。因此，对于我们来说，大部分ETL 工具都太过复杂了**。

​	我们极力建议将Kafka 当成是一个支持数据集成（使用Connect ）、应用集成（使用生产者和消费者）和流式处理的平台。Kafka 完全可以成为ETL 工具的替代品。

### 7.4.3 流式处理框架

​	几乎所有的流式处理框架都具备从Kafka 读取数据并将数据写入外部系统的能力。如果你的目标系统支持流式处理，井且你已经打算使用流式框架处理来自Kafka 的数据，那么使用相同的框架进行数据集成看起来是很合理的。这样可以省掉一个处理步骤（不需要保存来自Kafka 的数据，而是直接从Kafka 读取数据然后写到其他系统） ， 不过在发生数据丢失或者出现脏数据时，诊断问题会变得很困难，因为这些框架并不知道数据是什么时候丢失的，或者什么时候出现了脏数据。

## 7.5 总结

​	本章讨论了如何使用Kafka 进行数据集成，从解释为什么要使用Kafka 进行数据集成开始，到说明数据集成方案的一般性考虑点。我们先解释了为什么Kafka 和Connect API 是一种更好的选择，然后给出了一些例子， 演示如何在不同的场景下使用Connect ，井深入了解了Connect 的工作原理，最后介绍了一些Conn ect 之外的数据集成方案。

​	不管最终你选择了哪一种数据集成方案，都需要保证所有消息能够在各种恶劣条件下完成传递。我们相信，在与Kafka 的可靠性特性结合起来之后， Connect 具有了极高的可靠性。不过，我们仍然需要对它们进行严格的测试，以确保你选择的数据集成系统能够在发生进程停止、机器崩溃、网络延迟和高负载的情况下不丢失消息。毕竟，数据集成系统应该只做一件事情，那就是传递数据。

​	可靠性是数据集成系统唯一一个重要的需求。在选择数据系统时，首先要明确需求（可以参考7 .1节），并确保所选择的系统能够满足这些需求。除此之外，还要很好地了解数据集成方案，确保知道怎么使用它们来满足需求。虽然Kafka 支持至少一次传递的原语，但你也要小心谨慎，避免在配置上出现偏差，破坏了可靠性。

# 8. 跨集群数据镜像

​	在某些情况下，不同的集群之间相互依赖，管理员需要不停地在集群间复制数据。大部分数据库都支持**复制**（replication），也就是持续地在数据库服务器之间复制数据。不过，<u>因为前面已经使用过“复制”这个词来描述在同一个集群的节点间移动数据，所以我们把集群间的数据复制叫作**镜像**（mirroring）</u>。Kafka 内置的跨集群复制工具叫作MirrorMaker 。

​	在这一章，我们将讨论跨集群的数据镜像，它们既可以镜像所有数据，也可以镜像部分数据。我们先从一些常见的跨集群镜像场景开始，然后介绍这些场景所使用的架构模式，以及这些架构模式各自的优缺点。接下来我们会介绍MirrorMaker以及如何使用它， 然后说明在进行部署和性能调优时需要住意的一些事项。最后我们会对MirrorMaker的一些替代方案进行比较。

## 8.1 跨集群镜像的使用场景

​	下面列出了几个使用跨集群镜像的场景。

+ 区域集群和中心集群

  有时候， 一个公司会有多个数据中心，它们分布在不同的地理区域、不同的城市或不同的大洲。这些数据中心都有自己的Kafka 集群。有些应用程序只需要与本地的Kafka 集群通信，而有些则需要访问多个数据中心的数据（否则就没必要考虑跨数据中心的复制方案了）。有很多情况需要跨数据中心，比如一个公司根据供需情况修改商品价格就是一个典型的场景。该公司在每个城市都有一个数据中心，它们收集所在城市的供需信息，并调整商品价格。这些信息将会被镜像到一个中心集群上，业务分析员就可以在上面生成整个公司的收益报告。

+ 冗余（DR)

  一个Kafka 集群足以支撑所有的应用程序，不过你可能会担心集群因某些原因变得不可用，所以你希望有第二个Kafka 集群，它与第一个集群有相同的数据，如果发生了紧急情况，可以将应用程序重定向到第二个集群上。

+ 云迁移

  现今有很多公司将它们的业务同时部署在本地数据中心和云端。为了实现冗余，应用程序通常会运行在云供应商的多个服务区域里，或者使用多个云服务。本地和每个云服务区域都会有一个Kafka 集群。本地数据中心和云服务区域里的应用程序使用自己的Kafka 集群，当然也会在数据中心之间传输数据。例如，如果云端部署了一个新的应用程序，它需要访问本地的数据。本地的应用程序负责更新数据，井把它们保存在本地的数据库里。我们可以使用Connect 捕获这些数据库变更，并把它们保存到本地的Kafka集群里，然后再镜像到云端的Kafka 集群上。这样有助于控制跨数据中心的流量成本，同时也有助于改进流量的监管和安全性。

## 8.2 多集群架构

### * 8.2.1 跨数据中心通信的一些现实情况

​	以下是在进行跨数据中心通信时需要考虑的一些问题。

+ 高延迟

  Kafka 集群之间的通信延迟随着集群间距离的增长而增加。虽然光缆的速度是恒定的，但集群间的网络跳转所带来的缓冲和堵塞会增加通信延迟。

+ 有限的带宽

  单个数据中心的广域网带宽远比我们想象的要低得多，而且可用的带宽时刻在发生变化。另外，高延迟让如何利用这些带宽变得更加困难。

+ 高成本

  不管你是在本地还是在云端运行Kafka ，集群之间的通信都需要更高的成本。部分原因是因为带宽有限，而增加带宽是很昂贵的，当然，这个与供应商制定的在数据中心、区域和云端之间传输数据的收费策略也有关系。

​	<u>Kafka 服务器和客户端是按照单个数据中心进行设计、开发、测试和调优的</u>。我们假设服务器和客户端之间具有很低的延迟和很高的带宽，在使用默认的超时时间和缓冲区大小时也是基于这个前提。因此，我们不建议跨多个数据中心安装Kafka 服务器（不过稍后会介绍一些例外情况）。

​	大多数情况下，我们要避免向远程的数据中心生成数据，但如果这么做了，那么就要忍受高延迟，井且需要通过**增加重试次数**（ <u>Linkedln 曾经为跨集群镜像设置了32 000 多次重试次数</u>）和**增大缓冲区**来解决潜在的网络分区问题（生产者和服务器之间临时断开连接）。

​	**如果有了跨集群复制的需求，同时又禁用了从broker到broker之间的通信以及从生产者到broker 之间的通信，那么我们必须允许从broker 到消费者之间的通信。事实上，这是最安全的跨集群通信方式。在发生网络分区时，消费者无法从Kafka 读取数据，数据会驻留在Kafka 里， 直到通信恢复正常。因此，网络分区不会造成任何数据丢失**。

​	<u>不过，因为带宽有限，如果一个数据中心的多个应用程序需要从另一个数据中心的Kafka 服务器上读取数据，我们倾向于为每一个数据中心安装一个Kafka 集群，并在这些集群间复制数据，而不是让不同的应用程序通过广域网访问数据</u>。

​	在讨论更多有关跨数据中心通信的调优策略之前，我们需要先知道以下一些架构原则。

+ **每个数据中心至少需要一个集群**。
+ 每两个数据中心之间的数据复制要做到每个事件**仅复制一次**（除非出现错误需要重试） 。
+ **如果有可能，尽量从远程数据中心<u>读取数据</u>，而不是向远程数据中心写入数据**。

### 8.2.2 Hub和Spoke架构

​	这种架构适用于一个中心Kafka 集群对应多个本地Kafka 集群的情况，如图8 -1 所示。

![img](https://img2020.cnblogs.com/blog/1350843/202110/1350843-20211017081433832-510514946.png)

​	图8-1 ：一个中心 Kafka 集群对应多个本地Kafka 集群

​	这种架构有一个简单的变种，如果只有一个本地集群， 那么整个架构里就只剩下两个集群： 一个首领和一个跟随者，如图8 -2所示。

![img](https://img2020.cnblogs.com/blog/1350843/202110/1350843-20211017081647760-1592900958.png)

​	图8-2: 一个首领对应一个跟随者

​	当消费者需要访问的数据集分散在多个数据中心时，可以使用这种架构。如果每个数据中心的应用程序只处理自己所在数据中心的数据，那么也可以使用这种架构，只不过它们无法访问到全局的数据集。

​	**这种架构的好处在于，数据只会在本地的数据中心生成，而且每个数据中心的数据只会被镜像到中央数据中心一次**。只处理单个数据中心数据的应用程序可以被部署在本地数据中心里， 而需要处理多个数据中心数据的应用程序则需要被部署在中央数据中心里。<u>因为数据复制是单向的，而且消费者总是从同一个集群读取数据，所以这种架构易于部署、配置和监控</u>。

​	<u>不过这种架构的简单性也导致了一些不足。一个数据中心的应用程序无法访问另一个数据中心的数据</u>。为了更好地理解这种局限性，我们举一个例子来说明。

​	假设有一家银行，它在不同的城市有多家分行。每个城市的Kafka集群上保存了用户的信息和账号历史数据。我们把各个城市的数据复制到一个中心集群上， 这样银行就可以利用这些数据进行业务分析。在用户访问银行网站或去他们所属的分行办理业务时， 他们的请求被路由到本地集群上，同时从本地集群读取数据。假设一个用户去另一个城市的分行办理业务，因为他的信息不在这个城市，所以这个分行需要与远程的集群发生交互（不建议这么做）， 否则根本没有办桂访问到这个用户的信息（很尴尬） 。**因此，这种架构模式在数据访问方面有所局限，因为区域数据中心之间的数据是完全独立的**。

​	**在采用这种架构时，每个区域数据中心的数据都需要被镜像到中央数据中心上。镜像进程会读取每一个区域数据中心的数据，并将它们重新生成到中心集群上**。<u>如果多个数据中心出现了重名的主题，那么这些主题的数据可以被写到中心集群的单个主题上，也可以被写到多个主题上</u>。

### * 8.2.3 双活架构

​	<u>当有两个或多个数据中心需要共享数据并且每个数据中心都可以生产和读取数据时， 可以使用双活（ Active-Active）架构</u>，如图8-3所示。

![img](https://img2020.cnblogs.com/blog/1350843/202110/1350843-20211017082346578-1788024956.png)

​	图8-3 ：两个数据中的需要共享数据

​	这种架构的主要好处在于，它可以为就近的用户提供服务，具有性能上的优势，而且不会因为数据的可用性问题（在Hub 和Spoke 架构中就有这种问题）在功能方面作出牺牲。第二个好处是冗余和弹性。因为每个数据中心具备完整的功能， 一旦一个数据中心发生失效，就可以把用户重定向到另一个数据中心。这种重定向完全是网络的重定向，因此是一种最简单、最透明的失效备援方案。

​	**这种架构的主要问题在于，如何在进行多个位置的数据异步读取和异步更新时避免冲突**。比如镜像技术方面的问题一一如何确保同一个数据不会被无止境地来回镜像？而<u>数据一致性</u>方面的问题则更为关键。下面是可能遇到的问题。

+ 如果用户向一个数据中心发送数据，同时从第二个数据中心读取数据，那么在用户读取数据之前，他发送的数据有可能还没有被镜像到第二个数据中心。对于用户来说， 这就好比把一本书加入到购物车，但是在他点开购物车时，书却不在里面。<u>因此，在使用这种架构时，开发人员经常会将用户“粘”在同一个数据中心上，以确保用户在大多数情况下使用的是同一个数据中心的数据（除非他们从远程进行连接或者数据中心不可用）</u> 。

+ 一个用户在一个数据中心订购了书A ，而第二个数据中心几乎在同一时间收到了该用户订购书B 的订单，在经过数据镜像之后，每个数据中心都包含了这两个事件。两个数据中心的应用程序需要知道如何处理这种情况。我们是否应该从中挑选一个作为“正确”的事件？如果是这样，我们需要在两个数据中心之间定义一致的规则，用于确定哪个事件才是正确的。又或者把两个都看成是正确的事件，将两本书都发给用户，然后设立一个部门专门来处理退货问题？ Amazon 就是使用这种方式来处理冲突的，但对于股票交易部门来说，这种方案是行不通的。如何最小化冲突以及如何处理冲突要视具体情况而定。总之要记住，<u>如果使用了这种架构，必然会遇到冲突问题，还要想办注解决它们</u>。

​	如果能够很好地处理在从多个位置异步读取数据和异步更新数据时发生的冲突问题，那么我们强烈建议使用这种架构。这种架构是我们所知道的最具伸缩性、弹性、灵活性和成本优势的解决方案。所以，它值得我们投入精力去寻找一些办法，用于避免循环复制、把相同用户的请求粘在同一个数据中心，以及在发生冲突时解决冲突。

​	**双活镜像（特别是当数据中心的数量超过两个）的挑战之处在于，每两个数据中心之间都需要进行镜像，而且是双向的**。如果有5 个数据中心，那么就需要维护至少20 个镜像进程，还有可能达到40 个，因为为了高可用，每个进程都需要冗余。

​	**另外，我们还要避免循环镜像，相同的事件不能无止境地来回镜像**。<u>对于每一个“逻辑主题”，我们可以在每个数据中心里为它创建一个单独的主题，并确保不要从远程数据中心复制同名的主题</u>。例如，对于逻辑主题"users"，我们在一个数据中心为其创建"SF.users"主题，在另一个数据中心为其创建"NYC.users"主题。镜像进程将SF 的" SF.users"镜像到NYC ，同时将NYC 的"NYC.users"镜像到SF 。这样一来，每一个事件只会被镜像一次，不过在经过镜像之后，每个数据中心同时拥有了SF.users和NYC.users这两个主题，也就是说，每个数据中心都拥有相同的用户数据。消费者如果要读取所有的用户数据，就需要以"\*.users"的方式订阅主题。我们也可以把这种方式理解为数据中心的命名空间，比如在这个例子里， NYC 和SF就是命名空间。

​	在不久的将来， Kafka 将会增加记录头部信息。头部信息里可以包含源数据中心的信息，我们可以使用这些信息来避免循环镜像，也可以用它们来单独处理来自不同数据中心的数据。当然，你也可以通过使用结构化的数据格式（比如Avro ）来实现这一特性，并用它在数据里添加标签和头部信息。不过在进行镜像时，需要做一些额外的工作，因为现成的镜像工具井不支持自定义的头部信息格式。

### * 8.2.4 主备架构

​	**有时候，使用多个集群只是为了达到灾备的目的**。你可能在同一个数据中心安装了两个集群，它们包含相同的数据，平常只使用其中的一个。当提供服务的集群完全不可用时，就可以使用第二个集群。又或者你可能希望它们具备地理位置弹性，比如整体业务运行在加利福尼亚州的数据中心上，但需要在德克萨斯州有第二个数据中心，第二个数据中心平常不怎么用，但是一且第一个数据中心发生地震，第二个数据中心就能派上用场。德克萨斯州的数据中心可能拥有所有应用程序和数据的非活跃（“冷”）复制，在紧急情况下，管理员可以启动它们，让第二个集群发挥作用。这种需求一般是合规性的，业务不一定会将其纳入规划范畴，但还是要做好充分的准备。主备（Active-Standby）架构意图如图8-4 所示。

![img](https://img2020.cnblogs.com/blog/1350843/202110/1350843-20211017092005673-1157229634.png)

​	图8-4 ：主备架构示意图

​	这种架构的好处是易于实现，而且可以被用于任何一种场景。你可以安装第二个集群，然后使用镜像进程将第一个集群的数据完整镜像到第二个集群上，不需要担心数据的访问和冲突问题，也不需要担心它会带来像其他架构那样的复杂性。

​	这种架构的不足在于，它浪费了一个集群。**Kafka 集群间的失效备援比我们想象的要难得多。从目前的情况来看，要实现不丢失数据或无重复数据的Kafka 集群失效备援是不可能的**。<u>我们只能尽量减少这些问题的发生，但无法完全避免</u>。

​	让一个集群什么事也不傲，只是等待灾难的发生，这明显就是对资源的浪费。因为灾难是（或者说应该是）很少见的，所以在大部分时间里，灾备集群什么事也不做。有些组织尝试减小灾备集群的规模，让它远小于生产环境的集群规模。这种做法具有一定的风险，因为你无告保证这种小规模的集群能够在紧急情况下发挥应有的作用。有些组织则倾向于让灾备集群在平常也能发挥作用，他们把一些只读的工作负载定向到灾备集群上，也就是说，实际上运行的是Hub和Spoke架构的一个简化版本，因为架构里只有一个Spoke。

​	那么问题来了： 如何实现Kafka 集群的失效备援？

​	首先，不管选择哪一种失效备援方案， SRE（网站可靠性工程）团队都必须随时待命。今天能够正常运行的计划，在系统升级之后可能就无法正常工作，又或者已有的工具无法满足新场景的需求。每季度进行一次失效备援是最低限度的要求， 一个高效的SRE 团队会更频繁地进行失效备援。Chaos Monkey 是Netflix 提供的一个著名的服务，它随机地制造灾难，有可能让任何一天都成为失效备援日。

​	现在，让我们来看看失效备援都包括哪些内容。

1. **数据丢失和不一致性**

   <u>因为Kafka 的各种镜像解决方案都是**异步**的（ 8.2.5 节将介绍一种同步的方案），所以灾备集群总是无法及时地获取主集群的最新数据</u>。我们要时刻注意灾备集群与主集群之间拉开了多少距离，并保证不要出现太大的差距。不过， 一个繁忙的系统可以允许灾备集群与主集群之间有几百个甚至几千个消息的延迟。如果你的Kafka 集群每秒钟可以处理100 万个消息，而在主集群和灾备集群之间有5ms 的延迟，那么在最好的情况下，灾备集群每秒钟会有5000个消息的延迟。所以，不在计划内的失效备援会造成数据的丢失。在进行计划内的失效备援时，可以先停止主集群，等待镜像进程将剩余的数据镜像完毕，然后切换到灾备集群，这样可以避免数据丢失。在发生非计划内的失效备援时，可能会丢失数千个消息。目前Kafka 还不支持事务， 也就是说，如果多个主题的数据（比如销售数据和产品数据）之间有相关性，那么在失效备提过程中， 一些数据可以及时到达灾备集群，而有些则不能。那么在切换到灾备集群之后，应用程序需要知道该如何处理没有相关销售信息的产品数据。

2. 失效备援之后的起始偏移量

   在切换到灾备集群的过程中，最具挑战性的事情莫过于如何让应用程序知道该从什么地方开始继续处理数据。下面将介绍一些常用的方法，其中有些很简单，但有可能会造成额外的数据丢失或数据重复：有些则比较复杂，但可以最小化丢失数据和出现重复数据的可能性。

   + **偏移量自动重置**

     Kafka 消费者有一个配置选项，用于指定在没有上一个提交偏移量的情况下该作何处理。消费者要么从分区的起始位置开始读取数据，要么从分区的末尾开始读取数据。如果使用的是旧版本的消费者（偏移量保存在Zookeeper 上），而且因为某些原因，这些偏移量没有被纳入灾备计划，那么就需要从上述两个选项中选择一个。要么从头开始读取数据，并处理大量的重复数据，要么直接跳到末尾，放弃一些数据（希望只是少量的数据）。**如果重复处理数据或者丢失一些数据不会造成太大问题，那么重置偏移量是最为简单的方案。不过直接从主题的末尾开始读取数据这种方式或许更为常见**。

   + 复制偏移量主题

     如果使用新的Kafka消费者（0.9 或以上版本），消费者会把偏移量提交到一个叫作__consurner_offsets 的主题上。如果对这个主题进行了镜像，那么当消费者开始读取灾备集群的数据时，它们就可以从原先的偏移量位置开始处理数据。这个看起来很简单，不过仍然有很多需要注意的事项。

   <u>首先， 我们并不能保证主集群里的偏移量与灾备集群里的偏移量是完全匹配的</u>。假设主集群里的数据只保留3 天，而你在一个星期之后才开始镜像，那么在这种情况下，主集群里第一个可用的偏移量可能是57 000 000 （前4 天的旧数据已经被删除了），而灾备集群里的第一个偏移量是0 ，那么当消费者尝试从57 000 003 处（因为这是它要读取的下一个数据）开始读取数据时，就会失败。

   <u>其次，就算在主题创建之后立即开始镜像，让主集群和灾备集群的主题偏移量都从0开始，生产者在后续进行重试时仍然会造成偏移量的偏离</u>。

   **简而言之，目前的Kafka 镜像解决方案无法为主集群和灾备集群保留偏移量**。

   最后，就算偏移量被完美地保留下来，因为主集群和灾备集群之间的延迟以及Kafka 缺乏对事务的支持，消费者提交的偏移量有可能会在记录之前或者记录之后到达。<u>在发生失效备援之后，消费者可能会发现偏移量与记录不匹配，或者灾备集群里最新的偏移量比主集群里的最新偏移量小</u>。如图8-5 所示。

   ![img](https://img2018.cnblogs.com/blog/687300/201810/687300-20181009204023843-1127193754.png)

   图8-5 ：灾备集群偏穆量与主集群的最新偏穆量不匹配的示例

   **在这些情况下，我们需要接受一定程度的重复数据。如果灾备集群最新的偏移量比主集群的最新偏移量小，或者因为生产者进行重试导致灾备集群的记录偏移量比主集群的记录偏移量大，都会造成数据重复**。你还需要知道该怎么处理最新偏移量与记录不匹配的问题，此时要从主题的起始位置开始读取还是从末尾开始读取？

   复制偏移量主题的方式可以用于减少数据重复或数据丢失，而且实现起来很简单，只要及时地从0 开始镜像数据，井持续地镜像偏移量主题就可以了。不过一定要注意上述的几个问题。

   + **基于时间的失效备援**

     如果使用的是新版本（0.10.0及以上版本）的Kafka消费者，每个消息里都包含了一个时间戳，这个时间戳指明了消息发送给Kafka 的时间。在更新版本的Kafka (0.10.1.0及以上版本）里， broker提供了一个索引和一个API ，用于根据时间戳查找偏移量。于是，假设你正在进行失效备援， 井且知道失效事件发生在凌晨4:05 ，那么就可以让消费者从4: 03 的位置开始处理数据。在两分钟的时间差里会存在一些重复数据，不过这种方式仍然比其他方案要好得多，而且也很容易向其他人解释一一“我们将从凌晨4:03的位置开始处理数据”这样的解释要比“我们从一个不知道是不是最新的位置开始处理数据”要好得多。所以，这是一种更好的折中。问题是，如何让消费者从凌晨4:03的位置开始处理数据呢？

     可以让应用程序来完成这件事情。我们为用户提供一个配置参数，用于指定从什么时间点开始处理数据。如果用户指定了时间，应用程序可以通过新的API 获取指定时间的偏移量，然后从这个位置开始处理数据。

     如果应用程序在一开始就是这么设计的，那么使用这种方案就再好不过了。但如果应用程序在一开始不是这么设计的呢？开发一个这样的小工具也并不难一一接收一个时间戳，使用新的API获取相应的偏移量，然后提交偏移量。我们希望在未来的Kafka 版本里添加这样的工具，不过你也可以自己写一个。在运行这个工具时，应该先关闭消费者群组， 在工具完成任务之后再启动它们。

     该方案适用于那些使用了新版Kafka 、对失效备援有明确要求井且喜欢自己开发工具的人。

   + 偏移量外部映射

     我们知道，**镜像偏移量主题的一个最大问题在于主集群和灾备集群的偏移量会发生偏差**。因此， 一些组织选择使用外部数据存储（比如Apache Cassandra ） 来保存集群之间的偏移量映射。他们自己开发镜像工具，在一个数据被镜像到灾备集群之后，主集群和灾备集群的偏移量被保存到外部数据存储上。或者只有当两边的偏移量差值发生变化时，才保存这两个偏移量。比如， 主集群的偏移量495 被映射到灾备集群的偏移量500 ，在外部存储上记录为（ 495,500 ）。如果之后因为消息重复导致差值发生变化， 偏移量596 被映射为600 ，那么就保留新的映射（ 569,600 ） 。他们没有必要保留495 和596 之间的所有偏移量映射，他们假设差值都是一样的，所以主集群的偏移量550 会映射到灾备集群的偏移量555 。那么在发生失效备援时，他们将主集群的偏移量与灾备集群的偏移量映射起来，而不是在时间戳（通常会有点不准确）和偏移量之间做映射。他们通过上述技术手段之一来强制消费者使用映射当中的偏移量。<u>对于那些在数据记录之前达到的偏移量或者没有及时被镜像到灾备集群的偏移量来说，仍然会有问题一一不过这至少已经满足了部分场景的需求</u>。

     **这种方案非常复杂，我认为并不值得投入额外的时间。在索引还没有出现之前，或许可以考虑使用这种方案。但在今天，我倾向于将集群升级到新版本，并使用<u>基于时间戳</u>的解决方案，而不是进行偏移量映射，更何况偏移量映射并不能覆盖所有的失效备援场景**。

3. 在失效备援之后

   假设失效备援进行得很顺利，灾备集群也运行得很正常，现在需要对主集群做一些改动，比如把它变成灾备集群。

   如果能够通过简单地改变镜像进程的方向，让它将数据从新的主集群镜像到旧的主集群上面， 事情就完美了! 不过，这里还存在两个问题。

   + 怎么知道该从哪里开始镜像？我们同样需要解决与镜像程序里的消费者相关的问题。<u>而且不要忘了，所有的解决方案都有可能出现重复数据或者丢失数据，或者两者兼有</u>。

   + 之前讨论过，旧的主集群可能会有一些数据没有被镜像到灾备集群上，如果在这个时候把新的数据镜像回来，那么历史遗留数据还会继续存在，两个集群的数据就会出现不一致。

   <u>基于上述的考虑，最简单的解决方案是清理旧的主集群，删掉所有的数据和偏移量，然后从新的主集群上把数据镜像回来，这样可以保证两个集群的数据是一致的</u>。

4. 关于集群发现

   在设计灾备集群时，需要考虑一个很重要的问题，就是在发生失效备援之后，应用程序需要知道如何与灾备集群发起通信。不建议把主集群的主机地址硬编码在生产者和消费者的配置属性文件里。大多数组织为此创建了DNS 别名，将其指向主集群，一旦发生紧急情况，可以将其指向灾备集群。有些组织则使用服务发现工具，比如Zookeper 、Etcd或Consul。这些服务发现工具（ DNS 或其他）没有必要将所有broker 的信息都包含在内，Kafka 客户端只需要连接到其中的一个broker，就可以获取到整个集群的元数据，并发现集群里的其他broker。一般提供3 个broker 的信息就可以了。除了服务发现之外，在大多数情况下，需要重启消费者应用程序，这样它们才能找到新的可用偏移量，然后继续读取数据。

### * 8.2.5 延展集群

​	在主备架构里， 当Kafka 集群发生失效时，可以将应用程序重定向到另一个集群上，以保证业务的正常运行。而<u>在整个数据中心发生故障时，可以使用延展集群（stretch cluster)来避免Kafka 集群失效</u>。**延展集群就是跨多个数据中心安装的单个Kafka 集群**。

​	延展集群与其他类型的集群有本质上的区别。**首先，延展集群井非多个集群，而是单个集群，因此不需要对延展集群进行镜像**。<u>延展集群使用Kafka 内置的复制机制在集群的broker之间同步数据</u>。我们可以通过配置打开延展集群的<u>同步复制</u>功能，生产者会在消息、成功写入到其他数据中心之后收到确认。同步复制功能要求使用机架信息，确保每个分区在其他数据中心都存在副本，还需要配置min.ist和acks=all ，确保每次写入消息时都可以收到至少两个数据中心的确认。

​	同步复制是这种架构的最大优势。有些类型的业务要求灾备站点与主站点保持100% 的同步，这是一种合规性需求，可以应用在公司的任何一个数据存储上，包括Kafka 本身。这种架构的另一个好处是，数据中心及所有broker 都发挥了作用，不存在像主备架构那样的资源浪费。

​	这种架构的不足之处在于，它所能应对的灾难类型很有限，只能应对数据中心的故障， 无法应对应用程序或者Kafka 故障。运维的复杂性是它的另一个不足之处，它所需要的物理基础设施并不是所有公司都能够承担得起的。

​	如果能够在至少3 个具有高带宽和低延迟的数据中心上安装Kafka （包括Zookeeper ），那么就可以使用这种架构。如果你的公司有3 栋大楼处于同一个街区，或者你的云供应商在同一个地区有3 个可用的区域，那么就可以考虑使用这种方案。

​	为什么是3个数据中心？ 主要是因为Zookeeper 。Zookeeper 要求集群里的节点个数是奇数，而且只有当大多数节点可用时，整个集群才可用。如果只有两个数据中心和奇数个节点，那么其中的一个数据中心将包含大多数节点，也就是说，如果这个数据中心不可用，那么Zookeeper 和Kafka 也不可用。如果有3 个数据中心，那么在分配节点时，可以做到每个数据中心都不会包含大多数节点。如果其中的一个数据中心不可用， 其他两个数据中心包含了大多数节点，此时Zookeeper 和Kafka 仍然可用。

​	**从理论上说，在两个数据中心运行Zookeeper 和Kafka 是可能的，只要将Zookeeper 的群组配置成允许手动进行失效备援。不过在实际应用当中，这种做法并不常见**。

## 8.3 Kafka 的MirrorMaker

​	Kafka 提供了一个简单的工具，用于在两个数据中心之间镜像数据。这个工具叫MirrorMaker ，它包含了一组消费者（因为历史原因，它们在MirrorMaker 文档里被称为流），这些消费者属于同一个群组，井从主题上读取数据。每个MirrorMaker进程都有一个单独的生产者。**镜像过程很简单：MirrorMaker为每个消费者分配一个线程，消费者从源集群的主题和分区上读取数据，然后通过公共生产者将数据发送到目标集群上**。默认情况下，消费者每60 秒通知生产者发送所有的数据到Kafka ，并等待Kafka 的确认。然后消费者再通知源集群提交这些事件相应的偏移量。这样可以保证不丢失数据（在源集群提交偏移量之前， Kafka 对消息进行了确认），而且如果MirrorMaker 进程发生崩溃，最多只会出现60秒的重复数据。见图8-6 。

![img](https://img2018.cnblogs.com/blog/687300/201810/687300-20181009195238811-316502029.png)

​	图8-6 : MirrorMaker的镜像过程

> MirrorMaker 相关信息
>
> Mirror Maker 看起来很简单，不过出于对效率的考虑，以及尽可能地做到仅一次传递，它的实现并不容易。截止到Kafka 0.10.0.0 版本， MirrorMaker 已经被重写了4 次，而且在未来有可能会进行更多的重写。这里所描述的以及后续章节将提及的MirrorMaker 相关细节都基于0.9.0.0 到0.10.2.0 之间的版本。

### 8.3.1 如何配置

​	MirrorMaker 是高度可配置的。首先，它使用了一个生产者和多个消费者，所以生产者和消费者的相关配置参数都可以用于配置MirrorMaker。

​	MirrorMaker 的基本命令行参数。

+ consumer.config

  <u>该参数用于指定消费者的配置文件。所有的消费者将共用这个配置，也就是说，只能配置一个源集群和一个group.id</u> 。所有的消费者属于同一个消费者群组，这正好与我们的要求不谋而合。配置文件里有两个必选的参数： bootstrap.servers （源集群的服务器地址）和group.id 。除了这两个参数外，还可以为消费者指定其他任意的配置参数。<u>auto.commit.enable 参数一般不需要修改，用默认值false 就行。MirrorMaker 会在消息安全到达目标集群之后提交偏移量，所以不能使用自动提交。如果修改了这个参数，可能会导致数据丢失</u>。auto.offset.reset 参数一般需要进行修改，默认值是latest，也就是说， MirrorMaker 只对那些在MirrorMaker 启动之后到达源集群的数据进行镜像。如果想要镜像之前的数据，需要把该参数设为earliest。

+ producer.config

  该参数用于指定生产者的配置文件。配置文件里唯一必选的参数是bootstrap.servers（目标集群的服务器地址）。我们将在8.3.3 节介绍更多的配置属性。

+ new.consumer

  MirrorMaker 只能使用0.8 版本或者0.9 版本的消费者。建议使用0.9 版本的消费者，因为它更加稳定。

+ num.streams

  之前已经解释过， 一个流就是一个消费者。所有的消费者共用一个生产者， MirrorMaker将会使用这些流来填充同一个生产者。如果需要额外的吞吐量，就需要创建另一个MirrorMaker 进程。

+ whitelist

  这是一个正则表达式，代表了需要进行镜像的主题名字。所有与表达式匹配的主题都将被镜像。在这个例子里，我们希望镜像所有的主题，不过在实际当中最好使用类似"prod.\*"这样的表达式，避免镜像测试用的主题。在双活架构中， MirrorMaker 将NYC数据中心的数据镜像到SF，为其配置了whitelist ＝"NYC.\\*"这样就不会将SF 的主题重新镜像回来。

### 8.3.2 在生产环境部署MirrorMaker

​	在生产环境， MirrorMaker 一般是作为后台服务运行的，而且是以nohup 的方式运行，井将控制台的输出重定向到一个日志文件里。这个工具有一个`-deamon`命令行参数，可以进行后台运行。

​	大部分使用MirrorMaker 的公司都有自己的启动脚本，他们一般会使用部署系统（ 比如Ansible 、Puppet 、Chef和Salt）实现自动化的部署和配置管理。

​	<u>如果有可能，尽量让MirrorMaker 运行在**目标数据中心**里</u>。也就是说，如果要将NYC 的数据发送到SF，MirrorMaker 应该运行在SF 的数据中心里。因为长距离的外部网络比数据中心的内部网络更加不可靠，如果发生了网络分区，数据中心之间断开了连接， 那么一个无法连接到集群的消费者要比一个无法连接到集群的生产者要安全得多。<u>如果消费者无法连接到集群，最多也就是无挂读取数据，数据仍然会在Kafka 集群里保留很长的一段时间，不会有丢失的风险。相反，在发生网络分区时，如果MirrorMaker 已经读取了数据，但无法将数据生成到目标集群上，就会造成数据丢失。所以说， 远程读取比远程生成更加安全</u>。

​	那么，什么情况下需要在本地读取消息并将其生成到远程数据中心呢？如果需要加密传输数据，但又不想在数据中心进行加密，就可以使用这种方式。**消费者通过SSL 连接到Kafka 对性能有一定的影响，这个比生产者要严重得多，而且这种性能问题也会影响broker**。<u>如果跨数据中心流量需要加密，那么最好把MirrorMaker 放在源数据中心， 让它读取本地的非加密数据，然后通过SSL 连接将数据生成到远程的数据中心。这个时候， 使用SSL 连接的是生产者，所以性能问题就不那么明显了</u>。在使用这种方式时， 需要确保Mirror Maker 在收到目标broker 副本的有效确认之前不要提交偏移量，并在重试次数超出限制或者生产者缓冲区植出的情况下立即停止镜像。

​	如果希望减小源集群和目标集群之间的延迟，可以在不同的机器上运行至少两个MirrorMaker实例，而且它们要使用相同的消费者群组。也就是说，如果关掉其中一台服务器， 另一个MirrorMaker实例能够继续镜像数据。

​	在将MirrorMaker部署到生产环境时，最好要对以下几项内容进行监控。

**延迟监控**

​	我们绝对有必要知道目标集群是否落后于源集群。延迟体现在源集群最新偏移量和目标集群最新偏移茸的差异上。见图8-7 。

![img](https://www.freesion.com/images/532/7603a43456381381b07f73ce333daf2c.png)

​	如图8-7 所示，源集群的最后一个偏移量是7 ，而目标集群的最后一个偏移量是5 ，所以它们之间有两个消息的延迟。

​	有两种方式可用于跟踪延迟，不过它们都不是完美的解决方案。

+ **检查MirrorMaker 提交到源集群的最新偏移量**。可以使用kafka-consumer-groups 工具检查MirrorMaker 读取的每一个分区，查看分区的最新偏移量，也就是MirrorMaker 提交的最新偏移量。不过这个偏移量井不会100 % 的准确，因为MirrorMaker 井不会每时每刻都提交偏移量，默认情况下，它会每分钟提交一次。所以，我们最多会看到一分钟的延迟，然后延迟突然下降。图8-7 中的延迟是2 ，但kafka-consumer-groups 会认为是4，因为MirrorMaker 还没有提交最近的偏移量。Linkedln 的burrow 也会监控这些信息，不过它使用了更为复杂的方越来识别延迟的真实性，所以不会导致误报。

+ **检查MirrorMaker 读取的最新偏移量（即使还未提交）** 。消费者通过JMX发布关键性度量指标，其中有一个指标是指消费者的最大延迟（基于它所读取的所有分区计算得出的）。这个延迟也不是100% 的准确，因为它只反映了消费者读取的数据，并没有考虑生产者是否成功地将数据发送到目标集群上。在图8 -7 的示例里， MirrorMaker 消费者会认为延迟是1 ，而不是2 ，因为它已经读取了消息6 ，尽管这个消息还没有被生成到目标集群上。

​	要注意，如果MirrorMaker 跳过或丢弃部分消息，上述的两种方能是无法检测到的，因为它们只跟踪最新的偏移量。Confluent 的Control Center 通过监控消息的数量和校验和来提升监控的准确性。

度量指标监控

​	MirrorMaker 内嵌了生产者和消费者，它们都有很多可用的度量指标，所以建议对它们进行监控。Kafka 文档列出了所有可用的度量指标。下面列出了几个已经被证明能够提升MirrorMaker 性能的度量指标。

+ 消费者

  Fetch-size-avg、fetch-size-max、fetch-rate、fetch-throttle-time-avg以及fetch-throttle-time-max。

+ 生产者

  batch-size-avg、batch-size-max、requests-in-flight遗迹record-retry-rate。

+ 同时适用于两者

  io-ratio和io-wait-ratio。

canary

​	如果对所有东西都进行了监控，那么canary就不是必需的，不过对于多层监控来说，canary可能还是有必要的。我们可以每分钟往拥集群的某个特定主题上发送一个事件，然后尝试从目标集群读取这个事件。如果这个事件在给定的时间之后才到达，那么就发出告警，说明MirrorMaker 出现了延迟或者已经不正常了。

### 8.3.3 MirrorMaker调优

​	MirrorMaker 集群的大小取决于对吞吐量的需求和对延迟的接受程度。如果不允许有任何延迟，那么MirrorMaker 集群的容量需要能够支撑吞吐量的上限。如果可以容忍一些延迟，那么可以在95% ～99% 的时间里只使用75%-80% 的容量。在吞吐量高峰时可以允许一些延迟，高峰期结束时，因为MirrorMaker 有一些空余容量，可以很容易地消除延迟。

​	Kafka 提供了kafka-performance-producer 工具，用于在源集群上制造负载，然后启动MirrorMaker 对这个负载进行镜像。分别为MirrorMaker 配置1 、2 、4 、8 、16 、24 和32 个消费者线程，井观察性能在哪个点开始下降，然后将num.streams 的值设置为一个小于当前点的整数。如果数据经过压缩（因为网络带宽是跨集群镜像的瓶颈，所以建议将数据压缩后再传输），那么MirrorMaker还要负责解压并重新压这些数据。这样会消耗很多的CPU 资源，所以在增加线程数量时，要注意观察CPU 的使用情况。通过这种方式，可以得到单个MirrorMaker 实例的最大吞吐量。如果单个实例的吞吐量还达不到要求，可以增加更多的MirrorMaker实例和服务器。

​	另外，你可能想要分离比较敏感的主题，它们要求很低的延迟，所以其镜像必须尽可能地接近源集群和MirrorMaker 集群。这样可以避免主题过于臃肿，或者避免出现失控的生产者拖慢数据管道。

​	我们能够对MirrorMaker 进行的调优也就是这些了。不过，我们仍然有其他办陆可以增加每个消费者和每个MirrorMaker 的吞吐量。

​	如果MirrorMaker 是跨数据中心运行的，可以在Linux 上对网络进行优化。

![img](https://img2018.cnblogs.com/blog/687300/201810/687300-20181009201115226-209154851.png)

​		除此以外，你可能还想对 MirrorMaker 里的生产者和消费者进行调优。首先，你想知道生产者或消费者是不是瓶颈所在一一生产者是否在等待消费者提供更多的数据，或者其他的什么？通过查看生产者和消费者的度量指标就可以知道问题所在了 ，如果其中的一个进程空闲 ， 而另外一个很忙，那么就知道该对哪个进行调优了。另外一种办法是查看线程转储 (thread dump ），可以使用 jstack 获得线程转储。<u>如果 MirrorMaker 的大部分时间用在轮询上 ，那么说明消费者出现了瓶颈，如果大部分时间用在发送上，那么就是生产者出现了瓶颈</u>。

 如果需要对<u>生产者</u>进行调优，可以使用下列参数。

+ max.in.flight.requests.per.connection
  默认情况下， MirrorMaker 只允许存在一个处理中的请求。也就是说，生产者在发送下一个消息之前 ， 当前发送的消息必须得到目标集群的确认。这样会对吞吐量造成限制，特别是当 broker 在对消息进行确认之前出现了严重的延迟。 MirrorMaker 之所以要限定请求的数量，是因为有些消息在得到成功确认之前需要进行重试，而这是唯一能够保证消息次序的方法。如果不在乎消息的次序，那么可以通过增加 max.in.flight.requests.per.connection 的值来提升吞吐量。
+ linger.ms和batch.size
  如果在进行监控时发现生产者总是发送未填满的批次（比如，度量指标 batch-size-avg和 batch-size-max 的值总是比 batch.size 低），那么就可以通过增加一些延迟来提升吞吐量。通过增加 latency.ms 可以让生产者在发送批次之前等待几毫秒，让批次填充更多的数据。如果发送的数据都是搞批次的，同时还有空余的内存，那么可以配置更大的batch.size ，以便发送更大的批次 。

下面的配置用于提升<u>消费者</u>的吞吐量。

+ range。 MirrorMaker 默认使用 range 策略（用于确定将哪些分区分配给哪个消费者的算法）进行分区分配。 range 策略有一定的优势不过range 策略会导致不公平现象。对于 MirrorMaker 来说，最好可以把策略改为round robin ，特别是在镜像大量的主题和分区的时候。将策略改为round robin 算法，需要在消费者配置属性文件里加上`partition.assignment.strategy=org.apache.kafka.clients.consumer.RoundRobinAssignor`。
+ fetch.max.bytes 。如果度盘指标显示 fetch-size-avg 和 fetch-size-max 的数值与 fetch.max.bytes 很接近，说明消费者读取的数据已经接近上限。如果有更多的可用内存，可以配置更大的 fetch.max.bytes ，消费者就可以在每个请求里读取更多的数据。
+  fetch.min.bytes 和 fetch.max.wait。如果度量指标 fetch-rate 的值很高，说明消费者发送的请求太多了，而且获取不到足够的数据。这个时候可以配置更大的 fetch.min.bytes 和 fetch.max.wait这样消费者的每个请求就可以获取到更多的数据， broker 会等到有足够多的可用数据时才将响应返回。

## 8.4 其他跨集群镜像方案

### 8.4.1 优步的uReplicator

### 8.4.2 Confluent的Replicator

## 8.5 总结

+ 多集群架构
+ 失效备援
+ MirrorMaker

# 9. 管理Kafka

## 9.1 主题操作

​	使用kafka-topics.sh工具可以执行主题的大部分操作（配置变更部分已经被弃用并被移动到kafka-configs.sh工具当中）。我们可以用它创建、修改、删除和查看集群里的主题。

> 检查版本
>
> Kafka 的大部分命令行工具直接操作Zookeeper 上的元数据，并不会连接到broker 上。因此，要确保所使用工具的版本与集群里的broker 版本相匹配。直接使用集群broker 自带的工具是最保险的。

### 9.1.1 创建主题

​	在集群里创建一个主题需要用到3 个参数。这些参数是必须提供的，尽管有些已经有了broker 级别的默认值。

+ 主题名称
+ 复制系数
+ 分区

### 9.1.2 增加分区

​	有时候，我们需要为主题增加分区数量。主题基于分区进行伸缩和复制，增加分区主要是为了扩展主题容量或者降低单个分区的吞吐量。如果要在单个消费者群组内运行更多的消费者，那么主题数量也需要相应增加，因为**一个分区只能由群组里的一个消费者读取**。

> 减少分区数量
>
> 我们无法减少主题的分区数量。因为如果删除了分区，分区里的数据也一并被删除，导致数据不一致。我们也无法将这些数据分配给其他分区，因为这样做很难，而且会出现消息乱序。所以，**如果一定要减少分区数量，只能删除整个主题，然后重新创建它**。

### 9.1.3 删除主题

​	如果一个主题不再被使用，<u>只要它还存在于集群里，就会占用一定数量的磁盘空间和文件句柄</u>。把它删除就可以释放被占用的资源。为了能够删除主题， broker 的`delete.topic.enable`参数必须被设置为true 。如果该参数被设为false ，删除主题的请求会被忽略。

​	删除主题会丢弃主题里的所有数据。这是一个不可逆的操作，所以在执行时要十分小心。

### 9.1.4 列出集群里的所有主题

## 9.2 消费者群组

​	在Kafka 里，有两个地方保存着消费者群组的信息。对于旧版本的消费者来说，它们的信息保存在Zookeeper 上；对于新版本的消费者来说，它们的信息保存在broker 上。kafka-consumer-groups.sh工具可以用于列出上述两种消费者群组。它也可以用于删除消费者群组和偏移量信息，不过这个功能仅限于旧版本的消费者群组（信息保存在Zookeeper上） 。在对旧版本的消费者群组进行操作时，需要通过`--zookeeper`参数指定Zookeeper 的地址；在对新版本的消费者群组进行操作时，则需要使用`--bootstrap-server`参数指定broker 的主机名和端口。

### 9.2.1 列出并描述群组

### 9.2.2 删除群组

​	只有旧版本的消费者客户端才支持删除群组的操作。删除群组操作将从Zookeeper 上移除整个群组，包括所有己保存的偏移量。在执行该操作之前，必须关闭所有的消费者。如果不先执行这一步，可能会导致消费者出现不可预测的行为，因为群组的元数据已经从Zookeeper 上移除了。

### 9.2.3 偏移量管理

​	除了可以显示和删除消费者群组（使用了旧版本消费者客户端）的偏移量外，还可以获取偏移量， 并保存批次的最新偏移量，从而实现偏移量的重置。在需要重新读取悄息或者因消费者无桂正常处理悄息（比如包含了非能格式的消息）需要跳过偏移量时， 需要进行偏移量重置。

> 管理已经提交到Kafka的偏移量
>
> 目前还没有工具可以用于管理由消费者客户端提交到Kafka 的偏移量，管理功能只对提交到Zookeeper 的偏移量可用。另外，为了能够管理提交到Kafka的消费者群组偏移量，需要在客户端使用相应的API 来提交群组的偏移量。

1. 导出偏移量

   Kafka 没有为导出偏移量提供现成的脚本，不过可以使用kafka-run-class.sh 脚本调用底层的Java 类来实现导出。在导出偏移量时， 会生成一个文件，文件里包含了分区和偏移量的信息。偏移量信息以一种导人工具能够识别的格式保存在文件里。

2. 导入偏移量

   偏移量导入工具与导出工具做的事情刚好相反，它使用之前导出的文件来重置消费者群组的偏移量。一般情况下，我们会导出消费者群组的当前偏移量，并将导出的文件复制一份（这样就有了一个备份），然后修改复制文件里的偏移量。这里要注意， 在使用导入命令时，不需要使用`--group`参数，因为文件里已经包含了消费者群组的名字。

> 先关闭消费者
>
> 在导入偏移量之前，必须先关闭所有的消费者。如果消费者群组处于活跃状态，它们不会读取新的偏移量， 反而有可能将导入的偏移量覆盖掉。

## 9.3 动态配置变更

​	我们可以在集群处于运行状态时覆盖**主题**配置和**客户端**的配额参数。我们打算在未来增加更多的动态配置参数，这也是为什么这些参数被单独放进了kafka-configs.sh。这样就可以为特定的主题和客户端指定配置参数。一旦设置完毕，它们就成为集群的永久配置，被保存在Zookeeper 上， broker 在启动时会读取它们。不管是在工具里还是文档里，它们所说的动态配置参数都是基于“主题”实例或者“客户端”实例的，都是可以被“覆盖”的。

### 9.3.1 覆盖主题的默认配置

### 9.3.2 覆盖客户端的默认配置

​	对于Kafka 客户端来说，只能覆盖生产者配额和消费者配额参数。这两个配额都以字节每秒为单位，表示客户端在每个broker 上的生产速率或消费速率。也就是说，如果集群里有5个broker ，生产者的配额是10MB/s ，那么它可以以10MB/s 的速率在单个broker上生成数据， 总共的速率可以达到50MB/s 。

### 9.3.3 列出被覆盖的配置

### 9.3.4 移除被覆盖的配置

​	动态的配置完全可以被移除，从而恢复到集群的默认配置。可以使用`--alter`命令和`--delete-config`参数来删除被覆盖的配置。

## 9.4 分区管理

​	Kafka 工具提供了两个脚本用于管理分区， 一个用于重新选举首领，另一个用于将分区分配给broker 。结合使用这两个工具，就可以实现集群流量的负载均衡。

### 9.4.1 首选的首领选举

​	第6章提到，使用多个分区副本可以提升可靠性。不过，**只有其中的一个副本可以成为分区首领，而且只有首领所在的broker 可以进行生产和消费活动**。<u>**Kafka 将副本清单里的第一个同步副本选为首领**，但在关闭并重启broker 之后，并不会自动恢复原先首领的身份</u>。

> 自动首领再均衡
>
> **broker有一个配置可以用于启用自动首领再均衡，不过到目前为止，并不建议在生产环境使用该功能。自动均衡会带来严重的性能问题，在大型的集群里，它会造成客户端流量的长时间停顿**。

​	因为集群包含了大量的分区，首选的副本选举有可能无法正常进行。在进行选举时， 集群的元数据必须被写到Zookeeper 的节点上，如果元数据超过了节点允许的大小（默认是1MB ），那么选举就会失败。这个时候，需要将分区清单的信息写到一个JSON 文件里，并将请求分为多个步骤进行（具体JSON格式见原书）。

### 9.4.2 修改分区副本

在某些时候，可能需要修改分区的副本。以下是一些需要修改分区副本的场景。

+ 主题分区在整个集群里的不均衡分布造成了集群负载的不均衡。

+ broker 离线造成分区不同步。

+ 新加入的broker 需要从集群里获得负载。

> **为重新分配副本进行网络优化**
>
> 如果要从单个broker 上移除多个分区，比如将broker 移出集群，那么在重新分配副本之前最好先关闭或者重启broker 。这样，这个broker 就不再是任何一个分区的首领，它的分区就可以被分配给集群里的其他broker （只要没有启用自动首领选举）。<u>这可以显著提升重分配的性能，并减少对集群的影响，因为复制流量将会被分发给多个broker</u>。

> **分批重分配**
>
> 分区重分配对集群的性能有很大影响，因为它会引起内存页缓存发生变化，并占用额外的网络和磁盘资源。将重分配过程拆分成多个小步骤可以将这种影响降到最低。

### 9.4.3 修改复制系数

​	分区重分配工具提供了一些特性，用于改变分区的复制系数， 这些特性井没有在文挡里说明。如果在创建分区时指定了错误的复制系数（比如在创建主题时没有足够多可用的broker ），那么就有必要修改它们。

### 9.4.4 转储日志片段

### 9.4.5 副本验证

​	分区复制的工作原理与消费者客户端类似：跟随者broker定期将上一个偏移量到当前偏移量之间的数据复制到磁盘上。如果复制停止井重启，它会从上一个检查点继续复制。**如果之前复制的日志片段被删除，跟随者不会做任何补偿**。

​	可以使用kafka-replica-verification.sh 工具来验证集群分区副本的一致性。它会从指定分区的副本上获取消息，并检查所有副本是否具有相同的消息。我们必须使用正则表达式将待验证主题的名字传给它。如果不提供这个参数， 它会验证所有的主题。除此之外，还需要显式地提供broker的地址清单。

> 副本验证对集群的影晌
>
> 副本验证工具也会对集群造成影响，因为它需要读取所有的消息。另外，它的读取过程是并行进行的，所以使用的时候要小心。

## 9.5 消费和生产

### 9.5.1 控制台消费者

> 检查工具版本
>
> 使用与 Kafka broker相同版本的消费者客户端，这一点是非常重要的。旧版本的控制台消费者与 Zookeeper之间不恰当的交互行为可能会影响到集群。

### 9.5.2 控制台生产者

## 9.6 客户端ACL

## 9.7 不安全的操作

本节介绍的操作将涉及保存在Zookeeper 上的元数据。除了这里提到的内容以外，不要直接修改Zookeeper的其他任何信息， 一定要小心谨慎，因为这些操作都是很危险的。

### 9.7.1 移动集群控制器

​	每个Kafka 集群都有一个控制器，它是运行在集群某个broker 上的一个线程。控制器负责看管集群的操作，有时候需要将控制器从一个broker 迁移到另一个broker 上。例如， 因为出现了某些异常，控制器虽然还在运行，但已无怯提供正常的功能。这时候可以迁移控制器， 但毕竟这也不是一般性的操作，所以不应该经常迁移控制器。

​	<u>当前控制器将自己注册到Zookeeper的一个节点上，这个节点处于集群路径的最顶层，名字叫作/controller。手动删除这个节点会释放当前控制器，集群将会进行新的控制器选举</u>。

### 9.7.2 取消分区重分配

​	分区重分配的一般流程如下。

1. 发起重分配请求（创建Zookeeper节点）。

2. 集群控制器将分区添加到broker上。

3. 新的broker 开始复制分区，直到副本达到同步状态。

4. 集群控制器从分区副本清单里移除旧的broker。

​	因为分区重分配是并行进行的，所以一般情况下没有理由取消一个正在进行中的重分配任务。不过有一个例外的情况，比如在重分配进行到一半时， broker 发生了故障井且无法立即重启，这会导致重分配过程无法结束，进而妨碍其他重分配任务的进行（比如将故障broker 的分区分配给其他broker ）。如果发生了这种情况，可以让集群忽略这个重分配任务。

​	移除一个进行中的分区重分配任务的步骤如下。

1. 从Zookeeper 上删除`/admin/reassign_partitions`节点。
2. 重新选举控制器（参见9.7.1节）

> 检查复制系数
>
> <u>在取消进行中的分区重分配任务时，对于任何一个未完成重分配的分区来说，旧的broker 都不会从副本清单里移除。也就是说，有些分区的复制系数会比正常的大</u>。如果主题的分区包含不一致的复制系数，那么broker 是不允许对其进行操作的（比如增加分区）。所以建议检查分区是否仍然可用，并确保分区的复制系数是正确的。

### 9.7.3 移除待删除的主题

### 9.7.4 手动删除主题

> 先关闭broker
>
> 在集群还在运行的时候修改Zookeeper 里的元数据是很危险的，这会造成集群不稳定。所以，不要在集群还在运行的时候删除或修改Zookeeper 里的主题元数据。

从集群里手动删除主题的过程如下。

1. 关闭集群里所有的broker 。

2. 删除Zookeeper路径/brokers/topics/TOPICNAME ，注意要先删除节点下的子节点。

3. 删除每个broker 的分区目录，这些目录的名字可能是TOPICNAME-NUM ，其中NUM 是指分区的ID 。

4. 重启所有的broker 。

## 9.8 总结

+ 主题管理、客户端配置
+ 检查日志片段
+ 不安全的操作举例

​	如果没有进行适当的监控，管理集群就是一个不可能完成的任务。第10 章将会讨论如何对broker 和集群的健康状况以及操作进行监控，这样就可以知道Kaf'ka 的运行状态了。我们也会提供一些有关客户端（包括生产者和消费者）监控的最佳实践。

# 10. 监控Kafka

## 10.1 度量指标基础

### 10.1.1 度量指标在哪里

​	Kafka 提供的所有度量指标都可以通过Java Management Extensions (JMX）接口来访问。要在外部监控系统里使用这些度量指标，最简单的方式是将负责收集度量指标的代理(agent）连接到Kafka 上。代理作为一个单独的进程运行在监控系统里，并连接到Kafka的JMX 接口上，例如使用Nagios XI check_jmx 插件或jmxtrans 来连接JMX 接队也可以直接在Kafka 进程里运行一个JMX 代理，然后通过HTIP 连接访问度量指标，比如Jolokia 或MX4J 。

> 找到JMX端口
>
> broker 将JMX 揣口作为整个broker 配置信息的一部分保存在Zookeeper上。所以，如果要通过编程的方式访问Kafka 的JMX ，比如管理工具需要在没有端口配置的情况下连接到JMX ，那么可以从Zookeeper 上获取端口信息。`/brokers/ids/<ID＞`节点包含了JSON 格式的broker 信息，里面有JMX 对应的主机名（hostname）和端口（jmx_ port）。

### 10.1.2 内部或外部度量

​	JMX接口提供的是内部度量指标，它们由被监控的应用程序生成。对于很多内部度量来说（比如各个请求阶段的时间），使用内部度量指标是最好的选择。没有什么能比应用程序更加了解自己了。还有一些度量指标，比如请求的整体时间、某种请求类型的可用性，可以在应用程序外部进行度量。也就是说，这些度量指标是由客户端或其他第三方应用（对于我们来说就是指Kafka broker）提供的。另外也有一些度量指标，比如可用性（broker 是否可达）或延迟（请求需要的时间）。这些度量指标为被监控的应用程序提供了很有用的外部视图。

​	网站健康监控就是我们所熟知的一种外部度量。一个正常运行的Web服务器能够正常处理请求， 它向监控系统发送度量指标， 一切看起来都很好。不过， <u>Web服务器本身的防火墙或服务器所处网络的防火墙可能导致客户端无法连接到服务器上。负责检查网站可访问性的外部监控系统将会检测到这个问题，井发送告警</u>。

### 10.1.3 应用程序健康检测

​	不管通过哪一种方式从Kafka 收集监控信息，都要确保能够通过简单的健康检测来监控应用程序本身的健康状况，这可以通过两种方式来实现。

+ 使用外部进程来报告broker的运行状态（健康检测） 。

+ 在broker停止发送度量指标时发出告警（也叫作**stale 度量指标**）。

​	<u>虽然第二种方式也是可行的，但有时候很难区分是broker出现了问题还是监控系统本身出现了问题</u>。

​	如果监控的是broker，可以直接连接到它的外部端口（就是客户端连接到broker所使用的端口），看看是否可以得到响应。如果监控的是Kafka客户端，就会比较复杂，需要检查进程是否处于运行状态，或者通过内部提供的方位来确定应用程序的健康状况。

### 10.1.4 度量指标的覆盖面

​	Kafka 提供了很多度量指标，如何选择合适的度量指标非常关键，特别是在基于这些度量指标定义告警的时候。太多难以确定严重程度的告警很容易让人们陷入"告警疲劳"，我们难以为每一个度量指标定义恰当的阈值并保持更新，告警的可信度也会因此而下降。

​	<u>一些大覆盖面的告警用处更大。也就是说，这类告警会告诉我们某处出现了问题， 然后我们去收集更多的信息，以便确定问题的细节</u>。想象一下汽车的"检查引擎"告警灯，如果仪表盘上有100 个不同的指示器，比如空气过谑器、油箱、排气管等，那么就会让人感到很困惑。相反，如果用一个指示器就能告诉我们汽车出现了问题，然后我们再去找出问题的其他细节，事情就会变得简单很多。这一章将介绍具有大覆盖面的度量指标，它们可以让告警变得更简单。

## 10.2 broker 的度量指标

​	本节将从讨论非同步分区度量指标开始，介绍如何根据这些度量指标采取行动，然后讨论其他的度量指标，以便对broker 的度量指标有一个全面的认识。这里不会列出broker 的所有度量指标，但会列出那些在监控broker 和集群时必须用到的部分。在介绍客户端度量指标之前，还会针对日志展开详细的讨论。

### * 10.2.1 非同步分区

​	如果说broker只有一个可监控的度量指标，那么它一定是指非同步分区的数量。该度量指明了作为首领的bro ker有多少个分区处于非同步状态。这个度量可以反映Kafka的很多内部问题，从broker 的崩溃到资源的过度消耗。因为这个度量指标可以说明很多问题，所以当它的值大于零时，就应该想办法采取相应的行动。

​	<u>如果集群里多个broker的非同步分区数量一直保持不变，那说明集群中的某个broker已经离线了。整个集群的非同步分区数量等于离线broker的分区数量，而且离线broker不会生成任何度量指标</u>。这个时候，需要检查这个broker出了什么问题，井解决问题。通常有可能是硬件问题，也有可能是操作系统问题或者Java问题，导致进程出现中断或挂起。

> 默认的副本选举
>
> <u>在诊断问题之前，尝试过运行默认的副本选举（参见第9 章）吗？ broker在释放首领角色（发生崩溃或被关闭）之后不会自动恢复首领角色（除非启用了首领自动再均衡，不过不建议启用这个功能）</u>。也就是说，集群里的首领副本很容易出现不均衡。运行默认的副本选举是很容易的，也很安全，所以在出现这类问题时，建议先重新选举首领，看看能否解决问题。

​	<u>如果非同步分区的数量是被动的，或者虽然数量稳定但并没有broker离线，说明集群出现了性能问题</u>。这类问题繁复多样，难以诊断，不过可以通过一些步骤来缩小问题的范围。第一步，先确认问题是与单个broker 有关还是与整个集群有关。不过有时候这个也难有定论。如果非同步分区属于单个broker ，那么这个broker 就是问题的根源，表象是其他broker 无告从它那里复制消息。

​	<u>如果多个broker都出现了非同步分区， 那么有可能是集群的问题，也有可能是单个broker的问题。这时候有可能是因为一个broker无法从其他broker那里复制数据。为了找出这个broker，可以列出集群的所有非同步分区，井检查它们的共性</u>。使用kafka-topics.sh工具（第9 章已详细介绍过）可以获取非同步分区清单。

1. 集群级别的问题

   集群问题一般分为以下两类。

   + 不均衡的负载
   + 资源过度消耗

   分区或首领的不均衡问题虽然解决起来有点复杂，但问题的定位是很容易的。为了诊断这个问题，需要用到broker 的以下度量指标。

   +  分区的数量

   + 首领分区的数量

   + 主题流入字节速率
   + 主题流入消息速率

   > 实现集群负载均衡的辅助工具
   >
   > broker 本身无站在整个集群里实现自动的分区重分配。也就是说， Kafka 集群的流量均衡是一个十分费劲的过程，需要手动检查一大串度量指标，然后进行均衡的副本重分配。为了解决这个问题，有一些组织开发了自动化工具来帮助完成这个任务。例如， Linkedin 发布了一个叫作kafka-assigner 的工具，可以在Github 的开源代码仓库kafka-tools 里找到。Kafka 提供的企业支持服务也包含了这一功能。

   Kafka 集群的另一个性能问题是容量瓶颈。有很多潜在的瓶颈会拖慢整个系统： CPU 、磁盘IO 和网络吞吐量是其中最为常见的<u>。磁盘的使用并不在其列，因为当磁盘被填满时，broker 会在进行适当的操作之后直接崩溃</u>。为了诊断容量问题，可以对如下一些操作系统级别的度量指标进行监控。

   + CPU 使用
   + 网络输入吞吐量

   + 网络输出吞吐量

   + 磁盘平均等待时间
   + 磁盘使用百分比

   **上述任何一种资源出现过度消耗，都会表现为分区的不同步。要记住， broker 的复制过程使用的也是Kafka 客户端**。<u>如果集群的数据复制出现了问题，那么集群的用户在生产消息或读取消息时也会出现问题</u>。所以，有必要为这些度量指标定义一个基线，并设定相应的阔值，以便在容量告急之前定位问题。随着集群流量的增长，对这些度量指标的趋势走向进行检查也是很有必要的。其中， All Topics Bytes In Rate 最适合用于显示集群的使用情况。

2. 主机级别的问题

   如果性能问题不是出现在整个集群上，而是出现在一两个broker 里，那么就要检查broker所在的主机，看看是什么导致它与集群里的其他broker 不一样。主机级别的问题可以分为以下几类。

   + 硬件问题
   + 进程冲突
   + 本地配置的不一致

   硬件问题很容易被发现，因为服务器会直接停止工作。不过，引起性能衰退的硬件问题却不那么明显。当出现这类问题时，系统仍旧保持运行，但会降低行为能力。比如内存出现了坏点，系统检测到坏点，直接跳过这个片段（可用的内存因此减少了）。类似的问题也会发生在CPU 上。对于这类问题，可以使用硬件提供的工具来监控硬件的健康状况，比如智能平台管理接口（IPMI）。出现问题时，可以通过dmesg 查看输出到系统控制台的内核缓冲区日志。

   <u>能够导致Kafka 性能衰退的一个比较常见的硬件问题是磁盘故障</u>。Kafka 使用磁盘来存储消息，生产者的性能与磁盘的写入速度有直接关系。这里出现的任何偏差都会表现为生产者和复制消息者的性能问题，而后者会导致分区的不同步。因此，应该持续地监控磁盘，并在出现问题时马上进行修复。

   > 一粒老鼠屎
   >
   > 一个broker 的磁盘问题可能会影响到整个集群的性能。因为生产者客户端会连接到所有的broker 上，如果操作得当，这些broker 的分区几乎能够均等地分布在整个集群里。如果一个broker 性能出现衰退并拖慢了处理请求的速度，就会导致生产者的回压，从而拖慢发给所有broker 的请求。

   假设你正在通过IPMI 或其他硬件管理接口来监控磁盘的状态，与此同时，在操作系统上运行了SMART (Self-Monitoring, Analysis and Reporting Technology ，自行监控、分析和报告技术）工具来监控和测试磁盘。在故障即将发生时，它会发出告警。<u>除此之外，还要注意查看磁盘控制器，不管是否使用了RAID 硬件都要注意查看。很多磁盘控制器都有板载的缓存，这个缓存只在控制器和电地备份单元（ BBU ）正常工作时才会被使用。如果BBU发生故障，缓存就会被禁用，磁盘的性能就会衰退</u>。

   在网络方面，局部的故障也会带来很大的问题。有些问题是硬件引起的，比如糟糕的光缆或连接器。有些问题是配置不当造成的，比如更改了服务器或上游网络硬件的网络连接速度和双工设置。网络配置问题还有可能出现在操作系统上，比如网络缓冲区太小，或者太多的网络连接占用了大量的系统内存。<u>在这方面，网络接口的错误数量是一个最为关键的指标。如果这个数字一直在增长，说明网络连接出现了问题</u>。

   如果硬件没有问题，那么需要注意系统里的其他应用程序，它们也会消耗系统的资源，而且有可能会给Kafka 带来压力。它们有可能是没有被正常安装的软件，或者一个非正常运行的进程，比如监控代理进程。对于这种情况，可以使用top 工具来识别那些大量消耗CPU 或内存的进程。

   <u>如果经过上述的检查还是找不出主机的问题根源，那么有可能是broker 或者系统配置不一致造成的</u>。一个服务器上运行着多个应用程序，每个应用程序有多个配置选项，要找出它们的差别真是一项艰巨的任务。这就是为什么要使用配置管理工具（如Chef 或Puppet )来维护操作系统和应用程序（包括Kafka ）的配置一致性。

### 10.2.2 broker 度量指标

​	除了非同步分区数量外，还有其他很多broker级别的度量指标需要监控。虽然不一定会为所有的度量指标设定告警阔值，但它们的确提供了关于broker和集群的有价值的信息。它们都应该出现在监控仪表盘上。

1. 活跃控制器数量

   该指标表示broker是否就是当前的集群控制器，其值可以是0或1。如果是1 ，表示broker 就是当前的控制器。**任何时候，都应该只有一个broker 是控制器，而且这个broker必须一直是集群控制器**。<u>如果出现了两个控制器，说明有一个本该退出的控制器线程被阻塞了，这会导致管理任务无法正常执行，比如移动分区。为了解决这个问题，需要将这两个broker 重启，而且不能通过正常的方式重启，因为此时它们无法被正常关闭</u>。

   <u>如果集群里没有控制器，集群就无法对状态的变更作出恰当的响应，状态的变更包括主题或分区的创建和broker 故障</u>。这时候要注意检查为什么控制器线程没有正常运行，比如，Zookeeper 集群的网络分区就会造成这样的问题。解决底层的问题之后，重启集群里的所有broker ，重置控制器线程的状态。

2. 请求处理器空闲率

   **Kafka 使用了两个线程池来处理客户端的请求：网络处理器线程池和请求处理器线程池**。

   + 网络处理器线程地负责通过网络读入和写出数据。这里没有太多的工作要做， 也就是说，不用太过担心这些线程会出现问题。
   + 请求处理器线程地负责处理来自客户端的请求，包括从磁盘读取消息和往磁盘写入消息。因此， broker 负载的增长对这个线程池有很大的影响。

   > 智能地使用线程
   >
   > 这样看来，好像需要数百个请求处理器线程，但实际上，请求处理器线程数没必要超过CPU 的核数。Kafka 在使用请求处理器时是非常智能的， 它会分流需要很长时间来处理的请求。例如，当请求的配额被限定或每个生产请求需要多个确认时， Kafka 就会使用这个功能。

   请求处理器平均空闲百分比这个度量指标表示请求处理器空闲时间的百分比。数值越低，说明broker的负载越高。经验表明，如果空闲百分比低于20% ，说明存在潜在的问题，如果低于10% ，说明出现了性能问题。除了集群的规模太小之外，还有其他两个原因会增大这个线程地的使用量。首先，线程池里没有足够的线程。<u>一般来说，请求处理器线程的数应该与系统的处理器核数一样（包括多线程处理器）</u>。

   另一个常见的原因是线程做了不该做的事。<u>在Kafka 0. 10 之前，请求处理器线程负责解压传入的消息批次、验证消息、分配偏移量，并在写入磁盘之前重新压缩消息。糟糕的是，压缩方法使用了同步锁</u>。Kafka 0.10 版本引入了一种新的格式，偏移量可以直接附加在消息批次里。也就是说，生产者在发送消息批次之前可以设置相对的偏移量，这样broker 就可以避免解压缩和重新压缩。如果使用了支持0 .1 0 版本悄息格式的生产者和消费者客户端， 并且把broker 的消息格式也升级到了0.10 版本，可以发现性能有了显著的改进。这样可以降低对请求处理器线程的消耗。

3. 主题流入字节

   主题流入字节速率使用b/s来表示，在对broker 接收的生产者客户端消息流量进行度量时，这个度量指标很有用。该指标可以用于确定何时该对集群进行扩展或开展其他与规模增长相关的工作。它也可以用于评估一个broker 是否比集群里的其他broker 接收了更多的流量， 如果出现了这种情况，就需要对分区进行再均衡。

4. 主题流出字节

   主题流出字节速率与流入字节速率类似，是另一个与规模增长有关的度量指标。流出字节速率显示的是消费者从broker 读取消息的速率。流出速率与流入速率的伸缩方式是不一样的，这要归功于Kafka 对多消费者客户端的支持。很多Kafka 的流出速率可以达到流入速率的6倍！所以，单独对流出速率进行观察和走势分析是非常重要的。

   > 把复制消费者包括在内
   >
   > 流出速率也包括副本流量，也就是说，如果所有主题都设置了复制系数2，那么在没有消费者客户端的情况下，流出速率与流入速率是一样的。如果有一个消费者客户端从集群读取所有的消息，那么流出速率会是流入速率的2倍。如果不知道这一点，光是看着这些指标就会感到很疑惑。

5. 主题流入的消息

   之前介绍的字节速率以字节的方式来表示broker 的流量， 而消息速率则以每秒生成消息个数的方式来表示流量，而且不考虑消息的大小。这也是一个很有用的生产者流量增长规模度量指标。它也可以与字节速率一起用于计算消息的平均大小。与字节速率一样，该指标也能反映集群的不均衡情况。

   > 为什么没有消息的流出速率
   >
   > <u>经常会有人问，为什么没有broker 的"流出消息"度量指标？因为在消息被读取时， broker 将整个消息批次发送给消费者，并没有展开批次，也就不会去计算每个批次包含了多少个消息，所以， broker 也就不知道发送了多少个消息。broker 为此提供了一个度量指标叫作每秒获取次数，它指的是请求速率，而不是消息个数</u>。

6. 分区数量

   <u>broker 的分区数量一般不会经常发生改变，它是指分配给broker 的分区总数</u>。它包括broker 的每一个分区副本，不管是首领还是跟随者。如果一个集群启用了自动创建主题的功能，那么监控这个度量指标会变得很有意思，因为你会发现，这样可以让主题的创建游离于控制之外。

7. 首领数量

   该度量指标表示broker拥有的首领分区数量。与broker的其他度量一样，该度量指标也应该在整个集群的broker上保持均等。我们需要对该指标进行周期性地检查，井适时地发出告警，即使在副本的数量和大小看起来都很完美的时候，它仍然能够显示出集群的不均衡问题。因为broker有可能出于各种原因释放掉一个分区的首领身份，比如Zookeeper会话过期，而在会话恢复之后，这个分区并不会自动拿回首领身份（除非启用了自动首领再均衡功能）。在这些情况下，该度量指标会显示较少的首领分区数，或者直接显示为零。这个时候需要运行一个默认的副本选举，重新均衡集群的首领。

   可以使用该指标与分区数量一起计算出broker 首领分区的百分比。一个均衡的集群，如果它的复制系数为2 ，那么所有的broker都应该差不多是它们的50%分区的首领。如果复制系数是3 ，这个百分比应该降到33%。

8. 离线分区

   与非同步分区数量一样，离线分区数量也是一个关键的度量指标。**该度量只能由集群控制器提供（对于其他broker 来说，该指标的值为零），它显示了集群里没有首领的分区数量**。发生这种情况主要有两个原因。

   + 包含分区副本的所有broker都关闭了
   + 由于消息数量不匹配，没有同步副本能够拿到首领身份（井且禁用了不完全首领选举）

   **在一个生产环境Kafka 集群里，离线分区会影响生产者客户端，导致消息丢失，或者造成回压。这属于“站点宕机”问题，需要立即解决**。

9. 请求度量指标

   第5 章描述了Kafka协议，它有多种不同的请求，每种请求都有相应的度量指标。（略，看原书）

### 10.2.3 主题和分区的度量指标

1. 主题实例的度量指标

   主题实例的度量指标与之前描述的broker 度量指标非常相似。事实上，它们之间唯一的区别在于这里指定了主题名称，也就是说，这些度量指标属于某个指定的主题。主题实例的度量指标数量取决于集群主题的数量，而且用户极有可能不会监控这些度量指标或设置告警。它们一般提供给客户端使用，客户端依此评估它们对Kafka 的使用情况，并进行问题调试。

2. 分区实例的度量指标

   分区实例的度量指标不如主题实例的度量指标那样有用。另外，它们的数量会更加庞大，因为几百个主题就可能包含数千个分区。不过不管怎样，在某些情况下，它们还是有一定用处的。

> 非同步分区度量指标
>
> 在分区实例的度量指标中， 有一个指标用于表示分区是否处于非同步状态。一般情况下，该指标对于日常的运维起不到太大作用，因为这类指标太多了。可以直接监控broker 的非同步分区数量，然后使用命令行工具（参见第9 章） 确定哪些分区处于非同步状态。

### 10.2.4 Java 虚拟机监控

​	除了broker 的度量指标外，还应该对服务器提供的一些标准度量进行监控，包括Java 虚拟机（ JVM ）。如果JVM 频繁发生垃圾回收，就会影响broker 的性能，在这种情况下， 就应该得到告警。JVM 的度量指标还能告诉我们为什么broker 下游的度量指标会发生变化。

1. 垃圾回收
2. Java 操作系统监控

### 10.2.5 操作系统监控

​	<u>对于broker 来说，跟踪CPU 的使用情况是很有必要的，因为它们在处理请求时使用了大量的CPU 时间。而内存使用情况的跟踪就显得没有那么重要了，因为运行Kaflea 并不需要太大的内存</u>。它会使用堆外的一小部分内存来实现压缩功能， 其余大部分内存则用于缓存。不过，我们还是要对内存使用情况进行跟踪，确保其他的应用不会影响到broker 。可以通过监控总内存空间和可用交换内存空间来确保内存交换空间不会被占用。

​	**对于Kafka 来说，磁盘是最重要的子系统。所有的消息都保存在磁盘上，所以Kaflea 的性能严重依赖磁盘的性能**。我们需要对磁盘空间和索引节点进行监控，确保磁盘空间不会被用光。对于保存数据的分区来说就更是如此。对磁盘IO 进行监控也是很有必要的， 它们揭示了磁盘的运行效率。我们需要监控磁盘的每秒种读写速度、读写平均队列大小、平均等待时间和磁盘的使用百分比。

​	最后，还需要监控broker的网络使用情况。简单地说，就是指流入和流出的网络流量， 一般使用b/s 来表示。要记住，在没有消费者时， 1个流入比特对应1个或多个流出比特，这个数字与主题的复制系数相等。根据消费者的实际数量，流入流量很容易比输出流量高出一个数量级。在设置告警阈值时要切记这一点。

### 10.2.6 日志

​	在讨论监控肘，如果不涉及日志，那么这个监控就是不完整的。与其他应用程序一样，Kafka 的broker 也可以被配置成定期向磁盘写入日志。为了能够从日志里获得有用的信息，选择合适的日志和日志级别是很重要的。通过记录INFO 级别的日志，可以捕捉到很多有关broker 运行状态的重要信息。为了获得一系列清晰的日志文件，很有必要对日志进行分类。

​	可以考虑使用两种日志。第一种是kafka.controller，可以将它设置为INFO 级别。这个日志用于记录集群控制器的信息。在任何时候，集群里都只有一个控制器，因此只有一个broker会使用这个日志。日志里包含了主题的创建和修改操作、broker 状态的变更，以及集群的活动，比如默认的副本选举和分区的移动。另一个日志是kafka.server.ClientQuotaManager，也可以将它设置为INFO 级别。这个日志用于记录与生产和消费配额活动相关的信息。因为这些信息很有用，所以最好不要把它们记录在broker 的主日志文件里。

​	在调试问题时，还有一些日志也很有用，比如kafka.request.logger， 可以将它设置为DEBUG 或TRACE 级别。这个日志包含了发送给broker 的每一个请求的详细信息。如果日志级别被设置为DEBUG ，那么它将包含连接端点、请求时间和慨要信息。如果日志级别被设置为TRACE ，那么它将包含主题和分区的信息， 以及除消息体之外的所有与请求相关的信息。不管被设置成什么级别，这个日志会产生大量的数据，所以如果不是出于调试的目的，不建议启用这个日志。

​	日志压缩线程的运行状态也是一个有用的信息。不过，这些线程并没有单独的度量指标，一个分区的压缩失败有可能造成压缩线程的整体崩愤，而且是悄然发生的。启用kafka.log.LogCleaner、kafka.log.Cleaner和kafka.log.LogCleanerManager这些日志，井把它们设置为DEBUG 级别，就可以输出日志压缩钱程的运行状态。这些日志包含了每个被压缩分区的大小和消息个数。一般情况下，这些日志的数量不会很大。也就是说，默认启用这些日志并不会带来什么麻烦。

## 10.3 客户端监控

### 10.3.1 生产者度量指标

新版本Kafka 生产者客户端的度量指标经过调整变得更加简洁，只用了少量的MBean 。相反，之前版本的客户端（不再受支持的版本） 使用了大量的MBean ，而且度量指标包含了大量的细节（提供了大量的百分位和各种移动平均数） 。这些度量指标提供了很大的覆盖面，但这样会让跟踪异常情况变得更加困难。

1. 生产者整体度量指标
2. Per-broker 和Per-topic 度量指标

### 10.3.2 消费者度量指标

​	与新版本的生产者客户端类似，新版本的消费者客户端将大量的度量指标属性塞进了少数的几个MBean里。

1. Fetch Manager 度量指标
2. Per-broker 和Per-topic 度量指标
3. Coordinator度量指标

### 10.3.3 配额

​	Kafka 可以对客户端的请求进行限流，防止客户端拖垮整个集群。对于消费者和生产者客户端来说，这都是可配的，可以使用每秒钟允许单个客户端访问单个broker 的流量字节数来表示。它有一个broker 级别的默认值，客户端可以对其进行覆盖。当broker 发现客户端的流量已经超出配额时，它就会暂缓向客户端返回响应，等待足够长的时间，直到客户端流量降到配额以下。

​	<u>broker 并不会在响应消息里提供客户端被限流的错误码，也就是说，对于应用程序来说，如果不监控这些指标，可能就不知道发生了限流</u>。

​	<u>默认情况下， broker 不会开启配额功能。不过不管有没有使用配额，监控过些指标总是没有问题的。况且，它们有可能在未来某个时刻被启用，而且从一开始就监控它们要比在后期添加更加容易</u>。

## 10.4 延时监控

​	<u>对于消费者来说，最需要被监控的指标是消费者的延时</u>。它表示分区最后一个消息和消费者最后读取的消息之间相差的消息个数。在之前的小节里已经说过，在这里，使用外部监控要比使用客户端自己的监控好得多。虽然消费者提供了延时指标，但该指标存在一个问题，它只表示单个分区的延时，也就是具有最大延时的那个分区，所以它不能准确地表示消费者的延时。另外，它需要消费者做一些额外的操作，因为该指标是由消费者对每个发出的请求进行计算得出的。如果消费者发生崩捕或者离线，那么该指标要么不准确， 要么不可用。

​	监控消费者延时最好的办法是使用外部进程，它能够观察broker 的分区状态，跟踪最近消息的偏移量，也能观察消费者的状态，跟踪消费者提交的最新偏移量。这种方式提供了一种持续更新的客观视图，而且不依赖消费者的状态。我们需要在每一个分区上进行这种检查。对于大型消费者来说，比如MirrorMaker ， 可能意味着要监控成千上万个分区。

## 10.5 端到端监控

​	我们推荐使用的另一种外部监控系统是端到端的监控系统，它为Kafka 集群的健康状态、提供了一种客户端视图。消费者和生产者客户端有一些度量指标能够说明集群可能出现了问题，但这里有猜想的成分，因为延时的增加有可能是由客户端、网络或Kafka 本身引起的。另外，用户原本的工作可能只是管理Kafka 集群，但现在也需要监控客户端。

## 10.6 总结

+ 监控

# 11. 流式处理

## 11.1 什么是流式处理

​	先来看看什么是数据流（也被称为“事件流”或“流数据”）。首先，数据流是无边界数据集的抽象表示。无边界意味着无限和持续增长。无边界数据集之所以是无限的，是因为随着时间的推移，新的记录会不断加入进来。这个定义已经被包括Google 和Amazon 在内的大部分公司所采纳。

​	这个简单的模型（事件流）可以表示很多业务活动，比如信用卡交易、股票交易、包裹递送、流经交换机的网络事件、制造商设备传感器发出的事件、发送出去的邮件、游戏里物体的移动，等等。这个清单是无穷无尽的，因为几乎每一件事情都可以被看成事件的序列。

​	除了没有边界外，事件流模型还有其他一些属性。

+ 事件流是有序的

  事件的发生总是有个先后顺序。

+ 不可变的数据记录

  事件一旦发生，就不能被改变。一个金融交易被取消，并不是说它就消失了，相反，这需要往事件流里添加一个额外的事件，表示前一个交易的取消操作。

+ 事件流是可重播的

  这是事件流非常有价值的一个属性。用户可以很容易地找出那些不可重播的流（流经套接字的TCP 数据包就是不可重播的），但对于大多数业务来说，重播发生在几个月前（甚至几年前）的原始事件流是一个很重要的需求。可能是为了尝试使用新的分析方法纠正过去的错误，或是为了进行审计。这也就是为什么我们相信Kafka 能够让现代业务领域的流式处理大获成功一一**可以借助Kafka 来捕捉和重播事件流**。如果没有这项能力，流式处理充其量只是数据科学实验室里的一个玩具而已。

​	知道什么是事件流以后，是时候了解“流式处理”的真正含义了。**流式处理是指实时地处理一个或多个事件流。流式处理是一种编程范式，就像请求与响应范式和批处理范式那样**。下面将对这3 种范式进行比较，以便更好地理解如何在软件架构中应用流式处理。

+ 请求与响应

  这是延迟最小的一种范式，响应时间处于亚毫秒到毫秒之间，而且响应时间一般非常稳定。这种处理模式一般是阻塞的，应用程序向处理系统发出请求，然后等待响应。在数据库领域，这种范式就是**线上交易处理**（ OLTP ）。销售点（ POS ）系统、信用卡处理系统和基于时间的追踪系统一般都使用这种范式。

+ 批处理

  这种范式具有高延迟和高吞吐量的特点。处理时间从几分钟到几小时不等，并且用户从结果里读到的都是旧数据。在数据库领域，它们就是数据仓库（ DWH ）或商业智能（ BI) 系统。

+ 流式处理

  这种范式介于上述两者之间。大部分的业务不要求亚毫秒级的响应，不过也接受不了要等到第二天才知道结果。大部分业务流程都是持续进行的，只要业务报告保持更新，业务产品线能够持续响应，那么业务流程就可以进行下去，而无需等待特定的响应，也不要求在几毫秒内得到响应。一些业务流程具有持续性和非阻塞的特点，比如针对可疑信用卡交易的警告、网络警告、根据供应关系实时调整价格、跟踪包裹。

​	流的定义不依赖任何一个特定的框架、API 或特性。只要持续地从一个无边界的数据集读取数据，然后对它们进行处理并生成结果，那就是在进行流式处理。重点是，整个处理过程必须是持续的。一个在每天凌晨两点启动的流程，从流里读取500 条记录，生成结果，然后结束，这样的流程不是流式处理。

## 11.2 流式处理的一些概念

### * 11.2.1 时间

​	时间或许就是流式处理最为重要的概念，也是最让人感到困惑的。在讨论分布式系统时，如何理解复杂的时间概念？推荐阅读Justin Sheehy 的论文“ There is No Now ”。**在流式处理里，时间是一个非常重要的概念，因为大部分流式应用的操作都是基于时间窗口的**。例如，流式应用可能会计算股价的5 分钟移动平均数。如果生产者因为网络问题离线了2小时，然后带着2 小时的数据重新连线，我们需要知道该如何处理这些数据。这些数据大部分都已经超过了5 分钟，而且没有参与之前的计算。

​	流式处理系统一般包含如下几个时间概念。

+ 事件时间

  **事件时间是指所追踪事件的发生时间和记录的创建时间**。例如，度量的获取时间、商店里商品的出售时间、网站用户访问网页的时间，等等。在Kafka 0.10.0 和更高版本里，生产者会自动在记录中添加记录的创建时间。如果这个时间戳与应用程序对“事件时间”的定义不一样，例如， Kafka 的记录是基于事件发生后的数据库记录创建的，那就需要自己设置这个时间戳字段。在处理数据流肘，事件时间是很重要的。

+ 日志追加时间

  **日志追加时间是指事件保存到broker 的时间**。在Kafka 0.10. 0 和更高版本里，如果启用了自动添加时间戳的功能，或者记录是使用旧版本的生产者客户端生成的，而且没有包含时间戳，那么broker 会在接收这些记录时自动添加时间戳。<u>这个时间戳一般与流式处理没有太大关系，因为用户一般只对事件的发生时间感兴趣</u>。例如，如果要计算每天生产了多少台设备，就需要计算在那一天实际生产的设备数量，尽管这些事件有可能因为网络问题到了第二天才进入Kafka 。不过，如果真实的事件时间没有被记录下来，那么就可以使用日志追加时间，在记录创建之后，这个时间就不会发生改变。

+ 处理时间

  **处理时间是指应用程序在收到事件之后要对其进行处理的时间**。这个时间可以是在事件发生之后的几毫秒、几小时或几天。同一个事件可能会被分配不同的时间戳，这取决于应用程序何时读取这个事件。<u>如果应用程序使用了两个线程来读取同一个事件，这个时间戳也会不一样！所以这个时间戳非常不可靠，应该避免使用它</u>。

> 注意时区问题
>
> **在处理与时间有关的问题时，需要注意时区问题。<u>整个数据管道应该使用同一个时区，否则操作的结果就会出现棍淆，变得毫无意义</u>。如果时区问题不可避免，那么在处理事件之前需要将它们转换到同一个时区，这就要求记录里同时包含时区信息**。

### 11.2.2 状态

​	如果操作里包含了多个事件，流式处理就会变得很有意思，比如根据类型计算事件的数量、移动平均数、合并两个流以便生成更丰富的信息流。在这些情况下，光处理单个事件是不够的，用户需要跟踪更多的信息，比如这个小时内看到的每种类型事件的个数、需要合井的事件、将每种类型的事件值相加， 等等。**事件与事件之间的信息被称为"状态"**。

​	这些状态一般被保存在应用程序的本地变量里。例如，使用散列表来保存移动计数器。事实上，本书的很多例子就是这么做的。不过，这不是一种可靠的方法，因为如果应用程序关闭，状态就会丢失，结果就会发生变化，而这并不是用户希望看到的。所以，要小心地持久化最近的状态，如果应用程序重启，要将其恢复。

​	流式处理包含以下几种类型的状态。

+ 本地状态或内部状态

  这种状态只能被单个应用程序实例访问，它们一般使用内嵌在应用程序里的数据库进行维护和管理。本地状态的优势在于它的速度，不足之处在于它受到内存大小的限制。所以，流式处理的很多设计模式都将数据拆分到多个子流，这样就可以使用有限的本地状态来处理它们。

+ 外部状态

  这种状态使用外部的数据存储来维护， 一般使用NoSQL 系统，比如Cassandra。使用外部存储的优势在于，它没有大小的限制，而且可以被应用程序的多个实例访问，甚至被不同的应用程序访问。不足之处在于，引人额外的系统会造成更大的延迟和复杂性。<u>大部分流式处理应用尽量避免使用外部存储，或者将信息缓存在本地，减少与外部存储发生交互，以此来降低延迟，而这就引入了如何维护内部和外部状态一致’性的问题</u>。

### * 11.2.3 流和表的二元性

​	在将表与流进行对比时，可以这么想：流包含了变更一一流是一系列事件，每个事件就是一个变更。表包含了当前的状态，是多个变更所产生的结果。所以说， 表和流是同一个硬币的两面世界总是在发生变化，用户有时候关注变更事件，有时候则关注世界的当前状态。如果一个系统允许使用这两种方式来查看数据，那么它就比只支持一种方式的系统强大。

​	为了将表转化成流，需要捕捉到在表上所发生的变更，将"insert"、"update"和"delete"事件保存到流里。大部分数据库提供了用于捕捉变更的"Change Data Capture" (CDC）解决方案， Kafka 连接器将这些变更发送到Kafka ，用于后续的流式处理。

​	<u>为了将流转化成表， 需要“应用”流里所包含的所有变更，这也叫作流的"**物化**"。首先在内存里、内部状态存储或外部数据库里创建一个表，然后从头到尾遍历流里的所有事件，逐个地改变状态。在完成这个过程之后，得到了一个表，它代表了某个时间点的状态</u>。

### * 11.2.4 时间窗口

​	大部分针对流的操作都是基于时间窗口的，比如移动平均数、一周内销量最好的产品、系统的99 百分位等。两个流的合并操作也是基于时间窗口的，我们会合并发生在相同时间片段上的事件。不过，很少人会停下来仔细想想时间窗口的类型。例如，在计算移动平均数时，需要知道以下几个问题。

+ 窗口的大小

  是基于5 分钟进行平均，还是15 分钟，或者一天？窗口越小，就能越快地发现变更，不过噪声也越多。窗口越大，变更就越平滑，不过延迟也越严重，如果价格涨了，需要更长的时间才能看出来。

+ 窗口移动的频率（“移动间隔”）

  5 分钟的平均数可以每分钟变化一次，或者每秒钟变化一次，或者每当有新事件到达时发生变化。如果“移动间隔”与窗口大小相等，这种情况被称为“滚动窗口（tumbling window）"。如果窗口随着每一条记录移动，这种情况被称为"滑动窗口（sliding window）"。

+ **窗口的可更新时间多长**

  假设计算了00: 00 到00:05 之间的移动平均数， 一个小时之后又得到了一些“事件时间”是00:02 的事件，那么需要更新00:00 到00:05 这个窗口的结果吗？或者就这么算了？理想情况下，可以定义一个时间段，在这个时间段内， 事件可以被添加到与它们相应的时间片段里。如果事件处于4 个小时以内，那么就更新它们，否则就忽略它们。

## 11.3 流式处理的设计模式

### 11.3.1 单个事件处理

​	处理单个事件是流式处理最基本的模式。这个模式也叫map 或filter 模式，因为它经常被用于过滤无用的事件或者用于转换事件（ map 这个术语是从Map-Reduce 模式中来的， map阶段转换事件， reduce 阶段聚合转换过的事件）。

​	这种模式可以使用一个生产者和一个消费者来实现，如图11-3 所示。

![kafka：(9) 流式处理](https://www.likecs.com/default/index/img?u=aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTM1MDg0My8yMDIxMTAvMTM1MDg0My0yMDIxMTAyMTA5NDEzNjU0OC0xNjc1NzIwOTM0LnBuZw==)

​	图11-3 ：单事件处理拓扑

### 11.3.2 使用本地状态

​	大部分流式处理应用程序关心的是如何聚合信息，特别是基于时间窗口进行聚合。例如，找出每天最低和最高的股票交易价格井计算移动平均数。

​	要实现这些聚合操作，需要维护流的状态。在本例中，为了计算每天的最小价格和平均价格， 需要将最小值和最大值保存下来，并将它们与每一个新值进行对比。

​	这些操作可以通过本地状态（而不是共享状态）来实现，因为本例中的每一个操作都是基于组的聚合操作，如图11-4 所示。例如，基于各个股票代码进行聚合，而不是基于整个股票市场。我们使用了一个Kafka 分区器来确保具有相同股票代码的事件总是被写入相同的分区。应用程序的每个实例从分配给它们的分区上获取事件（这是Kafka 的消费者保证）。也就是说，应用程序的每一个实例都可以维护一个股票代码子集的状态。

![kafka：(9) 流式处理](https://www.likecs.com/default/index/img?u=aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTM1MDg0My8yMDIxMTAvMTM1MDg0My0yMDIxMTAyMTA5NDEwNTQzMy0xNDM4MDM5NDAxLnBuZw==)

​	图11-4 ：使用本地状态的事件拓扑

​	如果流式处理应用程序包含了本地状态，情况就会变得非常复杂，而且还需要解决下列的一些问题。

+ 内存使用

  应用实例必须有可用的内存来保存本地状态。

+ 持久化

  要确保在应用程序关闭时不会丢失状态，并且在应用程序重启后或者切换到另一个应用实例时可以恢复状态。Streams可以很好地处理这些问题，它使用内嵌的RocksDB将本地状态保存在内存里，同时持久化到磁盘上，以便在重启后可以恢复。本地状态的变更也会被发送到Kafka 主题上。如果Streams节点崩溃，本地状态井不会丢失，可以通过重新读取Kafka 主题上的事件来重建本地状态。例如，如果本地状态包含“IBM 当前最小价格是167.19 ”，并且已经保存到了Kafka 上，那么稍后就可以通过读取这些数据来重建本地缓存。这些Kafka 主题使用了压缩日志，以确保它们不会无限量地增长，方便重建状态。

+ 再均衡

  有时候，分区会被重新分配给不同的消费者。在这种情况下，失去分区的实例必须把最后的状态保存起来， 同时获得分区的实例必须知道如何恢复到正确的状态。

​	不同的流式处理框架为开发者提供了不同的本地状态支持。如果应用程序需要维护本地状态，那么就要知道框架是否提供了支持。本章的末尾将会对一些框架进行简要的对比，不过软件发展变化太快，而流式处理框架更是如此。

### 11.3.3 多阶段处理和重分区

![kafka：(9) 流式处理](https://www.likecs.com/default/index/img?u=aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTM1MDg0My8yMDIxMTAvMTM1MDg0My0yMDIxMTAyMTA5NDgxMzA2MS03MTkxNzkxMjIucG5n)

​	这种多阶段处理对于写过Map-Reduce 代码的人来说应该很熟悉，因为他们经常要使用多个reduce 步骤。如果写过Map-Reduce 代码，就应该知道，处理每个reduce步骤的应用需要被隔离开来。与Map-Reduce不同的是，大多数流式处理框架可以将多个步骤放在同一个应用里，框架会负责调配每一步需要运行哪一个应用实例（或worker ）。

### * 11.3.4 使用外部查找一一流和表的连接

​	有时候，流式处理需要将外部数据和流集成在一起，比如使用保存在外部数据库里的规则来验证事务，或者将用户信息填充到点击事件当中。

​	很明显，为了使用外部查找来实现数据填充，可以这样做： 对于事件流里的每一个点击事件， 从用户信息表里查找相关的用户信息，从中抽取用户的年龄和性别信息，把它们包含在点击事件里，然后将事件发布到另一个主题上。

​	这种方式最大的问题在于，外部查找会带来严重的延迟， 一般在5 ～ 15ms 之间。这在很多情况下是不可行的。另外，外部数据存储也无法接受这种额外的负载一一流式处理系统每秒钟可以处理10～50 万个事件，而数据库正常情况下每秒钟只能处理1万个事件，所以需要伸缩性更强的解决方案。

​	**为了获得更好的性能和更强的伸缩性，需要将数据库的信息缓存到流式处理应用程序里**。不过，要管理好这个缓存也是一个挑战。比如，如何保证缓存里的数据是最新的？<u>如果刷新太频繁，那么仍然会对数据库造成压力，缓存也就失去了作用。如果刷新不及时， 那么流式处理中所用的数据就会过时</u>。

​	如果能够捕捉数据库的变更事件，井形成事件流，流式处理作业就可以监听事件流， 井及时更新缓存。捕捉数据库的变更事件井形成事件流，这个过程被称为**CDC——变更数据捕捉（Change Data Capture）** 。如果使用了Connect ，就会发现，有一些连接器可以用于执行CDC 任务，把数据库表转成变更事件流。这样就拥有了数据库表的私有副本， 一旦数据库发生变更，用户会收到通知，井根据变更事件更新私有副本里的数据，如图11-7 所示。

![kafka：(9) 流式处理](https://www.likecs.com/default/index/img?u=aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTM1MDg0My8yMDIxMTAvMTM1MDg0My0yMDIxMTAyMTIyMjY0MjIzMC00NDg3NTUyNTYucG5n)

​	图11-7 ：连接流和表的拓扑， 不需要外部数据源

​	这样一来，当收到点击事件时，可以从本地的缓存里查找user_id ， 并将其填充到点击事件里。因为使用的是本地缓存，它具有更强的伸缩’性，而且不会影响数据库和其他使用数据库的应用程序。

​	<u>之所以将这种方案叫作流和表的连接，是因为其中的一个流代表了本地缓存表的变更</u>。

### 11.3.5 流与流的连接

​	有时候需要连接两个真实的事件流。什么是“真实”的流？本章开始的时候曾经说过，流是无边界的。如果使用一个流来表示一个表，那么就可以忽略流的大部分历史事件，因为你只关心表的当前状态。不过，<u>如果要连接两个流，那么就是在连接所有的历史事件一一将两个流里具有相同键和发生在相同时间窗口内的事件匹配起来。这就是为什么流和流的连接也叫作基于时间窗口的连接（ windowed-join ）</u>。

​	假设有一个由网站用户输入的搜索事件流和一个由用户对搜索结果进行点击的事件流。对用户的搜索和用户对搜索结果的点击进行匹配，就可以知道哪一个搜索的热度更高。很显然，我们需要基于搜索关键词进行匹配，而且每个关键词只能与一定时间窗口内的事件进行匹配——假设用户在输入搜索关键词后几秒钟就会点击搜索结果。因此，我们为每一个流维护了以几秒钟为单位的时间窗口，并对这些时间窗口事件结果进行匹配，如图11-8 所示。

![kafka：(9) 流式处理](https://www.likecs.com/default/index/img?u=aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTM1MDg0My8yMDIxMTAvMTM1MDg0My0yMDIxMTAyMTIyMzMyMTYwOC0xMTg3Mzg4NDA3LnBuZw==)

​	图11-8 连接两个流，通常包含一个移动时间窗

​	在Streams 中，上述的两个流都是通过相同的键来进行分区的，这个键也是用于连接两个流的键。这样一来， user_id:42 的点击事件就被保存在点击主题的分区5 上，而所有user_id:42的搜索事件被保存在搜索主题的分区5 上。Streams 可以确保这两个主题的分区5 的事件被分配给同一个任务，这个任务就会得到所有与user_id:42 相关的事件。Streams 在内嵌的RocksDB 里维护了两个主题的连接时间窗口，所以能够执行连接操作。

### 11.3.6 乱序的事件

​	不管是对于流式处理还是传统的ETL 系统来说，处理乱序事件都是一个挑战。物联网领域经常发生乱序事件： 一个移动设备断开WiFi 连接几个小时，在重新连上WiFi 之后将几个小时累积的事件一起发送出去。这在监控网络设备（故障交换机被修复之前不会发送任何诊断数据）或进行生产（装置间的网络连接非常不可靠） 时也时有发生。

​	要让流处理应用程序处理好这些场景，需要做到以下几点。

+ 识别乱序的事件。应用程序需要检查事件的时间，并将其与当前时间进行比较。

+ 规定一个时间段用于重排乱序的事件。比如3 个小时以内的事件可以重排，但3周以外的事件就可以直接扔掉。

+ 具有在一定时间段内重排乱序事件的能力。这是流式处理应用与批处理作业的一个主要不同点。假设有一个每天运行的作业， 一些事件在作业结束之后才到达，那么可以重新运行昨天的作业来更新事件。<u>而在流式处理中，“重新运行昨天的作业”这种情况是不存在的，乱序事件和新到达的事件必须一起处理</u>。
+ 具备更新结果的能力。如果处理的结果保存到数据库里，那么可以通过put 或update 对结果进行更新。如果流应用程序通过邮件发送结果，那么要对结果进行更新，就需要很巧妙的手段。

​	有一些流式处理框架，比如Google 的Dataflow 和Kafka的Streams ，都支持独立于处理时间发生的事件，井且能够处理比当前处理时间更晚或更早的事件。它们在本地状态里维护了多个聚合时间窗口，用于更新事件，并为开发者提供配置时间窗口大小的能力。当然，时间窗口越大，维护本地状态需要的内存也越大。

​	Streams API 通常将聚合结果写到主题上。这些主题一般是压缩日志主题，也就是说，它们只保留每个键的最新值。如果一个聚合时间窗口的结果需要被更新为晚到事件的结果，Streams 会直接为这个聚合时间窗口写入一个新的结果，将前一个结果覆盖掉。

### 11.3.7 重新处理

​	最后一个很重要的模式是重新处理事件，该模式有两个变种。

+ 我们对流式处理应用进行了改进， 使用新版本应用处理同一个事件流，生成新的结果，井比较两种版本的结果，然后在某个时间点将客户端切换到新的结果流上。
+ 现有的流式处理应用出现了缺陷，修复缺陷之后，重新处理事件流并重新计算结果。

​	对于第一种情况， Kafka 将事件流长时间地保存在可伸缩的数据存储里。也就是说，要使用两个版本的流式处理应用来生成结果，只需要满足如下条件：

+ 将新版本的应用作为一个新的消费者群组

+ 让它从输入主题的第一个偏移量开始读取数据（这样它就拥有了属于自己的输入流事件副本）
+ 检查结果流，在新版本的处理作业赶上进度时，将客户端应用程序切换到新的结果流上

​	第二种情况有一定的挑战性。它要求“重置”应用，让应用回到输入流的起始位置开始处理，同时重置本地状态（这样就不会将两个版本应用的处理结果棍淆起来了），而且还可能需要清理之前的输出流。虽然Streams 提供了一个工具用于重置应用的状态，不过如果有条件运行两个应用程序井生成两个结果流，还是建议使用第一种方案。第一种方案更加安全，多个版本可以来回切换，可以比较不同版本的结果，而且不会造成数据的丢失，也不会在清理过程中引入错误。

## 11.4 Streams 示例

​	Kafka 有两个基于流的API ， 一个是底层的Processor API ， 一个是高级的Streams DSL 。下面的例子中将使用Streams DSL 。通过为事件流定义转换链可以实现流式处理。转换可以是简单的过滤器，也可以是复杂的流与流的连接。我们可以通过底层的API 实现自己的转换，不过没必要这么做。

​	在使用DSL API 时， 一般会先用StreamBuilder 创建一个**拓扑**（ topology ）。拓扑是一个有向图（ DAG ），包含了各个转换过程，将会被应用在流的事件上。在创建好拓扑后，使用拓扑创建一个KafkaStreams 执行对象。多个线程会随着KafkaStreams对象启动，将拓扑应用到流的事件上。在关闭KafkaStreams对象时，处理也随之结束。

### 11.4.1 字数统计

### 11.4.2 股票市场统计

### 11.4.3 填充点击事件流

## 11.5 Kafka Streams 的架构概览

### 11.5.1 构建拓扑

​	每个流式应用程序至少会实现和执行一个**拓扑**。拓扑（在其他流式处理框架里叫作DAG,即有向无环图）是一个操作和变换的集合，每个事件从输入到输出都会流经它。在之前的字数统计示例里，拓扑结构如图11- 10 所示。

![kafka：(9) 流式处理](https://www.likecs.com/default/index/img?u=aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTM1MDg0My8yMDIxMTAvMTM1MDg0My0yMDIxMTAyMTIyMzY0NzUyMy0zODI4MjQ1MTYucG5n)

​	图11-10 ：字数统计示例的拓扑结构

​	哪怕是一个很简单的应用，都需要一个拓扑。拓扑是由处理器组成的，这些处理器是拓扑图里的节点（用椭圆表示）。大部分处理器都实现了一个数据操作一一过滤、映射、聚合等。数据源处理器从主题上读取数据，井传给其他组件， 而数据地处理器从上一个处理器接收数据，并将它们生成到主题上。拓扑总是从一个或多个数据源处理器开始，井以一个或多个数据池处理器结束。

### 11.5.2 对拓扑进行伸缩

​	Streams 通过在单个实例里运行多个线程和在分布式应用实例间进行负载均衡来实现伸缩。用户可以在一台机器上运行Streams应用，并开启多个线程，也可以在多台机器上运行Streams 应用。不管采用何种方式，所有的活动线程将会均衡地处理工作负载。

​	Streams引擎将拓扑拆分成多个子任务来并行执行。拆分成多少个任务取决于Streams引擎，同时也取决于主题的分区数量。每个任务负责一些分区：任务会订阅这些分区，并从分区读取事件数据， 在将结果写到数据池之前，在每个事件上执行所有的处理步骤。这些任务是Streams引擎最基本的井行单元，因为每个任务可以彼此独立地执行，如图11-11所示。

![kafka：(9) 流式处理](https://www.likecs.com/default/index/img?u=aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTM1MDg0My8yMDIxMTAvMTM1MDg0My0yMDIxMTAyMTIyNDkyNzgzNS0xNTA5OTI2MTkwLnBuZw==)

​	图11-11 ：运行相同拓扑的两个任务一一每个读取主题的一个分区

​	<u>开发人员可以选择每个应用程序使用的线程数。如果使用了多个线程，每个线程将会执行一部分任务。如果有多个应用实例运行在多个服务器上，每个服务器上的每一个线程都会执行不同的任务。这就是流式应用的伸缩方式： 主题里有多少分区，就会有多少任务。果想要处理得更快，就添加更多的线程。如果一台服务器的资源被用光了，就在另一台服务器上启动应用实例。Kafka 会自动地协调工作，它为每个任务分配属于它们的分区， 每个任务独自处理自己的分区</u>，井维护与聚合相关的本地状态，如图11-12 所示。

![kafka：(9) 流式处理](https://www.likecs.com/default/index/img?u=aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTM1MDg0My8yMDIxMTAvMTM1MDg0My0yMDIxMTAyMTIyNTQzMjA3Ny0zMzcyNDk4MjQucG5n)

​	图11-12 ：处理任务可以运行在多个线程和多个服务器上

​	大家或许已经注意到，<u>有时候一个步骤需要处理来自多个分区的结果，这样就会在任务之间形成依赖</u>。例如，在点击事件流的例子里对两个流进行了连接，在生成结果之前， 需要从每一个流的分区里获取数据。<u>Streams 将连接操作所涉及的分区全部分配给相同的任务，这样，这个任务就可以从相关的分区读取数据，井独立执行连接操作</u>。这也就是为什么Streams 要求同一个连接操作所涉及的主题必须要有相同数目的分区，而且要基于连接所使用的键进行分区。

​	<u>如果应用程序需要进行重新分区，也会在任务之间形成依赖</u>。例如，在点击事件流的例子里，所有的事件使用用户ID 作为键。如果想要基于页面或者邮政编码生成统计信息该怎么办？此时就需要使用邮政编码对数据进行重新分区，并在新分区上运行聚合操作。如果任务1 处理来自分区1 的数据，这些数据到达另一个处理器，这个处理器对数据进行重新分区（groupBy操作），它需要对数据进行shuffle ，也就是把数据发送给其他任务进行处理。与其他流式处理框架不一样的是， Streams 通过使用新的键和分区将事件写到新的主题来实现重新分区，并启动新的任务从新主题上读取和处理事件。<u>重新分区的步骤是将拓扑拆分成两个子拓扑，每个子拓扑都有自己的任务集</u>，如图11- 13 所示。第二个任务集依赖第一个任务集，因为它们处理的是第一个子拓扑的结果。不过，它们仍然可以独立地井行执行，因为第一个任务集以自己的速率将数据写到一个主题上，而第二个任务集也以自己的速率从这个主题读取和处理事件。两个任务集之间不需要通信， 也没有共享资源，而且它们也不需要运行在相同的线程里戎相同的服务器上。这是<u>Kafka 提供的最有用的特性之一一一减少管道各个部分之间的依赖</u>。

![kafka：(9) 流式处理](https://www.likecs.com/default/index/img?u=aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTM1MDg0My8yMDIxMTAvMTM1MDg0My0yMDIxMTAyMTIzMDEzMjY0OC04OTQ0NDEwMDQucG5n)

​	图11- 13 ：处理主题分区事件的两组任务

### 11.5.3 从故障中存活下来

​	Streams 的伸缩模型不仅允许伸缩应用，还能优雅地处理故障。首先，包括本地状态在内的所有数据被保存到有高可用性的Kafka 上。如果应用程序出现故障需要重启，可以从Kafka 上找到上一次处理的数据在流中的位置，井从这个位置开始继续处理。如果本地状态丢失（ 比如可能需要将服务器替换掉），应用程序可以从保存在Kafka 上的变更日志重新创建本地状态。

​	<u>Streams 还利用了消费者的协调机制来实现任务的高可用性。如果一个任务失败，只要还有其他线程或者应用程序实例可用，就可以使用另一个线程来重启该任务</u>。这类似于消费者群组的故障处理，如果一个消费者失效，就把分区分配给其他活跃的消费者。

## 11.6 流式处理使用场景

​	本章的开头解释过，如果想快速处理事件，而不是为每个批次等上几个小时，但又不是真的要求毫秒级的响应，那么流式处理（或者说持续处理）就可以派上用场了。话是没错，不过听起来仍然十分抽象。下面来看一些例子，它们都使用流式处理来解决实际的问题。

+ 客户服务

  自动发送确认邮件、发送票据等。

+ 物联网

  实时质量监控设备

+ 欺诈检测（异常检查）

  识别作弊行为等

## 11.7 如何选择流式处理框架

​	在比较两个流式处理系统时，要着重考虑使用场景是什么。以下是一些需要考虑的应用类别。

+ 摄取

  摄取的目的是将数据从一个系统移动到另一个系统，井在传输过程中对数据进行一些修改，使其更适用于目标系统。

+ 低延迟

  任何要求立即得到响应的应用。有些欺诈检测场景就属于这一类。

+ 异步微服务

  这些微服务为大型的业务流程执行一些简单操作，比如更新仓储信息。这些应用需要通过维护本地状态缓存来提升性能。

+ 几近实时的数据分析

  这些流式媒体应用程序执行复杂的聚合和连接，以便对数据进行切分，井生成有趣的业务见解。

​	选择何种流式处理系统取决于要解决什么问题。

+ 如果要解决摄取问题，那么需要考虑一下是需要一个流式处理系统还是一个更简单的专注于摄取的系统，比如Kafka Connect 。如果确定需要一个流式处理系统，那就要确保它拥有可用的连接器，并且要保证目标系统也有高质量的连接器可用。
+ <u>如果要解决的问题要求毫秒级的延迟，那么就要考虑一下是否一定要用流。一般来说，请求与响应模式更加适用于这种任务。如果确定需要一个流式处理系统，那就需要选择一个支持低延迟的模型，而不是基于微批次的模型</u>。
+ 如果要构建异步微服务，那么需要一个可以很好地与消息总线（希望是Kafka ） 集成的流式处理系统。它应该具备变更捕捉能力，这样就可以将上游的变更传递到微服务本地的缓存里，而且它要支持本地存储，可以作为微服务数据的缓存和物化视图。
+ 如果要构建复杂的数据分析引擎，那么也需要一个支持本地存储的流式处理系统，不过这次不是为了本地缓存和物化视图，而是为了支持高级的聚合、时间窗口和连接，因为如果没有本地存储，就很难实现这些特性。API 需要支持自定义聚合、基于时间窗口的操作和多类型连接。

​	除了使用场景外，还有如下一些全局的考虑点。

+ 系统的可操作性

  它是否容易部署？是否容易监控和调试？是否易于伸缩？它是否能够很好地与已有的基础设施集成起来？如果出现错误，需要重新处理数据，这个时候该怎么办？

+ API 的可用性和调试的简单性

  为了开发出高质量的应用，同一种框架的不同版本可能需要耗费不同的时间，这类情况很常见。开发时间和上市时机太重要了，所以我们需要选择一个高效率的系统。

+ 让复杂的事情简单化

  几乎每一个系统都声称它们支持基于时间窗口的高级聚合操作和本地缓存，但问题是，它们够简单吗？它们是处理了规模伸缩和故障恢复方面的细节问题，还是只提供了脆弱的抽象，然后让你来处理剩下的事情？系统提供的API 越简洁，封装的细节越多，开发人员的效率就越高。

+ 社区

  大部分流式处理框架都是开掘的。对于开源软件来说， 一个充满生气的社区是不可替代的。好的社区意味着用户可以定期获得新的功能特性，而且质量相对较高（没有人会使用糟糕的软件），缺陷可以很快地得到修复，而且用户的问题可以及时得到解答。这也意味着，如果遇到一个奇怪的问题并在Google 上搜索，可以搜索到相关的信息，因为其他人也在使用这个系统，而且也遇到了相同的问题。

## 11.8 总结

+ 流式处理
+ Kafka Streams
+ 流式处理使用场景
