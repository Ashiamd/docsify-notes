# Kafka权威指南-学习笔记02

# 7. 构建数据管道

​	Kafka 为数据管道带来的主要价值在于，它可以作为数据管道各个数据段之间的大型缓冲区， 有效地解耦管道数据的生产者和消费者。Kafka 的解耦能力以及在安全和效率方面的可靠性，使它成为构建数据管道的最佳选择。

> **数据集成的场景**
>
> 有些组织把Kafka 看成是数据管道的一个端点，他们会想“我怎么才能把数据从Kafka 移到ElasticSearch 里”。这么想是理所当然的一一特别是当你需要的数据在到达ElasticSearch 之前还停留在Kafka 里的时候，其实我们也是这么想的。不过我们要讨论的是如何在更大的场景里使用Kafka ，这些场景至少包含两个端点（可能会更多） ，而且这些端点都不是Kafka 。**对于那些面临数据集成问题的人来说，我们建议他们从大局考虑问题，而不只是把注意力集中在少量的端点上。过度聚焦在短期问题上，只会增加后期维护的复杂性，付出更高的成本。**

​	本章将讨论在构建**数据管道**时需要考虑的几个常见问题。这些问题并非Kafka 独有， 它们都是与数据集成相关的一般性问题。我们将解释为什么可以使用Kafka 进行数据集成，以及它是如何解决这些问题的。我们将讨论Connect API 与普通的客户端API (Producer 和Consumer）之间的区别，以及这些客户端API 分别适合在什么情况下使用。然后我们会介绍Connect。Connect 的完整手册不在本章的讨论范围之内，不过我们会举几个例子来帮助你入门，而且会告诉你可以从哪里了解到更多关于Connect 的信息。最后介绍其他数据集成系统，以及如何将它们与Kafka 集成起来。

## 7.1 构建数据管道时需要考虑的问题

### 7.1.1 及时性

​	有些系统希望每天一次性地接收大量数据，而有些则希望在数据生成几毫秒之内就能拿到它们。大部分数据管道介于这两者之间。一个好的数据集成系统能够很好地支持数据管道的各种及时性需求，而且在业务需求发生变更时，具有不同及时性需求的数据表之间可以方便地进行迁移。<u>Kafka 作为一个基于流的数据平台，提供了可靠且可伸缩的数据存储，可以支持几近实时的数据管道和基于小时的批处理。生产者可以频繁地向Kafka 写入数据，也可以按需写入：消费者可以在数据到达的第一时间读取它们，也可以每隔一段时间读取一次积压的数据</u>。

​	Kafka 在这里扮演了一个大型缓冲区的角色，降低了生产者和消费者之间的时间敏感度。实时的生产者和基于批处理的消费者可以同时存在，也可以任意组合。实现回压策略也因此变得更加容易， Kafka 本身就使用了回压策略（必要时可以延后向生产者发送确认），消费速率完全取决于消费者自己。

### * 7.1.2 可靠性

​	我们要避免单点故障，并能够自动从各种故障中快速恢复。数据通过数据管道到达业务系统，哪怕出现几秒钟的故障，也会造成灾难性的影响，对于那些要求毫秒级的及时性系统来说尤为如此。数据传递保证是可靠性的另一个重要因素。<u>有些系统允许数据丢失，不过在大多数情况下，它们要求**至少一次传递**。也就是说，源系统的每一个事件都必须到达目的地，不过有时候需要进行重试，而重试可能造成重复传递。有些系统甚至要求**仅一次传递**一一源系统的每一个事件都必须到达目的地，不允许丢失，也不允许重复</u>。

​	我们已经在第6章深入讨论了Kafka 的可用性和可靠性保证。**Kafka 本身就支持“至少一次传递”**，<u>如果再结合具有事务模型或唯一键特性的外部存储系统， Kafka 也能实现“仅一次传递”</u>。因为大部分的端点都是数据存储系统，它们提供了“仅一次传递”的原语支持，所以基于Kafka 的数据管道也能实现“仅一次传递”。

​	<u>值得一提的是， Connect APl 为集成外部系统提供了处理偏移量的APl ，连接器因此可以构建仅一次传递的端到端数据管道。实际上，很多开源的连接器都支持仅一次传递</u>。

### * 7.1.3 高吞吐量和动态吞吐量

​	为了满足现代数据系统的要求，数据管道需要支持非常高的吞吐量。<u>更重要的是，在某些情况下，数据管道还需要能够应对突发的吞吐量增长</u>。

​	由于我们将Kafka 作为生产者和消费者之间的缓冲区，消费者的吞吐量和生产者的吞吐量就不会耦合在一起了。我们也不再需要实现复杂的回压机制，如果生产者的吞吐量超过了消费者的吞吐量，可以把数据积压在Kafka 里，等待消费者追赶上来。通过增加额外的消费者或生产者可以实现Kafka 的伸缩，因此我们可以在数据管道的任何一边进行动态的伸缩，以便满足持续变化的需求。

​	因为Kafka 是一个高吞吐量的分布式系统， 一个适当规模的集群每秒钟可以处理数百兆的数据，所以根本无需担心数据管道无住满足伸缩性需求。另外， <u>Connect API 不仅支持伸缩，而且擅长并行处理任务。稍后，我们将会介绍数据源和数据池（Data Sink）如何在多个线程间拆分任务，最大限度地利用CPU资源，哪怕是运行在单台机器上</u>。

​	Kafka 支持多种类型的压缩，**在增长吞吐量时， Kafka 用户和管理员可以通过压缩来调整网络和存储资源的使用**。

### 7.1.4 数据格式

> [s3（S3 Simple Storage Service 简单存储服务）_百度百科 (baidu.com)](https://baike.baidu.com/item/s3/1409766?fr=aladdin)

​	数据管道需要协调各种数据格式和数据类型，这是数据管道的一个非常重要的因素。数据类型取决于不同的数据库和数据存储系统。你可能会通过Avro 将XML 或关系型数据加载到Kafka 里，然后将它们转成JSON 写入ElasticSearch ，或者转成Parquet 写入HDFS ，或者转成csv 写入S3 。

​	Kafka 和Connect API 与数据格式无关。我们已经在之前的章节介绍过，生产者和消费者可以使用各种序列化器来表示任意格式的数据。Connect API 有自己的内存对象模型，包括数据类型和schema 。不过，可以使用一些可插拔的转换器将这些对象保存成任意的格式，也就是说，不管数据是什么格式的，都不会限制我们使用连接器。

​	<u>很多数据源和数据池都有schema ，我们从数据源读取schema ，把它们保存起来，井用它们验证数据格式的兼容性，甚至用它们更新数据池的schema。从MySQL 到Hive的数据管道就是一个很好的例子。如果有人在MySQL 里增加了一个字段，那么在加载数据时，数据管道可以保证Hive 里也添加了相应的字段</u>。

​	另外，<u>数据池连接器将Kafka 的数据写入外部系统，因此需要负责处理数据格式</u>。有些连接器把数据格式的处理做成可插拔的，比如HDFS 的连接器就支持Avro 和Parquet 。

​	**通用的数据集成框架不仅要支持各种不同的数据类型，而且要处理好不同数据源和数据池之间的行为差异**。例如，在关系型数据库向Syslog 发起抓取数据请求时， Syslog 会将数据推送给它们，而HDFS 只支持追加写入模式，只能向HDFS 写入新数据，而对于其他很多系统来说，既可以追加数据， 也可以更新已有的数据。

### * 7.1.5 转换

​	**数据转换比其他需求更具争议性。数据管道的构建可以分为两大阵营，即ETL 和ELT** 。ETL 表示**提取一转换一加载**（ Extract-Transform-Load ），也就是说，当数据流经数据管道时，数据管道会负责处理它们。这种方式为我们节省了时间和存储空间，因为不需要经过保存数据、修改数据、再保存数据这样的过程。不过，这种好处也要视情况而定。有时候，这种方式会给我们带来实实在在的好处，但也有可能给数据管道造成不适当的计算和存储负担。<u>这种方式有一个明显不足，就是数据的转换会给数据管道下游的应用造成一些限制，特别是当下游的应用希望对数据进行进一步处理的时候。假设有人在MongoDB 和MySQL 之间建立了数据管道，井且过滤掉了一些事件记录，或者移除了一些字段，那么下游应用从MySQL 中访问到的数据是不完整的。如果它们想要访问被移除的字段， 只能重新构建管道，井重新处理历史数据（如果可能的话）</u>。

​	ELT 表示**提取－加载－转换**（Extract-Load-Transform）。<u>在这种模式下，数据管道只做少量的转换（主要是数据类型转换），确保到达数据地的数据尽可能地与数据源保持一致。**这种情况也被称为高保真（high fidelity）数据管道或数据湖（data lake）架构**</u>。目标系统收集“原始数据”，并负责处理它们。这种方式为目标系统的用户提供了最大的灵活性，因为它们可以访问到完整的数据。在这些系统里诊断问题也变得更加容易，因为数据被集中在同一个系统里进行处理，而不是分散在数据管道和其他应用里。<u>这种方式的不足在于，数据的转换占用了目标系统太多的CPU 和存储资源。有时候，目标系统造价高昂，如果有可能，人们希望能够将计算任务移出这些系统</u>。

### * 7.1.6 安全性

> [sasl_百度百科 (baidu.com)](https://baike.baidu.com/item/sasl/5292142?fr=aladdin)

​	安全性是人们一直关心的问题。对于数据管道的安全性来说，人们主要关心如下几个方面。

+ <u>我们能否保证流经数据管道的数据是经过加密的</u>？这是跨数据中心数据管道通常需要考虑的一个主要方面。
+ 谁能够修改数据管道？

+ 如果数据管道需要从一个不受信任的位置读取或写入数据，是否有适当的<u>认证机制</u>？

​	**Kafka 支持加密传输数据，从数据源到Kafka ，再从Kafka 到数据池**。

​	**它还支持认证（通过SASL 来实现）和授权**，所以你可以确信，如果一个主题包含了敏感信息，在不经授权的情况下，数据是不会流到不安全的系统里的。

​	**Kafka 还提供了审计日志用于跟踪访问记录**。通过编写额外的代码，还可能跟踪到每个事件的来源和事件的修改者，从而在每个记录之间建立起整体的联系。

### 7.1.7 故障处理能力

​	我们不能总是假设数据是完美的，而要事先做好应对故障的准备。能否总是把缺损的数据挡在数据管道之外？能否恢复无法解析的记录？能否修复（或许可以手动进行）并重新处理缺损的数据？如果在若干天之后才发现原先看起来正常的数据其实是缺损数据，该怎么办？

​	因为Kafka 会长时间地保留数据，所以我们可以在适当的时候回过头来重新处理出错的数据。

### * 7.1.8 耦合性和灵活性

​	数据管道最重要的作用之一是解耦数据源和数据池。它们在很多情况下可能发生耦合。

+ 临时数据管道

  有些公司为每一对应用程序建立单独的数据管道。例如， 他们使用Logstash向ElasticSearch 导入日志，使用Flume向HDFS 导入日志，使用GoldenGate 将Oracle 的数据导到HDFS ，使用Informatica 将MySQL 的数据或XML 导到Oracle ，等等。他们将数据管道与特定的端点耦合起来，井创建了大量的集成点，需要额外的部署、维护和监控。当有新的系统加入肘，他们需要构建额外的数据管道，从而增加了采用新技术的成本，同时遏制了创新。

+ 元数据丢失

  如果数据管道没有保留schema元数据，而且不允许schema 发生变更，那么最终会导致生产者和消费者之间发生紧密的耦合。没有了schema ，生产者和消费者需要额外的信息来解析数据。假设数据从Oracle 流向HDFS ，如果DBA 在Oracle 里添加了一个字段，而且没有保留schema 信息，也不允许修改schema ，那么从HDFS 读取数据时可能会发生错误，因此需要双方的开发人员同时升级应用程序才能解决这个问题。不管是哪一种情况，它们的解决方案都不具备灵活性。如果数据管道允许schema 发生变更，应用程序各方就可以修改自己的代码，无需担心对整个系统造成破坏。

+ 末端处理

  我们在讨论数据转换时就已提到，数据管道难免要做一些数据处理。在不同的系统之间移动数据肯定会碰到不同的数据格式和不同的应用场景。不过，如果数据管道过多地处理数据，那么就会给下游的系统造成一些限制。在构建数据管道时所做的设计决定都会对下游的系统造成束缚，比如应该保留哪些字段或应该如何聚合数据，等等。如果下游的系统有新的需求，那么数据管道就要作出相应的变更，这种方式不仅不灵活，而且低效、不安全。**<u>更为灵活的方式是尽量保留原始数据的完整性，让下游的应用自己决定如何处理和聚合数据</u>**。

## 7.2 如何在Connect API 和客户端API 之间作出选择

​	在向Kafka 写入数据或从Kafka 读取数据时，要么使用传统的生产者和消费者客户端，就像第3章和第4章所描述的那样，要么使用后面即将介绍的Connect API 和连接器。在具体介绍Connect API 之前，我们不妨先问自己一个问题：“什么时候适合用哪一个？”

​	我们知道， Kafka 客户端是要被内嵌到应用程序里的，应用程序使用它们向Kafka 写入数据或从Kafka 读取数据。如果你是开发人员，你会使用Kafka 客户端将应用程序连接到Kafka ，井修改应用程序的代码，将数据推送到Kafka 或者从Kafka 读取数据。

​	如果要将Kafka连接到数据存储系统，可以使用Connect ，因为这些系统不是你开发的，你无法或者也不想修改它们的代码。**Connect 可以用于从外部数据存储系统读取数据， 或者将数据推送到外部存储系统**。如果数据存储系统提供了相应的连接器，那么非开发人员就可以通过配置连接器的方式来使用Connect。

​	如果你要连接的数据存储系统没有相应的连接器，那么可以考虑使用客户端API 或Connect API 开发一个应用程序。**我们建议首选Connect ，因为它提供了一些开箱即用的特性，比如配置管理、偏移量存储、井行处理、错误处理，而且支持多种数据类型和标准的REST管理API** 。开发一个连接Kafka 和外部数据存储系统的小应用程序看起来很简单，但其实还有很多细节需要处理，比如数据类型和配置选项，这些无疑加大了开发的复杂性一一<u>Connect 处理了大部分细节，让你可以专注于数据的传输</u>。

## 7.3 Kafka Connect

​	Connect是Kafka 的一部分，它为在Kafka和外部数据存储系统之间移动数据提供了一种可靠且可伸缩的方式。它为连接器插件提供了一组API 和一个运行时——Connect 负责运行这些插件， 它们则负责移动数据。Connect 以worker进程集群的方式运行，我们基于worker进程安装连接器插件，然后使用REST API 来管理和配置connector，这些worker进程都是长时间持续运行的作业。连接器启动额外的task ，有效地利用工作节点的资源，以并行的方式移动大量的数据。数据源的连接器负责从源系统读取数据，井把数据对象提供给worker进程。数据地的连接器负责从worker进程获取数据，井把它们写入目标系统。Connect通过connector在Kafka里存储不同格式的数据。Kafka 支持JSON ，而且Confluent Schema Registry 提供了Avro 转换器。开发人员可以选择数据的存储格式，这些完全独立于他们所使用的连接器。

​	本章的内容无也完全覆盖Connect的所有细节和各种连接器，这些内容可以单独写成一本书。不过，我们会提供Connect的概览，还会介绍如何使用它，井提供一些额外的参考资料。

### 7.3.1 运行Connect

​	Connect随着Kafka一起发布，所以无需单独安装。<u>如果你打算在生产环境使用Connect来移动大量的数据，或者打算运行多个连接器，那么最好把Connect 部署在独立于broker 的服务器上</u>。在所有的机器上安装Kafka，并在部分服务器上启动broker，然后在其他服务器上启动Connect 。

​	启动Connect 进程与启动broker差不多， 在调用脚本时传入一个属性文件即可。

```shell
bin/connect-distributed.sh config/connect-distributed.properties
```

​	Connect 进程有以下几个重要的配置参数。

+ `bootstrap.servers` ：该参数列出了将要与Connect 协同工作的broker 服务器，连接器将会向这些broker 写入数据或者从它们那里读取数据。你不需要指定集群所有的broker，不过建议至少指定3 个。
+ `group.id`：<u>具有相同group id的worker属于Ω同一个Connect集群</u>。集群的连接器和它们的任务可以运行在任意一个worker 上。
+ `key.convert`和`value.converter`：Connect 可以处理存储在Kafka 里的不同格式的数据。这两个参数分别指定了消息的键和值所使用的转换器。默认使用Kafka 提供的JSONConverter ，当然也可以配置成Confluent Schema Registry 提供的AvroConverter 。

​	有些转换器还包含了特定的配置参数。例如，通过将key.converter.schema.enable设置成 true 或者 false 来指定 JSON 消息是否可以包含 schema。值转换器也有类似的配置，不过它的参数名是 value.converter.schema.enable 。Avro 消息也包含了 schema，不过需要通过 key.converter.schema.registry.url 和 value.converter.schema.registry.url 来指定 Schema Registry 的位置。

​	我们一般通过Connect的REST API来配置和监控rest.host.name和rest.port连接器。你可以为 REST API 指定特定的端口。

​	在启动worker集群之后，可以通过REST API来验证它们是否运行正常：

```
curl http://localhost:8083/
```

![img](https://www.icode9.com/i/ll/?i=20201216221034596.png)

​	这个REST URI 应该要返回当前Connect 的版本号。我运行的是Kafka 0.10.1.0（预发行）快照版本。我们还可以检查已经安装好的连接器插件：

```
curl http://localhost:8083/connector-plugins
```

![img](https://www.icode9.com/i/ll/?i=20201216221109405.png)

> **单机模式**
>
> 要注意，Connect也支持单机模式。单机模式与分布式模式类似， 只是在启动时使用 bin/connect-standalone.sh代替bin/connect-distributed.sh，也可以通过命令行传入连接器的配置文件，这样就不需要使用REST API了。
>
> 在单机模式下，所有的连接器和任务都运行在单独的worker进程上。单机模式使用起来更简单，特别是在开发和诊断问题的时候，或者是在需要让连接器和任务运行在某台特定机器上的时候（比如Syslog连接器会监听某个端口，所以你需要知道它运行在哪台机器上）

### 7.3.2 连接器示例一一文件数据源和文件数据池

...

​	在删除连接器之后，如果查看Connect 的日志，你会发现其他的连接器会重启它们的任务。这是为了在worker 进程间平衡剩余的任务，确保删除连接器之后可以保持负载的均衡。

### 7.3.3 连接器示例一一从MySQL 到ElasticSearch

​	将一个MySQL 的表数据导入到一个Kafka 主题上，再将它们加载到ElasticSearch 里，然后对它们的内容进行索引。

​	...

> 构建自己的连接器
>
> 任何人都可以基于公开的Connector API 创建自己的连接器。事实上，人们创建了各种连接器，然后把它们发布到连接器中心（Connector Hub），并告诉我们怎么使用它们。如果你在连接器中心找不到可以适配你要集成的数据存储系统的连接器，可以开发自己的连接器。你也可以把自己的连接器贡献给社区，让更多的人知道和使用它们。

### 7.3.4 深入理解Connect

​	要理解Connect 的工作原理，需要先知道3 个基本概念，以及它们之间是如何进行交互的。我们已经在之前的示例里慎示了如何运行worker进程集群以及如何启动和关闭**连接器**。不过我们并没有深入解释**转化器**是如何处理数据的一一转换器把MySQL 的数据行转成JSON 记录，然后由连接器将它们写入Kafka。

​	现在让我们深入理解每一个组件，以及它们之间是如何进行交互的。

1. 连接器和任务

   连接器插件实现了Connector API，API 包含了两部分内容。

   **连接器**

   连接器负责以下3 件事情。

   + 决定需要运行多少个任务。
   +  按照任务来拆分数据复制。
   + 从worker 进程获取任务配置并将其传递下去。例如， JDBC 连接器会连接到数据库，统计需要复制的数据表，井确定需要执行多少个任务，然后在配置参数`max.tasks`和实际数据量之间选择数值较小的那个作为任务数。在确定了任务数之后，连接器会为每个任务生成一个配置，配置里包含了连接器的配置项（比如connection.url）和该任务需要复制的数据表。`taskConfigs()`方法也返回一个映射列表，这些映射包含了任务的相关配置。worker 进程负责启动和配置任务，每个任务只复制配置项里指定的数据表。如果通过REST API 启动连接器，有可能会启动任意节点上的连接器，那么连接器的任务就会在该节点上执行。

   **任务**

   任务负责将数据移入或移出Kafka。任务在初始化时会得到由worker 进程分配的一个上下文： 源系统上下文（ Source Context ）包含了一个对象，可以将源系统记录的偏移量保存在上下文里（ 例如，文件连接器的偏移量就是文件里的字节位置， JDBC 连接器的偏移量可以是数据表的主键ID ） 。目标系统连接器的上下文提供了一些方法，连接器可以用它们操作从Kafka 接收到的数据，比如进行数据清理、错误重试，或者将偏移量保存到外部系统以便实现仅一次传递。任务在完成初始化之后，就开始按照连接器指定的配置（包含在一个Properties 对象里）启动工作。源系统任务对外部系统进行轮询，并返回一些记录， worker 进程将这些记录发送到Kafka 。数据地任务通过worker 进程接收来自Kafka 的记录，井将它们写入外部系统。

2. worker进程

   worker 进程是连接器和任务的“容器”。它们负责处理HTTP请求，这些请求用于定义连接器和连接器的配置。它们还负责保存连接器的配置、启动连接器和连接器任务，并把配置信息传递给任务。<u>如果一个worker 进程停止工作或者发生崩溃，集群里的其他worker进程会感知到（ Kafka 的消费者协议提供了心跳检测机制），井将崩溃进程的连接器和任务重新分配给其他进程</u>。如果有新的进程加入集群，其他进程也会感知到，并将自己的连接器和任务分配给新的进程，确保工作负载的均衡。进程还负责提交偏移量，如果任务抛出异常，可以基于这些偏移量进行重试。

   <u>为了更好地理解worker 进程，我们可以将其与连接器和任务进行简单的比较。连接器和任务负责“数据的移动”， 而worker 进程负责REST API 、配置管理、可靠性、高可用性、伸缩性和负载均衡</u>。

   <u>这种关注点分离是Connect API 给我们带来的最大好处，而这种好处是普通客户端API 所不具备的。有经验的开发人员都知道，编写代码从Kafka 读取数据并将其插入数据库只需要一到两天的时间，但是如果要处理好配置、异常、REST API 、监控、部署、伸缩、失效等问题，可能需要几个月。如果你使用连接器来实现数据复制，连接器插件会为你处理掉一大堆复杂的问题</u>。

3. 转化器和Connect的数据模型

   数据模型和转化器是Connect API 需要讨论的最后一部分内容。Connect 提供了一组数据API——它们包含了数据对象和用于描述数据的schema。例如， JDBC 连接器从数据库读取了一个字段，并基于这个字段的数据类型创建了一个Connect Schema 对象。然后使用这些Schema 对象创建一个包含了所有数据库字段的Struct一一我们保存了每一个字段的名字和它们的值。<u>源连接器所做的事情都很相似一一从源系统读取事件，并为每个事件生成schema 和值（值就是数据对象本身） 。目标连接器正好相反，它们获取schema 和值，井使用schema 来解析值，然后写入到目标系统</u>。

   源连接器只负责基于Data API 生成数据对象，那么worker 进程是如何将这些数据对象保存到Kafka 的？这个时候，转换器就派上用场了。用户在配置worker 进程（或连接器）时可以选择使用合适的转化器，用于将数据保存到Kafka 。目前可用的转化器有Avro 、JSON和String。JSON 转化器可以在转换结果里附带上schema ，当然也可以不使用schema ，这个是可配的。Kafka 系统因此可以支持结构化的数据和半结构化的数据。连接器通过DataAPI 将数据返回给worker 进程， worker 进程使用指定的转化器将数据转换成Avro 对象、JSON 对象或者字符串，然后将它们写入Kafka 。

   对于目标连接器来说，过程刚好相反一一在从Kafka 读取数据时， worker 进程使用指定的转换器将各种格式（ Avro 、JSON 或String ）的数据转换成Data API 格式的对象，然后将它们传给目标连接器，目标连接器再将它们插入到目标系统。

   Connect API 因此可以支持多种类型的数据，数据类型与连接器的实现是相互独立的一一只要有可用的转换器，连接器和数据类型可以自由组合。

4. 偏移量管理

   worker 进程的REST API 提供了部署和配置管理服务，除此之外， worker 进程还提供了偏移量管理服务。连接器只要知道哪些数据是已经被处理过的，就可以通过Kafka 提供的API 来维护偏移量。

   源连接器返回给worker 进程的记录里包含了一个逻辑分区和一个逻辑偏移量。它们并非Kafka 的分区和偏移量，而是源系统的分区和偏移量。例如，对于文件源来说，分区可以是一个文件，偏移量可以是文件里的一个行号或者字符号：而对于JDBC 源来说，分区可以是一个数据表，偏移量可以是一条记录的主键。<u>在设计一个据连接器时，要着重考虑如何对源系统的数据进行分区以及如何跟踪偏移量，这将影响连接器的井行能力，也决定了连接器是否能够实现至少一次传递或者仅一次传递</u>。

   源连接器返回的记录里包含了源系统的分区和偏移量， worker 进程将这些记录发送给Kafka 。如果Kafka 确认记录保存成功， worker 进程就把偏移量保存下来。偏移量的存储机制是可插拔的， 一般会使用Kafka 主题来保存。如果连接器发生崩溃井重启，它可以从最近的偏移量继续处理数据。

   目标连接器的处理过程恰好相反，不过也很相似。它们从Kafka 上读取包含了主题、分区和偏移量信息的记录，然后调用连接器的put() 方法，该方法会将记录保存到目标系统里。如果保存成功，连接器会通过消费者客户端将偏移量提交到Kafka 上。

   框架提供的偏移量跟踪机制简化了连接器的开发工作，并在使用多个连接器时保证了一定程度的行为一致性。

## 7.4 Connect之外的选择

### 7.4.1 用于其他数据存储的摄入框架

​	虽然我们很想说Kafka 是至高无上的明星，但肯定会有人不同意这种说法。有些人将Hadoop 或ElasticSearch 作为他们数据架构的基础，这些系统都有自己的数据摄入工具。Hadoop 使用了Flume，ElasticSearch 使用了Logstash 或Fluentd 。如果架构里包含了Kafka ，并且需要连接大量的源系统和目标系统，那么建议使用Connect API 作为摄入工具。如果构建的系统是以Hadoop 或ElasticSearch 为中心的， Kafka 只是数据的来源之一，那么使用Flume 或Logstash 会更合适。

### 7.4.2 基于图形界面的ETL工具

​	从保守的Informatica 到一些开源的替代方案，比如Talend 和Pentaho ， 或者更新的Apache NiFi 和StreamSets一一这些ETL 解决方案都支持将Kafka 作为数据源和数据地。如果你已经使用了这些系统，比如Pentaho ， 那么就可能不会为了Kafka 而在系统里增加另一种集成工具。如果你已经习惯了基于图形界面的ETL 数据管道解决方案，那就继续使用它们。不过， 这些系统有一些不足的地方，那就是它们的工作流比较复杂，如果你只是希望从Kafka 里获取数据或者将数据写入Kafka ，那么它们就显得有点笨重。我们在本章的开头部分已经说过，**在进行数据集成时，应该将注意力集中在消息的传输上。因此，对于我们来说，大部分ETL 工具都太过复杂了**。

​	我们极力建议将Kafka 当成是一个支持数据集成（使用Connect ）、应用集成（使用生产者和消费者）和流式处理的平台。Kafka 完全可以成为ETL 工具的替代品。

### 7.4.3 流式处理框架

​	几乎所有的流式处理框架都具备从Kafka 读取数据并将数据写入外部系统的能力。如果你的目标系统支持流式处理，井且你已经打算使用流式框架处理来自Kafka 的数据，那么使用相同的框架进行数据集成看起来是很合理的。这样可以省掉一个处理步骤（不需要保存来自Kafka 的数据，而是直接从Kafka 读取数据然后写到其他系统） ， 不过在发生数据丢失或者出现脏数据时，诊断问题会变得很困难，因为这些框架并不知道数据是什么时候丢失的，或者什么时候出现了脏数据。

## 7.5 总结

​	本章讨论了如何使用Kafka 进行数据集成，从解释为什么要使用Kafka 进行数据集成开始，到说明数据集成方案的一般性考虑点。我们先解释了为什么Kafka 和Connect API 是一种更好的选择，然后给出了一些例子， 演示如何在不同的场景下使用Connect ，井深入了解了Connect 的工作原理，最后介绍了一些Conn ect 之外的数据集成方案。

​	不管最终你选择了哪一种数据集成方案，都需要保证所有消息能够在各种恶劣条件下完成传递。我们相信，在与Kafka 的可靠性特性结合起来之后， Connect 具有了极高的可靠性。不过，我们仍然需要对它们进行严格的测试，以确保你选择的数据集成系统能够在发生进程停止、机器崩溃、网络延迟和高负载的情况下不丢失消息。毕竟，数据集成系统应该只做一件事情，那就是传递数据。

​	可靠性是数据集成系统唯一一个重要的需求。在选择数据系统时，首先要明确需求（可以参考7 .1节），并确保所选择的系统能够满足这些需求。除此之外，还要很好地了解数据集成方案，确保知道怎么使用它们来满足需求。虽然Kafka 支持至少一次传递的原语，但你也要小心谨慎，避免在配置上出现偏差，破坏了可靠性。

# 8. 跨集群数据镜像

​	在某些情况下，不同的集群之间相互依赖，管理员需要不停地在集群间复制数据。大部分数据库都支持**复制**（replication），也就是持续地在数据库服务器之间复制数据。不过，<u>因为前面已经使用过“复制”这个词来描述在同一个集群的节点间移动数据，所以我们把集群间的数据复制叫作**镜像**（mirroring）</u>。Kafka 内置的跨集群复制工具叫作MirrorMaker 。

​	在这一章，我们将讨论跨集群的数据镜像，它们既可以镜像所有数据，也可以镜像部分数据。我们先从一些常见的跨集群镜像场景开始，然后介绍这些场景所使用的架构模式，以及这些架构模式各自的优缺点。接下来我们会介绍MirrorMaker以及如何使用它， 然后说明在进行部署和性能调优时需要住意的一些事项。最后我们会对MirrorMaker的一些替代方案进行比较。

## 8.1 跨集群镜像的使用场景

​	下面列出了几个使用跨集群镜像的场景。

+ 区域集群和中心集群

  有时候， 一个公司会有多个数据中心，它们分布在不同的地理区域、不同的城市或不同的大洲。这些数据中心都有自己的Kafka 集群。有些应用程序只需要与本地的Kafka 集群通信，而有些则需要访问多个数据中心的数据（否则就没必要考虑跨数据中心的复制方案了）。有很多情况需要跨数据中心，比如一个公司根据供需情况修改商品价格就是一个典型的场景。该公司在每个城市都有一个数据中心，它们收集所在城市的供需信息，并调整商品价格。这些信息将会被镜像到一个中心集群上，业务分析员就可以在上面生成整个公司的收益报告。

+ 冗余（DR)

  一个Kafka 集群足以支撑所有的应用程序，不过你可能会担心集群因某些原因变得不可用，所以你希望有第二个Kafka 集群，它与第一个集群有相同的数据，如果发生了紧急情况，可以将应用程序重定向到第二个集群上。

+ 云迁移

  现今有很多公司将它们的业务同时部署在本地数据中心和云端。为了实现冗余，应用程序通常会运行在云供应商的多个服务区域里，或者使用多个云服务。本地和每个云服务区域都会有一个Kafka 集群。本地数据中心和云服务区域里的应用程序使用自己的Kafka 集群，当然也会在数据中心之间传输数据。例如，如果云端部署了一个新的应用程序，它需要访问本地的数据。本地的应用程序负责更新数据，井把它们保存在本地的数据库里。我们可以使用Connect 捕获这些数据库变更，并把它们保存到本地的Kafka集群里，然后再镜像到云端的Kafka 集群上。这样有助于控制跨数据中心的流量成本，同时也有助于改进流量的监管和安全性。

## 8.2 多集群架构

### * 8.2.1 跨数据中心通信的一些现实情况

​	以下是在进行跨数据中心通信时需要考虑的一些问题。

+ 高延迟

  Kafka 集群之间的通信延迟随着集群间距离的增长而增加。虽然光缆的速度是恒定的，但集群间的网络跳转所带来的缓冲和堵塞会增加通信延迟。

+ 有限的带宽

  单个数据中心的广域网带宽远比我们想象的要低得多，而且可用的带宽时刻在发生变化。另外，高延迟让如何利用这些带宽变得更加困难。

+ 高成本

  不管你是在本地还是在云端运行Kafka ，集群之间的通信都需要更高的成本。部分原因是因为带宽有限，而增加带宽是很昂贵的，当然，这个与供应商制定的在数据中心、区域和云端之间传输数据的收费策略也有关系。

​	<u>Kafka 服务器和客户端是按照单个数据中心进行设计、开发、测试和调优的</u>。我们假设服务器和客户端之间具有很低的延迟和很高的带宽，在使用默认的超时时间和缓冲区大小时也是基于这个前提。因此，我们不建议跨多个数据中心安装Kafka 服务器（不过稍后会介绍一些例外情况）。

​	大多数情况下，我们要避免向远程的数据中心生成数据，但如果这么做了，那么就要忍受高延迟，井且需要通过**增加重试次数**（ <u>Linkedln 曾经为跨集群镜像设置了32 000 多次重试次数</u>）和**增大缓冲区**来解决潜在的网络分区问题（生产者和服务器之间临时断开连接）。

​	**如果有了跨集群复制的需求，同时又禁用了从broker到broker之间的通信以及从生产者到broker 之间的通信，那么我们必须允许从broker 到消费者之间的通信。事实上，这是最安全的跨集群通信方式。在发生网络分区时，消费者无法从Kafka 读取数据，数据会驻留在Kafka 里， 直到通信恢复正常。因此，网络分区不会造成任何数据丢失**。

​	<u>不过，因为带宽有限，如果一个数据中心的多个应用程序需要从另一个数据中心的Kafka 服务器上读取数据，我们倾向于为每一个数据中心安装一个Kafka 集群，并在这些集群间复制数据，而不是让不同的应用程序通过广域网访问数据</u>。

​	在讨论更多有关跨数据中心通信的调优策略之前，我们需要先知道以下一些架构原则。

+ **每个数据中心至少需要一个集群**。
+ 每两个数据中心之间的数据复制要做到每个事件**仅复制一次**（除非出现错误需要重试） 。
+ **如果有可能，尽量从远程数据中心<u>读取数据</u>，而不是向远程数据中心写入数据**。

### 8.2.2 Hub和Spoke架构

​	这种架构适用于一个中心Kafka 集群对应多个本地Kafka 集群的情况，如图8 -1 所示。

![img](https://img2020.cnblogs.com/blog/1350843/202110/1350843-20211017081433832-510514946.png)

​	图8-1 ：一个中心 Kafka 集群对应多个本地Kafka 集群

​	这种架构有一个简单的变种，如果只有一个本地集群， 那么整个架构里就只剩下两个集群： 一个首领和一个跟随者，如图8 -2所示。

![img](https://img2020.cnblogs.com/blog/1350843/202110/1350843-20211017081647760-1592900958.png)

​	图8-2: 一个首领对应一个跟随者

​	当消费者需要访问的数据集分散在多个数据中心时，可以使用这种架构。如果每个数据中心的应用程序只处理自己所在数据中心的数据，那么也可以使用这种架构，只不过它们无法访问到全局的数据集。

​	**这种架构的好处在于，数据只会在本地的数据中心生成，而且每个数据中心的数据只会被镜像到中央数据中心一次**。只处理单个数据中心数据的应用程序可以被部署在本地数据中心里， 而需要处理多个数据中心数据的应用程序则需要被部署在中央数据中心里。<u>因为数据复制是单向的，而且消费者总是从同一个集群读取数据，所以这种架构易于部署、配置和监控</u>。

​	<u>不过这种架构的简单性也导致了一些不足。一个数据中心的应用程序无法访问另一个数据中心的数据</u>。为了更好地理解这种局限性，我们举一个例子来说明。

​	假设有一家银行，它在不同的城市有多家分行。每个城市的Kafka集群上保存了用户的信息和账号历史数据。我们把各个城市的数据复制到一个中心集群上， 这样银行就可以利用这些数据进行业务分析。在用户访问银行网站或去他们所属的分行办理业务时， 他们的请求被路由到本地集群上，同时从本地集群读取数据。假设一个用户去另一个城市的分行办理业务，因为他的信息不在这个城市，所以这个分行需要与远程的集群发生交互（不建议这么做）， 否则根本没有办桂访问到这个用户的信息（很尴尬） 。**因此，这种架构模式在数据访问方面有所局限，因为区域数据中心之间的数据是完全独立的**。

​	**在采用这种架构时，每个区域数据中心的数据都需要被镜像到中央数据中心上。镜像进程会读取每一个区域数据中心的数据，并将它们重新生成到中心集群上**。<u>如果多个数据中心出现了重名的主题，那么这些主题的数据可以被写到中心集群的单个主题上，也可以被写到多个主题上</u>。

### * 8.2.3 双活架构

​	<u>当有两个或多个数据中心需要共享数据并且每个数据中心都可以生产和读取数据时， 可以使用双活（ Active-Active）架构</u>，如图8-3所示。

![img](https://img2020.cnblogs.com/blog/1350843/202110/1350843-20211017082346578-1788024956.png)

​	图8-3 ：两个数据中的需要共享数据

​	这种架构的主要好处在于，它可以为就近的用户提供服务，具有性能上的优势，而且不会因为数据的可用性问题（在Hub 和Spoke 架构中就有这种问题）在功能方面作出牺牲。第二个好处是冗余和弹性。因为每个数据中心具备完整的功能， 一旦一个数据中心发生失效，就可以把用户重定向到另一个数据中心。这种重定向完全是网络的重定向，因此是一种最简单、最透明的失效备援方案。

​	**这种架构的主要问题在于，如何在进行多个位置的数据异步读取和异步更新时避免冲突**。比如镜像技术方面的问题一一如何确保同一个数据不会被无止境地来回镜像？而<u>数据一致性</u>方面的问题则更为关键。下面是可能遇到的问题。

+ 如果用户向一个数据中心发送数据，同时从第二个数据中心读取数据，那么在用户读取数据之前，他发送的数据有可能还没有被镜像到第二个数据中心。对于用户来说， 这就好比把一本书加入到购物车，但是在他点开购物车时，书却不在里面。<u>因此，在使用这种架构时，开发人员经常会将用户“粘”在同一个数据中心上，以确保用户在大多数情况下使用的是同一个数据中心的数据（除非他们从远程进行连接或者数据中心不可用）</u> 。

+ 一个用户在一个数据中心订购了书A ，而第二个数据中心几乎在同一时间收到了该用户订购书B 的订单，在经过数据镜像之后，每个数据中心都包含了这两个事件。两个数据中心的应用程序需要知道如何处理这种情况。我们是否应该从中挑选一个作为“正确”的事件？如果是这样，我们需要在两个数据中心之间定义一致的规则，用于确定哪个事件才是正确的。又或者把两个都看成是正确的事件，将两本书都发给用户，然后设立一个部门专门来处理退货问题？ Amazon 就是使用这种方式来处理冲突的，但对于股票交易部门来说，这种方案是行不通的。如何最小化冲突以及如何处理冲突要视具体情况而定。总之要记住，<u>如果使用了这种架构，必然会遇到冲突问题，还要想办注解决它们</u>。

​	如果能够很好地处理在从多个位置异步读取数据和异步更新数据时发生的冲突问题，那么我们强烈建议使用这种架构。这种架构是我们所知道的最具伸缩性、弹性、灵活性和成本优势的解决方案。所以，它值得我们投入精力去寻找一些办法，用于避免循环复制、把相同用户的请求粘在同一个数据中心，以及在发生冲突时解决冲突。

​	**双活镜像（特别是当数据中心的数量超过两个）的挑战之处在于，每两个数据中心之间都需要进行镜像，而且是双向的**。如果有5 个数据中心，那么就需要维护至少20 个镜像进程，还有可能达到40 个，因为为了高可用，每个进程都需要冗余。

​	**另外，我们还要避免循环镜像，相同的事件不能无止境地来回镜像**。<u>对于每一个“逻辑主题”，我们可以在每个数据中心里为它创建一个单独的主题，并确保不要从远程数据中心复制同名的主题</u>。例如，对于逻辑主题"users"，我们在一个数据中心为其创建"SF.users"主题，在另一个数据中心为其创建"NYC.users"主题。镜像进程将SF 的" SF.users"镜像到NYC ，同时将NYC 的"NYC.users"镜像到SF 。这样一来，每一个事件只会被镜像一次，不过在经过镜像之后，每个数据中心同时拥有了SF.users和NYC.users这两个主题，也就是说，每个数据中心都拥有相同的用户数据。消费者如果要读取所有的用户数据，就需要以"\*.users"的方式订阅主题。我们也可以把这种方式理解为数据中心的命名空间，比如在这个例子里， NYC 和SF就是命名空间。

​	在不久的将来， Kafka 将会增加记录头部信息。头部信息里可以包含源数据中心的信息，我们可以使用这些信息来避免循环镜像，也可以用它们来单独处理来自不同数据中心的数据。当然，你也可以通过使用结构化的数据格式（比如Avro ）来实现这一特性，并用它在数据里添加标签和头部信息。不过在进行镜像时，需要做一些额外的工作，因为现成的镜像工具井不支持自定义的头部信息格式。

### * 8.2.4 主备架构

​	**有时候，使用多个集群只是为了达到灾备的目的**。你可能在同一个数据中心安装了两个集群，它们包含相同的数据，平常只使用其中的一个。当提供服务的集群完全不可用时，就可以使用第二个集群。又或者你可能希望它们具备地理位置弹性，比如整体业务运行在加利福尼亚州的数据中心上，但需要在德克萨斯州有第二个数据中心，第二个数据中心平常不怎么用，但是一且第一个数据中心发生地震，第二个数据中心就能派上用场。德克萨斯州的数据中心可能拥有所有应用程序和数据的非活跃（“冷”）复制，在紧急情况下，管理员可以启动它们，让第二个集群发挥作用。这种需求一般是合规性的，业务不一定会将其纳入规划范畴，但还是要做好充分的准备。主备（Active-Standby）架构意图如图8-4 所示。

![img](https://img2020.cnblogs.com/blog/1350843/202110/1350843-20211017092005673-1157229634.png)

​	图8-4 ：主备架构示意图

​	这种架构的好处是易于实现，而且可以被用于任何一种场景。你可以安装第二个集群，然后使用镜像进程将第一个集群的数据完整镜像到第二个集群上，不需要担心数据的访问和冲突问题，也不需要担心它会带来像其他架构那样的复杂性。

​	这种架构的不足在于，它浪费了一个集群。**Kafka 集群间的失效备援比我们想象的要难得多。从目前的情况来看，要实现不丢失数据或无重复数据的Kafka 集群失效备援是不可能的**。<u>我们只能尽量减少这些问题的发生，但无法完全避免</u>。

​	让一个集群什么事也不傲，只是等待灾难的发生，这明显就是对资源的浪费。因为灾难是（或者说应该是）很少见的，所以在大部分时间里，灾备集群什么事也不做。有些组织尝试减小灾备集群的规模，让它远小于生产环境的集群规模。这种做法具有一定的风险，因为你无告保证这种小规模的集群能够在紧急情况下发挥应有的作用。有些组织则倾向于让灾备集群在平常也能发挥作用，他们把一些只读的工作负载定向到灾备集群上，也就是说，实际上运行的是Hub和Spoke架构的一个简化版本，因为架构里只有一个Spoke。

​	那么问题来了： 如何实现Kafka 集群的失效备援？

​	首先，不管选择哪一种失效备援方案， SRE（网站可靠性工程）团队都必须随时待命。今天能够正常运行的计划，在系统升级之后可能就无法正常工作，又或者已有的工具无法满足新场景的需求。每季度进行一次失效备援是最低限度的要求， 一个高效的SRE 团队会更频繁地进行失效备援。Chaos Monkey 是Netflix 提供的一个著名的服务，它随机地制造灾难，有可能让任何一天都成为失效备援日。

​	现在，让我们来看看失效备援都包括哪些内容。

1. **数据丢失和不一致性**

   <u>因为Kafka 的各种镜像解决方案都是**异步**的（ 8.2.5 节将介绍一种同步的方案），所以灾备集群总是无法及时地获取主集群的最新数据</u>。我们要时刻注意灾备集群与主集群之间拉开了多少距离，并保证不要出现太大的差距。不过， 一个繁忙的系统可以允许灾备集群与主集群之间有几百个甚至几千个消息的延迟。如果你的Kafka 集群每秒钟可以处理100 万个消息，而在主集群和灾备集群之间有5ms 的延迟，那么在最好的情况下，灾备集群每秒钟会有5000个消息的延迟。所以，不在计划内的失效备援会造成数据的丢失。在进行计划内的失效备援时，可以先停止主集群，等待镜像进程将剩余的数据镜像完毕，然后切换到灾备集群，这样可以避免数据丢失。在发生非计划内的失效备援时，可能会丢失数千个消息。目前Kafka 还不支持事务， 也就是说，如果多个主题的数据（比如销售数据和产品数据）之间有相关性，那么在失效备提过程中， 一些数据可以及时到达灾备集群，而有些则不能。那么在切换到灾备集群之后，应用程序需要知道该如何处理没有相关销售信息的产品数据。

2. 失效备援之后的起始偏移量

   在切换到灾备集群的过程中，最具挑战性的事情莫过于如何让应用程序知道该从什么地方开始继续处理数据。下面将介绍一些常用的方法，其中有些很简单，但有可能会造成额外的数据丢失或数据重复：有些则比较复杂，但可以最小化丢失数据和出现重复数据的可能性。

   + **偏移量自动重置**

     Kafka 消费者有一个配置选项，用于指定在没有上一个提交偏移量的情况下该作何处理。消费者要么从分区的起始位置开始读取数据，要么从分区的末尾开始读取数据。如果使用的是旧版本的消费者（偏移量保存在Zookeeper 上），而且因为某些原因，这些偏移量没有被纳入灾备计划，那么就需要从上述两个选项中选择一个。要么从头开始读取数据，并处理大量的重复数据，要么直接跳到末尾，放弃一些数据（希望只是少量的数据）。**如果重复处理数据或者丢失一些数据不会造成太大问题，那么重置偏移量是最为简单的方案。不过直接从主题的末尾开始读取数据这种方式或许更为常见**。

   + 复制偏移量主题

     如果使用新的Kafka消费者（0.9 或以上版本），消费者会把偏移量提交到一个叫作__consurner_offsets 的主题上。如果对这个主题进行了镜像，那么当消费者开始读取灾备集群的数据时，它们就可以从原先的偏移量位置开始处理数据。这个看起来很简单，不过仍然有很多需要注意的事项。

   <u>首先， 我们并不能保证主集群里的偏移量与灾备集群里的偏移量是完全匹配的</u>。假设主集群里的数据只保留3 天，而你在一个星期之后才开始镜像，那么在这种情况下，主集群里第一个可用的偏移量可能是57 000 000 （前4 天的旧数据已经被删除了），而灾备集群里的第一个偏移量是0 ，那么当消费者尝试从57 000 003 处（因为这是它要读取的下一个数据）开始读取数据时，就会失败。

   <u>其次，就算在主题创建之后立即开始镜像，让主集群和灾备集群的主题偏移量都从0开始，生产者在后续进行重试时仍然会造成偏移量的偏离</u>。

   **简而言之，目前的Kafka 镜像解决方案无法为主集群和灾备集群保留偏移量**。

   最后，就算偏移量被完美地保留下来，因为主集群和灾备集群之间的延迟以及Kafka 缺乏对事务的支持，消费者提交的偏移量有可能会在记录之前或者记录之后到达。<u>在发生失效备援之后，消费者可能会发现偏移量与记录不匹配，或者灾备集群里最新的偏移量比主集群里的最新偏移量小</u>。如图8-5 所示。

   ![img](https://img2018.cnblogs.com/blog/687300/201810/687300-20181009204023843-1127193754.png)

   图8-5 ：灾备集群偏穆量与主集群的最新偏穆量不匹配的示例

   **在这些情况下，我们需要接受一定程度的重复数据。如果灾备集群最新的偏移量比主集群的最新偏移量小，或者因为生产者进行重试导致灾备集群的记录偏移量比主集群的记录偏移量大，都会造成数据重复**。你还需要知道该怎么处理最新偏移量与记录不匹配的问题，此时要从主题的起始位置开始读取还是从末尾开始读取？

   复制偏移量主题的方式可以用于减少数据重复或数据丢失，而且实现起来很简单，只要及时地从0 开始镜像数据，井持续地镜像偏移量主题就可以了。不过一定要注意上述的几个问题。

   + **基于时间的失效备援**

     如果使用的是新版本（0.10.0及以上版本）的Kafka消费者，每个消息里都包含了一个时间戳，这个时间戳指明了消息发送给Kafka 的时间。在更新版本的Kafka (0.10.1.0及以上版本）里， broker提供了一个索引和一个API ，用于根据时间戳查找偏移量。于是，假设你正在进行失效备援， 井且知道失效事件发生在凌晨4:05 ，那么就可以让消费者从4: 03 的位置开始处理数据。在两分钟的时间差里会存在一些重复数据，不过这种方式仍然比其他方案要好得多，而且也很容易向其他人解释一一“我们将从凌晨4:03的位置开始处理数据”这样的解释要比“我们从一个不知道是不是最新的位置开始处理数据”要好得多。所以，这是一种更好的折中。问题是，如何让消费者从凌晨4:03的位置开始处理数据呢？

     可以让应用程序来完成这件事情。我们为用户提供一个配置参数，用于指定从什么时间点开始处理数据。如果用户指定了时间，应用程序可以通过新的API 获取指定时间的偏移量，然后从这个位置开始处理数据。

     如果应用程序在一开始就是这么设计的，那么使用这种方案就再好不过了。但如果应用程序在一开始不是这么设计的呢？开发一个这样的小工具也并不难一一接收一个时间戳，使用新的API获取相应的偏移量，然后提交偏移量。我们希望在未来的Kafka 版本里添加这样的工具，不过你也可以自己写一个。在运行这个工具时，应该先关闭消费者群组， 在工具完成任务之后再启动它们。

     该方案适用于那些使用了新版Kafka 、对失效备援有明确要求井且喜欢自己开发工具的人。

   + 偏移量外部映射

     我们知道，**镜像偏移量主题的一个最大问题在于主集群和灾备集群的偏移量会发生偏差**。因此， 一些组织选择使用外部数据存储（比如Apache Cassandra ） 来保存集群之间的偏移量映射。他们自己开发镜像工具，在一个数据被镜像到灾备集群之后，主集群和灾备集群的偏移量被保存到外部数据存储上。或者只有当两边的偏移量差值发生变化时，才保存这两个偏移量。比如， 主集群的偏移量495 被映射到灾备集群的偏移量500 ，在外部存储上记录为（ 495,500 ）。如果之后因为消息重复导致差值发生变化， 偏移量596 被映射为600 ，那么就保留新的映射（ 569,600 ） 。他们没有必要保留495 和596 之间的所有偏移量映射，他们假设差值都是一样的，所以主集群的偏移量550 会映射到灾备集群的偏移量555 。那么在发生失效备援时，他们将主集群的偏移量与灾备集群的偏移量映射起来，而不是在时间戳（通常会有点不准确）和偏移量之间做映射。他们通过上述技术手段之一来强制消费者使用映射当中的偏移量。<u>对于那些在数据记录之前达到的偏移量或者没有及时被镜像到灾备集群的偏移量来说，仍然会有问题一一不过这至少已经满足了部分场景的需求</u>。

     **这种方案非常复杂，我认为并不值得投入额外的时间。在索引还没有出现之前，或许可以考虑使用这种方案。但在今天，我倾向于将集群升级到新版本，并使用<u>基于时间戳</u>的解决方案，而不是进行偏移量映射，更何况偏移量映射并不能覆盖所有的失效备援场景**。

3. 在失效备援之后

   假设失效备援进行得很顺利，灾备集群也运行得很正常，现在需要对主集群做一些改动，比如把它变成灾备集群。

   如果能够通过简单地改变镜像进程的方向，让它将数据从新的主集群镜像到旧的主集群上面， 事情就完美了! 不过，这里还存在两个问题。

   + 怎么知道该从哪里开始镜像？我们同样需要解决与镜像程序里的消费者相关的问题。<u>而且不要忘了，所有的解决方案都有可能出现重复数据或者丢失数据，或者两者兼有</u>。

   + 之前讨论过，旧的主集群可能会有一些数据没有被镜像到灾备集群上，如果在这个时候把新的数据镜像回来，那么历史遗留数据还会继续存在，两个集群的数据就会出现不一致。

   <u>基于上述的考虑，最简单的解决方案是清理旧的主集群，删掉所有的数据和偏移量，然后从新的主集群上把数据镜像回来，这样可以保证两个集群的数据是一致的</u>。

4. 关于集群发现

   在设计灾备集群时，需要考虑一个很重要的问题，就是在发生失效备援之后，应用程序需要知道如何与灾备集群发起通信。不建议把主集群的主机地址硬编码在生产者和消费者的配置属性文件里。大多数组织为此创建了DNS 别名，将其指向主集群，一旦发生紧急情况，可以将其指向灾备集群。有些组织则使用服务发现工具，比如Zookeper 、Etcd或Consul。这些服务发现工具（ DNS 或其他）没有必要将所有broker 的信息都包含在内，Kafka 客户端只需要连接到其中的一个broker，就可以获取到整个集群的元数据，并发现集群里的其他broker。一般提供3 个broker 的信息就可以了。除了服务发现之外，在大多数情况下，需要重启消费者应用程序，这样它们才能找到新的可用偏移量，然后继续读取数据。

### * 8.2.5 延展集群

​	在主备架构里， 当Kafka 集群发生失效时，可以将应用程序重定向到另一个集群上，以保证业务的正常运行。而<u>在整个数据中心发生故障时，可以使用延展集群（stretch cluster)来避免Kafka 集群失效</u>。**延展集群就是跨多个数据中心安装的单个Kafka 集群**。

​	延展集群与其他类型的集群有本质上的区别。**首先，延展集群井非多个集群，而是单个集群，因此不需要对延展集群进行镜像**。<u>延展集群使用Kafka 内置的复制机制在集群的broker之间同步数据</u>。我们可以通过配置打开延展集群的<u>同步复制</u>功能，生产者会在消息、成功写入到其他数据中心之后收到确认。同步复制功能要求使用机架信息，确保每个分区在其他数据中心都存在副本，还需要配置min.ist和acks=all ，确保每次写入消息时都可以收到至少两个数据中心的确认。

​	同步复制是这种架构的最大优势。有些类型的业务要求灾备站点与主站点保持100% 的同步，这是一种合规性需求，可以应用在公司的任何一个数据存储上，包括Kafka 本身。这种架构的另一个好处是，数据中心及所有broker 都发挥了作用，不存在像主备架构那样的资源浪费。

​	这种架构的不足之处在于，它所能应对的灾难类型很有限，只能应对数据中心的故障， 无法应对应用程序或者Kafka 故障。运维的复杂性是它的另一个不足之处，它所需要的物理基础设施并不是所有公司都能够承担得起的。

​	如果能够在至少3 个具有高带宽和低延迟的数据中心上安装Kafka （包括Zookeeper ），那么就可以使用这种架构。如果你的公司有3 栋大楼处于同一个街区，或者你的云供应商在同一个地区有3 个可用的区域，那么就可以考虑使用这种方案。

​	为什么是3个数据中心？ 主要是因为Zookeeper 。Zookeeper 要求集群里的节点个数是奇数，而且只有当大多数节点可用时，整个集群才可用。如果只有两个数据中心和奇数个节点，那么其中的一个数据中心将包含大多数节点，也就是说，如果这个数据中心不可用，那么Zookeeper 和Kafka 也不可用。如果有3 个数据中心，那么在分配节点时，可以做到每个数据中心都不会包含大多数节点。如果其中的一个数据中心不可用， 其他两个数据中心包含了大多数节点，此时Zookeeper 和Kafka 仍然可用。

​	**从理论上说，在两个数据中心运行Zookeeper 和Kafka 是可能的，只要将Zookeeper 的群组配置成允许手动进行失效备援。不过在实际应用当中，这种做法并不常见**。

## 8.3 Kafka 的MirrorMaker

​	Kafka 提供了一个简单的工具，用于在两个数据中心之间镜像数据。这个工具叫MirrorMaker ，它包含了一组消费者（因为历史原因，它们在MirrorMaker 文档里被称为流），这些消费者属于同一个群组，井从主题上读取数据。每个MirrorMaker进程都有一个单独的生产者。**镜像过程很简单：MirrorMaker为每个消费者分配一个线程，消费者从源集群的主题和分区上读取数据，然后通过公共生产者将数据发送到目标集群上**。默认情况下，消费者每60 秒通知生产者发送所有的数据到Kafka ，并等待Kafka 的确认。然后消费者再通知源集群提交这些事件相应的偏移量。这样可以保证不丢失数据（在源集群提交偏移量之前， Kafka 对消息进行了确认），而且如果MirrorMaker 进程发生崩溃，最多只会出现60秒的重复数据。见图8-6 。

![img](https://img2018.cnblogs.com/blog/687300/201810/687300-20181009195238811-316502029.png)

​	图8-6 : MirrorMaker的镜像过程

> MirrorMaker 相关信息
>
> Mirror Maker 看起来很简单，不过出于对效率的考虑，以及尽可能地做到仅一次传递，它的实现并不容易。截止到Kafka 0.10.0.0 版本， MirrorMaker 已经被重写了4 次，而且在未来有可能会进行更多的重写。这里所描述的以及后续章节将提及的MirrorMaker 相关细节都基于0.9.0.0 到0.10.2.0 之间的版本。

### 8.3.1 如何配置

​	MirrorMaker 是高度可配置的。首先，它使用了一个生产者和多个消费者，所以生产者和消费者的相关配置参数都可以用于配置MirrorMaker。

​	MirrorMaker 的基本命令行参数。

+ consumer.config

  <u>该参数用于指定消费者的配置文件。所有的消费者将共用这个配置，也就是说，只能配置一个源集群和一个group.id</u> 。所有的消费者属于同一个消费者群组，这正好与我们的要求不谋而合。配置文件里有两个必选的参数： bootstrap.servers （源集群的服务器地址）和group.id 。除了这两个参数外，还可以为消费者指定其他任意的配置参数。<u>auto.commit.enable 参数一般不需要修改，用默认值false 就行。MirrorMaker 会在消息安全到达目标集群之后提交偏移量，所以不能使用自动提交。如果修改了这个参数，可能会导致数据丢失</u>。auto.offset.reset 参数一般需要进行修改，默认值是latest，也就是说， MirrorMaker 只对那些在MirrorMaker 启动之后到达源集群的数据进行镜像。如果想要镜像之前的数据，需要把该参数设为earliest。

+ producer.config

  该参数用于指定生产者的配置文件。配置文件里唯一必选的参数是bootstrap.servers（目标集群的服务器地址）。我们将在8.3.3 节介绍更多的配置属性。

+ new.consumer

  MirrorMaker 只能使用0.8 版本或者0.9 版本的消费者。建议使用0.9 版本的消费者，因为它更加稳定。

+ num.streams

  之前已经解释过， 一个流就是一个消费者。所有的消费者共用一个生产者， MirrorMaker将会使用这些流来填充同一个生产者。如果需要额外的吞吐量，就需要创建另一个MirrorMaker 进程。

+ whitelist

  这是一个正则表达式，代表了需要进行镜像的主题名字。所有与表达式匹配的主题都将被镜像。在这个例子里，我们希望镜像所有的主题，不过在实际当中最好使用类似"prod.\*"这样的表达式，避免镜像测试用的主题。在双活架构中， MirrorMaker 将NYC数据中心的数据镜像到SF，为其配置了whitelist ＝"NYC.\\*"这样就不会将SF 的主题重新镜像回来。

### 8.3.2 在生产环境部署MirrorMaker

​	在生产环境， MirrorMaker 一般是作为后台服务运行的，而且是以nohup 的方式运行，井将控制台的输出重定向到一个日志文件里。这个工具有一个`-deamon`命令行参数，可以进行后台运行。

​	大部分使用MirrorMaker 的公司都有自己的启动脚本，他们一般会使用部署系统（ 比如Ansible 、Puppet 、Chef和Salt）实现自动化的部署和配置管理。

​	<u>如果有可能，尽量让MirrorMaker 运行在**目标数据中心**里</u>。也就是说，如果要将NYC 的数据发送到SF，MirrorMaker 应该运行在SF 的数据中心里。因为长距离的外部网络比数据中心的内部网络更加不可靠，如果发生了网络分区，数据中心之间断开了连接， 那么一个无法连接到集群的消费者要比一个无法连接到集群的生产者要安全得多。<u>如果消费者无法连接到集群，最多也就是无挂读取数据，数据仍然会在Kafka 集群里保留很长的一段时间，不会有丢失的风险。相反，在发生网络分区时，如果MirrorMaker 已经读取了数据，但无法将数据生成到目标集群上，就会造成数据丢失。所以说， 远程读取比远程生成更加安全</u>。

​	那么，什么情况下需要在本地读取消息并将其生成到远程数据中心呢？如果需要加密传输数据，但又不想在数据中心进行加密，就可以使用这种方式。**消费者通过SSL 连接到Kafka 对性能有一定的影响，这个比生产者要严重得多，而且这种性能问题也会影响broker**。<u>如果跨数据中心流量需要加密，那么最好把MirrorMaker 放在源数据中心， 让它读取本地的非加密数据，然后通过SSL 连接将数据生成到远程的数据中心。这个时候， 使用SSL 连接的是生产者，所以性能问题就不那么明显了</u>。在使用这种方式时， 需要确保Mirror Maker 在收到目标broker 副本的有效确认之前不要提交偏移量，并在重试次数超出限制或者生产者缓冲区植出的情况下立即停止镜像。

​	如果希望减小源集群和目标集群之间的延迟，可以在不同的机器上运行至少两个MirrorMaker实例，而且它们要使用相同的消费者群组。也就是说，如果关掉其中一台服务器， 另一个MirrorMaker实例能够继续镜像数据。

​	在将MirrorMaker部署到生产环境时，最好要对以下几项内容进行监控。

**延迟监控**

​	我们绝对有必要知道目标集群是否落后于源集群。延迟体现在源集群最新偏移量和目标集群最新偏移茸的差异上。见图8-7 。

![img](https://www.freesion.com/images/532/7603a43456381381b07f73ce333daf2c.png)

​	如图8-7 所示，源集群的最后一个偏移量是7 ，而目标集群的最后一个偏移量是5 ，所以它们之间有两个消息的延迟。

​	有两种方式可用于跟踪延迟，不过它们都不是完美的解决方案。

+ **检查MirrorMaker 提交到源集群的最新偏移量**。可以使用kafka-consumer-groups 工具检查MirrorMaker 读取的每一个分区，查看分区的最新偏移量，也就是MirrorMaker 提交的最新偏移量。不过这个偏移量井不会100 % 的准确，因为MirrorMaker 井不会每时每刻都提交偏移量，默认情况下，它会每分钟提交一次。所以，我们最多会看到一分钟的延迟，然后延迟突然下降。图8-7 中的延迟是2 ，但kafka-consumer-groups 会认为是4，因为MirrorMaker 还没有提交最近的偏移量。Linkedln 的burrow 也会监控这些信息，不过它使用了更为复杂的方越来识别延迟的真实性，所以不会导致误报。

+ **检查MirrorMaker 读取的最新偏移量（即使还未提交）** 。消费者通过JMX发布关键性度量指标，其中有一个指标是指消费者的最大延迟（基于它所读取的所有分区计算得出的）。这个延迟也不是100% 的准确，因为它只反映了消费者读取的数据，并没有考虑生产者是否成功地将数据发送到目标集群上。在图8 -7 的示例里， MirrorMaker 消费者会认为延迟是1 ，而不是2 ，因为它已经读取了消息6 ，尽管这个消息还没有被生成到目标集群上。

​	要注意，如果MirrorMaker 跳过或丢弃部分消息，上述的两种方能是无法检测到的，因为它们只跟踪最新的偏移量。Confluent 的Control Center 通过监控消息的数量和校验和来提升监控的准确性。

度量指标监控

​	MirrorMaker 内嵌了生产者和消费者，它们都有很多可用的度量指标，所以建议对它们进行监控。Kafka 文档列出了所有可用的度量指标。下面列出了几个已经被证明能够提升MirrorMaker 性能的度量指标。

+ 消费者

  Fetch-size-avg、fetch-size-max、fetch-rate、fetch-throttle-time-avg以及fetch-throttle-time-max。

+ 生产者

  batch-size-avg、batch-size-max、requests-in-flight遗迹record-retry-rate。

+ 同时适用于两者

  io-ratio和io-wait-ratio。

canary

​	如果对所有东西都进行了监控，那么canary就不是必需的，不过对于多层监控来说，canary可能还是有必要的。我们可以每分钟往拥集群的某个特定主题上发送一个事件，然后尝试从目标集群读取这个事件。如果这个事件在给定的时间之后才到达，那么就发出告警，说明MirrorMaker 出现了延迟或者已经不正常了。

### 8.3.3 MirrorMaker调优

​	MirrorMaker 集群的大小取决于对吞吐量的需求和对延迟的接受程度。如果不允许有任何延迟，那么MirrorMaker 集群的容量需要能够支撑吞吐量的上限。如果可以容忍一些延迟，那么可以在95% ～99% 的时间里只使用75%-80% 的容量。在吞吐量高峰时可以允许一些延迟，高峰期结束时，因为MirrorMaker 有一些空余容量，可以很容易地消除延迟。

​	Kafka 提供了kafka-performance-producer 工具，用于在源集群上制造负载，然后启动MirrorMaker 对这个负载进行镜像。分别为MirrorMaker 配置1 、2 、4 、8 、16 、24 和32 个消费者线程，井观察性能在哪个点开始下降，然后将num.streams 的值设置为一个小于当前点的整数。如果数据经过压缩（因为网络带宽是跨集群镜像的瓶颈，所以建议将数据压缩后再传输），那么MirrorMaker还要负责解压并重新压这些数据。这样会消耗很多的CPU 资源，所以在增加线程数量时，要注意观察CPU 的使用情况。通过这种方式，可以得到单个MirrorMaker 实例的最大吞吐量。如果单个实例的吞吐量还达不到要求，可以增加更多的MirrorMaker实例和服务器。

​	另外，你可能想要分离比较敏感的主题，它们要求很低的延迟，所以其镜像必须尽可能地接近源集群和MirrorMaker 集群。这样可以避免主题过于臃肿，或者避免出现失控的生产者拖慢数据管道。

​	我们能够对MirrorMaker 进行的调优也就是这些了。不过，我们仍然有其他办陆可以增加每个消费者和每个MirrorMaker 的吞吐量。

​	如果MirrorMaker 是跨数据中心运行的，可以在Linux 上对网络进行优化。

![img](https://img2018.cnblogs.com/blog/687300/201810/687300-20181009201115226-209154851.png)

​		除此以外，你可能还想对 MirrorMaker 里的生产者和消费者进行调优。首先，你想知道生产者或消费者是不是瓶颈所在一一生产者是否在等待消费者提供更多的数据，或者其他的什么？通过查看生产者和消费者的度量指标就可以知道问题所在了 ，如果其中的一个进程空闲 ， 而另外一个很忙，那么就知道该对哪个进行调优了。另外一种办法是查看线程转储 (thread dump ），可以使用 jstack 获得线程转储。<u>如果 MirrorMaker 的大部分时间用在轮询上 ，那么说明消费者出现了瓶颈，如果大部分时间用在发送上，那么就是生产者出现了瓶颈</u>。

 如果需要对<u>生产者</u>进行调优，可以使用下列参数。

+ max.in.flight.requests.per.connection
  默认情况下， MirrorMaker 只允许存在一个处理中的请求。也就是说，生产者在发送下一个消息之前 ， 当前发送的消息必须得到目标集群的确认。这样会对吞吐量造成限制，特别是当 broker 在对消息进行确认之前出现了严重的延迟。 MirrorMaker 之所以要限定请求的数量，是因为有些消息在得到成功确认之前需要进行重试，而这是唯一能够保证消息次序的方法。如果不在乎消息的次序，那么可以通过增加 max.in.flight.requests.per.connection 的值来提升吞吐量。
+ linger.ms和batch.size
  如果在进行监控时发现生产者总是发送未填满的批次（比如，度量指标 batch-size-avg和 batch-size-max 的值总是比 batch.size 低），那么就可以通过增加一些延迟来提升吞吐量。通过增加 latency.ms 可以让生产者在发送批次之前等待几毫秒，让批次填充更多的数据。如果发送的数据都是搞批次的，同时还有空余的内存，那么可以配置更大的batch.size ，以便发送更大的批次 。

下面的配置用于提升<u>消费者</u>的吞吐量。

+ range。 MirrorMaker 默认使用 range 策略（用于确定将哪些分区分配给哪个消费者的算法）进行分区分配。 range 策略有一定的优势不过range 策略会导致不公平现象。对于 MirrorMaker 来说，最好可以把策略改为round robin ，特别是在镜像大量的主题和分区的时候。将策略改为round robin 算法，需要在消费者配置属性文件里加上`partition.assignment.strategy=org.apache.kafka.clients.consumer.RoundRobinAssignor`。
+ fetch.max.bytes 。如果度盘指标显示 fetch-size-avg 和 fetch-size-max 的数值与 fetch.max.bytes 很接近，说明消费者读取的数据已经接近上限。如果有更多的可用内存，可以配置更大的 fetch.max.bytes ，消费者就可以在每个请求里读取更多的数据。
+  fetch.min.bytes 和 fetch.max.wait。如果度量指标 fetch-rate 的值很高，说明消费者发送的请求太多了，而且获取不到足够的数据。这个时候可以配置更大的 fetch.min.bytes 和 fetch.max.wait这样消费者的每个请求就可以获取到更多的数据， broker 会等到有足够多的可用数据时才将响应返回。

## 8.4 其他跨集群镜像方案

### 8.4.1 优步的uReplicator

### 8.4.2 Confluent的Replicator

## 8.5 总结

+ 多集群架构
+ 失效备援
+ MirrorMaker

# 9. 管理Kafka

## 9.1 主题操作

P155
