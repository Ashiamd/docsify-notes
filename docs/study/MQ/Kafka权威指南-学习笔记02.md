# Kafka权威指南-学习笔记02

# 7. 构建数据管道

​	Kafka 为数据管道带来的主要价值在于，它可以作为数据管道各个数据段之间的大型缓冲区， 有效地解耦管道数据的生产者和消费者。Kafka 的解耦能力以及在安全和效率方面的可靠性，使它成为构建数据管道的最佳选择。

> **数据集成的场景**
>
> 有些组织把Kafka 看成是数据管道的一个端点，他们会想“我怎么才能把数据从Kafka 移到ElasticSearch 里”。这么想是理所当然的一一特别是当你需要的数据在到达ElasticSearch 之前还停留在Kafka 里的时候，其实我们也是这么想的。不过我们要讨论的是如何在更大的场景里使用Kafka ，这些场景至少包含两个端点（可能会更多） ，而且这些端点都不是Kafka 。**对于那些面临数据集成问题的人来说，我们建议他们从大局考虑问题，而不只是把注意力集中在少量的端点上。过度聚焦在短期问题上，只会增加后期维护的复杂性，付出更高的成本。**

​	本章将讨论在构建**数据管道**时需要考虑的几个常见问题。这些问题并非Kafka 独有， 它们都是与数据集成相关的一般性问题。我们将解释为什么可以使用Kafka 进行数据集成，以及它是如何解决这些问题的。我们将讨论Connect API 与普通的客户端API (Producer 和Consumer）之间的区别，以及这些客户端API 分别适合在什么情况下使用。然后我们会介绍Connect。Connect 的完整手册不在本章的讨论范围之内，不过我们会举几个例子来帮助你入门，而且会告诉你可以从哪里了解到更多关于Connect 的信息。最后介绍其他数据集成系统，以及如何将它们与Kafka 集成起来。

## 7.1 构建数据管道时需要考虑的问题

### 7.1.1 及时性

​	有些系统希望每天一次性地接收大量数据，而有些则希望在数据生成几毫秒之内就能拿到它们。大部分数据管道介于这两者之间。一个好的数据集成系统能够很好地支持数据管道的各种及时性需求，而且在业务需求发生变更时，具有不同及时性需求的数据表之间可以方便地进行迁移。<u>Kafka 作为一个基于流的数据平台，提供了可靠且可伸缩的数据存储，可以支持几近实时的数据管道和基于小时的批处理。生产者可以频繁地向Kafka 写入数据，也可以按需写入：消费者可以在数据到达的第一时间读取它们，也可以每隔一段时间读取一次积压的数据</u>。

​	Kafka 在这里扮演了一个大型缓冲区的角色，降低了生产者和消费者之间的时间敏感度。实时的生产者和基于批处理的消费者可以同时存在，也可以任意组合。实现回压策略也因此变得更加容易， Kafka 本身就使用了回压策略（必要时可以延后向生产者发送确认），消费速率完全取决于消费者自己。

### * 7.1.2 可靠性

​	我们要避免单点故障，并能够自动从各种故障中快速恢复。数据通过数据管道到达业务系统，哪怕出现几秒钟的故障，也会造成灾难性的影响，对于那些要求毫秒级的及时性系统来说尤为如此。数据传递保证是可靠性的另一个重要因素。<u>有些系统允许数据丢失，不过在大多数情况下，它们要求**至少一次传递**。也就是说，源系统的每一个事件都必须到达目的地，不过有时候需要进行重试，而重试可能造成重复传递。有些系统甚至要求**仅一次传递**一一源系统的每一个事件都必须到达目的地，不允许丢失，也不允许重复</u>。

​	我们已经在第6章深入讨论了Kafka 的可用性和可靠性保证。**Kafka 本身就支持“至少一次传递”**，<u>如果再结合具有事务模型或唯一键特性的外部存储系统， Kafka 也能实现“仅一次传递”</u>。因为大部分的端点都是数据存储系统，它们提供了“仅一次传递”的原语支持，所以基于Kafka 的数据管道也能实现“仅一次传递”。

​	<u>值得一提的是， Connect APl 为集成外部系统提供了处理偏移量的APl ，连接器因此可以构建仅一次传递的端到端数据管道。实际上，很多开源的连接器都支持仅一次传递</u>。

### * 7.1.3 高吞吐量和动态吞吐量

​	为了满足现代数据系统的要求，数据管道需要支持非常高的吞吐量。<u>更重要的是，在某些情况下，数据管道还需要能够应对突发的吞吐量增长</u>。

​	由于我们将Kafka 作为生产者和消费者之间的缓冲区，消费者的吞吐量和生产者的吞吐量就不会耦合在一起了。我们也不再需要实现复杂的回压机制，如果生产者的吞吐量超过了消费者的吞吐量，可以把数据积压在Kafka 里，等待消费者追赶上来。通过增加额外的消费者或生产者可以实现Kafka 的伸缩，因此我们可以在数据管道的任何一边进行动态的伸缩，以便满足持续变化的需求。

​	因为Kafka 是一个高吞吐量的分布式系统， 一个适当规模的集群每秒钟可以处理数百兆的数据，所以根本无需担心数据管道无住满足伸缩性需求。另外， <u>Connect API 不仅支持伸缩，而且擅长并行处理任务。稍后，我们将会介绍数据源和数据池（Data Sink）如何在多个线程间拆分任务，最大限度地利用CPU资源，哪怕是运行在单台机器上</u>。

​	Kafka 支持多种类型的压缩，**在增长吞吐量时， Kafka 用户和管理员可以通过压缩来调整网络和存储资源的使用**。

### 7.1.4 数据格式

> [s3（S3 Simple Storage Service 简单存储服务）_百度百科 (baidu.com)](https://baike.baidu.com/item/s3/1409766?fr=aladdin)

​	数据管道需要协调各种数据格式和数据类型，这是数据管道的一个非常重要的因素。数据类型取决于不同的数据库和数据存储系统。你可能会通过Avro 将XML 或关系型数据加载到Kafka 里，然后将它们转成JSON 写入ElasticSearch ，或者转成Parquet 写入HDFS ，或者转成csv 写入S3 。

​	Kafka 和Connect API 与数据格式无关。我们已经在之前的章节介绍过，生产者和消费者可以使用各种序列化器来表示任意格式的数据。Connect API 有自己的内存对象模型，包括数据类型和schema 。不过，可以使用一些可插拔的转换器将这些对象保存成任意的格式，也就是说，不管数据是什么格式的，都不会限制我们使用连接器。

​	<u>很多数据源和数据池都有schema ，我们从数据源读取schema ，把它们保存起来，井用它们验证数据格式的兼容性，甚至用它们更新数据池的schema。从MySQL 到Hive的数据管道就是一个很好的例子。如果有人在MySQL 里增加了一个字段，那么在加载数据时，数据管道可以保证Hive 里也添加了相应的字段</u>。

​	另外，<u>数据池连接器将Kafka 的数据写入外部系统，因此需要负责处理数据格式</u>。有些连接器把数据格式的处理做成可插拔的，比如HDFS 的连接器就支持Avro 和Parquet 。

​	**通用的数据集成框架不仅要支持各种不同的数据类型，而且要处理好不同数据源和数据池之间的行为差异**。例如，在关系型数据库向Syslog 发起抓取数据请求时， Syslog 会将数据推送给它们，而HDFS 只支持追加写入模式，只能向HDFS 写入新数据，而对于其他很多系统来说，既可以追加数据， 也可以更新已有的数据。

### * 7.1.5 转换

​	**数据转换比其他需求更具争议性。数据管道的构建可以分为两大阵营，即ETL 和ELT** 。ETL 表示**提取一转换一加载**（ Extract-Transform-Load ），也就是说，当数据流经数据管道时，数据管道会负责处理它们。这种方式为我们节省了时间和存储空间，因为不需要经过保存数据、修改数据、再保存数据这样的过程。不过，这种好处也要视情况而定。有时候，这种方式会给我们带来实实在在的好处，但也有可能给数据管道造成不适当的计算和存储负担。<u>这种方式有一个明显不足，就是数据的转换会给数据管道下游的应用造成一些限制，特别是当下游的应用希望对数据进行进一步处理的时候。假设有人在MongoDB 和MySQL 之间建立了数据管道，井且过滤掉了一些事件记录，或者移除了一些字段，那么下游应用从MySQL 中访问到的数据是不完整的。如果它们想要访问被移除的字段， 只能重新构建管道，井重新处理历史数据（如果可能的话）</u>。

​	ELT 表示**提取－加载－转换**（Extract-Load-Transform）。<u>在这种模式下，数据管道只做少量的转换（主要是数据类型转换），确保到达数据地的数据尽可能地与数据源保持一致。**这种情况也被称为高保真（high fidelity）数据管道或数据湖（data lake）架构**</u>。目标系统收集“原始数据”，并负责处理它们。这种方式为目标系统的用户提供了最大的灵活性，因为它们可以访问到完整的数据。在这些系统里诊断问题也变得更加容易，因为数据被集中在同一个系统里进行处理，而不是分散在数据管道和其他应用里。<u>这种方式的不足在于，数据的转换占用了目标系统太多的CPU 和存储资源。有时候，目标系统造价高昂，如果有可能，人们希望能够将计算任务移出这些系统</u>。

### * 7.1.6 安全性

> [sasl_百度百科 (baidu.com)](https://baike.baidu.com/item/sasl/5292142?fr=aladdin)

​	安全性是人们一直关心的问题。对于数据管道的安全性来说，人们主要关心如下几个方面。

+ <u>我们能否保证流经数据管道的数据是经过加密的</u>？这是跨数据中心数据管道通常需要考虑的一个主要方面。
+ 谁能够修改数据管道？

+ 如果数据管道需要从一个不受信任的位置读取或写入数据，是否有适当的<u>认证机制</u>？

​	**Kafka 支持加密传输数据，从数据源到Kafka ，再从Kafka 到数据池**。

​	**它还支持认证（通过SASL 来实现）和授权**，所以你可以确信，如果一个主题包含了敏感信息，在不经授权的情况下，数据是不会流到不安全的系统里的。

​	**Kafka 还提供了审计日志用于跟踪访问记录**。通过编写额外的代码，还可能跟踪到每个事件的来源和事件的修改者，从而在每个记录之间建立起整体的联系。

### 7.1.7 故障处理能力

​	我们不能总是假设数据是完美的，而要事先做好应对故障的准备。能否总是把缺损的数据挡在数据管道之外？能否恢复无法解析的记录？能否修复（或许可以手动进行）并重新处理缺损的数据？如果在若干天之后才发现原先看起来正常的数据其实是缺损数据，该怎么办？

​	因为Kafka 会长时间地保留数据，所以我们可以在适当的时候回过头来重新处理出错的数据。

### * 7.1.8 耦合性和灵活性

​	数据管道最重要的作用之一是解耦数据源和数据池。它们在很多情况下可能发生耦合。

+ 临时数据管道

  有些公司为每一对应用程序建立单独的数据管道。例如， 他们使用Logstash向ElasticSearch 导入日志，使用Flume向HDFS 导入日志，使用GoldenGate 将Oracle 的数据导到HDFS ，使用Informatica 将MySQL 的数据或XML 导到Oracle ，等等。他们将数据管道与特定的端点耦合起来，井创建了大量的集成点，需要额外的部署、维护和监控。当有新的系统加入肘，他们需要构建额外的数据管道，从而增加了采用新技术的成本，同时遏制了创新。

+ 元数据丢失

  如果数据管道没有保留schema元数据，而且不允许schema 发生变更，那么最终会导致生产者和消费者之间发生紧密的耦合。没有了schema ，生产者和消费者需要额外的信息来解析数据。假设数据从Oracle 流向HDFS ，如果DBA 在Oracle 里添加了一个字段，而且没有保留schema 信息，也不允许修改schema ，那么从HDFS 读取数据时可能会发生错误，因此需要双方的开发人员同时升级应用程序才能解决这个问题。不管是哪一种情况，它们的解决方案都不具备灵活性。如果数据管道允许schema 发生变更，应用程序各方就可以修改自己的代码，无需担心对整个系统造成破坏。

+ 末端处理

  我们在讨论数据转换时就已提到，数据管道难免要做一些数据处理。在不同的系统之间移动数据肯定会碰到不同的数据格式和不同的应用场景。不过，如果数据管道过多地处理数据，那么就会给下游的系统造成一些限制。在构建数据管道时所做的设计决定都会对下游的系统造成束缚，比如应该保留哪些字段或应该如何聚合数据，等等。如果下游的系统有新的需求，那么数据管道就要作出相应的变更，这种方式不仅不灵活，而且低效、不安全。**<u>更为灵活的方式是尽量保留原始数据的完整性，让下游的应用自己决定如何处理和聚合数据</u>**。

## 7.2 如何在Connect API 和客户端API 之间作出选择

​	在向Kafka 写入数据或从Kafka 读取数据时，要么使用传统的生产者和消费者客户端，就像第3章和第4章所描述的那样，要么使用后面即将介绍的Connect API 和连接器。在具体介绍Connect API 之前，我们不妨先问自己一个问题：“什么时候适合用哪一个？”

​	我们知道， Kafka 客户端是要被内嵌到应用程序里的，应用程序使用它们向Kafka 写入数据或从Kafka 读取数据。如果你是开发人员，你会使用Kafka 客户端将应用程序连接到Kafka ，井修改应用程序的代码，将数据推送到Kafka 或者从Kafka 读取数据。

​	如果要将Kafka连接到数据存储系统，可以使用Connect ，因为这些系统不是你开发的，你无法或者也不想修改它们的代码。**Connect 可以用于从外部数据存储系统读取数据， 或者将数据推送到外部存储系统**。如果数据存储系统提供了相应的连接器，那么非开发人员就可以通过配置连接器的方式来使用Connect。

​	如果你要连接的数据存储系统没有相应的连接器，那么可以考虑使用客户端API 或Connect API 开发一个应用程序。**我们建议首选Connect ，因为它提供了一些开箱即用的特性，比如配置管理、偏移量存储、井行处理、错误处理，而且支持多种数据类型和标准的REST管理API** 。开发一个连接Kafka 和外部数据存储系统的小应用程序看起来很简单，但其实还有很多细节需要处理，比如数据类型和配置选项，这些无疑加大了开发的复杂性一一<u>Connect 处理了大部分细节，让你可以专注于数据的传输</u>。

## 7.3 Kafka Connect

​	Connect是Kafka 的一部分，它为在Kafka和外部数据存储系统之间移动数据提供了一种可靠且可伸缩的方式。它为连接器插件提供了一组API 和一个运行时——Connect 负责运行这些插件， 它们则负责移动数据。Connect 以worker进程集群的方式运行，我们基于worker进程安装连接器插件，然后使用REST API 来管理和配置connector，这些worker进程都是长时间持续运行的作业。连接器启动额外的task ，有效地利用工作节点的资源，以并行的方式移动大量的数据。数据源的连接器负责从源系统读取数据，井把数据对象提供给worker进程。数据地的连接器负责从worker进程获取数据，井把它们写入目标系统。Connect通过connector在Kafka里存储不同格式的数据。Kafka 支持JSON ，而且Confluent Schema Registry 提供了Avro 转换器。开发人员可以选择数据的存储格式，这些完全独立于他们所使用的连接器。

​	本章的内容无也完全覆盖Connect的所有细节和各种连接器，这些内容可以单独写成一本书。不过，我们会提供Connect的概览，还会介绍如何使用它，井提供一些额外的参考资料。

### 7.3.1 运行Connect

​	Connect随着Kafka一起发布，所以无需单独安装。<u>如果你打算在生产环境使用Connect来移动大量的数据，或者打算运行多个连接器，那么最好把Connect 部署在独立于broker 的服务器上</u>。在所有的机器上安装Kafka，并在部分服务器上启动broker，然后在其他服务器上启动Connect 。

​	启动Connect 进程与启动broker差不多， 在调用脚本时传入一个属性文件即可。

```shell
bin/connect-distributed.sh config/connect-distributed.properties
```

​	Connect 进程有以下几个重要的配置参数。

+ `bootstrap.servers` ：该参数列出了将要与Connect 协同工作的broker 服务器，连接器将会向这些broker 写入数据或者从它们那里读取数据。你不需要指定集群所有的broker，不过建议至少指定3 个。
+ `group.id`：<u>具有相同group id的worker属于Ω同一个Connect集群</u>。集群的连接器和它们的任务可以运行在任意一个worker 上。
+ `key.convert`和`value.converter`：Connect 可以处理存储在Kafka 里的不同格式的数据。这两个参数分别指定了消息的键和值所使用的转换器。默认使用Kafka 提供的JSONConverter ，当然也可以配置成Confluent Schema Registry 提供的AvroConverter 。

​	有些转换器还包含了特定的配置参数。例如，通过将key.converter.schema.enable设置成 true 或者 false 来指定 JSON 消息是否可以包含 schema。值转换器也有类似的配置，不过它的参数名是 value.converter.schema.enable 。Avro 消息也包含了 schema，不过需要通过 key.converter.schema.registry.url 和 value.converter.schema.registry.url 来指定 Schema Registry 的位置。

​	我们一般通过Connect的REST API来配置和监控rest.host.name和rest.port连接器。你可以为 REST API 指定特定的端口。

​	在启动worker集群之后，可以通过REST API来验证它们是否运行正常：

```
curl http://localhost:8083/
```

![img](https://www.icode9.com/i/ll/?i=20201216221034596.png)

​	这个REST URI 应该要返回当前Connect 的版本号。我运行的是Kafka 0.10.1.0（预发行）快照版本。我们还可以检查已经安装好的连接器插件：

```
curl http://localhost:8083/connector-plugins
```

![img](https://www.icode9.com/i/ll/?i=20201216221109405.png)

> **单机模式**
>
> 要注意，Connect也支持单机模式。单机模式与分布式模式类似， 只是在启动时使用 bin/connect-standalone.sh代替bin/connect-distributed.sh，也可以通过命令行传入连接器的配置文件，这样就不需要使用REST API了。
>
> 在单机模式下，所有的连接器和任务都运行在单独的worker进程上。单机模式使用起来更简单，特别是在开发和诊断问题的时候，或者是在需要让连接器和任务运行在某台特定机器上的时候（比如Syslog连接器会监听某个端口，所以你需要知道它运行在哪台机器上）

### 7.3.2 连接器示例一一文件数据源和文件数据池

...

​	在删除连接器之后，如果查看Connect 的日志，你会发现其他的连接器会重启它们的任务。这是为了在worker 进程间平衡剩余的任务，确保删除连接器之后可以保持负载的均衡。

### 7.3.3 连接器示例一一从MySQL 到ElasticSearch

​	将一个MySQL 的表数据导入到一个Kafka 主题上，再将它们加载到ElasticSearch 里，然后对它们的内容进行索引。

​	...

> 构建自己的连接器
>
> 任何人都可以基于公开的Connector API 创建自己的连接器。事实上，人们创建了各种连接器，然后把它们发布到连接器中心（Connector Hub），并告诉我们怎么使用它们。如果你在连接器中心找不到可以适配你要集成的数据存储系统的连接器，可以开发自己的连接器。你也可以把自己的连接器贡献给社区，让更多的人知道和使用它们。

### 7.3.4 深入理解Connect

​	要理解Connect 的工作原理，需要先知道3 个基本概念，以及它们之间是如何进行交互的。我们已经在之前的示例里慎示了如何运行worker进程集群以及如何启动和关闭**连接器**。不过我们并没有深入解释**转化器**是如何处理数据的一一转换器把MySQL 的数据行转成JSON 记录，然后由连接器将它们写入Kafka。

​	现在让我们深入理解每一个组件，以及它们之间是如何进行交互的。

1. 连接器和任务

   连接器插件实现了Connector API，API 包含了两部分内容。

   **连接器**

   连接器负责以下3 件事情。

   + 决定需要运行多少个任务。
   +  按照任务来拆分数据复制。
   + 从worker 进程获取任务配置并将其传递下去。例如， JDBC 连接器会连接到数据库，统计需要复制的数据表，井确定需要执行多少个任务，然后在配置参数`max.tasks`和实际数据量之间选择数值较小的那个作为任务数。在确定了任务数之后，连接器会为每个任务生成一个配置，配置里包含了连接器的配置项（比如connection.url）和该任务需要复制的数据表。`taskConfigs()`方法也返回一个映射列表，这些映射包含了任务的相关配置。worker 进程负责启动和配置任务，每个任务只复制配置项里指定的数据表。如果通过REST API 启动连接器，有可能会启动任意节点上的连接器，那么连接器的任务就会在该节点上执行。

   **任务**

   任务负责将数据移入或移出Kafka。任务在初始化时会得到由worker 进程分配的一个上下文： 源系统上下文（ Source Context ）包含了一个对象，可以将源系统记录的偏移量保存在上下文里（ 例如，文件连接器的偏移量就是文件里的字节位置， JDBC 连接器的偏移量可以是数据表的主键ID ） 。目标系统连接器的上下文提供了一些方法，连接器可以用它们操作从Kafka 接收到的数据，比如进行数据清理、错误重试，或者将偏移量保存到外部系统以便实现仅一次传递。任务在完成初始化之后，就开始按照连接器指定的配置（包含在一个Properties 对象里）启动工作。源系统任务对外部系统进行轮询，并返回一些记录， worker 进程将这些记录发送到Kafka 。数据地任务通过worker 进程接收来自Kafka 的记录，井将它们写入外部系统。

2. worker进程

   worker 进程是连接器和任务的“容器”。它们负责处理HTTP请求，这些请求用于定义连接器和连接器的配置。它们还负责保存连接器的配置、启动连接器和连接器任务，并把配置信息传递给任务。<u>如果一个worker 进程停止工作或者发生崩溃，集群里的其他worker进程会感知到（ Kafka 的消费者协议提供了心跳检测机制），井将崩溃进程的连接器和任务重新分配给其他进程</u>。如果有新的进程加入集群，其他进程也会感知到，并将自己的连接器和任务分配给新的进程，确保工作负载的均衡。进程还负责提交偏移量，如果任务抛出异常，可以基于这些偏移量进行重试。

   <u>为了更好地理解worker 进程，我们可以将其与连接器和任务进行简单的比较。连接器和任务负责“数据的移动”， 而worker 进程负责REST API 、配置管理、可靠性、高可用性、伸缩性和负载均衡</u>。

   <u>这种关注点分离是Connect API 给我们带来的最大好处，而这种好处是普通客户端API 所不具备的。有经验的开发人员都知道，编写代码从Kafka 读取数据并将其插入数据库只需要一到两天的时间，但是如果要处理好配置、异常、REST API 、监控、部署、伸缩、失效等问题，可能需要几个月。如果你使用连接器来实现数据复制，连接器插件会为你处理掉一大堆复杂的问题</u>。

3. 转化器和Connect的数据模型

   数据模型和转化器是Connect API 需要讨论的最后一部分内容。Connect 提供了一组数据API——它们包含了数据对象和用于描述数据的schema。例如， JDBC 连接器从数据库读取了一个字段，并基于这个字段的数据类型创建了一个Connect Schema 对象。然后使用这些Schema 对象创建一个包含了所有数据库字段的Struct一一我们保存了每一个字段的名字和它们的值。<u>源连接器所做的事情都很相似一一从源系统读取事件，并为每个事件生成schema 和值（值就是数据对象本身） 。目标连接器正好相反，它们获取schema 和值，井使用schema 来解析值，然后写入到目标系统</u>。

   源连接器只负责基于Data API 生成数据对象，那么worker 进程是如何将这些数据对象保存到Kafka 的？这个时候，转换器就派上用场了。用户在配置worker 进程（或连接器）时可以选择使用合适的转化器，用于将数据保存到Kafka 。目前可用的转化器有Avro 、JSON和String。JSON 转化器可以在转换结果里附带上schema ，当然也可以不使用schema ，这个是可配的。Kafka 系统因此可以支持结构化的数据和半结构化的数据。连接器通过DataAPI 将数据返回给worker 进程， worker 进程使用指定的转化器将数据转换成Avro 对象、JSON 对象或者字符串，然后将它们写入Kafka 。

   对于目标连接器来说，过程刚好相反一一在从Kafka 读取数据时， worker 进程使用指定的转换器将各种格式（ Avro 、JSON 或String ）的数据转换成Data API 格式的对象，然后将它们传给目标连接器，目标连接器再将它们插入到目标系统。

   Connect API 因此可以支持多种类型的数据，数据类型与连接器的实现是相互独立的一一只要有可用的转换器，连接器和数据类型可以自由组合。

4. 偏移量管理

   worker 进程的REST API 提供了部署和配置管理服务，除此之外， worker 进程还提供了偏移量管理服务。连接器只要知道哪些数据是已经被处理过的，就可以通过Kafka 提供的API 来维护偏移量。

   源连接器返回给worker 进程的记录里包含了一个逻辑分区和一个逻辑偏移量。它们并非Kafka 的分区和偏移量，而是源系统的分区和偏移量。例如，对于文件源来说，分区可以是一个文件，偏移量可以是文件里的一个行号或者字符号：而对于JDBC 源来说，分区可以是一个数据表，偏移量可以是一条记录的主键。<u>在设计一个据连接器时，要着重考虑如何对源系统的数据进行分区以及如何跟踪偏移量，这将影响连接器的井行能力，也决定了连接器是否能够实现至少一次传递或者仅一次传递</u>。

   源连接器返回的记录里包含了源系统的分区和偏移量， worker 进程将这些记录发送给Kafka 。如果Kafka 确认记录保存成功， worker 进程就把偏移量保存下来。偏移量的存储机制是可插拔的， 一般会使用Kafka 主题来保存。如果连接器发生崩溃井重启，它可以从最近的偏移量继续处理数据。

   目标连接器的处理过程恰好相反，不过也很相似。它们从Kafka 上读取包含了主题、分区和偏移量信息的记录，然后调用连接器的put() 方法，该方法会将记录保存到目标系统里。如果保存成功，连接器会通过消费者客户端将偏移量提交到Kafka 上。

   框架提供的偏移量跟踪机制简化了连接器的开发工作，并在使用多个连接器时保证了一定程度的行为一致性。

## 7.4 Connect之外的选择

### 7.4.1 用于其他数据存储的摄入框架

​	虽然我们很想说Kafka 是至高无上的明星，但肯定会有人不同意这种说法。有些人将Hadoop 或ElasticSearch 作为他们数据架构的基础，这些系统都有自己的数据摄入工具。Hadoop 使用了Flume，ElasticSearch 使用了Logstash 或Fluentd 。如果架构里包含了Kafka ，并且需要连接大量的源系统和目标系统，那么建议使用Connect API 作为摄入工具。如果构建的系统是以Hadoop 或ElasticSearch 为中心的， Kafka 只是数据的来源之一，那么使用Flume 或Logstash 会更合适。

### 7.4.2 基于图形界面的ETL工具

​	从保守的Informatica 到一些开源的替代方案，比如Talend 和Pentaho ， 或者更新的Apache NiFi 和StreamSets一一这些ETL 解决方案都支持将Kafka 作为数据源和数据地。如果你已经使用了这些系统，比如Pentaho ， 那么就可能不会为了Kafka 而在系统里增加另一种集成工具。如果你已经习惯了基于图形界面的ETL 数据管道解决方案，那就继续使用它们。不过， 这些系统有一些不足的地方，那就是它们的工作流比较复杂，如果你只是希望从Kafka 里获取数据或者将数据写入Kafka ，那么它们就显得有点笨重。我们在本章的开头部分已经说过，**在进行数据集成时，应该将注意力集中在消息的传输上。因此，对于我们来说，大部分ETL 工具都太过复杂了**。

​	我们极力建议将Kafka 当成是一个支持数据集成（使用Connect ）、应用集成（使用生产者和消费者）和流式处理的平台。Kafka 完全可以成为ETL 工具的替代品。

### 7.4.3 流式处理框架

​	几乎所有的流式处理框架都具备从Kafka 读取数据并将数据写入外部系统的能力。如果你的目标系统支持流式处理，井且你已经打算使用流式框架处理来自Kafka 的数据，那么使用相同的框架进行数据集成看起来是很合理的。这样可以省掉一个处理步骤（不需要保存来自Kafka 的数据，而是直接从Kafka 读取数据然后写到其他系统） ， 不过在发生数据丢失或者出现脏数据时，诊断问题会变得很困难，因为这些框架并不知道数据是什么时候丢失的，或者什么时候出现了脏数据。

## 7.5 总结

​	本章讨论了如何使用Kafka 进行数据集成，从解释为什么要使用Kafka 进行数据集成开始，到说明数据集成方案的一般性考虑点。我们先解释了为什么Kafka 和Connect API 是一种更好的选择，然后给出了一些例子， 演示如何在不同的场景下使用Connect ，井深入了解了Connect 的工作原理，最后介绍了一些Conn ect 之外的数据集成方案。

​	不管最终你选择了哪一种数据集成方案，都需要保证所有消息能够在各种恶劣条件下完成传递。我们相信，在与Kafka 的可靠性特性结合起来之后， Connect 具有了极高的可靠性。不过，我们仍然需要对它们进行严格的测试，以确保你选择的数据集成系统能够在发生进程停止、机器崩溃、网络延迟和高负载的情况下不丢失消息。毕竟，数据集成系统应该只做一件事情，那就是传递数据。

​	可靠性是数据集成系统唯一一个重要的需求。在选择数据系统时，首先要明确需求（可以参考7 .1节），并确保所选择的系统能够满足这些需求。除此之外，还要很好地了解数据集成方案，确保知道怎么使用它们来满足需求。虽然Kafka 支持至少一次传递的原语，但你也要小心谨慎，避免在配置上出现偏差，破坏了可靠性。

# 8. 跨集群数据镜像

​	在某些情况下，不同的集群之间相互依赖，管理员需要不停地在集群间复制数据。大部分数据库都支持**复制**（replication），也就是持续地在数据库服务器之间复制数据。不过，<u>因为前面已经使用过“复制”这个词来描述在同一个集群的节点间移动数据，所以我们把集群间的数据复制叫作**镜像**（mirroring）</u>。Kafka 内置的跨集群复制工具叫作MirrorMaker 。

​	在这一章，我们将讨论跨集群的数据镜像，它们既可以镜像所有数据，也可以镜像部分数据。我们先从一些常见的跨集群镜像场景开始，然后介绍这些场景所使用的架构模式，以及这些架构模式各自的优缺点。接下来我们会介绍MirrorMaker以及如何使用它， 然后说明在进行部署和性能调优时需要住意的一些事项。最后我们会对MirrorMaker的一些替代方案进行比较。

## 8.1 跨集群镜像的使用场景

​	下面列出了几个使用跨集群镜像的场景。

+ 区域集群和中心集群

  有时候， 一个公司会有多个数据中心，它们分布在不同的地理区域、不同的城市或不同的大洲。这些数据中心都有自己的Kafka 集群。有些应用程序只需要与本地的Kafka 集群通信，而有些则需要访问多个数据中心的数据（否则就没必要考虑跨数据中心的复制方案了）。有很多情况需要跨数据中心，比如一个公司根据供需情况修改商品价格就是一个典型的场景。该公司在每个城市都有一个数据中心，它们收集所在城市的供需信息，并调整商品价格。这些信息将会被镜像到一个中心集群上，业务分析员就可以在上面生成整个公司的收益报告。

+ 冗余（DR)

  一个Kafka 集群足以支撑所有的应用程序，不过你可能会担心集群因某些原因变得不可用，所以你希望有第二个Kafka 集群，它与第一个集群有相同的数据，如果发生了紧急情况，可以将应用程序重定向到第二个集群上。

+ 云迁移

  现今有很多公司将它们的业务同时部署在本地数据中心和云端。为了实现冗余，应用程序通常会运行在云供应商的多个服务区域里，或者使用多个云服务。本地和每个云服务区域都会有一个Kafka 集群。本地数据中心和云服务区域里的应用程序使用自己的Kafka 集群，当然也会在数据中心之间传输数据。例如，如果云端部署了一个新的应用程序，它需要访问本地的数据。本地的应用程序负责更新数据，井把它们保存在本地的数据库里。我们可以使用Connect 捕获这些数据库变更，并把它们保存到本地的Kafka集群里，然后再镜像到云端的Kafka 集群上。这样有助于控制跨数据中心的流量成本，同时也有助于改进流量的监管和安全性。

## 8.2 多集群架构

### * 8.2.1 跨数据中心通信的一些现实情况

​	以下是在进行跨数据中心通信时需要考虑的一些问题。

+ 高延迟

  Kafka 集群之间的通信延迟随着集群间距离的增长而增加。虽然光缆的速度是恒定的，但集群间的网络跳转所带来的缓冲和堵塞会增加通信延迟。

+ 有限的带宽

  单个数据中心的广域网带宽远比我们想象的要低得多，而且可用的带宽时刻在发生变化。另外，高延迟让如何利用这些带宽变得更加困难。

+ 高成本

  不管你是在本地还是在云端运行Kafka ，集群之间的通信都需要更高的成本。部分原因是因为带宽有限，而增加带宽是很昂贵的，当然，这个与供应商制定的在数据中心、区域和云端之间传输数据的收费策略也有关系。

​	<u>Kafka 服务器和客户端是按照单个数据中心进行设计、开发、测试和调优的</u>。我们假设服务器和客户端之间具有很低的延迟和很高的带宽，在使用默认的超时时间和缓冲区大小时也是基于这个前提。因此，我们不建议跨多个数据中心安装Kafka 服务器（不过稍后会介绍一些例外情况）。

​	大多数情况下，我们要避免向远程的数据中心生成数据，但如果这么做了，那么就要忍受高延迟，井且需要通过**增加重试次数**（ <u>Linkedln 曾经为跨集群镜像设置了32 000 多次重试次数</u>）和**增大缓冲区**来解决潜在的网络分区问题（生产者和服务器之间临时断开连接）。

​	**如果有了跨集群复制的需求，同时又禁用了从broker到broker之间的通信以及从生产者到broker 之间的通信，那么我们必须允许从broker 到消费者之间的通信。事实上，这是最安全的跨集群通信方式。在发生网络分区时，消费者无法从Kafka 读取数据，数据会驻留在Kafka 里， 直到通信恢复正常。因此，网络分区不会造成任何数据丢失**。

​	<u>不过，因为带宽有限，如果一个数据中心的多个应用程序需要从另一个数据中心的Kafka 服务器上读取数据，我们倾向于为每一个数据中心安装一个Kafka 集群，并在这些集群间复制数据，而不是让不同的应用程序通过广域网访问数据</u>。

​	在讨论更多有关跨数据中心通信的调优策略之前，我们需要先知道以下一些架构原则。

+ **每个数据中心至少需要一个集群**。
+ 每两个数据中心之间的数据复制要做到每个事件**仅复制一次**（除非出现错误需要重试） 。
+ **如果有可能，尽量从远程数据中心<u>读取数据</u>，而不是向远程数据中心写入数据**。

### 8.2.2 Hub和Spoke架构

​	这种架构适用于一个中心Kafka 集群对应多个本地Kafka 集群的情况，如图8 -1 所示。

![img](https://img2020.cnblogs.com/blog/1350843/202110/1350843-20211017081433832-510514946.png)

​	图8-1 ：一个中心 Kafka 集群对应多个本地Kafka 集群

​	这种架构有一个简单的变种，如果只有一个本地集群， 那么整个架构里就只剩下两个集群： 一个首领和一个跟随者，如图8 -2所示。

![img](https://img2020.cnblogs.com/blog/1350843/202110/1350843-20211017081647760-1592900958.png)

​	图8-2: 一个首领对应一个跟随者

​	当消费者需要访问的数据集分散在多个数据中心时，可以使用这种架构。如果每个数据中心的应用程序只处理自己所在数据中心的数据，那么也可以使用这种架构，只不过它们无法访问到全局的数据集。

​	**这种架构的好处在于，数据只会在本地的数据中心生成，而且每个数据中心的数据只会被镜像到中央数据中心一次**。只处理单个数据中心数据的应用程序可以被部署在本地数据中心里， 而需要处理多个数据中心数据的应用程序则需要被部署在中央数据中心里。<u>因为数据复制是单向的，而且消费者总是从同一个集群读取数据，所以这种架构易于部署、配置和监控</u>。

​	<u>不过这种架构的简单性也导致了一些不足。一个数据中心的应用程序无法访问另一个数据中心的数据</u>。为了更好地理解这种局限性，我们举一个例子来说明。

​	假设有一家银行，它在不同的城市有多家分行。每个城市的Kafka集群上保存了用户的信息和账号历史数据。我们把各个城市的数据复制到一个中心集群上， 这样银行就可以利用这些数据进行业务分析。在用户访问银行网站或去他们所属的分行办理业务时， 他们的请求被路由到本地集群上，同时从本地集群读取数据。假设一个用户去另一个城市的分行办理业务，因为他的信息不在这个城市，所以这个分行需要与远程的集群发生交互（不建议这么做）， 否则根本没有办桂访问到这个用户的信息（很尴尬） 。**因此，这种架构模式在数据访问方面有所局限，因为区域数据中心之间的数据是完全独立的**。

​	**在采用这种架构时，每个区域数据中心的数据都需要被镜像到中央数据中心上。镜像进程会读取每一个区域数据中心的数据，并将它们重新生成到中心集群上**。<u>如果多个数据中心出现了重名的主题，那么这些主题的数据可以被写到中心集群的单个主题上，也可以被写到多个主题上</u>。

### 8.2.3 双活架构

P140



