# 数据密集型应用系统设计-学习笔记-03

# 第三部分 派生数据

# 第10章 批处理系统

​	本书前两部分讨论了很多有关请求和查询以及对应的响应或结果等方面的内容。许多现代数据系统都假设以这种方式来处理数据：用户请求某种信息，或者发送指令，一段时间后(希望)系统会返回结果。数据库、高速缓存、搜索索引、Web服务器和其他许多系统都是这样工作的。

​	对于这种在线系统，无论是请求页面的浏览器还是调用远程API的服务，往往都假定请求是由用户触发，而且用户正在等待响应。通常等待不应太久，所以我们非常重视这些系统的响应时间(参阅第1章“描述性能”)。

​	Web和越来越多基于HTTP/REST的API使得请求/响应的交互模式变得如此普遍，以至于很容易将其视为理所当然。但是我们应当记住，这并不是构建系统的唯一途径，其他方法也有其优点。下面我们来区分三种不同类型的系统：

+ 在线服务(或称在线系统)

  服务等待客户请求或指令的到达。当收到请求或指令时，服务试图尽可能快地处理它，并发回一个响应。响应时间通常是服务性能的主要衡量指标，而可用性同样非常重要(如果客户端无法访问服务，用户可能会收到一个报错消息)。

+ 批处理系统(或称离线系统)

  批处理系统接收大量的输入数据，运行一个作业来处理数据，并产生输出数据。作业往往需要执行一段时间(从几分钟到几天)，所以用户通常不会等待作业完成。相反，批量作业通常会定期运行(例如，每天一次)。批处理作业的主要性能衡量标准通常是吞吐量(处理一定大小的输入数据集所需的时间)。本章主要讨论批处理。

+ 流处理系统(或称近实时系统)

  <u>流处理介于在线与离线/批处理之间(所以有时称为近实时或近线处理)</u>。与批处理系统类似，流处理系统处理输入并产生输岀(而不是响应请求)。但是，流式作业在事件发生后不久即可对事件进行处理，而批处理作业则使用固定的组输入数据进行操作。这种差异使得流处理系统比批处理系统具有更低的延迟。由于流处理是在批处理的基础上进行的，我们将在第11章讨论流处理系统。

​	正如我们将在本章中所看到的，批处理是构建可靠、可扩展与可维护应用的重要组成部分。例如，2004年发表的著名批处理算法 MapReduce“使得 Google具有如此大规模的可扩展性能力”(虽然该说法有些夸大和简单化)。该算法随后在各科开源数据系统中被陆续实现，包括 Hadoop、 CouchDB和 MongoDB。

​	与多年前为数据仓库开发的并行处理系统相比， MapReduce是一个相当低级别的编程模型，但是对于运行在商用硬件上的处理规模来讲，它绝对是一个重大的进步。虽然 MapReduce的重要性现在有些下降，但它仍然值得深入理解，因为它清晰地解释了批处理为什么有用以及如何有用。

​	事实上，批处理是一种非常古老的计算形式。早在可编程数字计算机诞生之前，打孔卡制表机(例如1890年美国人口普查中使用的 Hollerith机器)就实现了半机械化的批量处理，以计算来自大量输入的汇总统计信息。而 MapReduce与1940年和950年泛应用于商业数据处理的IBM卡片分类机器有着惊人的相似之处。是的，历史总是在重演。

​	本章将介绍MapReduce和其他一些批处理算法和框架，并探讨它们在现代数据系统中的应用。但首先，我们从使用标准UNIX工具的数据处理开始。或许你已经很熟悉，但考虑到UNIX的思想和经验普遍见于大规模、异枃分布式数据系统，因此值得重温UNIX哲学。

## 10.1 使用UNIX工具进行批处理

​	我们从一个简单的例子开始。假设有一个Web服务器，每次响应请求时都会在日志文件中追加一行记录。例如，使用 nginx默认访问日志格式，日志中的一行记录如下所示：

```shell
216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] "GET /css/typography.css HTTP/1.1" 
200 3377 "http://martin.kleppmann.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) 
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36"
```

​	实际上这只是一行，分成多行只是为了方便阅读。这一行中包含很多信息。为了解释它，你需要了解日志格式的定义，如下所示：

```shell
 $remote_addr - $remote_user [$time_local] "$request"
 $status $body_bytes_sent "$http_referer" "$http_user_agent"
```

​	因此，这一行日志表示2015年2月27日17:55:11 UTC，服务器从IP地址为216.58.210.78的客户端接收到一个对文件/css/typography. cssl的请求。用户为非认证用户，所以$remote_addr被设置为连字符(-)。响应状态是200(即请求成功)，响应大小是3377字节。浏览器是 Chrome40，并且它加载了该文件，因为这个文件在URL为`http://martin.kleppmann.com`的页面中被引用。

### 10.1.1 简单日志分析

​	有很多工具可以用来处理日志文件，并生成关于网站流量的漂亮报告。但是出于实践的目的，我们从基本的UNIX工具做起。例如，假设想找出网站中前五个最受欢迎的网页，可以在 UNIX Shell执行下列操作：

```shell
cat /var/log/nginx/access.log | #1
    awk '{print $7}' | #2
    sort             | #3
    uniq -c          | #4
    sort -r -n       | #5
    head -n 5          #6
```

1. 读取日志文件。
2. 将毎一行按空格分割成不同的字段，每行只输出第七个字段，即请求的URL地址。在我们的示例行中，这个请求URL地址是/css/typography.css。
3. 按字母顺序排列URL地址列表。如果某个URL被请求过n次，那么排序后，结果中将包含连续n次的重复URL。
4. uniq命令通过检査两条相邻的行是否相同来过滤掉其输入中的重复行。-c选项为输出一个计数器：对于每个不同的URL，它会报告输入中出现该URL的次数。
5. 第二种排序按毎行起始处的数字(-n)排序，也就是URL的请求次数。然后以反向(-r)顺序输出结果，即结果中最大的数字首先返回。
6. 最后，head只输出输入中的前五行(-n 5)，并丢弃其他数据。

​	该命令序列的输出如下所示：

```shell
    4189 /favicon.ico
    3631 /2013/05/24/improving-security-of-ssh-private-keys.html
    2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
    1369 /
     915 /css/typography.css
```

​	如果你不熟悉UNIX工具，上面的命令行可能看起来令人费解，但是它的确非常强大。它将在几秒钟内处理千兆字节的日志文件，你可以轻松修改分析指令以满足自己的需求。例如，如果要省略CSS文件，将awk参数更改为`$7 !~ /\.css$/ {print $7}`即可。如果想得到请求次数最多的客户端IP地址而不是页面，那么就将awk参数更改为`{print $1}`。

​	本书并不打算详细探讨UNIX工具，但是这些工具非常值得学习。令人惊讶的是，使用awk，sed，grep，sort，uniq和 xargs的组合，可以在几分钟内完成许多数据分析任务，并且表现令人十分满意。

#### 命令链与自定义程序

​	你也可以写一个简单的程序来做同样的事情，而不是使用UNIX命令链。例如，Ruby代码的实现可能看起来像这样：

```ruby
counts = Hash.new(0)         # 1
File.open('/var/log/nginx/access.log') do |file| 
    file.each do |line|
        url = line.split[6]  # 2
        counts[url] += 1     # 3
    end
end
top5 = counts.map{|url, count| [count, url] }.sort.reverse[0...5] # 4
top5.each{|count, url| puts "#{count} #{url}" }                   # 5
```

1. counts是一个哈希表，保存了用于记录每个URL出现次数的计数器。计数器默认值为0。
2. 对日志的每一行，都把URL作为第七个被空格分隔的字段(这里的数组索引是6，因为Ruby数组是零索引的)。

3. 为日志当前行中URL地址的计数器加1
4. 按计数器值(降序)对哈希表内容进行排序，并取前五位。
5. 打印前五个条目。

​	这个程序并不像UNIX管道那样简洁，但是它的可读性很强，所以喜欢哪一个完全是个人爱好。除了表面上的语法差异之外，执行流程也存在很大差异。如果在大文件上运行此分析，差异会变得更加明显。

#### 排序与内存中聚合

​	Ruby脚本需要一个URL的内存哈希表，其中每个URL地址都会映射到被访问的次数。UNIX流水线例子中则没有这样的哈希表，而是依赖于对URL列表进行排序，在这个URL列表中多次出现的相同URL仅仅是简单重复。

​	哪种方法更好呢？这取决于有多少个不同的网址。对于大多数中小型网站，也许可以在内存中(比如说1GB内存)存储所有不同的URL，并且可以为每个URL提供一个计数器。在此示例中，作业的工作集(作业需要随机访问的内存量)仅取决于不同URL的数量：如果单个URL有一百万条日志条目，则哈希表中所需的空间表仍然只是一个URL加上计数器的大小。如果这个工作集足够小，那么内存哈希表工作正常，甚至在笔记本计算机上都能工作。

​	另一方面，如果作业的工作集大于可用内存，则排序方法的优点是可以高效地使用磁盘。这与第3章的“SSTables和LSM-Tree”中讨论的原理相同：<u>数据块可以在内存中排序并作为段文件写入磁盘，然后多个排序的段可以合并为一个更大的排序文件</u>。<u>归并排序在磁盘上有良好的顺序访问模式(请记住，优化为顺序IO是第3章反复出现的主题，这里则再次强调)</u>。

​	<u>GNU Coreutils(Linux)中的sort实用程序通过自动唤出到磁盘的方式支持处理大于内存的数据集，且排序可以自动并行化以充分利用多核CPU。这意味着之前简单的UNIX命令链可以很容易地扩展到大数据集，而不会耗尽内存。从磁盘读取输入文件倒可能会成为瓶颈</u>。

### * 10.1.2 UNIX 设计哲学

​	我们可以非常容易地使用类似例子中命令链来分析日志文件，这并不是巧合：事实上，这是UNIX的关键设计思想之一，而且时至今日依然令人惊奇。这里，我们更深入地硏究一下，目的是从中挖掘、借鉴一些不错的想法。

​	UNX管道的发明者Doug Mcllroy在1964年首先描述了这种用例：“当需要以另一种方式处理数据时，我们应该有一些连接程序的方法，就像花园软管互相拧在一起。这就是计算机的I/O。”管道类比一直沿用了下来，通过管道将程序连接起来的想法成为如今的UNIX哲学，在开发人员和UNIX用户中逐渐变得流行的一系列设计原则。1978年，对于这种哲学有了更为完整的描述：

1. 每个程序做好一件事。如果要做新的工作，则建立一个全新的程序，而不是通过增加新“特征”使旧程序变得更加复杂。
2. 期待每个程序的输出成为另一个尚未确定的程序的输入。不要将输出与无关信息混淆在一起。避免使用严格的表格状或二进制输入格式。不要使用交互式输入。
3. 尽早尝试设计和构建软件，甚至是操作系统，最好在几周内完成。需要扔掉那些笨拙的部分时不要犹豫，并立即进行重建。
4. 优先使用工具来减轻编程任务，即使你不得不额外花费时间去构建工具，并且预期在使用完成后会将其中一些工具扔掉。

​	这种方法(自动化、快速原型设计、增量式迭代、测试友好、将大型项目分解为可管理的模块等)听起来非常像今天的敏捷开发与 DevOps运动。令人惊讶的是，四十年来并没发生太大变化。

​	sort命令是一个很好的“一个程序只做好一件事情”的例了。可以说它比大多数编程语言标准库内置的排序算法实现的更好(后者通常不会自动唤出到磁盘，且不使用多线程，即使这样做会更加利于算法)。然而，单独使用sort工具几乎用处不大。只有将其与其他UNIX工具(如uniq)结合使用时，它才会变得无比强大。

​	像bash这样的 UNIX shell可以让我们轻松地将这些小程序组合成强大的数据处理作业。尽管这些程序中是由不同人所编写的，但它们可以灵活地结合在一起。那么，UNIX是如何实现这种可组合性的呢?

#### * 统一接口

​	如果希望某个程序的输出成为另一个程序的输入，也就意味着这些程序必须使用相同的数据格式，换句话说，需要兼容的接口。<u>如果你希望能够将任何程序的输岀连接到任何程序的输入，那意味着所有程序都必须使用相同的输入/输出接口</u>。

​	<u>在UNⅠX中，这个接口就是文件(更准确地说，是**文件描述符**)，文件只是一个有序的字节序列。这是一个非常简单的接口，因此可以使用相同的接口表示许多不同的东西：文件系统上的实际文件，到另一个进程(UNIX socket， stdin， stdout)的通信通道，设备驱动程序(比如/dev/audio或/dev/lp0)，表示TCP连接的套接字等。说起来容易，但实际上对于这些差异明显的不同的事物通过共享统一的接口，使得它们能够轻松连接在一起，确实不可思议</u>。

​	按照惯例，许多(但不是全部)UNIX程序将这个字节序列视为ASCII文本。我们的日志分析示例也基于这个事实：awk，sort，uniq和head都将它们的输入文件视为由\n(换行符，ASCII 0x0A)字符分隔的记录列表。其实ASCI|记录分隔符0x1E或许是一个更好的选择，因为它本来就是为了这个目的而设计的。但是无论如何，所有这些程序都使用标准相同的记录分隔符以支持交互操作。

​	对毎条记录(即一行输入)的解析则没有明确定义。UNIX工具通常使用空格或制表符将行分割成字段，但也使用CSV(逗号分隔)、管道符分隔或其他编码字符。即使像 xargs这样相当简单的工具也有六个命令行选项，用于指定如何解析输入。

​	以 ASCII文本作为统一接口的数据格式通常工作得很好，但这种方式并不见得很漂亮：我们的日志分析示例使用{print $7}来提取网址，但可读性并不好。理想情况下，它看起来应该类似于{print$ request_url}。我们稍后会再讨论这个想法。

​	尽管经过几十年的发展依然不够完美，但UNIX统一接口的表现仍然称得上卓越。不过，并没有太多软件能像UNIX工具一样实现交互操作与组合：不能通过自定义分析工具轻松地将电子邮件内容和在线购物历史记录传送到电子表格中，并将结果发布到社交网络或维基百科上。可以说，今天能像UNIX工具一样流畅地连接多个程序绝对是一个“异类”，而并不是常态。

​	即使是具有相同数据模型的数据库，从一个数据库中导出数据然后导入到另一个数据库也并非易事。集成性的缺失导致了数据的巴尔干化(或称碎片化)。

#### 逻辑与布线分离

​	UNIX工具的另一个特点是使用标准输入(stdin)和标准输出(stdout)。如果运行一个程序而不指定任何参数，那么标准输入来自键盘，标淮输出为屏幕。当然，也可以将文件作为输入和/或将输出重定向到文件。管道允许将一个进程的stdout附加到另一个进程的stdin(具有小的内存缓冲区，而不需要将全部中间数据流写入磁盘)。

​	程序仍然可以在需要时直接读取和写入文件。但如果程序不依赖特定的文件路径，只使用 stdin和 stdout，则UNIX方法的效果最好。这允许shell用户以任何他们想要的方式连接输入和输出；程序并不知道也不关心输入来自哪里以及输出到哪里。也可以说这是一种松耦合，后期绑定或控制反转)。将输入/输出的布线连接与程序逻辑分开，可以更容易地将小工具组合成更大的系统。

​	用户甚至可以编写自己的程序，并将它们与操作系统提供的工具组合在一起。程序只需要从 stdin读取输入并输出至 stdout，从而参与数捃处理流水线。在日志分析示例中，我们可以编写一个工具，将用户代理字、串转换为更为合理的浏览器标识符，或者将IP地址转换为国家代码，并将其插入流水线中。sort程序其实并不关心它正在与操作系统进行通信，还是与用户编写的程序进行通信。

​	但是，使用stdin和stdout也有其局限性。需要多个输入或输出的程序会变得很棘手。用户不能将程序的输出传输给一个网络连接。如果一个程序直接打开文件进行读写，或者将另一个程序作为子进程启动，或者打开一个网络连接，那么这个I/O就被程序本身连接起来。虽然支持一些可配置选项(例如通过命令行)，但是减少了在shell中连接输入和输出的灵活性。

#### 透明与测试

​	UNIX工具如此成功的部分原因在于，它可以非常轻松地观察事情的进展：

+ <u>UNIX命令的输入文件通常被视为是不可变的。这意味着可以随意运行命令，尝试各种命令行选项，而不会损坏输入文件</u>。
+ 可以在任何时候结束流水线，将输出管道输送到less，然后査看它是否具有预期的形式。这种检查能力对调试非常有用。
+ 可以将流水线某个阶段的输出写入文件，并将该文件用作下一阶段的输入。这使得用户可以重新启动后面的阶段，而无需重新运行整个流水线。

​	因此，与关系数据库的查询优化器相比，尽管UNIX工具相当简单，但是非常有用，特别是对于测试而言。

​	然而，UNIX工具的最大局限在于它们只能在一台机器上运行，而这正是像 Hadoop这样的工具的工作场景。

## 10.2 MapReduce 与分布式文件系统

P372