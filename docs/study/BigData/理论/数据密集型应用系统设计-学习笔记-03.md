# 数据密集型应用系统设计-学习笔记-03

# 第三部分 派生数据

# 第10章 批处理系统

​	本书前两部分讨论了很多有关请求和查询以及对应的响应或结果等方面的内容。许多现代数据系统都假设以这种方式来处理数据：用户请求某种信息，或者发送指令，一段时间后(希望)系统会返回结果。数据库、高速缓存、搜索索引、Web服务器和其他许多系统都是这样工作的。

​	对于这种在线系统，无论是请求页面的浏览器还是调用远程API的服务，往往都假定请求是由用户触发，而且用户正在等待响应。通常等待不应太久，所以我们非常重视这些系统的响应时间(参阅第1章“描述性能”)。

​	Web和越来越多基于HTTP/REST的API使得请求/响应的交互模式变得如此普遍，以至于很容易将其视为理所当然。但是我们应当记住，这并不是构建系统的唯一途径，其他方法也有其优点。下面我们来区分三种不同类型的系统：

+ 在线服务(或称在线系统)

  服务等待客户请求或指令的到达。当收到请求或指令时，服务试图尽可能快地处理它，并发回一个响应。响应时间通常是服务性能的主要衡量指标，而可用性同样非常重要(如果客户端无法访问服务，用户可能会收到一个报错消息)。

+ 批处理系统(或称离线系统)

  批处理系统接收大量的输入数据，运行一个作业来处理数据，并产生输出数据。作业往往需要执行一段时间(从几分钟到几天)，所以用户通常不会等待作业完成。相反，批量作业通常会定期运行(例如，每天一次)。批处理作业的主要性能衡量标准通常是吞吐量(处理一定大小的输入数据集所需的时间)。本章主要讨论批处理。

+ 流处理系统(或称近实时系统)

  <u>流处理介于在线与离线/批处理之间(所以有时称为近实时或近线处理)</u>。与批处理系统类似，流处理系统处理输入并产生输岀(而不是响应请求)。但是，流式作业在事件发生后不久即可对事件进行处理，而批处理作业则使用固定的组输入数据进行操作。这种差异使得流处理系统比批处理系统具有更低的延迟。由于流处理是在批处理的基础上进行的，我们将在第11章讨论流处理系统。

​	正如我们将在本章中所看到的，批处理是构建可靠、可扩展与可维护应用的重要组成部分。例如，2004年发表的著名批处理算法 MapReduce“使得 Google具有如此大规模的可扩展性能力”(虽然该说法有些夸大和简单化)。该算法随后在各科开源数据系统中被陆续实现，包括 Hadoop、 CouchDB和 MongoDB。

​	与多年前为数据仓库开发的并行处理系统相比， MapReduce是一个相当低级别的编程模型，但是对于运行在商用硬件上的处理规模来讲，它绝对是一个重大的进步。虽然 MapReduce的重要性现在有些下降，但它仍然值得深入理解，因为它清晰地解释了批处理为什么有用以及如何有用。

​	事实上，批处理是一种非常古老的计算形式。早在可编程数字计算机诞生之前，打孔卡制表机(例如1890年美国人口普查中使用的 Hollerith机器)就实现了半机械化的批量处理，以计算来自大量输入的汇总统计信息。而 MapReduce与1940年和950年泛应用于商业数据处理的IBM卡片分类机器有着惊人的相似之处。是的，历史总是在重演。

​	本章将介绍MapReduce和其他一些批处理算法和框架，并探讨它们在现代数据系统中的应用。但首先，我们从使用标准UNIX工具的数据处理开始。或许你已经很熟悉，但考虑到UNIX的思想和经验普遍见于大规模、异枃分布式数据系统，因此值得重温UNIX哲学。

## 10.1 使用UNIX工具进行批处理

​	我们从一个简单的例子开始。假设有一个Web服务器，每次响应请求时都会在日志文件中追加一行记录。例如，使用 nginx默认访问日志格式，日志中的一行记录如下所示：

```shell
216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] "GET /css/typography.css HTTP/1.1" 
200 3377 "http://martin.kleppmann.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) 
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36"
```

​	实际上这只是一行，分成多行只是为了方便阅读。这一行中包含很多信息。为了解释它，你需要了解日志格式的定义，如下所示：

```shell
 $remote_addr - $remote_user [$time_local] "$request"
 $status $body_bytes_sent "$http_referer" "$http_user_agent"
```

​	因此，这一行日志表示2015年2月27日17:55:11 UTC，服务器从IP地址为216.58.210.78的客户端接收到一个对文件/css/typography. cssl的请求。用户为非认证用户，所以$remote_addr被设置为连字符(-)。响应状态是200(即请求成功)，响应大小是3377字节。浏览器是 Chrome40，并且它加载了该文件，因为这个文件在URL为`http://martin.kleppmann.com`的页面中被引用。

### 10.1.1 简单日志分析

​	有很多工具可以用来处理日志文件，并生成关于网站流量的漂亮报告。但是出于实践的目的，我们从基本的UNIX工具做起。例如，假设想找出网站中前五个最受欢迎的网页，可以在 UNIX Shell执行下列操作：

```shell
cat /var/log/nginx/access.log | #1
    awk '{print $7}' | #2
    sort             | #3
    uniq -c          | #4
    sort -r -n       | #5
    head -n 5          #6
```

1. 读取日志文件。
2. 将毎一行按空格分割成不同的字段，每行只输出第七个字段，即请求的URL地址。在我们的示例行中，这个请求URL地址是/css/typography.css。
3. 按字母顺序排列URL地址列表。如果某个URL被请求过n次，那么排序后，结果中将包含连续n次的重复URL。
4. uniq命令通过检査两条相邻的行是否相同来过滤掉其输入中的重复行。-c选项为输出一个计数器：对于每个不同的URL，它会报告输入中出现该URL的次数。
5. 第二种排序按毎行起始处的数字(-n)排序，也就是URL的请求次数。然后以反向(-r)顺序输出结果，即结果中最大的数字首先返回。
6. 最后，head只输出输入中的前五行(-n 5)，并丢弃其他数据。

​	该命令序列的输出如下所示：

```shell
    4189 /favicon.ico
    3631 /2013/05/24/improving-security-of-ssh-private-keys.html
    2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
    1369 /
     915 /css/typography.css
```

​	如果你不熟悉UNIX工具，上面的命令行可能看起来令人费解，但是它的确非常强大。它将在几秒钟内处理千兆字节的日志文件，你可以轻松修改分析指令以满足自己的需求。例如，如果要省略CSS文件，将awk参数更改为`$7 !~ /\.css$/ {print $7}`即可。如果想得到请求次数最多的客户端IP地址而不是页面，那么就将awk参数更改为`{print $1}`。

​	本书并不打算详细探讨UNIX工具，但是这些工具非常值得学习。令人惊讶的是，使用awk，sed，grep，sort，uniq和 xargs的组合，可以在几分钟内完成许多数据分析任务，并且表现令人十分满意。

#### 命令链与自定义程序

​	你也可以写一个简单的程序来做同样的事情，而不是使用UNIX命令链。例如，Ruby代码的实现可能看起来像这样：

```ruby
counts = Hash.new(0)         # 1
File.open('/var/log/nginx/access.log') do |file| 
    file.each do |line|
        url = line.split[6]  # 2
        counts[url] += 1     # 3
    end
end
top5 = counts.map{|url, count| [count, url] }.sort.reverse[0...5] # 4
top5.each{|count, url| puts "#{count} #{url}" }                   # 5
```

1. counts是一个哈希表，保存了用于记录每个URL出现次数的计数器。计数器默认值为0。
2. 对日志的每一行，都把URL作为第七个被空格分隔的字段(这里的数组索引是6，因为Ruby数组是零索引的)。

3. 为日志当前行中URL地址的计数器加1
4. 按计数器值(降序)对哈希表内容进行排序，并取前五位。
5. 打印前五个条目。

​	这个程序并不像UNIX管道那样简洁，但是它的可读性很强，所以喜欢哪一个完全是个人爱好。除了表面上的语法差异之外，执行流程也存在很大差异。如果在大文件上运行此分析，差异会变得更加明显。

#### 排序与内存中聚合

​	Ruby脚本需要一个URL的内存哈希表，其中每个URL地址都会映射到被访问的次数。UNIX流水线例子中则没有这样的哈希表，而是依赖于对URL列表进行排序，在这个URL列表中多次出现的相同URL仅仅是简单重复。

​	哪种方法更好呢？这取决于有多少个不同的网址。对于大多数中小型网站，也许可以在内存中(比如说1GB内存)存储所有不同的URL，并且可以为每个URL提供一个计数器。在此示例中，作业的工作集(作业需要随机访问的内存量)仅取决于不同URL的数量：如果单个URL有一百万条日志条目，则哈希表中所需的空间表仍然只是一个URL加上计数器的大小。如果这个工作集足够小，那么内存哈希表工作正常，甚至在笔记本计算机上都能工作。

​	另一方面，如果作业的工作集大于可用内存，则排序方法的优点是可以高效地使用磁盘。这与第3章的“SSTables和LSM-Tree”中讨论的原理相同：<u>数据块可以在内存中排序并作为段文件写入磁盘，然后多个排序的段可以合并为一个更大的排序文件</u>。<u>归并排序在磁盘上有良好的顺序访问模式(请记住，优化为顺序IO是第3章反复出现的主题，这里则再次强调)</u>。

​	<u>GNU Coreutils(Linux)中的sort实用程序通过自动唤出到磁盘的方式支持处理大于内存的数据集，且排序可以自动并行化以充分利用多核CPU。这意味着之前简单的UNIX命令链可以很容易地扩展到大数据集，而不会耗尽内存。从磁盘读取输入文件倒可能会成为瓶颈</u>。

### * 10.1.2 UNIX 设计哲学

​	我们可以非常容易地使用类似例子中命令链来分析日志文件，这并不是巧合：事实上，这是UNIX的关键设计思想之一，而且时至今日依然令人惊奇。这里，我们更深入地硏究一下，目的是从中挖掘、借鉴一些不错的想法。

​	UNX管道的发明者Doug Mcllroy在1964年首先描述了这种用例：“当需要以另一种方式处理数据时，我们应该有一些连接程序的方法，就像花园软管互相拧在一起。这就是计算机的I/O。”管道类比一直沿用了下来，通过管道将程序连接起来的想法成为如今的UNIX哲学，在开发人员和UNIX用户中逐渐变得流行的一系列设计原则。1978年，对于这种哲学有了更为完整的描述：

1. 每个程序做好一件事。如果要做新的工作，则建立一个全新的程序，而不是通过增加新“特征”使旧程序变得更加复杂。
2. 期待每个程序的输出成为另一个尚未确定的程序的输入。不要将输出与无关信息混淆在一起。避免使用严格的表格状或二进制输入格式。不要使用交互式输入。
3. 尽早尝试设计和构建软件，甚至是操作系统，最好在几周内完成。需要扔掉那些笨拙的部分时不要犹豫，并立即进行重建。
4. 优先使用工具来减轻编程任务，即使你不得不额外花费时间去构建工具，并且预期在使用完成后会将其中一些工具扔掉。

​	这种方法(自动化、快速原型设计、增量式迭代、测试友好、将大型项目分解为可管理的模块等)听起来非常像今天的敏捷开发与 DevOps运动。令人惊讶的是，四十年来并没发生太大变化。

​	sort命令是一个很好的“一个程序只做好一件事情”的例了。可以说它比大多数编程语言标准库内置的排序算法实现的更好(后者通常不会自动唤出到磁盘，且不使用多线程，即使这样做会更加利于算法)。然而，单独使用sort工具几乎用处不大。只有将其与其他UNIX工具(如uniq)结合使用时，它才会变得无比强大。

​	像bash这样的 UNIX shell可以让我们轻松地将这些小程序组合成强大的数据处理作业。尽管这些程序中是由不同人所编写的，但它们可以灵活地结合在一起。那么，UNIX是如何实现这种可组合性的呢?

#### * 统一接口

​	如果希望某个程序的输出成为另一个程序的输入，也就意味着这些程序必须使用相同的数据格式，换句话说，需要兼容的接口。<u>如果你希望能够将任何程序的输岀连接到任何程序的输入，那意味着所有程序都必须使用相同的输入/输出接口</u>。

​	<u>在UNⅠX中，这个接口就是文件(更准确地说，是**文件描述符**)，文件只是一个有序的字节序列。这是一个非常简单的接口，因此可以使用相同的接口表示许多不同的东西：文件系统上的实际文件，到另一个进程(UNIX socket， stdin， stdout)的通信通道，设备驱动程序(比如/dev/audio或/dev/lp0)，表示TCP连接的套接字等。说起来容易，但实际上对于这些差异明显的不同的事物通过共享统一的接口，使得它们能够轻松连接在一起，确实不可思议</u>。

​	按照惯例，许多(但不是全部)UNIX程序将这个字节序列视为ASCII文本。我们的日志分析示例也基于这个事实：awk，sort，uniq和head都将它们的输入文件视为由\n(换行符，ASCII 0x0A)字符分隔的记录列表。其实ASCI|记录分隔符0x1E或许是一个更好的选择，因为它本来就是为了这个目的而设计的。但是无论如何，所有这些程序都使用标准相同的记录分隔符以支持交互操作。

​	对毎条记录(即一行输入)的解析则没有明确定义。UNIX工具通常使用空格或制表符将行分割成字段，但也使用CSV(逗号分隔)、管道符分隔或其他编码字符。即使像 xargs这样相当简单的工具也有六个命令行选项，用于指定如何解析输入。

​	以 ASCII文本作为统一接口的数据格式通常工作得很好，但这种方式并不见得很漂亮：我们的日志分析示例使用{print $7}来提取网址，但可读性并不好。理想情况下，它看起来应该类似于{print$ request_url}。我们稍后会再讨论这个想法。

​	尽管经过几十年的发展依然不够完美，但UNIX统一接口的表现仍然称得上卓越。不过，并没有太多软件能像UNIX工具一样实现交互操作与组合：不能通过自定义分析工具轻松地将电子邮件内容和在线购物历史记录传送到电子表格中，并将结果发布到社交网络或维基百科上。可以说，今天能像UNIX工具一样流畅地连接多个程序绝对是一个“异类”，而并不是常态。

​	即使是具有相同数据模型的数据库，从一个数据库中导出数据然后导入到另一个数据库也并非易事。集成性的缺失导致了数据的巴尔干化(或称碎片化)。

#### 逻辑与布线分离

​	UNIX工具的另一个特点是使用标准输入(stdin)和标准输出(stdout)。如果运行一个程序而不指定任何参数，那么标准输入来自键盘，标淮输出为屏幕。当然，也可以将文件作为输入和/或将输出重定向到文件。管道允许将一个进程的stdout附加到另一个进程的stdin(具有小的内存缓冲区，而不需要将全部中间数据流写入磁盘)。

​	程序仍然可以在需要时直接读取和写入文件。但如果程序不依赖特定的文件路径，只使用 stdin和 stdout，则UNIX方法的效果最好。这允许shell用户以任何他们想要的方式连接输入和输出；程序并不知道也不关心输入来自哪里以及输出到哪里。也可以说这是一种松耦合，后期绑定或控制反转)。将输入/输出的布线连接与程序逻辑分开，可以更容易地将小工具组合成更大的系统。

​	用户甚至可以编写自己的程序，并将它们与操作系统提供的工具组合在一起。程序只需要从 stdin读取输入并输出至 stdout，从而参与数捃处理流水线。在日志分析示例中，我们可以编写一个工具，将用户代理字、串转换为更为合理的浏览器标识符，或者将IP地址转换为国家代码，并将其插入流水线中。sort程序其实并不关心它正在与操作系统进行通信，还是与用户编写的程序进行通信。

​	但是，使用stdin和stdout也有其局限性。需要多个输入或输出的程序会变得很棘手。用户不能将程序的输出传输给一个网络连接。如果一个程序直接打开文件进行读写，或者将另一个程序作为子进程启动，或者打开一个网络连接，那么这个I/O就被程序本身连接起来。虽然支持一些可配置选项(例如通过命令行)，但是减少了在shell中连接输入和输出的灵活性。

#### 透明与测试

​	UNIX工具如此成功的部分原因在于，它可以非常轻松地观察事情的进展：

+ <u>UNIX命令的输入文件通常被视为是不可变的。这意味着可以随意运行命令，尝试各种命令行选项，而不会损坏输入文件</u>。
+ 可以在任何时候结束流水线，将输出管道输送到less，然后査看它是否具有预期的形式。这种检查能力对调试非常有用。
+ 可以将流水线某个阶段的输出写入文件，并将该文件用作下一阶段的输入。这使得用户可以重新启动后面的阶段，而无需重新运行整个流水线。

​	因此，与关系数据库的查询优化器相比，尽管UNIX工具相当简单，但是非常有用，特别是对于测试而言。

​	然而，UNIX工具的最大局限在于它们只能在一台机器上运行，而这正是像 Hadoop这样的工具的工作场景。

## 10.2 MapReduce 与分布式文件系统

​	MapReduce有点像分布在数千台机器上的UNIX工具。与UNIX工具类似，这是一个相当直接、蛮力，却又有效的神奇组件。 MapReduce作业可以和UNIX进程相媲美：需要一个或多个输入，并产生一个或多个输出。

​	<u>和大多数UNIX工具一样，运行MapReduce作业通常不会修改输入</u>，除了生成输出外没有任何副作用。输出文件以序列方式一次性写入<u>(在写文件时，不会修改任何现有的文件)</u>。

​	UNIX工具使用stdin和stdout作为输入和输出，而MapReduce作业在分布式文件系统上读写文件。在Hadoop的MapReduce实现中，该文件系统被称为HDFS [Hadoop Distributed File System，一个 Google文件系统(GFS)的开源实现版本]。

​	除HDFS外，还有其他各种分布式文件系统，包括 GlusterFS和 Quantcast File System(QFS)等。诸如 Amazon S3， Azure Blob存储和OpenStack Swift2)象存储服务也有很多相似之处。在这一章我们主要以HDFS为运行实例，不过这些原则也适用于其他分布式文件系统。

​	与网络连接存储(NAS)和存储区域网络(SAN)架构的共享磁盘方法相比，HDFS基于**无共享原则**(参见第二部分的介绍)。<u>共享磁盘存储由集中式存储设备实现，通常使用定制硬件和特殊网络基础设施(如光纤通道)。而无共享方法则不需要特殊硬件，只需要通过传统数据中心网络连接的计算机</u>。

​	<u>HDFS包含一个在每台机器上运行的守护进程，并会开放一个网络服务以允许其他节点访问存储在该机器上的文件(假设数据中心的每台节点都附带一些本地磁盘)。**名为 NameNode 的中央服务器会跟踪哪个文件块存储在哪台机器上**。因此，从概念上讲，HDFS创建了一个庞大的文件系统，来充分利用每个守护进程机器上的磁盘资源</u>。

​	<u>考虑到机器和磁盘的容错，文件块被复制到多台机器上</u>。复制意味着位于多个机器上的相同数据的多个副本(参阅第5章)；或者像Reed-Solomon代码这样的纠删码方案，相比于副本技术，**纠删码**可以以更低的存储开销来恢复数据。这些技术与RAID相似，它可以在同台机器的多个磁盘级别提供数据冗余；不同的是，在分布式文件系统中，文件访问和复制是在传统的数据中心网络上完成的，而不依赖特殊的硬件。

​	HDFS具有很好的扩展性：在撰写本书时，最大的HDFS集群运行在上万台机器上，总存储容量达到了几百PB。如此大规模的部署主要得益于商用硬件和开源软件的使用，使得HDFS上的数据存储与访问成本远低于同等容量的专用存储设备。

### 10.2.1 MapReduce 作业执行

​	MapReduce是一个编程框架，可以使用它编写代码来处理HDFS等分布式文件系统中的大型数据集。最简单的理解方法是参考本章前面的“简单目志分析”中的Web日志分析示例。 MaprReduce中的数据处理模式与此非常相似：

1. 读取一组输入文件，并将其分解成记录。在Web日志示例中，每个记录都是日志中的一行(即\n是记录分隔符)。
2. 调用 mapper函数从每个输入记录中提取一个键值对。在前面的例子中， mapper函数是`awk '{print $7}'`：它提取`URL($7)`作为关键字，并将相应的值留为空。
3. 按关键宇将所有的键值对排序。在日志示例中，这由第一个sort命令完成。
4. 调用 reducer函数遍历排序后的键值对。如果同一个键出现多次，排序会使它们在列表中相邻，所以很容易组合这些值，而不必在内存中保留过多状态。在前面的例子中， reducer是由`uniq -c`命令实现的，该命令对具有相同关键字的相邻记录进行计数。

​	这四个步骤可以由一个 MapReduce作业执行。步骤2(map)和4(reduce)是用户编写自定义数据处理的代码。步骤1(将文件分解成记录)由输入格式解析器处理。步骤3中的**排序步骤sort隐含在MapReduce中，无需用户编写， mapper的输出始终会在排序之后再传递给reducer**。

​	要创建 MapReduce作业，需要实现两个回调函数，即mapper和reducer，其行为如下(另请参阅第2章的“MapReduce查询”)。

+ Mapper

  <u>每个输入记录都会调用一次 mapper程序</u>，其任务是从输入记录中提取关键字和值。对于毎个输入，它可以生成任意数量的键值对(包括空记录)。<u>它不会保留从一个输入记录到下一个记录的任何状态，因此每个记录都是独立处理的</u>。

+ Reducer

  MapReduce框架使用由 mapper生成的键值对，**收集属于<u>同一个关键字</u>的所有值，并使用迭代器调用 reducer以使用该值的集合**。 Reducer可以生成输出记录(例如相同URL出现的次数)。

​	在web日志示例中，第5步由第二个sort命令按请求数对URL进行排序。**在MapReduce中，如果需要第二个排序阶段，则可以编写另一个 Mapreduce作业并将第个作业的输出用作第二个作业的输入。这样来看的话， mapper的作用是将数据放入一个适合排序的表单中，而 reducer的作用则是处理排序好的数据**。

#### * MapReduce的分布式执行

​	MapReduce与UNIX命令管道的主要区别在于它可以跨多台机器并行执行计算，其不必编写代码来指示如何并行化。 <u>**mapper和 reducer一次只能处理一条记录**，它们不需要知道输入来自哪里或者输岀到什么地方，所以框架可以处理复杂的跨机器移动数据的情形</u>。

​	在分布式计算中可以使用标准的UNIX工具作为 mapper和 reducer，但更为常见的是用传统编程语言实现的函数。在 Hadoop MapReduce中， mapper和reducer都是实现特定接口的Java类。在 MongoDB和 CouchDB中， mapper和 Reducer是 Javascript函数(参阅第2章的“ MapReduce查询”)。

​	图10-1展示了 Hadoop MapReduce作业中的数据流。其**并行化**基于**分区**(参阅第6章)实现：<u>作业的输入通常是HDFS中的一个目录，且输入目录中的**每个文件或文件块都被视为一个单独的分区，可以由一个单独的map任务来处理**</u>(在图10-1 标记为map任务1，map任务2和map任务3)。

​	一个输入文件的大小通常是几百兆字节。<u>只要有足够的空闲内存和CPU资源，MapReduce调度器(图中未显示)会尝试在输入文件副本的某台机器上运行mapper任务。这个原理被称为**将计算靠近数据：它避免将输入文件通过网络进行复制减少了网络负载，提高了访问局部性**</u>。

![MapReduce和分布式文件系统 - 图1](https://static.sitestack.cn/projects/ddia/img/fig10-1.png)

​	<u>大多数情况下，将要在map任务中所运行的应用程序代码在分配运行任务的节点上并不存在，所以MapReduce框架首先要复制代码(例如Java程序中的JAR文件)到该节点。然后启动map任务并开始读取输入文件，每次将一条记录传递给回调函数mapper。mapper的输出由键值对组成</u>。

​	<u>Reduce任务中的计算也被分割成块</u>。**Map任务的数量由输入文件块的数量决定，而reduce任务的数量则是由作业的作者来配置的(可以不同于map任务的数量)**。**<u>为了确保具有相同关键字的所有键值对都在相同的reducer任务中处理，框架使用关键字的哈希值来确定哪个reduce任务接收特定的键值对</u>**(请参阅第6章“基于关键字哈希值分区”)。

​	**键值对必须进行排序**。如果数据集太大，可能无法在单台机器上使用常规排序算法。事实上，**排序是分阶段进行的。首先，每个map任务都基于关键字哈希值，按照reducer对输出进行分块**。<u>每一个分区都被写入 mapper程序所在本地磁盘上的已排序文件</u>，使用的技术类似我们在第3章讨论的“SSTables和LSM-Trees”。

​	**当mapper完成读取输入文件并写入经过排序的输出文件， MapReduce调度器就会通知reducer开始从 mapper中获取输出文件**。 reducer与每个 mapper相连接，并按照其分区从 mapper下载<u>排序后的</u>键值对文件。**按照reducer分区，排序和将数据分区从 mapper复制到reducer，这样一个过程被称为shuffle**(这是一个容易令人困惑的术语，它并不完全与洗牌一样，<u>在MapReduce中其实没有随机性</u>)。

​	reduce任务从 mapper中获取文件并将它们合并在一起，同时保持数据的排序。因此，<u>如果不同的 mapper使用相同的关键字生成记录，则这些记录会在合并后的 reducer输入中位于相邻位置</u>。

​	reducer通过关键字和迭代器进行调用，而迭代器逐步扫描所有具有相同关键字的记录(某些情况下可能无法完全在内存中完成)。 reducer可以使用任意逻辑来处理这些记录，并且生成任意数量的输出记录。这些输出记录被写入分布式文件系统中的文件(通常是在运行 reducer机器的本地磁盘中的一个拷贝，在其他机器上存在副本)。

#### * MapReduce工作流

​	单个MapReduce作业可以解决的问题范围有限。回顾一下日志分析的例子，一个MapReduce作业可以确定每个URL页面的浏览次数，但不是最受欢迎的那些URL，因为这需要第二轮排序。

​	因此，将 Mapreduce作业链接到工作流中是非常普遍的，这样，<u>作业的输出将成为下个作业的输入</u>。 Hadoop mapReduce框架对工作流并没有任何特殊的支持，所以链接方式是通过目录名隐式完成的：第一个作业必须配置为将其输出写入HDFS中的指定目录，而第二个作业必须配置为读取相同的目录名作为输入。从 MapReduce框架的角度来看，它们仍然是两个独立的作业。

​	因此，<u>链接方式的MapReduce作业并不像UNIX命令流水线(它直接将一个进程的输出作为输入传递至另一个进程，只需要很小的内存缓冲区)，而更像是一系列命令，其中每个命令的输出被写入临时文件，下一个命令从临时文件中读取</u>。这种设计有利有弊，我们将在本章后面的“中间状态实体化”中讨论。	

​	**只有当作业成功完成时，批处理作业的输出才会被视为有效(MapReduce会丢弃失败作业的部分输出)**。<u>因此，工作流中的一个作业只有在先前的作业(即生成其输入目录的作业)成功完成时才能开始</u>。为了处理这些作业执行之间的依赖关系，已经开发了各种Hadoop的工作流调度器，包括Oozie， Azkaban，Luigi，Airflow和Pinball。

​	这些调度程序还具有管理功能，在维护大量批处理作业时非常有用。在构建推荐系统时，由50到100 MapReduce作业组成的工作流是非常常见的。而在大型组织中，许多不同的团队可能运行不同的作业来读取彼此的输出。良好的工具支持对于管理如此复杂的数据流非常重要。

​	Hadoop的各种高级工具(如Pig，Hive， Cascading， Crunch和FlumeJava)则支持设置多个MapReduce阶段的工作流，这些不同的阶段会被恰当地自动链接起来。

### 10.2.2 Reduce端的join与分组

​	我们在第2章中讨论了数据模型和查询语言的联结操作，但是我们还没有深入探讨join是如何实现的。现在我们将再次开始这个话题。

​	在许多数据集中，通常一条记录会与另一条记录存在关联，例如：关系模型中的外键，文档模型中的文档引用或图模型中的边。只要有代码需要访问该关联两边的记录(包含引用的记录和被引用的记录)，那么就需要join操作。正如第2章所讨论的那样，**反规范化可以减少对join的需求，但通常无法完全避免join**。

​	在数据库中，如果执行的杳询只涉及少量记录，那么数据库通常会使用索引来加速查找(请参阅第3章)。如果査询涉及到join臊作，则可能需要对多个索引进行査找。然而， MapReduce没有索引的概念，至少不是通常意义上的索引。

​	当给定一组文件作为MapReduce输入时，它读取所有文件的全部内容；数据库将其称作全表扫描。如果只想读取少量记录，则与索引查找相比，全表扫描的成本非常昂贵。但是，分析査询(参阅第3章“事务处理与分析处理”)通常需要计算大量记录的聚合。在这种情况下，扫描整个数据集是比较合理的，特别是如果可以在多台机器上并行处理。

​	在批处理的背景下讨论join时，我们主要是解决数据集内存在关联的所有事件。例如，假设一个作业是同时为所有用户处理数据，而不仅仅是为一个特定的用户查找数捃(这可以通过索引更高效地完成)。

#### 示例：分析用户活动事件

​	图10-2 给出了批处理作业中典型的join示例。图中左侧是事件日志，描述登录用户在网站上的活动(称为活动事件或单击流数据)，右侧是用户数据库。可以将此示例视为一种星型模式的一部分(请参阅第3章的“星型与雪花型分析模式”)：<u>事件日志是事实数据表，用户数据库是维度之一</u>。

![MapReduce和分布式文件系统 - 图2](https://static.sitestack.cn/projects/ddia/img/fig10-2.png)

​	分析任务可能需要将用户活动与用户描述信息相关联：例如，如果描述中包含用户年龄或出生日期，则系统可以确定哪些年龄组最受欢迎。但是，活动事件中仅包含用户标识，而不包含完整的用户描述信息。而在每个活动事件中嵌入这些描述信息又会太浪费。因此，活动事件需要与用户描述数据库进行join。

​	<u>join的最简单实现是逐个遍历活动事件，并在(远程服务器上的)用户数据库中查询每个遇到的用户ID。该方案首先是可行的，但性能会非常差：吞吐量将受到数据库服务器的往返时间的限制，本地缓存的有效性将很大程度上取决于数据的分布，并且同时运行的大量并行查询很容易使数据库不堪重负</u>。

​	**为了在批处理过程中实现良好的吞吐量，计算必须(尽可能)在一台机器上进行。如果通过网络对每条记录进行随机访问则请求太慢。而且，考虑到远程数据库中的数据可能会发生变化，查询远程数据库意味着会增加批处理作业的不确定性**。

​	因此，**更好的方法是获取用户数据库的副本(例如，使用ETL进程从数据库备份中提取数据，请参阅第3章的“数据仓库”)，并将其放入与用户活动事件日志相同的分布式文件系统。然后，可以将用户数据库放在HDFS中的一组文件中，并将用户活动记录放在另一组文件中，使用 MapReduce将所有相关记录集中到一起，从而有效地处理它们**。

#### * 排序-合并join

​	回想一下， mapper的目的是从每个输入记录中提取关键字和值。在图10-2的情况下这个关键字就是用户ID：一组mapper会扫描活动事件(提取用户ID作为关键字，而活动事件作为值)，而另一组 mapper将会遍历用户数据库(提取用户ID作为关键字，用户出生日期作为值)。过程如图10-3所示。

![MapReduce和分布式文件系统 - 图3](https://static.sitestack.cn/projects/ddia/img/fig10-3.png)

​	当MapReduce框架通过关键字对 mapper输出进行分区，然后对键值对进行排序时，结果是所有活动事件和用户ID相同的用户记录在reducer的输入中彼此相邻。 MapReduce作业甚至可以对记录进行排序，以便reducer会首先看到用户数据库中的记录，然后按时间戳顺序查看活动事件，这种技术称为**次级排序**。

​	<u>然后reducer可以很容易地执行真正的join逻辑：为每个用户ID调用一次 reducer函数</u>。由于次级排序，第一个值应该是来自用户数据库的出生日期记录。 Reducer将出生日期存储在局部变量中，然后使用相同的用户ⅠD遍历活动事件，输出相应的已观看网址和观看者年龄。随后的MapReduce作业可以计算每个URL的查看者年龄分布，并按年龄组进行聚类。

​	<u>由于 reducer每次处理一个特定用户ID的所有记录，因此只需要将用户记录在内存中保存一次，而不需要通过网络发出任何请求。这个算法被称为排序-合并join，因为mapper的输出是按关键字排序的，然后 reducer将来自join两侧的已排序记录列表合并在一起</u>。

#### 将相关数据放在一起

​	**在排序合并join中， mapper和排序过程确保将执行特定用户ID join操作的所有必要数据都放在一起，这样就只需要一次reducer调用**。<u>因为所有需要的数据已经预先排列好，所以 reducer是一段相当简单的单线程代码，以**高吞吐量**和**低内存开销**来处理记录</u>。

​	一种理解这种架构的方法是mapper发送“消息”给 reducer。当mapper发出一个键值对时，关键字就像传递值的目标地址一样。即使关键字只是一个任意字符串(不是那种像IP地址和端口一样的实际网络地址)，但是它的行为就像一个地址：所有具有相同关键字的键值对都将被传送到相同的目的地(即对reducer的调用)。

​	<u>**使用MapReduce编程模型将计算中的物理网络通信部分(从正确的机器获取数据)从应用逻辑(处理数据)中分离出来**</u>。这种分离与数据库的典型使用方式形成了鲜明对比：从数据库中获取数据的请求经常发生在应用程序代码的深处。<u>由于 MapReduce能够处理所有的网络通信，因此它也避免了在应用程序代码中处理局部故障，例如**某个节点的崩溃: MapReduce会在不影响应用程序逻辑的情况下透明地重试失败的任务**</u>。

#### 分组

​	**除了join之外，“将相关数据放在一起”模式的另一个常见用法是通过某个关键字(如SQL中的 GROUP BY子句)对记录进行分组。所有具有相同关键字的记录形成一个组，然后在每个组内执行某种聚合操作**，例如：

+ 计算每个组中记录的数量[例如，在统计页面视图的示例中，SQL将其表示为`COUNT(*)`聚合]。
+ 对SQL中的特定字段进行求和[`SUM(fieldname)`]。
+ 根据排名函数选择前k个记录。

​	<u>使用MapReduce实现这种分组操作的最简单方法是设置 mapper，使其生成的键值对使用所需的分组关键字</u>。然后，分区和排序过程将相同reducer中所有具有相同关键字的记录集合在一起。因此，在 MapReduce上实现的分组和join看起来非常相似。

​	<u>分组的另一个常见用途是收集特定用户会话的所有活动事件，以便发现用户的活动序列，称为**会话流程**</u>。例如，可以使用这种分析来确定选择网站新版本的用户是否比选择旧版本(A/B测试)的用户更有可能产生购买行为，或计算某个营销活动是否有效。

​	如果有多个Web服务器处理用户请求，则特定用户的活动事件很可能分散在各个不同服务器的日志文件中。可以通过使用会话 cookie、用户ID或类似的标识符作为分组关键字来实现访问流程，将特定用户的所有活动事件放在一起，同时将不同用户的事件分配到不同的分区。

#### * 处理数据倾斜

​	如果与单个关键字相关的数据量非常大，那么会破坏掉“将所有具有相同关键字的记录放在一起”的模式。例如，在社交网络中，大多数用户会有上百人关注者，但少数名人则可能有数百万的追随者。这种不成比例的活跃数据库记录被称为**关键对象**或**热键**。

​	在单个reducer中收集与名人相关的所有活动(例如对他们发布内容的回复)可能会导致严重的**数据倾斜**(也称为热点)。也就是说，某个reducer必须处理比其他 reducer更多的记录(请参阅第6章的“负载倾斜与热点”)。**由于 MapReduce作业只有在其所有mapper和reducer都完成时才能完成，因此<u>所有后续作业必须等待最慢的reducer完成之后才能开始</u>**。

​	<u>如果join输入中存在热键，则可以使用算法进行补偿。例如，Pig中的倾斜join方法首先运行一个抽样作业来确定哪些属于热键。**在真正开始执行join时， mapper将任何与热键有关的记录发送到随机选择的若干个reducer中的一个(传统 MapReduce基于关键字哈希来确定性地选择 reducer)**。对于join的其他输入，与热键相关的记录需要被复制到所有处理该关键字的reducer中</u>。

​	**这种技术将处理热键的工作分散到多个reducer上，可以更好地实现并行处理，代价是不得不将join的其他输入复制到多个reducer**。Crunch中的共亨join方法与此类似，但需要明确指定热键，而不是使用抽样作业。<u>这种技术也非常类似我们在第6章“负载倾斜与热点”所讨论的技术，**使用随机化来缓解分区数据库中的热点**</u>。

​	<u>Hive的倾斜join优化采取了另一种方法。它需要在表格元数据中明确指定热键，并将与这些键相关的记录与其余文件分开存放。在该表上执行join时，它将对热键使用map端join(请参阅下一节)</u>。

​	<u>使用热键对记录进行分组并汇总时，可以分两个阶段进行分组。第一个MapReduce阶段将记录随机发送到 reducer，以便毎个 reducer对热键的记录子集执行分组，并为毎个键输出更紧凑的聚合值。然后第二个 MapReduce作业将来自所有第一阶段reducer的值合并为每个键的单一值</u>。

### * 10.2.3 map端join操作

​	上一节描述的join算法在 reducer中执行实际的join逻辑，因此被称为**reducer端join**。mapper负责准备输入数据：从毎个输入记录中提取关键字和值，将键值对分配给reducer分区，并按关键字排序。

​	**Reduce端join方法的优点是不需要对输入数据做任何假设：无论其属性与结构如何，mapper都可以将数据处理好以谁备join**。然而，<u>**不利的一面是，所有这些排序，复制到reducer以及合并reducer输入可能会是非常昂贵的操作，这取决于可用的内存缓冲区，当数据通过MapReduce阶段时，数据可能需要写入磁盘若干次**</u>。

​	另一方面，如果可以对输入数据进行某些假设，则可以通过使用所谓的**map端join**来加快速度。这种方法使用了一个<u>缩减版本的 MapReduce作业，其中没有 reducer，也没有排序；相反，每个 mapper只需从分布式文件系统中读取输入文件块，然后将输出文件写入文件系统即可</u>。

#### * 广播哈希join

​	<u>实现map端join的最简单方法特别适合**大数据集与小数据集join**，尤其是小数据集能够全部加载到每个 mapper的内存中</u>。

​	例如，假设对于图10-2情况，用户数据库可以完全放入内存。在这种情况下，当mapper程序执行时，它可以首先将用户数据库从分布式文件系统读取到内存的哈希表中。然后， mapper程序扫描用户活动事件，并简单地查找哈希表中每个事件的用户ID。

​	<u>Map任务依然可以有多个：大数据集的每个文件块对应一个mapper(在图10.2的例子中，活动事件是大输入数据集)。**每个mapper还负责将小数据集全部加载到内存中**</u>。

​	<u>这种简单而有效的算法被称为**广播哈希join**：“**广播**”一词主要是指大数据集每个分区的 mapper还读取整个小数据集(即小数据集实际被“广播”给大数据集)，“**哈希**”意味着使用哈希表</u>。有多种系统都支持该方法，包括Pig(称为 replicated join)，Hive(称为 Map Join)， Cascading和Crunch等。它也用于数据仓库查询引擎，例如Impala。

​	<u>另一种方法并不需要将小数据集加载至内存哈希表中，而是将其保存在本地磁盘上的只读索引中。由于频繁访问，索引大部分内容其实是驻留在操作系统的页面缓存中，因此这种方法可以提供与内存哈希表几乎一样快的随机访问性能，而实际上并不要求整个数据集读入内存</u>。

#### * 分区哈希join

​	**如果以相同方式对map端join的输入进行分区，则哈希join方法可以独立作用于毎个分区**。在图10-2的情况下，可以根据用户ID的最后一位十进制数字(因此每一边都有10个分区)来分配活动事件和用户数据库中的记录。例如， Mapper 3首先将所有以3结尾的ID的用户加载到哈希表中，然后扫描ID以3结尾的每个用户的所有活动事件。

​	**如果分区操作正确完成，就可以确定所有要join的记录都位于相同编号的分区中，因此毎个mapper只需从毎个输入数据集中读取一个分区就足够了。这样的优点是每个mapper都可以将较少的数据加载到其哈希表中**。

​	**<u>这种方法只适用于两个join的输入具有相同数量的分区，根据相同的关键字和相同的哈希函数将记录分配至分区</u>**。如果输入是由之前已经执行过这个分组的MapReduce作业生成的，那么这是一个合理的假设。

​	分区哈希join在Hive中称为**bucketed map join**。

#### * map端合并join

​	如果输入数据集不仅以相同的方式进行分区，而且还基于相同的关键字进行了排序，则可以应用map端join的另一种变体。这时，输入是否足够小以载入内存并不重要，因为 **mapper可以执行通常由Reducer执行的合并操作：按关键宇升序增量读取两个输入文件，并且匹配具有相同关键字的记录**。

​	如果map端合并join是可能的，则意味着先前的 MapReduce作业会首先将输入数据集引入到这个经过分区和排序的表单中。<u>原则上，join可以在之前作业的 reduce阶段进行。但是，在独立的map作业中执行合并join更为合适，例如，除了特定的join操作之外，分区和排序后的数据集还可用于其他目的</u>。

#### * 具有map端join的MapReduce工作流

​	当下游作业使用 MapReduce join的输出时，map端或 reduce端 join的不同选择会影响到输出结构。 **reduce端join的输岀按join关键宇进行分区和排序，而map端join的输岀按照与大数据集相同的方式进行分区和排序**(因为对大数据集的每个文件块都会启动个map任务，无论是使用分区join还是广播join)。

​	正如所讨论的，map端join也存在对输入数据集的大小、排序和分区方面的假设。在优化join策略时，了解分布式文件系统中数据集的物理布局非常重要：仅仅知道编码格式和数据存储目录的名称是不够的；还必须知道数据分区数量，以及分区和排序的关键字。

​	**在Hadoop生态系统中，关于数据集分区的元数据经常在HCatalog和Hive metastore中维护**。

### 10.2.4 批处理工作流的输出

​	我们已经讨论了很多关于实现 MapReduce工作流的算法，但仍然忽略了一个重要问题：一旦作业完成，处理的最终结果是什么？为什么要把这些作业放在首位？

​	数据库査询中，根据分析目的我们区分了事务处理(OLTP)和分析型处理(参阅第3章“事务处理与分析处理”)。我们看到OLTP查询通常使用索引按关键字查找少量记录，然后将査询结果呈现给用户(例如在网页上)。另一方面，分析查询通常会扫描大量记录，执行分组与聚合，输出完整的报告，例如：显示度量随时间变化的图表，或某种排名中的前10项，或将某一数量分解成若干子类别。这种报告的消费者通常是需要做出商业决策的分析师或经理。

​	批处理即不是事务处理，也不是分析，那么，把它放在哪里更合适？批处理与分析更为接近，因为批处理过程通常会扫描大部分的输入数据集。但是， MapReduce作业的工作流与分析中SQL查询不同(请参阅本章后面的“对比 Hadoop与分布式数据库”)。批处理过程的输出通常不是报告，而是其他类型的数据结构。

#### * 生成搜索索引

​	<u>Google最初使用MapReduce的目的是为其搜索引擎建立索引，这个索引被实现为5到10个MapReduce作业的工作流</u>。尽管 Google后来不再使用 MapReduce，但是如果从构建搜索索引的角度来看 MapReduce，会更加有助于理解 MapReduce(<u>即使在今天， Hadoop MapReduce仍然是构建Lucene/Solr索引的好方法</u>)。

​	在第3章的“全文搜索与模糊索引”中简要讨论了像 Lucene这样的全文搜索索引是如何工作的：它是一个文件(术语字典)，可以在其中有效地查找特定关键字，并找到包含该关键字的所有文档ID列表(发布列表)。这是一个非常简单的搜索索引视图，实际上它还需要各种附加数据，以便按相关性对检索结果进行排序、拼写检査、解析同义词等等，但基本原则类似。

​	<u>如果需要对一组固定文档进行全文检索，则批处理是构建索引的有效方法：mapper根据需要对文档集进行分区，每个 reducer构建其分区索引，并将索引文件写入分布式文件系统。并行处理非常适用于构建这样的文档分区索引</u>(请参阅第5章的“分区与级索引”)。

​	**由于按关键字査询搜索索引是只读操作，因此这些索引文件一旦创建就是不可变的**。<u>如果索引的文档集合发生更改，则可以选择定期重新运行整个索引工作流，并在完成后用新的索引文件批量替换之前的索引文件。如果只有少量文档发生了变化，这种方法在计算上可能比较昂贵，但是它的优点是索引过程非常清晰合理：文档作为输入，索引作为输出</u>。

​	<u>增量建立索引是一种替代方法。如第3章所述，如果要添加、删除或更新索引中的文档， Lucene会生成新的段文件，并在后台异步合并和压缩段文件</u>。我们将在第11章中看到更多这样的增量处理。

#### * 批处理输出键值

​	搜索索引只是批处理工作流输出的一个示例。批处理的另一个常见用途是**构建机器学习系统**，如**分类器**(例如垃圾邮件过滤器，异常检测，图像识别)和**推荐系统**(例如你可能认识的人，你可能感兴趣的产品或相关搜索)。

​	这些批量作业的输出通常是某种数据库：例如，在用户数据库中通过用户ID进行查询以获取建议的好友，或者在产品数据库中通过产品ID查询以获取相关产品列表。

​	查询数据库需要在处理用户请求的Web应用中进行，而这些请求通常与 Hadoop基础架构是分离的。那么批处理过程的输出如何返回至数据库中以供Web应用杳询？

​	最明显的选择可能是直接在 mapper或 reducer中使用你最喜欢的数据库客户端软件包，而批处理作业则直接写入至数据库服务器，**一次写入一条记录**。这样的方法可行(假设防火墙规则允许从 Hadoop环境直接访问生产数据库)，但由于以下原因，这并不是一个好方案：

+ **正如前面讨论join时提到的，为每个记录发送一个网络请求比批处理任务的正常吞吐量要慢几个数量级。即使客户端软件包支持批处理，性能也可能很差**。
+ **MapReduce作业经常并行处理许多任务。如果所有的 mapper或 reducer都同时写入同一个输出数椐库，并以批处理期望的速率写入，那么数据库很容易过载，其查询性能会受到影响。这可能会进一步导致系统其他部分的操作问题**。
+ <u>**通常情况下， MapReduce为作业输出提供了一个干净的“全有或全无”的保证**：如果作业成功，则结果就是只运行一次任务的输出，即使中间发生了某些任务失败但最终重试成功。如果整个作业失败，则不会产生输出</u>。**然而，从作业内部写入外部系统会产生外部可见的副作用，而这种副作用无法彻底屏蔽。因此，<u>将不得不担心部分完成的作业产生对其他系统可见的结果</u>，以及 Hadoop任务尝试和推测性执行的复杂性**。

​	**更好的解决方案是，批处理作业创建一个全新的数据库，并将其作为文件写入分布式文件系统中的作业输出目录，就像上一节的搜索索引一样**。这些数据文件一旦写入就是不可变的，可以批量加载到处理只读查询的服务器中。各种键值存储都支持构建MapReduce作业中的数据库文件，包括Voldemort， Terrapin， ElephantDB和HBase批量加载。

​	<u>构建数据库文件是很好的MapReduce使用方法：使用 mapper提取关键字，然后使用该关键宇进行排序，这已经成为**构建索引**的必要步骤</u>。由于大多数键值存储是只读的(文件只能由批处理作业一次写入，而且是不可变的)，所以数据结构非常简单，例如，它们不需要预写日志(请参阅第3章的“可靠的B-tree”)。

​	将数据加载到Voldemort时，服务器将继续向旧数据文件发起请求，同时将新数据文件从分布式文件系统复制到服务器的本地磁盘。一旦复制完成，服务器会自动切换到新文件进行查询。如果在这个过程中出现任何问题，它可以很容易地再次切换回旧文件，因为这些旧文件依然存在，而且<u>不可变</u>。

#### * 批处理输出的哲学

​	本章前面讨论过的UNIX设计晢学倡导明确的数据流：一个程序读取输入并写回输出。在这个过程中，输入保持不变，任何以前的输出都被新输出完全替换，并且没有其他副作用。这意味着可以随心所欲地重新运行一个命令，进行调整或调试，而不会扰乱系统状态。

​	MapReduce作业的输出处理遵循相同的原理。将输入视为不可变，避免副作用(例如对外部数据库的写入)，批处理作业不仅实现了良好的性能，而且更容易维护：

+ **如果在代码中引入了漏洞，输出错误或者损坏，那么可以简单地回滚到先前版本，然后重新运行该作业，将再次生成正确的输出；或者更简单的办法是将旧的输出保存在不同的目录中，然后切换回原来的目录**。相比之下，具有读写事务的数据库就不具有这样的属性：如果部署了将错误数据写入数据库的错误代码，回滚代码并不能修复数据库中的数据。能够从错误代码中恢复的思想被称为**人为容错性**。
+  与发生错误即意味着不可挽回的损害相比，易于回滚的特性更有利于快速开发新功能。这种使不可逆性最小化的原则对于敏捷开发是有益的。
+ 如果map或reduce任务失败， MapReduce框架会自动重新安排作业并在同一个输入上再次运行。如果失败是由于代码漏洞造成的，那么它会一直崩溃，最终导致作业在数次尝试之后失败。但是如果故障是由于暂时问题引起的，则可以实现容错。**由于输入总是不可变，这种自动重试是安全的，而失败任务的输出则被MapReduce框架丢弃**。
+ **相同的文件可用作各种不同作业的输入**，其中包括监控作业，它可以搜集相关运行指标，并评估其他作业的输出是否满足预期特性(例如，将其与前一次运行的输出进行比较并测量差异)。
+ **与UNIX工具类似， MapReduce作业将逻辑与连线(配置输入和输出目录)分开，从而可以更好地隔离问题**，重用代码：一个团队可以专注于实现“做好一件事”的工作，而其他团队可以决定何时何地运行该作业。

​	在这些方面，那些UNIX验证过的设计原则似乎用在 Hadoop上也同样效果很好，但UNIX和 Hadoop在某些方面存在不同。例如，因为大多数UNIX工具都假定输入为没有类型的文本文件，所以必须做大量的输入解析工作(本章开头的日志分析示例使用`{print $7}`来提取URL)。在 Hadoop中，通过使用更多结构化的文件格式，可以消除一些低价值的语法转换：Avro(请参阅第4章的“Avro”)和 Parquet(请参阅第3章的“列式存储”)通常用于提供高效的基于模式的编码，并支持模式的不断演变(参见第4章)。

### 10.2.5 对比Hadoop与分布式数据库

​	正如我们所看到的， Hadoop有点像UNIX的分布式版本，其中HDFS是文件系统，而MapReduce则是UNIX进程的特殊实现(<u>它总是在map阶段和reduce阶段之间运行sort工具</u>)。我们讨论了如何在这些原语之上实现各种join和分组操作。

​	MapReduce论文发表时，在某种意义上来说它并不新鲜。我们在前几节讨论的所有处理和并行join算法都已经在十多年前所谓的大规模并行处理(MPP)数据库中实现了。 Gamma数据库机器， Teradata和 Tandem NonStop SQL是这方面的先驱。

​	**这其中最大的区别在于：**

+ **MPP数据库专注于在一个机器集群上并行执行SQL查询分析，**
+ **而 MapReduce和分布式文件系统的结合则更像是一个可以运行任意程序的通用操作系统**。

#### * 存储多样性

​	**数据库要求根据特定的模型(例如，关系或文档)来构造数据，而分布式文件系统中的文件只是字节序列，可以使用任何数据模型和编码来编写**。它们可能是数据库记录的集合，也同样可以是文本、图像、视频、传感器读数、稀疏矩阵、特征向量、基因组序列或任何其他类型的数据。

​	<u>更直接地讲， Hadoop开放了将数据不加区分地转存到HDFS的可能性，之后再去考虑如何进一步处理数据。相比之下，在将数据导入数据库的专有存储格式之前，MPP数据库通常需要对欻椐和査询模式进行仔细的前期建模</u>。

​	从纯粹主义者的角度来看，这种仔细的建模和导入方式似乎是可取的，因为这意味着数据库用户拥有质量更好的数据。**<u>然而在实践中，看起来只是简单地使数据可用；而即使是一个古怪的，难以使用的原始格式往往也比预先决定理想的数据模型更有价值</u>**。

​	这个想法与数据仓库类似(请参阅第3章的“数据仓库”)：只将大型组织中各个部分的数据集中在一起是非常有价值的，因为它可以在之前完全分离的数据集执行join操作。<u>MPP数据库所秉持的谨慎设计模式减慢了集中式数据的收集速度</u>；**因此，仅仅以原始形式收集数据，之后再考虑模式设计，从而使收集数据的速度加快(**这种形式有时也被称为“**数据湖**”或“企业数据中心”)。

​	<u>不加区分地数据转储也转移了**数据解释**的负担：不是强迫数据集的**生产者**将其转化为标准化格式，而是将解释数据变为**消费者**的问题(**读时模式**，请参阅第2章的“文档模型中的模式灵活性”)</u>。如果生产者和消费者分属不同优先级的团队，这会是个优势。甚至也可能并不存在一个理想的数据模型，而是针对不同的目的产生对数据不同的看法。<u>以原始形式简单地转储数据可以进行多次这样的转换。这种方法被称为**寿司原则**</u>。

​	因此， <u>**Hadoop经常被用于实现ETL过程**(参见第3章“数据仓库”)：来自事务处理系统的数据以某种原始形式转储到分布式文件系统中，然后编写MapReduce作业进行数据清理，将其转换为关系表单，并将其导入MPP数据仓库以进行分析</u>。**数据建模仍然会发生，但它位于一个单独步骤中，与数据收集是分离的。由于分布式文件系统支詩以任何格式编码的数据，所以这种解耦是可行的**。

#### 处理模型的多样性

​	<u>MPP数据库属于一体化、紧密集成的软件系统，包括磁盘存储布局、查询计划、调度和执行</u>。由于这些组件都可以针对数据库的特定需求进行调整和优化，因此整个系统可以在其设计的査询类型上实现非常好的性能。此外，SQL查询语言支持表达式查询，以及优雅的语义，而无需编写代码，因此，业务分析人员使用的图形化工具(如Tableau)即可执行查询。

​	另一方面，并非所有类型的处理都可以合理地表达为SQL查询。例如，如果正在构建机器学习和推荐系统，或具有相关性排名模型的全文搜索索引，或执行图像分析，则很可能需要更具一般性的数据处理模型。这些类型的处理通常特定于专门的应用程序(例如机器学习的特征工程，机器翻译的自然语言模型，欺诈预测的风险评估功能)，所以不可避免地需要编写代码，而不仅仅是查询。

​	MapReduce使工程师能够轻松地在大型数据集上运行自己的代码。<u>如果你有HDFS和MapReduce，可以在它上面建立一个SQL查询执行引擎，事实上这就是Hive项目所做的事情</u>。也可以编写许多其他形式的不适合用SQL查询表示的批处理。

​	随后，人们发现 MapReduce对于某些类型的处理局限性太大，而且执行得太差，因此在 Hadoop之上开发了各种其他的处理模型(我们将在本章后面的“超越 MapReduce中展开讨论)。仅有SQL和 MapReduce两种处理模型还不够，我们需要更多不同的模型！而且由于 Hadoop平台的开放性，实施一整套处理方法也是可行的，但对于一体化的MPP数据库来说则力所不及。

​	<u>至关重要的是，这些不同的处理模型都可以在一个共享的机器集群中运行，所有机器都可以访问分布式文件系统上的相同文件。在 Hadoop方法中，不需要将数据导入到几个不同的专用系统中进行不同类型的处理：系统已经足够灵活，可以支持同一集群中不同的工作负载。无需移动数据使得从数据中获取价值变得容易得多，而且更容易使用新的处理模型进行测试</u>。

​	Hadoop生态系统包括可以随机访问的OLTP数据库，如HBase(请参阅第3章的“ SSTables和 LSM-tree”)以及MPP模式的分析数据库，如Impala。 HBase和Impala都不使用MapReduce，但都使用HDFS进行存储。尽管它们访问和处理数据的方法差异很大，但是可以共存并被集成到同一个系统中。

#### * 针对频繁故障的设计

​	在比较MapReduce和MPP数据库时，设计方法的另外两个不同点是：如何处理故障和如何使用内存和磁盘。<u>与在线系统相比，批处理对故障的敏感度较低，因为如果遇到失败的任务，它们不会立即影响用户，而且总是可以重新运行</u>。

​	<u>如果一个节点在执行查询时崩溃，大多数MPP数据库会中止整个查询，并让用户重新提交査询或自动重新运行査询</u>。由于查询通常最多运行几秒钟或几分钟，所以这种处理错误的方式是可以接受的，因为重试的代价不是太大。<u>MPP数据库还倾向于在内存中保留尽可能多的数据(例如使用**哈希join**)以避免磁盘读取的成本</u>。

​	另一方面， **MapReduce可以容忍map或reduce任务的失败，通过以单个任务的粒度重试工作来避免影响整体作业。它也会期望将数据写入磁盘，一方面是为了容错，另一方面是假设数据集太大而不能载入内存**。

​	**MapReduce方法更适用于较大的作业**：处理大量的数据并运行很长时间的作业，以至于在此过程中可能至少会遇到一次任务故障。在这种情况下，<u>由于单个任务失败而重新运行整个工作将是巨大的浪费。虽然以单个任务粒度进行恢复带来的开销会使无故障处理变得更慢，但如果任务失败率足够高，这仍然是一种合理的权衡方式</u>。

​	但是这些假设的现实性究竟如何呢？在大多数集群中，机器故障确实在发生，但并不是很频繁，可能很少发生，大多数作业都不会遇到机器故障。为了容错，真的值得引入这么大的开销吗?

​	要了解MapReduce节省使用内存和任务级恢复的背后原因，查看最初设计MapReduce的环境会很有帮助。 Google拥有混合使用的数据中心，在线生产服务和离线批处理作业在同一台机器上运行。<u>任务通过使用容器来实现资源分配(CPU，内有，磁盘空间等)。毎个任务都具有优先级，如果优先级较高的任务需要更多的资源，则可以终止(抢占)同一机器上较低优先级的任务以释放资源。优先级还决定了计算资源的定价：团队必须为他们使用的资源付费，而优先级更高的任务花费也会更多</u>。

​	这种架构允许非生产(低优先级)计算资源被过度使用，因为系统知道可以在必要时回收资源。与那些隔离生产和非生产任务的系统相比，过度使用资源反过来可以更好地利用机器和提高效率。<u>但是，由于MapReduce作业以低优先级运行，当优先级较高的进程需要其资源时，随时都可以发生抢占。**批处理任何工作可以有效地利用高优先级进程剩下的任何可用计算资源**</u>。

​	在Google，运行一个小时的MapReduce任务大约有5%被终止的风险，<u>为更高优先级的进程腾出空间</u>。这个比例要比由于硬件问题、机器重启或其他原因导致的故障率高出一个数量级。以这样的抢占率，如果一个作业有100个任务，每个任务运行10分钟，那么至少有一个任务在完成之前将被终止的风险大于50%。

​	**<u>这就是为什么 MapReduce被设计为容忍意外任务终止的原因：不是因为硬件特别不可靠，而是因为任意终止进程的灵活性能够更好地利用集群资源</u>**。

​	而**在开源集群调度器中，抢占的使用情况则相对较少**。YARN的Capacity Scheduler支持抢占以平衡不同队列的资源分配。但在编写本书时，YARN， Mesos或Kubernetes不支持通用优先级抢占。**在任务不经常被终止的环境中， MapReduce的设计决策没有多少意义**。在下一节中，我们将看看 MapReduce的一些替代方法，这些替代方案做出了不同的设计决定。

## 10.3 超越MapReduce

​	尽管MapReduce在20世纪末变得非常流行并被大量炒作，但它只是分布式系统的许多可能的编程模型之一。取决于具体的数据量、数据结构以及处理类型，其他工具可能更适合特定的计算。

​	尽管如此，我们仍然在讨论MapReduce上花了很多篇幅，因为它是一个有用的学习工具，因为它是分布式文件系统的一个相当清晰和简单的抽象。在这里，简单是指从理解它在做什么的角度来说，而不是容易使用的角度。事实上与容易使用恰恰相反：使用原始的 MapReduce API来实现一个复杂的处理任务是相当困难和费力的，例如，你需要从头开始实现全部join算法。

​	针对直接使用的困难，在 MapReduce上创建了各种高级编程模型(Pig，Hive，Cascading， Crunch)进一步封装抽象。如果了解 MapReduce的工作原理，那么学习这些模型也相当容易，而且它们的高级构造使许多常见的批处理任务更加容易实现。

​	然而，MapReduce执行模型本身也存在一些问题，这些问题并没有通过增加另一个抽象层次而得到解决，而且在某些类型的处理中表现出糟糕的性能。一方面，MapReduce非常强大：可以使用它来处理那些运行在不可靠的多租户系统上、任务频繁终止的超大规模数据，并且仍然可以完成工作(虽然速度很慢)。另一方面，对于某些类型的处理，其他工具则可能要快几个数量级。

​	在本章的剩余部分中，我们将看到一些批处理的替代方案。第11章将转向流处理，这可以看作是加速批处理的另一种方法。

### 10.3.1 中间状态实体化

​	如前所述，<u>每个 MapReduce作业都独立于其他任何作业。作业与其他任务的主要联系点是分布式文件系统上的输入和输出目录</u>。**如果希望一个作业的输出成为第二个作业的输入，则需要将第二个作业的输入目录配置为与第一个作业的输出目录相同，并且外部工作流调度程序必须仅在第一个作业已经完成后才开始第二个作业**。

​	如果第一个作业的输出是要在组织内部广泛分发的数据集，则此设置是合理的。在这种情况下，需要能够通过名称来引用它，并将其用作多个不同作业(包括由其他团队开发的作业)的输入。将数据发布到分布式文件系统中众所周知的位置可以实现松耦合，这样作业就不需要知道谁在生成输出或者消耗输出(请参见本章前面的“逻辑与布线分离”)。

​	<u>但是，在很多情况下，我们知道一个作业的输出只能用作另一个作业的输入，这个作业由同一个团队维护。在这种情况下，分布式文件系统上的文件只是中间状态：一种将数据从一个作业传递到下一个作业的方式。在用于构建由50或100个 MapReduce作业组成的推荐系统的复杂工作流中，存在很多这样的中间状态</u>。

​	<u>将这个中间状态写入文件的过程称为**实体化(或物化)**</u>。我们在第3章“聚合：数据立方体与物化视图”中巳经在物化视图的背景下遇到了这个术语，它主要是指<u>提前计算某些操作的结果并将其写入磁盘，而不是在需要时才进行计算</u>。

​	相比之下，本章开头的日志分析示例使用UNIX管道将一个命令的输出与另一个命令的输入连接起来。管道并不完全实现中间状态，而是只使用一个小的内存缓冲区，逐渐将输出流式传输到输入。

​	与UNIX管道相比， MapReduce完全实体化中间状态的方法有一些不利之处：

+ **MapReduce作业只有在前面作业(生成其输入)中的所有任务都完成时才能启动**，而通过UNIX管道连接的进程同时启动，输出一旦生成就会被使用。<u>不同机器的差异或不同的负载意味着作业中往往会有一些任务需要比其他任务花費更长的时间。必须等待前面作业中所有任务的完成必然会减慢整个工作流的执行</u>。
+ **Mapper通常是冗余的：它们只是读取刚刚由 reducer写入的同一个文件，并为下个分区和排序阶段做准备**。<u>在许多情况下， mapper代码可能是之前 reducer的一部分：如果 reducer的输出被分区和排序的方式与 mapper输出相同，那么不同阶段的 reducer可以直接链接在一起，而不需要与 mapper阶段交错</u>。
+ **将中间状态存储在分布式文件系统中意味着这些文件被复制到多个节点，对于这样的临时数据来说通常是大材小用了**。

#### * 数据流引擎

​	为了解决 MapReduce的这些问题，开发了用于分布式批处理的新的执行引擎，其中最著名的是 Spark，Tez，和 Flink。它们的设计方式有很多不同之处，但有一个共同点：**它们把整个工作流作为一个作业来处理，而不是把它分解成独立的子作业**。

​	<u>由于通过若干个处理阶段明确地建模数据流，所以这些系统被称为数据流引擎。**像MapReduce一样，它们通过反复调用用户定义的函数来在单个线程上一次处理一条记录。它们通过对输入进行分区来并行工作，并将一个功能的输出复制到网络上，成为另一个功能的输入**</u>。

​	与 MapReduce不同，这些功能不需要严格交替map和reduce的角色，而是以更灵活的方式进行组合。我们称为**函数运算符**，数据流引擎提供了多种不同的选项来连接一个运算符的输出到另一个的输入：

+ 一个选项是通过关键字对记录进行重新分区和排序，就像在MapReduce的shuffle阶段一样(请参阅本章前面的“ MapReduce的分布式执行”)。此功能可以像在MapReduce I中一样进行排序-合并join和分组。
+ 另一个可能性是读取若干个输入，并以相同的方式进行分区，但忽略排序。这节省了分区哈希join的工作，其中记录的分区是重要的，但顺序不相关，因为构建哈希表实现了顺序的随机化。
+ 对于广播哈希join，可以将一个运算符的输出发送到join运算符的所有分区。

​	这种处理引擎的风格源于Dryad和Nephele这样的研究系统，与MapReduce模型相比，它有几个优点：

+ **排序等计算代价昂贵的任务只在实际需要的地方进行，而不是在每个map和reduce阶段之间默认发生**。
+ 没有不必要的map任务，因为mapper所做的工作通常可以合并到前面的reduce运算符中(mapper不会更改数据集的分区)。
+ **由于工作流中的所有join和数据依赖性都是明确声明的，因此调度器知道哪些数据在哪里是必需的，因此它可以进行本地优化**。<u>例如，可以尝试将占用某些数据的任务放在与生成它的任务相同的机器上，以便可以通过**共享内存缓冲区**来交换数据，而不必通过网络复制数据。</u>
+ **将运算符之间的中间状态保存在内存中或写入本地磁盘通常就足够了，这比将内容写入HDFS(必须将其复制到多个节点并写入到每个副本所在的磁盘)需要更少的IO**。 MapReduce已经将这种优化用于mapper的输出，但是数据流引擎将该思想推广到了所有的中间状态。
+ **运算符可以在输入准备就绪后立即开始执行，<u>在下一个开始之前不需要等待前个阶段全部完成</u>**。
+ 与 **MapReduce(为每个任务启动一个新的JVM)**相比，现有的Java虚拟机(JVM)进程可以被重用来运行新的运算符，从而减少启动开销。

​	可以使用数据流引擎来执行与MapReduce工作流相同的计算，并且由于这些优化，通常执行速度会明显加快。**<u>由于运算符是map和reduce的一个泛化，相同的处理代码可以在任一执行引擎上运行：在Pig，Hive或 Cascading中所实现的工作流可以通过简单的配置更改从MapReduce切换到Tez或 Spark，而无需修改代码</u>**。

​	<u>Tez是一个相当轻量的库，它依靠YARN的 shuffle服务来实现节点之间的数据复制，而Spark和Flink则是包含网络通信层、调度器和面向用户API的大型框架</u>。我们将很快开始讨论这些高级API。

#### * 容错

​	将中间状态完全实体化到分布式文件系统的一个优点是持久化，这使得在 MapReduce中实现容错变得相当容易：如果一个任务失败了，它可以在另一台机器上重新启动并从文件系统重新读取相同的输入。

​	<u>Spark， Flink和Tez避免将中间状态写入HDFS，所以它们采用不同的方法来容忍错误：如果机器发生故障，并且该机器上的中间状态丢失，则利用其他可用的数据重新计算(例如之前的中间阶段，如果可能的话；或原始输入数据，通常在HDFS上)</u>。

​	**为了实现重新计算，框架必须追踪给定数据是如何计算的，使用了哪个输入分区以及应用了哪个运算符。 Spark使用<u>弹性分布式数据集(Resilient Distributed Dataset，RDD)抽象</u>来追踪数据的祖先，而Flink对运算符状态建立<u>检查点</u>，从而允许将执行过程中遇到故障的运算符恢复运行**。

​	<u>在重新计算数据时，知道计算是否具有**确定性**非常重要</u>：也就是说，如果给定相同的输入数据，那么运算符是否始终产生相同的输出？如果部分丢失的数据已经发送给下游运算符，这个问题就非常关键。如果运算符重新启动，重新计算的数据与原有的丟失数据不一致，下游运算符会很难解决新旧数据之间的冲突。<u>如果出现非确定性运算符，解决方案通常是将下游运算符也终止掉，并在新数据上再次运行它们</u>。

​	**为了避免这种级联故障，<u>最好让运算符具有确定性</u>**。**<u>但是请注意，非确定性行为很容易意外发生</u>**：例如，许多编程语言在迭代哈希表的元素时不能保证任何特定顺序，许多概率和统计算法明确依赖于使用随机数，以及使用系统时钟或外部数据源都是不确定的。<u>为了可靠地从故障中恢复，就需要消除这些不确定性因素，例如通过使用固定的种子产生伪随机数</u>。

​	**<u>通过重新计算数据以实现从故障中恢复并不总是能得到正确的答案：如果中间数据比源数据小得多，或者如果计算量非常大，那么将中间数据转化为文件比重新计算文件的代价要小</u>**。

#### * 关于实体化的讨论

​	回到UNIX的类比，我们看到MapReduce就像是将每个命令的输出写入临时文件，而数据流引擎看起来更像是UNIX管道。尤其Flink是围绕流水线执行的思想而建立起来的，也就是将运算符的输岀递增地传递给其他运算符，并且<u>在开始处理之前不等待输入完成</u>。

​	<u>排序操作不可避免地需要消耗其全部输入，才可以产生输出，因为有可能最后的输入记录具有最小的关键字，因此它必须成为第一个输出记录。**任何需要分类的运算符都需要至少暂时地累积状态。但是工作流的许多其他部分可以以流水线方式执行**</u>。

​	<u>当作业完成时，它的输出需要在某个地方实现持久化，以便用户可以找到并使用它，很可能它会再次写入分布式文件系统</u>。**因此，在使用数据流引擎时，HDFS上的实体化数据集通常仍是作业的输入和最终输出**。<u>**和 MapReduce一样，输入是不可变的，输出被完全替换**</u>。

​	**<u>简单总结一下，数据流对MapReduce的改进是，不需要自己将所有中间状态写入文件系统</u>**。

### 10.3.2 图与迭代处理

​	在第2章中的“类图数据模型”中，我们讨论了使用图来建模数据，并使用图查询语言遍历图中的边和顶点。第2章的讨论集中在OLTP的使用方式上：快速査询某些少量符合特定条件的顶点。

​	在批处理环境中査看图也很有趣，其目标是<u>在整个图上执行某种离线处理或分析</u>。这种需求经常出现在机器学习应用程序(如推荐引擎)或排名系统中。<u>例如，最著名的图分析算法之一是 PageRank，它试图根据链接至某网页的其他网页来评估该网页的受欢迎程度。它是确定网络搜索引擎结果呈现顺序的标准之一</u>。

> 像 Spark， Flink和Tez这样的数据流引擎(参见本章前面的“中间状态实体化”)通常将运算符作为**有向无环图(DAG)**排列在作业中。这与图处理并不一样：**在数据流引擎中，运算符之间的数据流被构造成一个图，数据本身通常由关系式元组构成；而在图处理中，数据本身具有图的形式**。真不幸，又一个由命名引起的混淆！

​	**许多图算法通过一次遍历一个边，将一个顶点与相邻顶点join起来以便传递某种信息，重复该过程直到满足某种条件为止**。例如，直到没有更多的边需要遍历，或者直到某些度量值收敛。我们在图2-6中看过一个例子，它通过迭代跟踪那些能够表明某个位置位于哪些其他位置之内(这种算法被称为传递闭包)的边，列出了数据库中所有位于北美的位置。

​	可以在分布式文件系统(包含顶点和边的列表文件)中存储图，但是这种“重复直到完成”的想法不能用普通的 MapReduce来表示，因为它只执行一次数据传递。这种算法通常需要以**迭代**方式实现：

1. 外部调度程序运行批处理来执行算法的一个步骤。
2. 当批处理过程完成时，调度器检查遍历是否完成(基于特定完成条件，例如没有更多的边要遍历，或者与最后一次迭代相比的变化低于某个阈值)。
3. 如果尚未完成，则调度程序返回到步骤1并运行另一轮批处理。

​	这种方法有效，但是<u>用MapReduce来实现却非常低效，主要是因为**MapReduce没有考虑算法的迭代性质**：即使与上一次迭代相比，只有图的一小部分发生了改变，它也总是读取整个输入数据集并产生一个全新的输出数据集</u>。

#### * Pregel处理模型

​	<u>作为对图数据的批处理优化，计算的批量同步并行(bulk synchronous parallel，BSP)模型已经流行起来，典型系统包括Apache Giraph， Sparke的GraphX API和Flink的 Gelly API。由于最早是 Google的 Pregel论文将这种处理图的方法普及，因此它也被称为 Pregel模型</u>。

​	回想一下在MapReduce中， mapper在概念上“发送消息”给 reducer调用，因为框架将所有具有相同关键字的 mapper输出集中在一起。 **Pregel中的想法与此类似：一个顶点可以“发送消息”到另一个顶点，通常这些消息沿着图的边被发送**。

​	**在每次迭代中，<u>为毎个顶点调用函数</u>，将所有发送至该顶点的消息传递给它，就像调用reducer一样**。<u>与 MapReduce不同之处在于， **Pregel模型的迭代过程中，顶点保存它在内存中的状态，所以函数只需要处理新输入的消息**</u>。如果图的某个部分没有收到发送消息，则不需要做任何工作。

​	这与参与者模型有些相似(请参阅第4章的“分布式Actor model”)，可以将每个节点视为参与者，不同之处在于，<u>顶点状态和顶点之间的消息具有**容错性**和**持久性**，并且通信以固定的方式进行：毎一轮迭代中，框架会将前一次迭代中的所有消息都发送出去</u>。 Actor model通常没有这样的时序保证。

#### * 容错

​	**事实上，顶点只能通过消息传递进行通信(而不是通过彼此间的直接查询)有助于提高 Pregel作业的性能，因为消息可以被批量处理，并且等待通信的次数会减少**。<u>唯一的等待是在迭代之间：由于 **Pregel模型保证在一次迭代中发送的所有消息都会在下次迭代中被发送，所以先前的迭代必须全部完成，而且所有的消息必须在下一次迭代开始之前复制到网络中**</u>。

​	即使底层网络可能会丢弃、重复或任意延迟消息(请参阅第8章的“不可靠的网络”)，但 Pregel的实现可以保证在后续迭代中消息在目标顶点**<u>只会被处理一次</u>**。像MapReduce一样，该框架透明地从故障中恢复，以简化Pregel顶层算法的编程模型。

​	这种容错方式是通过在**<u>迭代结束时定期快照所有顶点的状态</u>**来实现的，即将其全部状态写入持久存储。<u>如果某个节点发生故障并且其内存中状态丢失，则最简单的解决方法是将整个图计算回滚到上一个检査点，然后重新开始计算。如果算法是确定性的，并且记录了消息，那么也可以选择性地只恢复丟失的分区(就像我们之前讨论过的数据流引擎)</u>。

#### * 并行执行

​	顶点不需要知道它运行在哪台物理机器上。当它发送消息到其他顶点时，只需要将消息发送至一个顶点ID。框架对图进行分区，即确定哪个顶点运行在哪个机器上，以及如何通过网络路由消息，以便它们都能到达正确的位置。

​	由于**编程模型一次仅处理一个顶点(有时也称为“像顶点一样思考”)，所以框架能够以任意方式划分图**。<u>理想情况下，如果顶点之间需要进行大量的通信，那么它们会被分区至同一台机器。但是，**找到这样的优化分区是很困难的，通常按照任意分配的顶点ID对图进行分区，而不会尝试将相关的顶点分组在一**起</u>。

​	因此，图算法往往会有很多跨机器通信的开销，**中间状态(节点之间发送的消息)往往比原始图大**。通<u>**过网络发送消息的开销会显著减慢分布式图算法**</u>。

​	出于这样的原因，**<u>如果图可以载入到计算机内存中，那么单机(甚至可能是单线程)算法的性能可能要比分布式批处理的性能更好</u>**。即使图大于内存，也可以放在单个计算机的磁盘中，使用GraphChi等框架进行单机处理也是一个可行的选择。如果图太大而不适合单个机器，则需要采用像Pregel这样的分布式方法。<u>有效的并行化图算法是一个正在研究中的领域</u>。

### 10.3.3 高级API和语言

​	自 MapReduce流行以来，分布式批处理的执行引擎已经逐渐成熟。到目前为止，基础设施已经足够强大，能够存储和处理超过10000台机器集群中的PB级数据。由于在这种规模下物理操作批处理过程的问题已经或多或少得到了解决，所以问题已经转向其他领域：<u>改进编程模型，提高处理效率，扩大解决的问题域</u>等。

​	如前所述，由于手工编写 MapReduce作业太过耗时费力，因此Hive，Pig， Cascading和 Crunch等高级语言API变得非常流行。<u>随着Tez的出现，这些高级语言还能够移植到新的数据流执行引擎，而无需重写作业代码</u>。 Spark和 Flink也包含他们自己的高级数据流API，其中很多灵感来自FlumeJava。

​	这些数据流API通常使用关系式构建块来表示计算：将数据集join到某个字段的值上；按关键宇对元组进行分组；按条件进行过滤；并通过计数、求和或其他函数来聚合元组。在内部，这些运算使用本章前面讨论过的各种join和分组算法来实现。

​	除了减少代码的明显优势之外，这些高级接口还允许交互式使用。在这种交互式使用中，可以在 shell中逐步编写分析代码，并随时运行以观察计算结果。这种开发方式在探索数据集和测试处理方法时非常有用。这一点很像前面讨论过的UNIX设计哲学中。

​	此外，这些高级接口不仅使提高了系统利用率，而且提高了机器级别的作业执行效率。

#### 转向声明式查询语言

​	**与编写执行join的代码相比，指定join作为关系运算符的优点在于，框架可以分析join輸入的属性，并自动决定哪个join算法最适合当前的任务**。<u>Hive， Spark和Flink利用基于成本的查询优化器来实现这样的功能，甚至还可以改变join顺序，使中间状态数量最小化</u>。

​	**join算法的选择会对批处理作业性能产生很大的影响**，当然也不需要理解和记住本章讨论过的所有join算法。如果以声明方式指定join，那么可以在应用程序中简单地说明哪些join是必需的，由查询优化器决定如何最佳执行。之前在第2章“数据查询语言”中讨论过这个想法。

​	但是，在其他方面， MapReduce及后续的数据流引擎与SQL的完全声明式查询模型有很大不同。 MapReduce是围绕函数回调的思想构建的：对于每个记录或者一组记录，调用由用户定义的函数( mapper或 reducer)，共且该函数可以灵活调用任意代码来决定输出。这种方法的优点是，可以利用现有库的大型生态系统来执行数据解析、自然语言分析、图像分析以及运行数值或统计算法等。

​	轻松运行任意代码是 MapReduce之类的批处理系统与MPP数据库的区别所在(参见本章前面的“对比 Hadoop与分布式数据库”)。尽管数据库具有编写用户定义函数的功能，但是使用起来通常会很麻烦，而且与大多数编程语言中广泛使川的程序包管理器和依赖管理系统(例如而向Java的 Maven， JavaScript的npm和Ruby的Rubygems)无法很好地集成。

​	<u>而且，除了join之外，数据流引擎已经证实在很多其他情形中，声明性特征也同样具有优势</u>。例如，如果回调函数只包含一个简单的过滤条件，或者只是从一条记录中选择了部分宇段，那么在调用每条记录的函数时会有相当多的CPU开销。**如果以声明式表示简单的过滤和map操作，那么查询优化器可以利用面向列的存储格式(请参阅第3章的“列式存储”)，从磁盘仅读取所需的列**。Hive， Spark DataFrames和 Impala也使用**向量化执行**(请参阅第3章的“内存带宽与向量化处理”)：在对CPU高速缓存很友好的内部循环中迭代数据，并避免函数调用。 Spark生成JVM字节码， Impala使用LLVM为这些内部循环生成本机代码。

​	<u>通过将声明式特征与高级API结合，使查询优化器在执行期间可以利用这些优化方法，批处理框架看起来就更像MPP数据库了(并且能够实现性能相当)。同时，通过具有运行任意代码和读取任意格式数据的可扩展性，它们依然保持了灵活性的优势</u>。

#### 不同领域的专业化

​	尽管能够运行任意代码的可扩展性是有用的，但是标准处理模式不断反复运行的情形也极其常见，因比有必要实现可重用的通用构建模块。传统上，MPP数据库满足商业智能分析和商业报告的需求，但这只是批处理的诸多应用领域之一。

​	另一个越来越重要的领域是统计和数值算法，这是诸如分类和推荐系统等机器学习应用所需要的。已经出现了一些**可重复使用的实现**：例如， **Mahout在 MapReduce、Spark和 Flink之上实现了用于机器学习的各种算法，而 MADliB在关系型MPP数据库(Apache HAWQ)中实现了类似的功能**。

​	<u>可重用实现对例如k-近邻这样的空间算法也是有用的，它在多维空间中搜索与给定项接近的数据项，是一种相似性搜索。近似搜索对于基因组分析算法也很重要，它们需要找到相似但不相同的字符串</u>。

​	批处理引擎正被用于日益广泛的算法领域的分布式执行。<u>随着批处理系统获得更丰富的内置功能和高级声明式运算符，而MPP数据库也变得更具可编程性和灵活性，两者开始变得更加相似：最终，它们都是用于存储和处理数据的系统</u>。

## 10.4 小结

​	本章探讨了批处理这一主题。我们首先查看了诸如awk，grep和sort等UNIX工具，然后讨论了这些工具的设计理念是如何运用到 MapReduce和最新的数据流引擎中。这些设计原则包括：**输入是不可变的**，**输出是为了成为另一个(还未知的)程序的输入**，而复杂的问题通过编写“做好一件事”的小工具来解决。

​	**在UNIX世界中，允许多个程序组合在一起的统一接口是文件和管道；在 MapReduce中，该接口是分布式文件系统**。<u>我们看到数据流引擎添加了自己的**管道式数据传输机制**，**以避免在分布式文件系统中将中间状态实体化**，而**作业的初始输入和最终输出通常仍然是HDFS**</u>。

​	分布式批处理框架需要解决的两个主要问题是：

+ 分区

  在 MapReduce中， mapper根据输入文件块进行分区。 mapper的输出被重新分区、排序，合并成一个可配置数量的 reducer分区。这个过程的目的是把所有的相关数据——例如，具有相同关键字的所有记录都放在同一个地方。

  <u>除非必需，后MapReduce的数据流引擎都**尽量避免排序**，但它们釆取了大致类似的分区方法</u>。

+ 容错

  **MapReduce需要频繁写入磁盘**，这使得可以从单个失败任务中轻松恢复，而无需重新启动整个作业，但**在无故障情况下则会减慢执行速度**。<u>**数据流引擎执行较少的中间状态实体化并保留更多的内存，这意味着如果节点出现故障，他们需要重新计算更多的数据。确定性运算符减少了需要重新计算的数据量**</u>。

​	我们讨论了几种 MapReduce的join算法，其中大部分也在MPP数据库和数据流引擎中使用。分区算法的示例包括：

+ **排序-合并join**

  毎个将要join的输入都会由一个join关键字的 mapper来处理。通过分区、排序与合并，具有相同关键字的所有记录最终都会进入对 reducer的同一个调用。然后这个函数输出join记录。

+ **广播哈希join**

  两个join输入中的某一个数据集很小，所以不需要分区，而完全加载到哈希表中。因此，<u>可以为大数据集的每个分区启动一个 mapper，将小数据集的哈希表加载到每个 mapper中，然后一次扫描大数据集的一条记录，对每条记录进行哈希表查询</u>。

+ **分区哈希join**

  如果两个join的输入以相同的方式分区(使用相同的关键字，相同的哈希函数和相同数量的分区)，则哈希表方法可以**独立**用于每个分区。

​	<u>分布式批处理引擎有一个有意限制的编程模型：回调函数(如 mapper和 reducer)被设定为**无状态**，并且除了指定输出之外没有外部可见的任何副作用</u>。这个限制使得框架隐藏了抽象背后的一些困难的分布式系统问题，从而在面对崩溃和网络问题时，可以安全地重试任务，并丢弃任何失败任务的输出。**如果针对某个分区的多个任务都成功了，则实际上只会有其中一个任务使其输出可见**。

​	得益于这样的框架，在批处理作业中的代码无需考虑容错机制的实现：框架可以保证作业的最终输出与没有发生错误的情况相同(即使实际上各种任务可能不得不重试)。<u>而在线服务在处理用户请求时，将写入数据库作为处理请求的副作用，与之相比，批处理的可靠性语义要强大得多</u>。

​	批处理作业的显著特点是它读取一些输入数据并产生一些输出数据，而**不修改输入**。换句话说，输出是从输入派生而来。至关重要的是，输入数据是有界的：数椐大小固定已知(例如，它包含一些时间点的日志文件或数据库内容的快照)。<u>因为它是有界的，所以一个作业总是可以知道何时完成了对整个输入的读取，何时作业最终完成</u>。

​	在下一章中，我们将转向**流处理，其中输入是无界的**。也就是说，作业的输入是永无止境的数据流。在这种情况下，作业永远不会完成，因为在任何时候都可能有更多的输入进来。我们将看到流处理和批处理在某些方面是相似的，但流数据无界的假设也会深刻改变我们设计系统的方法。

# 第11章 流处理系统

​	第10章讨论了批处理技术，它是读取一组文件作为输入并生成一组新的输出文件的技术。输出是**派生数据**的一种形式；也就是说，如果需要，可以通过再次运行批处理过程来重新创建数据集。这个看似简单但却强大的想法可以用来**创建搜索索引**、**推荐系统**以及**分析**等。

​	然而，在第10章中始终存在一个重要的假设：即输入是**有界**的，是已知的有限大小，所以批处理知道何时读完他们。比如， <u>MapReduce核心的排序操作必须先读取整个输入，然后才能开始生成输出：最后一个输入记录可能是具有最低键的输入记录，因此需要成为第一个输出记录，也正因为如此，不能提前开始输出</u>。

​	而实际上，有很多数据是无限的，而且随着时间的推移而逐渐到达：用户胙天和今天产生了数据，而明天还将继续产生更多的数据。除非业务停止，否则这个过程永远不会结束，因此数据集永远不会以任何有意义的方式“完成”。而<u>批处理器则必须人为地将数据划分为**固定时间段的数据块**</u>：例如在每天结束时处理一天的数据，或者在每小时结束时处理这一小时的数据。

​	批处理的问题是，输入的更改只会在一天之后的输出中反映出来，这对于许多没有耐心的用户来说太慢了。<u>为了减少这种延迟，可以更频繁地运行处理，即在每秒钟结束时(甚至是持续不断地)处理每秒的数据，完全放弃固定的时间片，每当有事件就开始处理。这就是流处理背后的思想</u>。

​	**一般来说，“流”是指随着时间的推移而持续可用的数据**。这个概念出现在很多地方：在UNIX的 stdin和 stdout、编程语言(lazy lists)、文件系统API(如Java的FileInputStream)、TCP连接、通过互联网传送音频和视频等。

​	本章，我们将把事件流视为一种数据管理机制：一种无界的、持续增量处理的方式，对应于上一章所介绍的批处理处理方式。我们将首先讨论如何通过网络来表示、存储和传输流。在“数据库和流”部分，将探讨流和数据库之间的关系。最后，在“处理流”中，将探索连续处理这些流的方法和工具，以及可以用来构建应用程序的方法。

## 11.1 发送事件流

​	**在批处理的世界里，作业的输入和输出是文件(或许在分布式文件系统上)**。那么等效的流是什么样的呢？

​	当输入是文件(字节序列)时，第一个处理步驟通常是将其解析为记录序列。<u>在流处理的上下文中，记录通常被称为**事件**，但它本质上是一回事：一个小的、独立的、不可变的对象，该对象包含某个时间点发生的事情的细节。每个事件通常包含一个时间戳，用于指示**事件发生**的**墙上时间**(参见第8章“单调时钟与墙上时钟”)</u>。

​	例如，发生的事情可能是用户的某些操作，例如浏览页面或下单购买。也可能源于机器设备，例如来自温度传感器的周期性测量或者CPU利用率度量。<u>在第10章的“使用UNIX工具进行批处理”的示例中，Web服务器日志的每一行都是一个事件</u>。

​	如第4章所述，事件可以被编码为文本字符串或JSON，或者某种二进制形式。通过这种编码方式，可以保存事件，比如将其追加到文件、插入到关系表，或将其写入文档数据库等。它还可以通过网络将事件发送到另一个节点以进行处理。

​	**在批处理中，文件被写入一次，然后可能被多个作业读取。类似地，在流术语中，事件由生产者(也称为发布者或发送者)生成一次，然后可能由多个消费者(订阅者或接收者)处理**。<u>在文件系统中，文件名标识一组相关记录；在流系统中，相关的事件通常被组合成主题或流</u>。

​	原则上，通过文件或数据库也可以连接生产者和消费者：生产者将其生成的每个事件写入数据存储，并且毎个消费者定期轮询数据存储以检查自上次运行以来出现的事件。这实际上正是批处理在每天结束时处理一天的数据的过程。

​	但是，如果数据存储不是为这种用途而设计的，那么在延迟时间较短的情况下进行连续处理时，轮询的代价会变得很大。<u>轮询次数越多，返回新事件的请求的百分比越低，开销越高。所以，当新事件出现时，最好通知消费者</u>。

​	数据库传统上无法很好地支持这种通知机制：关系型数据库通常具有触发器，可以对变化作出反应(例如，将行插入表时)，但是他们的功能非常有限，在数据库设计中有点事后考虑的意思。因此，开发了专门的工具用来提供事件通知。

### * 11.1.1 消息系统

​	向消费者通知新事件的常见方法是使用消息系统：生产者发送包含事件的消息，然后该消息被推送给一个或多个消费者。之前在第4章的“基于消息传递的数据流”只是简单的介绍了这些系统，现在我们来详细讨论这些系统。

​	像UNIX管道或TCP连接一样，在生产者和消费者之间的直接通信通道将是实现消息传递系统的简单方法。但是，大多数消息传递系统都在这个基本模型上进行扩展。特别是，<u>UNIX管道和TCP仅连接一个发送者与一个接收者，而消息系统允许多个生产者节点将消息发送到同一主题，并允许多个消费者节点接收主题中的消息</u>。

​	在这种**发布/订阅模式**中，不同的系统采取了不同的方法，没有一个标准的答案满足所有的目的。为了区分这些系统，提出以下两个问题对区分很有帮助：

1. **如果生产者发送消息的速度比消费者所能处理的快，会发生什么？**一般来说，有三种选择：系统丢弃消息；将消息缓存在队列中；或者<u>激活背压，也称为流量控制(即阻止生产者发送更多消息)</u>。<u>例如，UNIX管道和TCP使用背压：他们有一个固定大小的小緩冲区，如果它被填满，发送者将被阻塞，直到接收方将数据从缓冲区中取出(请参阅第8章“网络拥塞与排队”</u>)。如果消息被缓存在队列中，那么了解队列增长时会发生什么非常重要。如果内存无法容纳所有队列，系统是否会崩溃？还是会将消息写入磁盘？如果是这样，磁盘访问又会如何影响消息传递系统的性能。
2. **如果节点崩溃或者暂时离线，是否会有消息丢失？**<u>与数据库一样，持久性可能需要写入磁盘和/或结合复制方案(请参阅第7章“复制与持久性”)，而这都是有成本的</u>。如果能够接受有时候会丟失消息，那么在同样的硬件上可能获得更高的吞吐量和更低的延迟。

​	消息丢失是否可以接受在很大程度上取决于应用程序。例如，对于周期性传输的传感器读数和指标，偶尔丟失的数据点可能并不重要，因为更新的值马上就会发送过来。

​	但是，要注意的是，如果大量的消息被丢失，可能相关指标并不会立即发现异常。如果你正在对事件进行计数，由于每个丟失的消息都意味着计数器错误，因此它们能够被可靠地传送就显得更重要了。

​	第10章中所讨论的<u>批处理系统有一个很好的特性是，它们提供了强大的可靠性保证：失败的任务会自动重试，失败任务的部分输出会自动丢弃。这意味着输出与未发生故障时一样，这有助于简化编程模型</u>。本章稍后将硏究如何在流上下文中提供类似的保证。

#### 生产者与消费者之间的直接消息传递

​	许多消息系统将生产者直接连接到消费者，而不通过中间节点：

+ **UDP组播**广泛应用于金融行业，例如股票市场等低延迟场景。尽管UDP本身是不可靠的，但应用层协议可以恢复丢失的数据包(生产者必须记住它发送的数据包，以便它可以按需要重新发送数据包)。
+ **无代理的消息库**(如 ZeroMQ 和 nanomsg)，采取类似的方法，通过TCP或**IP多播**实现**发布/订阅消息**传递。
+ StatsD和Brubeck使用不可靠的UDP消息传递来收集网络中所有机器的指标并对其进行监控(在StatsD协议中，只有接收到所有消息时，计数器指标才是正确的；使用UDP只能使计数指近似准确。另请参阅第8章“TCP与UDP”)。
+ 如果消费者在网络上公开服务，则生产者可以直接发出HTTP或RPC请求(请参阅第4章“基于服务的数据流：REST和RPC”)以将消息推送给消费者。这正是<u>**webhooks**背后的想法：一个服务的回调URL被注册到另一个服务中，并且每当事件发生时都会向该URL发出请求</u>。

​	在其设计的目标场景下，这些直接消息传递方式运行效果不错，但是它们通常都要求应用程序代码意识消息丟失的可能性。<u>它们只能支持有限的容错：即使协议可以检测并重新传输在网络中丢失的数据包，但通常还是假定生产者和消费者需要一直在线</u>。

​	<u>如果消费者处于离线状态，则可能会遗漏当他们掉线时发送的消息</u>。**有些协议允许生产者重试失败的消息传递，然而，如果生产者崩溃，则这种方法可能会失败，从而丢失了本应该重试的消息缓冲区**。

#### * 消息代理

​	<u>一种广泛使用的替代方法是通过消息代理(也称为消息队列)发送消息，消息代理实质上是一种针对处理消息流而优化的数据库</u>。它作为服务器运行，生产者和消费者作为客户端连接到它。生产者将消息写入代理，消费者通过从消息代理那里读取消息来接收消息。

​	**通过将数据集中在代理中，这些系统可以更容易地适应不断变化的客户端(连接、断开连接和崩溃)，而持久性问题则被转移到代理那里**。一些消息代理只将消息保存在内存中，而另一些消息代理(取决于配置)将其写入磁盘，以便在代理崩溃的情况下不会丢失消息。对于速度慢的消费者，他们通常允许<u>无限队列(而不是丢弃消息或背压)</u>，不过这种选择也可能取决于配置。

​	排队的结果也通常导致消费者以**异步**方式工作：生产者发送消息时，它通常只等待代理确认它已经缓存了消息，而不会等待消息被消费者处理。向消费者的交付发生在将来某个不确定的时间点——通常是在几分之一秒内，但如果存在队列积压，有时会有很明显的延迟。

#### 消息代理与数据库对比

​	**一些消息代理甚至可以使用XA或JTA参与两阶段提交协议(请参阅第9的“实践中的分布式事务”)**。这个特性使它们在本质上与数据库非常相似，虽然消息代理和数据库之间仍然存在着重要的实际差异：

+ 数据库通常会保留数据直到被明确要求删除，而大多数消息代理在消息成功传递给消费者时就自动删除消息。这样的消息代理不适合长期的数据存储。
+ 由于消息代理很快删除了消息，多数消息系统会假定当前工作集相当小，即队列很短。如果因为消费者速度很慢，而使代理需要缓存很多消息的话(如果内存无法容纳所有的消息，可能会将部分消息唤出到磁盘)，那么每个消息就需要更长的时间来处理，整个吞吐量可能会因此降低。
+ <u>数据库通常支持二级索引和各种搜索数据的方式，而消息代理通常支持某种方式订阅匹配特定模式的主题</u>。这些机制虽然是不同的，但本质上都是让客户端可以选择它们想要了解的部分数据。
+ <u>查询数据库时，结果通常基于数据的时间点快照。如果另一个客户端随后向数据库写入更改査询结果的内容，那么第一个客户端不会发现之前的结果已经过期(除非它重复查询或轮询更改)。相比之下，消息代理不支持任意的查询，但是当数据发生变化时(即新消息可用时)，它们会通知客户端</u>。

​	这是消息代理的传统观点，体现在像JMS和AMQP这样的标准中，并有很多系统实现，包括RabbitMQ， ActiveMQ，HornetQ，Qpid，TIBCO Enterprise Message Service，IBM MQ，Azure Service Bus，以及Google Cloud Pub/Sub等。

#### * 多个消费者

​	当多个消费者读取同一个主题中的消息时，有两种主要的消息传递模式，如图11-1所示。

+ **负载均衡式**

  **每一条消息都只被传递给其中一个消费者**，所以消费者可以共享主题中处理消息的工作。代理可以任意分配消息给消费者。当处理消息的代价很高时，此模式非常有用，因此希望能够添加消费者来并行处理消息(在AMQP中，可以通过让多个客户端使用同一个队列消费来实现负载均衡，而在JMS中，它称为**共享订阅**)。

+ **扇出式**

  **每条消息都被传递给所有的消费者**。扇出式允许几个独立的消费者各自“收听”相同的消息广播，而不会相互影响，<u>流相当于多个读取相同输入文件的不同批处理作业</u>(此功能由JMS中的**主题订阅**提供和AMQP中的**交换**绑定)。

![传递事件流 - 图1](https://static.sitestack.cn/projects/ddia/img/fig11-1.png)

​	**这两种模式可以组合使用。例如，两个独立的消费者群组可以各自订阅一个主题，使得每个组都能共同接收所有消息，但是在每个组内，每一条信息只有一个节点接收**。

#### * 确认和重新传递

​	消费者可能会随时崩溃，所以可能会发生下面这些情况：代理向消费者传递消息，但消费者从不处理消息，或者在崩溃之前只对消息进行了部分处理。**为了确保消息不会丢失，消息代理使用确认：客户端必须在处理完消息后显式地告诉代理，以便代理可以将其从队列中移除**。

​	如果与客户端的连接关闭或超时，而代理没有收到确认，则认为消息未处理，因此它将消息重新传递给另一个消费者(<u>请注意，消息可能实际上已经完全处理，但是确认消息在网络传输过程中丢失。处理这种情况需要**原子提交**协议，正如在第9章中所讨论的“实践中的分布式事务”那样)</u>。

​	<u>当与负载均衡结合时，这种重新传递行为对消息的排序会产生一个有趣的影响。在图11-2中，消费者通常按照生产者发送的顺序处理消息。然而，消费者2在处理消息m3时崩溃，与此同时消费者1正在处理消息m4。未确认的消息m3随后被重新发送给消费者1，结果消费者1按照m4，m3，m5的顺序处理消息。因此，m3和m4不是以它们被生产者1发送顺序传递的</u>。

![传递事件流 - 图2](https://static.sitestack.cn/projects/ddia/img/fig11-2.png)

​	**即使消息代理试图保留消息的顺序(如JMS和AMQP标准所要求的)，<u>负载均衡</u>与<u>重新传递</u>的组合也不可避免地导致<u>消息被重新排序</u>**。为了避免此问题，可以为每个消费者使用单独的队列(即不使用负载均衡功能)。如果消息彼此完全独立，消息重新排序就不成问题，但是如果消息之间存在**因果依赖**关系，那么它就成为很重要的问题了，我们将在本章后面继续讨论。

### 11.1.2 分区日志

​	通过网络发送数据包或者向网络服务发送请求通常都是瞬间的操作，不会留下永久的痕迹。尽管可以永久记录(使用数据包捕获和日志记录)，但通常不会这么去做。<u>即使是将消息持久地写入磁盘的消息代理，在将消息传递给消费者之后，也会很快将其删除，因为**消息代理是基于瞬间的消息传递思维构建的**</u>。

​	数据库和文件系统采取相反的方式：在有人明确选择删除它之前，任何写入数据库或文件的内容通常都期望是永久保存。

​	思维方式上的这种差异对于如何创建派生数据有很大的影响。正如在第10章讨论的一样，批处理过程的一个关键特征是，可以反复地运行它们，尝试处理步骤，而且没有损坏输入的风险(因为输入是只读的)。**但是AMQP/JMS风格的消息系统则不是这样的：<u>如果确认后从代理中删除了消息，就无法再次接收消息，因此不能再次运行同一个消费者并期望得到相同的结果</u>**。

​	**如果将新的消費者添加到消息系统，通常它只会开始接收在它注册后发送的消息；任何之前的消息已经消失，无法恢复了**。而对于文件和数据库，则可以随时添加新的客户端，并且可以读取以前任意写入的数据(只要应用程序没有明确覆盖或删除数据)。

​	那么为什么不能混合使用，将数据库的持久存储方法与消息传递的低延迟功能相结合？这正是日志消息代理背后的想法。

#### 基于日志的消息存储

​	日志是磁盘上一个仅支持**追加式**修改记录的序列。之前在第三章日志结构化存储引擎和预写日志的上下文中我们已经讨论了日志，在第5章的复制部分也讨论了日志。

​	我们可以使用相同的结构来实现消息代理：<u>生产者通过将消息追加到日志的末尾来发送消息，消费者通过依次读取日志来接收消息。如果消费者读到日志的末尾，它就开始等待新消息被追加的通知</u>。UNIX工具`tail -f`正是基于这种工作思路的例子，它可以监视修改文件的尾部。

​	<u>为了突破单个磁盘所能提供的带宽吞吐的上限，可以对日志进行**分区**(参阅第6章)。不同的节点负责不同的分区，使每个分区成为一个单独的日志，并且可以独立于其他分区读取和写入。然后可以将主题定义为一组分区，他们都携带相同类型的消息</u>。这种方法如图11-3所示。

​	**在每个分区中，代理为每个消息分配一个单调递增的序列号或偏移量(在图11-3中，框中的数字是消息偏移量)。这样的序列号是非常有意义，因为分区只能追加，所以分区内的消息是完全有序的。<u>不同分区之间则没有顺序保证</u>**。

![传递事件流 - 图3](https://static.sitestack.cn/projects/ddia/img/fig11-3.png)

​	Apache Kafka，Amazon Kinesis Streams和Twitter DistributedLog都是这种方式工作的<u>基于日志的消息代理系统</u>。 Google Cloud Pub/Sub在架构上相似，但是开放JMS风格的API而不是日志抽象。尽管这些消息代理将所有消息写入磁盘，但<u>通过在多台机器上进行**分区**，能够实现毎秒数百万条消息的吞吐量，并且通过**复制**消息实现了容错性</u>。

#### * 对比日志与传统消息系统

​	因为多个消费者可以独立地读取日志而不会相互影响，读取消息不会将其从日志中删除，因此基于日志的方法很自然地支持扇出式消息传递。<u>为了在一组消费者之间实现负载均衡，代理可以将整个分区分配给消费者组中的节点，而不是将单个消息分配给消费者客户端</u>。

​	每个客户端都会使用分配给它所在分区中的所有消息。通常，当消费者被分配了一个日志分区时，它将以直接的单线程方式顺序读取分区中的消息。这种粗粒度的负载均衡方法有一些缺陷：

+ 因为同一分区内的消息将被传递到同一节点注，所以消费一个主题的节点数最多等于该主题中的日志分区数。

+ 如果单个消息处理缓慢，则会阻碍该分区中的后续消息的处理(一种队头阻塞的形式，请参阅第1章的“描述性能”)。

​	**因此，在消息处理的代价很高，希望在逐个消息的基础上并行处理，而且消息排序又不那么重要的情况下，JMS/AMQP类型的消息代理更可取。另一方面，在消息吞吐量高的情况下，每个消息处理速度快，消息顺序又很重要的情况下，基于日志的方法工作得很好**。

#### 消费者偏移量

​	<u>顺序读取一个分区可以很容易地判断哪些消息已经被处理：所有偏移量小于消费者当前偏移量的消息已经被处理，并且所有更大偏移量的消息还没有被看到。因此，代理不需要跟踪毎条消息的确认，**只需要定期记录消费者的偏移量**</u>。在这种方法中，减少的记录开销以及可以使用批处理和流水线操作的机会有助于提高基于日志的系统的吞吐量。

​	实际上，此偏移量与主从复制数据库常见的日志序列号非常相似，在第5章的“配置新的从节点”中讨论了这种情况。<u>在数据库复制中，日志序列号允许从节点断开连接后，重新连接到主节点，并在不跳过任何写入的情况下恢复复制</u>。这里使用了完全相同的原则：消息代理的行为就像一个主节点数据库，消费者就像一个从节点。

​	<u>如果消费者节点失败，则消费者组中的另一个节点将被分配到失败的消费者分区，并以最后记录的偏移量开始使用消息。**如果消费者已经处理了后续的消息，但还没有记录它们的偏移量，那么在重新启动后这些消息将被再次处理**。本章后面将讨论处理这个问题的方法</u>。

#### 磁盘空间使用

​	如果持续不断地追加日志，磁盘空间最终将被耗尽。为了回收磁盘空间，日志实际上是被分割成段，并且不时地将旧段删除或归档保存。稍后我们将讨论一种更复杂的释放磁盘空间的方法。

​	<u>这就意味着，如果一个消费者的速度慢到难以跟上消息产生的速度，并且远远落后以至于消费者偏移量指向了已经被删除的片段，那么消费者将会错过一些消息</u>。实际上，日志实现了一个有限大小的缓冲区，当缓冲区变满时，旧的消息就被丢弃，该缓冲区也被称为**循环缓冲区**或**环形缓冲区**。由于该缓冲区在磁盘上，因此它可以非常大。

​	让我们来做一个粗略的计算。在撰写本文时，一个典型的大容量硬盘为6TB，顺序写入吞吐量为150MB/s。如果以最快速度写入消息，则大约11小时就可以填满磁盘。因此，磁盘可以缓存11小时的消息，之后它将开始覆盖旧的消息。即使使用多个硬盘和多台机器，这个比率也是一样的。实际的部署中，很少会达到磁盘的满写带宽，所以日志通常可以保存几天甚至几周的消息缓冲区。

​	<u>不管保留多长时间的消息，因为每个消息都被写入到磁盘，因此日志的吞吐量基本保持不变。这种行为与将消息默认保存在内存中，仅当队列变得过大时才将它们写入磁盘的消息传递系统相比，差异明显：当队列很短的时候这些系统是很快的，当开始写入磁盘时，会变得很慢，因此吞吐量取决于保留的历史记录数量</u>。

#### * 当消费者跟不上生产者时

​	本章前面“消息传递系统”的开始部分，讨论了消费者无法跟上生产者发送消息的速度时三种选择：丟弃消息，缓冲或应用背压。在这个分类法中，基于日志的方法是一种缓冲形式，它具有较大但固定大小的缓冲区(受可用磁盘空间的限制)。

​	如果消费者落后得太多，以至于其所需的信息比保留在磁盘上的信息还要旧，那么它将无法读取这些信息，所以代理有效地丟弃缓冲区容量不能容纳的旧消息。**可以监控消费者落后日志头部的距离，并在落后明显时会发出警报**。<u>由于缓冲区很大，因此有足够的时间让操作员修复缓慢的消费者，并允许它在开始丢失消息前赶上</u>。

​	**即使消费者确实落后太多，并且开始丢失信息，也只有该消费者受到影响；它不会中断其他消费者的服**务。这是一个非常大的运营优势：可以通过实验性的方式使用生产日志进行开发、测试或调试，而不必担心中断生产服务。<u>当消费者关闭或崩溃时，它会停止消耗资源，唯一留下的就是消费者偏移量</u>。

​	<u>这种行为也与传统的消息代理不同，在传统的消息代理中，需要小心地删除消费者已经关闭的任何队列，否则他们将继续不必要地积累消息，并占用其他活动消费者的内存</u>。

#### * 重新处理信息

​	**我们之前提到过，使用AMQP和JMS风格的消息代理时，由于会导致消息在代理上被删除，因此处理和确认操作可视为带有一定的破坏性。另一方面，在基于日志的消息代理中，使用消息更像是从文件读取：这是<u>只读操作</u>，并不会更改日志**。

​	除了消费者的任何输出之外，处理的唯一副作用是消费者偏移量前移了。但是偏移量在消费者的控制之下，因此在必要时可以轻松地对其进行操作。例如，可以用昨天的偏移量启动一个消费者的副本，并将输出写到不同的位置，以便重新处理最后一天的消息。可以通过改变处理代码多次重复此操作。

​	**这个特点使得基于日志的消息系统更像上一章的批处理过程，其中派生数据通过可重复的转换过程与输入数据明确分离。它支持更多的实验性尝试，也更容易从错误和故障中进行恢复，从而成为集成数据流的不错选择**。

## 11.2 数据库与流

​	我们已经在消息代理和数据库之间进行了一些比较。尽管传统上他们被认为是单独的工具类别，但是我们看到基于日志的消息代理已经成功地从数据库中得到启发，并将其应用于消息传递。也可以反过来：从消息传递和流中获取一些启发，并将他们应用到数据库。

​	之前曾经说过，事件是某个时刻发生的事情的记录。发生的事情可能是用户操作(例如输入搜索査询)或传感器读取，但也可能是写入数据库。<u>将内容写入数据库的事实是一个可以被捕获、存储和处理的事件</u>。这一观察结果表明，数椐库和数据流之间的联系比磁盘上日志的物理存储更紧密，这点非常重要。

​	**实际上，复制日志(请参阅第5章的“复制日志的实现”)是数据库写入事件的流，由主节点在处理事务时生成**。从节点将写入流应用于他们自己的数据库副本，从而最终得到相同数据的准确副本。复制日志中的事件描述了数据变化。

​	我们还在第9章的“全序广播”中讨论过<u>**状态机复制原理**，该原理指出：如果每个事件代表对数据库的写入，并且每个副本按相同的顺序处理相同的事件，则所有副本最后都将收敛于相同的最终状态(处理事件被假设为确定性的操作)。这只是事件流的另一种情况</u>！

​	在本节中，我们将首先看看异构数据系统中出现的问题，然后探讨如何通过将事件流的想法引入到数据库来解决这个问题。

### 11.2.1 保持系统同步

​	正如在本书中所看到的，没有一个系统能够满足所有的数据存储、查询和处理需求。在实践中，大多数重要的应用程序都需要结合多种不同的技术来满足需求：例如，使用OLTP数据库来为用户请求提供服务，使用缓存来加速常见请求，使用全文索引处理搜索查询，以及使用数据仓库用于分析。每一个技术都有自己的数据副本，以自己的表示方法存储，并且针对自己的设计目标而优化。

​	由于相同或相关的数据出现在多个不同的地方，因此他们需要保持相互同步：如果数据库中的某个项更新，则也需要在缓存、搜索索引和数据仓库中进行更新。对于数据仓库，这种同步通常由**ETL进程**执行(请参阅第3章的“数据仓库”)，一般通过获取数据库的完整副本，对其进行转换并将其批量加载到数据仓库中——换句话说，就是批处理。同样，在第10章的“批处理工作流的输出”中介绍了如使用批处理过程创建搜索索引、推荐系统和其他派生数据系统。

​	如果定期的完整数据库转储过于缓慢，有时使用的替代方法是双重写入，其中程序代码在数据更改时显式地写入每个系统。例如，首先写入数据库，然后更新搜索索引，然后使缓存条目失效(或者甚至同时执行这些写入)。

​	但是，双重写入有一些严重的问题，其中一个就是图11-4所示的竞争条件。在这个例子中，两个客户同时想要更新项X：客户端1想要将值设置为A，客户端2想要将其设置为B。两个客户端首先将新值写入数据库，然后将其写入搜索索引。由于实机不凑巧，这些请求交叉了：数据库首先看到来自客户端1的写入，将值设置为A，然后看到来自客户端2的写入，将值设置为B，因此数据库中的最终值为B。而搜索索引首先看到来自客户端2的写入，然后才是客户端1的写入，所以搜索索引中的最终值是A。这两个系统将永远不一致，即使目前还没有发生错误。

![流与数据库 - 图1](https://static.sitestack.cn/projects/ddia/img/fig11-4.png)

​	除非有一些额外的并发检测机制，例如在第5章的“检测并发写”中讨论的版本向量，否则甚至不会注意到发生了并发写入值悄悄地覆盖另一个值。

​	<u>双重写入的另一个问题是其中一个写入可能会失败，而另一个却成功了。这其实是个容错问题而不是并发问题，但也会造成两个系统相互不一致的结果。确保他们都成功或者都失败属于原子提交范畴，这个问题解决起来代价很大(请参阅第9章“原子提交与两阶段提交”)</u>。

​	如果只有一个复制的数据库与一个主节点，那么主节点确定写入的顺序，所以状态机复制方法适用于数据库的副本。然而，在图11-4中没有一个单独的主节点：数据库可能有一个主节点，而搜索索引可能也有其主节点，但是两者都不会跟随对方的主节点，所以冲突可能就发生了(参阅第5章“多主节点复制”)。

​	如果实际上只有一个主节点(例如数据库)，并且如果可以使搜索索引成为数据库的从节点，情况会更好。但这在实践中可能吗?

### 11.2.2 变更数据捕获

​	大多数数据库的复制日志的问题在于，它们长期以来被认为是数据库的内部实现细节，而不是公开的API。客户端应该通过其数据模型和査询语言来査询数据库，而不是分析复制日志并尝试从中提取数据。

​	<u>数十年来，许多数据库根本没有一种详细的方法来获取所写入的变更日志。由于这个原因，很难将数据库中所做的所有更改复制到不同的存储技术，如搜索索引，缓存或数据仓库</u>。

​	最近，人们对**变更数据捕获(Change Data Capture，CDC)**越来越感兴趣。CDC记录了写入数据库的所有更改，并以可复制到其他系统的形式来提取数据。如果在写入时立即将更改作为一种流来发布，那么CDC就更有趣了。

​	例如，可以捕获数据库中的更改并不断将相同的更改应用于搜索索引。如果以相同顺序应用于更改日志，那么可以预期搜索索引中的数据与数据库中的数据匹配。搜索索引和任何其他派生的数据系统只是变更流的消费者，如图11-5所示。

![流与数据库 - 图2](https://static.sitestack.cn/projects/ddia/img/fig11-5.png)

#### 实现变更数据捕获

​	我们可以调用日志消费者的派生数据，如第三部分介绍中所述：存储在搜索索引和数据仓库中的数据只是记录系统中数据的另一个视图。变更数据捕获机制可以确保对记录系统所做的所有更改都反映在派生数据系统中，以便派生系统具有数据的准确副本。

​	<u>从本质上讲，变更数据捕获使得一个数据库成为主节点(从中捕获变化的数据库)并将其他变成从节点</u>。**由于基于日志的消息代理保留了消息的排序，因此它非常适合从源数据库传输更改事件**(避免了图11-2的重新排序问题)。

​	数据库触发器可以通过注册触发器来实现变更数据捕获(请参阅第5章“基于触发器的复制”)，这些触发器可观测到数据表的所有更改，并将相应的条目添加到更改日志表中。但是，他们往往不太稳定，并且有着非常大的性能开销。解析复制日志是种更健壮的方法，但它也带来了挑战，比如处理模式更改。

​	<u>基于该想法的Linkedin Databus，Facebook Wormhole和Yahoo! Sherpa已在大规模环境下得到部署。 Bottled Water使用解码预写日志的API来实现PostgreSQL的CDC，Maxwell和Debezium通过解析binlog为 MySQL数据库做类似的事情，Mongoriver读取MongoDB的oplog，Oracle GoldenGate也提供类似的功能</u>。

​	像消息代理一样，变更数据捕获通常是**异步**的：记录数据库系统不会在提交更改之前等待应用于消费者。<u>这种设计具有的操作上的优势是，添加缓慢的消费者不会对记录系统造成太大影响，但是它的缺点是所有**复制滞后**导致的问题在这里全部适用(请参阅第5章“复制滞后问题”)</u>。

#### 初始快照

​	如果有了数据库所有更改的日志，就可以通过replay日志来重建数据库的整个状态。<u>然而，在许多情况下，永久保留所有更改将会需要太多的磁盘空间，并且replay将花费太长时间，因此日志需要被**截断**</u>。

​	例如，构建新的全文索引需要整个数据库的完整副本，仅仅应用最近更改的日志还不够，因为它会丢失最近未更新的项目。因此，如果没有完整的日志历史记录，则需要从一致的快照开始，正如之前在第5章“配置新的从节点”所讨论的。

​	数据库的快照必须与更改日志中的已知位置或偏移量相对应，以便在快照处理完成后，知道在哪一点开始应用更改。一些CDC工具集成了此快照功能，而另外一些则需要手动操作。

#### 日志压缩

​	如果只能保留有限的日志历史记录，则每次需要添加新的派生数据系统时都需要执行快照过程。但是，日志压缩提供了一个很好的选择。

​	<u>之前在第3章的“哈希索引”部分的日志结构存储引擎(见图3-2中的例子)中，讨论了日志压缩功能。原理很简单：存储引擎定期查找具有相同key的日志记录，丟弃所有的重复项，并且只保留每个key的最新的更新。这个压缩和合并的过程是在后台运行的</u>。

​	在日志结构存储引擎中，具有特殊空值(逻辑删除)的更新实际意味着key被删除了，从而导致它在日志压缩过程中被清除。但只要kεy不被覆盖或删除，它就永远留在日志中。这种压缩日志所需的磁盘空间仅取决于数据库的当前内容，而不取决于数据库中发生的写入次数。如果相同的key经常被覆盖，则以前的值最终将被垃圾回收，只有最新的值将被保留。

​	**在基于日志的消息代理和变更数据捕获上下文中，相同的想法也适用。如果CDC系统设置为毎个更改都有一个key，并且每个key的更新都替换了之前的值，那么对一个特定的key，仅保留最近一次写入就足够了**。	

​	现在，无论何时重建派生数据系统(如搜索索引)，都可以从日志压缩主题的偏移量0的位置启动一个新的消费者，然后依次扫描日志中的所有消息。日志确保包含数据库中每个key的最新值(也可能是一些较旧的值)。换句话说，可以使用它来获取数据库内容的完整副本，而无需生成CDC源数据库的另一个快照。

​	**Apache Kafka支持此日志压缩功能。正如将在本章后面看到的，这样消息代理可用于永久存储，而不仅仅是用于临时消息传递**。

#### 对变更流的API 支持

​	<u>越来越多的数据库开始支持将变更流作为标准接口，而不是那些典型的改进和反向工程的CDC努力</u>。例如， RethinkDB支持订阅查询结果发生变化的通知， Firebase和CouchDB的数据同步基于change feed并同时提供给应用层，而 Meteor使用MongoDB oplog来订阅数据更改消息并更新用户界面。

​	VoltDB支持**事务**以流的形式连续地从数据库中导出数据。数据库将关系数据模型中的输出流表示为表，该表支持事务插入元组，但不支持查询。输出流包含了向该特殊表提交写事务的元组日志，并严格按照事务提交顺序排序。外部消费者可以异步使用此日志并使用它来更新派生数据系统。

​	Kafka Connect致力于将广泛的数据库系统变更数据采集工具与Kafka集成。一且更改事件流汲取到Kafka中，它就可以用来更新派生数据系统，比如搜索索引，也可以用于本章稍后讨论的流处理系统。

### 11.2.3 事件溯源

​	这里所讨论的想法与事件溯源之间有一些相似之处，它是一种在领域驱动设计(DDD)社区中开发的技术。接下来，我们将简要讨论事件溯源，它包含了一些和流系统相关并且有用的想法。

​	**与变更数据捕获类似，事件溯源涉及到将所有对应用程序状态的更改保存为更改事件的日志**。最大的区别在于事件溯源在不同抽象层次上应用了这个想法：

+ <u>在变更数据捕获中，应用程序以数据可变方式来操纵数据库，例如自由地更新和删除记录。从数据库中提取较低级别的变更日志(例如，通过解析复制日志)，从而确保从数据库提取的写入顺序与实际写入的顺序相匹配，从而避免图11-4中的竞争条件。写入数据库的应用程序不需要知道CDC正在发生</u>。
+ **<u>在事件溯源中，应用程序逻辑是基于写入事件口志的不可变事件构建的</u>。在这种情况下，事件存储仅支持追加，不鼓励甚至禁止更新或删除操作。事件旨在反映在应用程序级所发生的事情，而不是低级别的状态更改**。

​	事件溯源是一种强大的**数据建模**技术：**从应用程序的角度来看，将用户的行为记录为不可变的事件更有意义，而不是记录这些行为对可变数据库的影响**。<u>事件溯源使得随着时间的推移去演化应用程序变得更加容易，通过更容易理解事情发生的原因，以及防范应用程序错误(请参阅夲章后面的“不可变事件的优勢”)来帮助调试</u>。

​	<u>举个例子，“学生取消课程注册”事件以一种中立的方式清楚地表达了一个行为，而副作用“从入学表中删除一个条目，并且一条取消的原因被添加到学生反馈表”则嵌入了许多关于稍后使用数据方式的假设。如果引入新的应用程序功能，例如“将座位提供给等待列表中的下一个人”，则采用事件遡源的思路可以轻松地将其集成到现有事件上</u>。

​	事件溯源类似于编年史数据模型，事件日志和星型模式中的事实表也有类似之处(请参阅第3章的“星形与雪花形分析模式”)。

​	目前，业界已经开发了专门的数据库如Event Store来支持使用事件测源的应用程序，但通常这种方法独立于任何特定的工具。传统的数据库或基于日志的消息代理也可以用来构建这种风格的应用程序。

#### 从事件日志导出当前状态

​	<u>事件日志本身并不是很有用，因为用户通常期望看到系统的当前状态，而不是修改的历史记录</u>。例如，在购物网站上，用户期望能够看到他们购物车的当前内容，而不是对购物车里曾经的修改历史(追加式)。

​	**因此，使用事件溯源的应用程序需要记录事件的日志(表示写入系统的数据)，并将其转换为适合向用户显示的状态(从系统读取数据的方式)**。<u>这种转换可以使用任意的逻辑，但它应该是**确定性**的，以便可以再次运行它并从事件日志中派生相同的应用程序状态</u>。

​	与变更数据捕获一样， replay事件日志能够重建系统的当前状态。但是，日志压缩需要以不同的方式处理：

+ 用于更新记录的CDC事件通常包含记录的全部新版本，因此key的当前值完全由该key的最近事件确定，并且日志压缩可以丢弃相同key之前的事件。
+ 另一方面，使用事件溯源在更高的层次上对事件建模：**事件通常用来表达用户行为的意图，而不是一种对行为结果进行相应状态更新的机制**。在这种情况下，后来的事件通常不会覆盖以前的事件，所以需要事件的完整历史来重建最终状态。日志压缩不可能以相同的方式进行。

​	使用事件溯源的应用程序通常有一些机制来保存从导出的当前状态的快照，因此它们不需要重复处理全部的日志。但是，这只是一个性能优化，以加快读取速度和崩溃恢复。其目的是系统能够永久存储所有的原始事件，并在需要时重新处理完整的事件日志。我们会在本章后面的“不可变性的限制”中讨论这个假设。

#### * 命令和事件

​	**事件溯源的哲学是小心的区分事件和命令**。当来自用户的请求第一次到达时，它最初是一个命令：此时它可能仍然会失败，例如因为违反了某些完整性条件。应用程序必须首先验证它是否可以执行该命令。<u>如果验证成功并且命令被接受，它将变成一个持久且不可变的事件</u>。

​	例如，如果用户试图注册某个特定的用户名，或在飞机上或剧院中预订座位，则应用程序需要检查用户名或座位尚未占用(之前在第9章的“支持容错的共识”中讨论过这个例子)。当检查成功时，应用程序可以生成一个事件来指出特定的用户名被特定的用户ID注册了，或者特定的座位已预留给特定的某个客户了。

​	<u>在事件发生的时候，它成为了事实。即使客户之后决定更改或取消预订，他们以前曾为某个特定的座位进行预订依然是不争的事实，而更改或取消是稍后添加的单独事件</u>。

​	**不允许事件流的消费者拒绝事件**：<u>当消费者看到事件时，它已经是日志中不可变的部分，并且可能已经被其他消费者看到。因此，**任何命令的验证都需要在它成为事件之前同步发生**，例如，通过使用能够原子地验证命令并发布事件的可序列化事务</u>。

​	或者，预订座位的用户请求可以分成两个事件：第一个是暂时预约，第二个是确认预约后的单独确认事件(如第9章的“采用全序广播实现线性化存储”中所述)。这种划分允许验证过程异步进行。

### 11.2.4 状态，流与不可变性

​	在第10章介绍了批处理受益于其输入文件的不变性，所以可以在现有的输入文件上运行实验处理作业，而不用担心损坏它们。这种不变性原则也是使得事件溯源和变更数据捕获如此强大的原因。

​	我们通常将数据库看成是用来存储应用程序当前状态的，这种表示法针对读取进行了优化，并且通常对于查询服务来说是最方便的。状态的本质是它会发生变化，所以数据库支持更新、删除以及插入数据。这如何符合不变性？

​	每当状态改变，该状态就反映了随着时间推移而变化的事件的结果。例如，当前可用座位列表是已经处理的预订的结果，当前账户余额是账户中的信用和借记的结果，Web服务器的响应时间图是所有发生的Web请求的单个响应时间的集合。

​	无论状态如何变化，总会有一系列事件导致这些变化。不管事件已经结束或者尚在进行中，事件发生是不争的事实。关键的思路是可变状态和不变事件的追加日志不相互矛盾：它们是同一枚硬币的两面。所有变化的日志，更新日志，代表了随着时间的推移状态的演变。

​	如果你擅长数学，你可能会说应用状态是事件流对时间的积分得到的，而变化流是状态对时间的求导得到的，如图11-6所示。虽然这个比喻有一定的局限性(例如，状态的二阶导数似乎没有意义)，但可以帮助进一步认知数据。

![流与数据库 - 图3](https://static.sitestack.cn/projects/ddia/img/fig11-6.png)

​	如果持久化保存更新日志，不过是实现状态可重现效果。如果认为事件的日志是记录系统，并且从它派生出任何可变状态，那么就更容易推断通过系统的数据流。正如PatHelland所说：

​	<u>事务日志记录了对数据库所做的所有更改。高速追加是更改日志的唯一方法。从这个角度来看，数据库的内容保存了日志中最新记录值的缓存。日志是事实。数据库是日志子集的缓存。该缓存子集恰好是来自日志的每个记录和索引值的最新</u>。

​	<u>日志压缩(如本章前面的“日志压缩”中所述)则是链接日志与数据库区别的一种方式。它仅保留毎条记录的最新版本，并丟弃被覆盖的版本</u>。

#### 不变事件的优势

​	数据库中的不变性是一个古老的想法。例如，几个世纪以来，会计师一直在财务簿中使用不变性。交易发生时，它被记录在仅追加的账本中，这本质上是描述货币、商品或服务转手的事件日志。诸如损益或资产负债表之类的账户，则是从账本交易中累加而来。

​	如果发生错误，会计师不会删除或更改账本中的错误交易，而是增加另一笔交易来弥补错误，例如退还不正确的费用。不正确的交易将永远保留在分类账中，出于审计原因这可能很重要。如果从不正确的账本中得出的错误数字已经公布，那么在下一个会计周期会有对应的更正数字。这个过程在会计领域可以说是司空见惯。

​	尽管这种叫审计性在金融系统中尤其重要，但对于其他许多不受这种严格规定的系统也是有好处的。如第10章“批处理输出的哲学”中所述，如果意外地部署了将错误数据写入数据库的错误代码，并且代码能够破坏性地覆盖数据，恢复将变得更加的困难。通过不可变事件的追加日志，诊断问题和从问题中恢复就要容易得多。

​	<u>不可变的事件还会捕获更多的信息，而不仅仅是当前的状态。比如在购物网站，顾客可以将商品添加到购物车，然后再将其移除。从订单执行的角度，虽然第二个事件抵消了第一个事件，但是出于分析的目的，知道客户考虑过某个特定的商品，之后却决定不购买也很有用。也许他们会选择在未来购买，或者找到了替代品。事件日志会记录所有这些信息，而对于数据库，当他们从购物车中删除时，数据库也删除了相关记录</u>。

#### * 相同的事件日志派生多个视图

​	此外，通过**从不变事件日志中分离可变状态，可以从相同的事件日志派生出多个面向读取的表示方式**。这就像有多个消费者的流一样(见图11-5)。例如，分析数据库Druid使用这种方法直接从Kafka获取事件， Pistachio是一个分布式的键值存储，使用Kafka作为提交日志， Kafka Connect sinks可以将来自Kafka的数据导出到各种不同的数据库和索引中。对于许多其他存储和索引系统(如搜索服务器)来说，类似地从分布式日志中获取输入也是有意义的(请参阅本章前面的“保持系统同步”)。

​	从事件日志到数据库有一个明确的转换步骤，可以更加容易地随时间来演进应用程序：如果想要引入一个新的方式呈现现有数据，可以使用事件日志来构建一个单独针对新功能的读取优化视图，并与现有的系统一起运行，而不需要修改它们。<u>同时运行旧系统和新系统通常比在现有系统中执行复杂的模式迁移更容易。一旦旧的系统不再需要，就可以简单地关闭它并回收它的资源</u>。

​	**如果不必担心如何去査询和访问数据，那么存储数据通常是非常简单的。模式设计、索引和存储引擎的许多复杂性多是源于希望支持某些查询和访问模式(参看第3章)**。因比，将数据写入形式与读取形式分开，并允许多个不同的读取视图，可以获得很大的灵活性。这个想法有时时被称为**命令査询责仼分离**(Command Query Responsibility Segregation，CQRS)。

​	<u>数据库和模式设计的传统方法是基于数据查询必须与数据写入的形式相同这一谬误</u>。如果可以将数据从写优化的事件日志转换为读优化的应用程序状态，有关规范化和非规范化的争论(请参阅第2章的“多对一与多对多关系”)则会变得无关紧要：<u>由于转换过程提供了响应机制使其与源事件日志保持一致，因此在读优化的视图中对数据进行反规范化处理是完全合理的</u>。

​	在第1章的“描述负载”中，我们讨论了Twitter的主页时间线，一个特定用户正在关注的人最近写的推文的缓存(很像邮箱)。它是读优化的另一个例子：因为你的推文在所有关注你的人的时间线上都是重复的，所以主页时间线是高度非规范化的。然而，扇出服务可以保持重复状态与新的推文和新的关注之间的同步，从而确保对状态重复的可管理性。

#### 并发控制

​	**事件捕获和变更数据捕获的最大缺点是事件日志的消费者通常是异步的**，所以用户可能会写入日志，然后从日志派生的视图中读取，却发现这些写操作还没有反映在读取视图中。我们在第5章的“读自己的写”中讨论了这个问题和可能的解决方案。

​	一种解决方案是同步执行读取视图的更新，并将事件追加到日志中。这需要一个事务来将写入操作合并到一个原子单元中，所以要么需要将事件日志和读取视图保存在同个存储系统中，要么需要跨不同系统的分布式事务。或者，可以使用在第9章“采用全序广播实现线性化存储”中讨论的方法。	

​	另一方面，从事件日志导出当前状态也简化了并发控制。对于多对象事务的大部分需求(请參阅第7章的“单对象与多对象事务操作”)源自单个用户需要在不同地方改变数据的操作。通过事件溯源，可以设计一个事件，使其成为用户操作的独立描述。用户操作只需要在一个地方进行一次写操作，即将事件追加到日志中，这很容易使其原子化。

​	如果以相同的方式对事件日志和应用程序状态进行分区(例如，处理分区3中客户的事件仅需要更新应用程序状态的分区3)，则简单的单线程日志消费者不需要对写操作进行并发控制一通过构造，它一次只处理一个事件(另请参阅第7章“实际的串行执行”)。该日志通过在分区中定义事件的串行顺序来消除并发的不确定性。如果一个事件涉及多个状态分区，那么需要做更多的工作，将在第12章讨论。

#### 不变性的限制

​	许多不使用事件溯源模型的系统也依赖于不变性：各种数据库在内部使用不可变的数据结构或者多版本数据来支持时间点快照(请参阅第7章“索引与快照隔离”)。诸如Git、 Mercurial和Fossil等版本控制系统也依赖于不可变的数据来保存文件的版本历史记录。

​	在多大程度上，永远保存所有变化的历史记录是可行的？答案取决于数据集的变化情况。一些工作负载主要是添加数据，很少更新或删除数据，因此很容易支持不变性。其他工作负载在较小的数据集上有较高的更新和删除率，此时，不变的历史数据可能变得过于庞大，碎片化也可能成为一个问题，并且压缩和垃圾回收的性能对于运维的健壮性变得至关重要。

​	<u>除了性能之外，还可能在某种情况下，由于管理方面的原因需要删除数据，尽管这些数据都是不可变的</u>。例如，隐私条例可能要求在用户关闭账户后删除他们的个人信息，数据保护法规可能要求删除错误的信息，或者可能需要控制敏感信息的意外泄漏。

​	<u>在这种情况下，仅仅在日志中追加另一个事件来指示先前的数据被视为已删除是明显不够的。实际上，你是想重写历史数据并且假装数据从未写入。例如，Datomic称该特性为excision(切除)，而Fosil版本控制系统也有一个类似的概念，叫作shunning(回避)</u>。

​	**真正的删除数据反而会非常困难，这是因为数据副本可能在很多地方都有。例如，存储引擎、文件系统和固态硬盘通常会写入一个新的地址而不是覆盖原地址，而备份通常是不可改变的，以防止意外被删除或破坏。删除更多的是“使检索数据更加困难”之意，而不是“使检索数据彻底不可能”**。尽管如此，有时必须尝试一下如将在第12章的“立法与自律”中看到的那样。

## 11.3 流处理

​	本章到目前为止，已经讨论了流的来源(用户活动事件，传感器以及数据库的写操作)，讨论了流是如何传输的(通过直接消息传递，通过消息代理和事件日志)。

​	接下来需要讨论的是，有了流之后，可以用它来做什么，即怎么处理它。一般来说有三种选择：

1. 可以将事件中的数据写入数据库、缓存、搜索索引或者类似的存储系统，然后被其他客户端查询。如图11-5所示，这是保持数据库与系统其他部分同步的一种好方法，特别是在消费者是写入数据库的唯一客户端的情况下。写入存储系统的流等效于在第10章“批处理工作流的输出”讨论的内容。
2. 可以通过某种方式将事件推送给用户，例如通过发送电子邮件警报或推送通知，或者将事件以流的方式传输到实时仪表板进行可视化。在这种情况下，人是流的最终消费者。
3. 可以处理一个或多个输入流以产生一个或多个输出流。数据流可能会先经过由几个这样的处理阶段组成的流水线，最终在输出端结東(选项1或选项2)。

​	在本章的余下部分，我们将讨论选项3：处理流以产生其他派生流。处理流的代码逻辑被称为操作或者作业。它与第10章中讨论过的UNIX进程和 MapReduce作业密切相关，并且数据流的模式是类似的：<u>流处理器以**只读**的方式接收输入流，并以**仅追加**方式将处理输出写入新的位置</u>。

​	流处理器中的分区和并行化模式也非常类似于第10章中介绍的 MapReduce和数据流引擎，因此不在这里重复介绍。基本的映射操作(如转换和过滤记录)也是一样的。

​	流与批量作业的一个关键区别是，流不会结束。这种差别有很多含义：正如本章开始部分所讨论的，<u>排序对无界数据集没有意义，因此不能使用排序合并join</u>(请参阅第10章“reduce端的join与分组”)。容错机制也必须改变：对于已经运行了几分钟的批处理作业，可以简单地从头开始重新启动失败的任务，但是对于已经运行好几年的流处理作业，在崩溃之后重新开始几乎不可行。

### 11.3.1 流处理的适用场景

​	流处理长期以来一直被用于监控目的，即希望在发生某些特定事件时收到警报。例如：

+ 欺诈检测系统需要确定信用卡的使用方式是否发生了意外的变化，如果信用卡看起来可能已经被盜，就要将其冻结。
+ 交易系统需要检査金融市场的价格变化，并根据指定的规则进行交易。
+ 制造系统需要监控工厂中机器的状态，在出现故障时快速识别冋题。
+ 军事和情报系统需要追踪潜在的侵略者的活动，并在有迹象表明发生袭击时发出警报。

​	这些类型的应用需要相当复杂的模式匹配和相关性。然而，流处理的其他用途也随着时间的推移而出现了。本节，我们将简要比较一下这些应用。

#### 复杂事件处理

​	**复杂事件处理(Complex Event Processing，CEP)**是20世纪90年代为分析事件流而发展的一种方法，尤其适用需要搜索特定的事件模式。<u>与正则表达式支持在字符串中搜索特定字符模式的方式类似，CEP允许指定规则，从而可以在流中搜索特定模式的事件</u>。

​	CEP系统通常使用像SQL这样的高级声明式査询语言或图形用户界面，来描述应该检测到的事件模式。这些査询提交给一个处理引擎，该引擎使用输入流并在内部维护匹配所需的状态机。当发现匹配时，引擎产生一个复杂的事件，这个事件包括检测到的事件模式的细节信息。

​	在这些系统中，査询和数据之间的关系与普通数据库相比正好相反。通常情况下，数据库会持久存储数据并将查询视为暂时的：当查询到来时，数据库搜索与查询匹配的数据，然后在查询完成时忘记它。<u>CEP引擎则反转了这些角色：查询是长期存储的，来自输入流的事件不断流过他们以匹配事件模式</u>。

​	CEP的实现包括 Esper、 IBM Info Sphere Streams、Apama、TIBCO StreamBase和SQLstream。像Samza这样的分布式流处理器也对流支持声明式SQL查询。

#### 流分析

​	使用流处理的另一个领域是对流进行分析。CEP和流分析之间的界限有些模糊，但作为一般规则，分析往往不太关心找到特定的事件序列，而更多地面向大量事件的累计效果和统计指标，例如：

+ 测量某种类型事件的速率(每个时间间隔发生的频率)。
+ 计算一段时间内某个值的滚动平均值。
+ 将当前的统计数据与以前的时间间隔进行比较(例如，检测趋势或提醒与上周同一时间相比异常高或低的指标)。

​	这些统计信息通常是在固定的时间间隔内进行计算的，例如，过去5min内每秒对服务的平均查询次数是多少，以及在此期间的第99个百分位的响应时间；几分钟内的平均值可以消除从一秒内无关紧要的波动，同时还能及时了解流量的最新变化。<u>聚合操作的时间间隔称为**窗口**</u>，我们将在本章后面“关于时间的推理”中更详细地讨论窗口。

​	流分析系统有时使用概率算法，比如用于设置成员关系的布隆过滤器(在第3章的“性能优化”中介绍过)，用于基数估计的HyperLogLog，以及各种百分比估值计算法(请参阅第1章“实饯中的百分位数”)。概率算法只能产生近似的结果，但其优点是在流处理器中所需的内存明显少于精确算法。近似算法的使用有时会导致人们认为流处理系统总是失真和不够精确，但这种观点是错误的：**流处理本身并没有任何固有的近似处理，概率算法仅仅是一种优化**。

​	许多开源的分布式流处理框架在设计时都考虑了对分析的支持，例如， Apache Storm、 Spark Streaming、Fink、 Concord、 Samza和 Kafka Streams。类似的，还有一些托管服务如Google Cloud Dataflow和Azure Stream Analytics。

#### 维护物化视图

​	在本章前面“数据库和数据流”看到，可以使用数据库更改流来保持派生数据系统(如缓存、搜索索引和数据仓库等)与源数据库之间的同步。可以将这些示例视为种维护物化视图的例子(请参阅第3章“聚合:数据立方体与物化视图”)：对某个数据集导出一个特定的试图以便高效查询，并在底层数据更改时自动更新该导出视图。

​	同样，在事件溯源中，应用状态通过应用事件日志来维护，这里的应用状态也是一种物化视图。与流分析场景不同，仅考虑某个时间窗口内的事件通常还不够：除了那些可能被日志压缩丢弃的事件(请参阅本章前面“日志压缩”)，构建物化视图可能需要任意时间段内的所有事件。实际上，需要的是一个可以延伸到开始时间的足够长的窗口。

​	原则上，任何流处理器都可以用于物化视图维护，尽管永久维护事件的需求与面向分析的框架的假设背道而驰，这些框架主要在有限持续时间的窗口上运行。 Samza和Kafka Streams支持这种用法，基于Kafka的日志压缩功能。

#### 在流上搜索

​	CEP通常搜索包含多个事件的特定模式，除了CEP，有时还需要基于一些复杂条件(例如仝文搜索查询)来搜索单个事件。

​	举个例子，媒体监控服务订阅来自媒体机构的新闻文章或者公告，并支持搜索关于公司、产品或感兴趣主题的相关新闻。这是通过预先制定一个搜索查询来完成的，然后不断地将新闻流与这个查询进行匹配。在一些网站上也有类似的功能：例如，房地产网站的用户需要当市场上出现符合其搜索条件的新房产时被及时通知。 <u>Elasticsearch的过滤器功能是实现这种**流式搜索**的一种方式</u>。

​	传统的搜索引擎首先索引文档，然后在索引上运行查询。**相比之下，搜索流则是反过来：査询条件先保存下来，所有文档流过査询条件，就像CEP一样**。最极端的情况可以针对每个查询来测试每个文档，但是如果有大量的查询，这可能会非常缓慢。为了优化，可以对查询和文档都进行索引，从而缩小匹配的查询集合。

#### 消息传递和RPC

​	在第4章“基于消息传递的数据流”中，我们讨论了消息传递系统作为RPC的替代方法，即作为通信服务的一种机制，例如actor模型中所使用的机制。虽然这些系统也是基于消息和事件，但是我们通常并不把他们归类为流处理系统：

+ Actor框架主要是管理通信模块的并发和分布式执行的机制，而流处理主要是数据管理技术。
+ Actor之间的交流往往是短暂的，并且是一对一的，而事件日志是持久的、多用户的。
+ <u>Actor可以以任意方式进行通信(包括循环请求/响应模式)，但流处理器通常设置在非循环流水线中，其中毎个流是一个特定作业的输出，并且从一组定义明确的输入流派生而来</u>。

​	也就是说，RPC类系统和流处理之间有一些交叉的地方。例如，Apache Storm有一个称为分布式RPC的功能，它支持将用户查询分布到一组同时运行事件处理的节点上，然后这些查询与来自输入流的事件交织在一起，查询结果可以从多节点聚合而来然后返回给用户(另请参阅第12章的“多分区数据处理”)。

​	你也可以使用 Actor框架来处理流。然而，许多这样的框架在崩溃的情况下不能保证消息的传递，所以处理不是容错的，除非实现额外的重试逻辑。

### 11.3.2 流的时间问题

​	流处理系统经常需要和时间打交道，尤其是在用于分析目的时，这些分析通常使用时间窗口，例如“最近五分钟内的平均值”。似乎“最后五分钟”的含义应该明确无误，但不幸的是，这种定义处理起来非常棘手。

​	在批处理过程中，处理任务会快速处理大量的历史事件。如果需要按时间进行某种分解，批处理需要查看每个事件所嵌入的时间戳。<u>因为处理运行的时间与事件实际发生的时间无关，所以查看运行批处理的机器的系统时钟没有意义</u>。

​	批处理可以在几分钟内读取一年的历史事件；在大多数情况下，关注的是一年的历史事件，而不是几分钟的处理过程。而且，在事件中使用时间戳可以使得处理过程是确定性的：基于同一个输入，再次运行相同的处理过程可以得到相同的结果(请参阅第9章的“故障容错”)。

​	另一方面，许多流处理框架使用处理节点上的本地系统时钟(处理时间)来确定窗口。这种方法的优点是简单，当事件发生和事件处理之间的间隔可以忽略不计时这种方式也是合理的。然而，如果存在显著的处理滞后，即处理可能比事件实际发生的时间明显要晚，则该方法不再有效。

#### 事件时间与处理时间

​	发生事件处理滞后的原因有很多，例如排队、网络故障(请参阅第8章“不可靠的网络”)、导致消息代理或处理器中岀现竞争的性能问题，重新启动流的消费者，重新处理过去的事件(请参阅本章前面“重新处理消息”，例如从错误中恢复或修复了代码的错误)。

​	而且，消息延迟还可能导致消息的不可预知的排序。例如，假设用户首先发出一个Web请求(由Web服务器A处理)，然后发出第二个请求(由服务器B处理)。A和B发出描述它们所处理的请求的事件，但是B的事件在A的事件发生之前到达消息代理。现在，流处理器将首先看到B事件，然后看到A事件，即使它们实际上是以相反的顺序发生的。

​	如果找一个类比的话，可以考虑一下“星球大战”这部电影：第四部于1977年上映，1980年的第五部，1983年的第六部，之后分别于1999年，2002年和2005年上映第一部，第二部，第三部，以及2015年的第七部。如果以上映顺序来观看电影则观看(“处理”)电影的顺序与它们叙述的事件顺序就是不一致的(每部的编号就像事件时间戳一样，而观看电影的日期就是处理时间)。人类能够应付这样的不连续性，但是流处理算法则需要专门的代码处理，以适应这样的时间和排序问题。

​	**混淆事件时间与处理时间会导致错误的结果**。例如，假设有一个流处理操作来测量请求频率(计算毎秒的请求数)。如果重新部署了该流处理系统，则中间可能会关闭分钟，在重启后继续处理积压的事件。<u>如果根据处理时间来衡量请求频率，那么看起来好像在处理积压事件时突然出现了异常的请求高峰，而事实上则是请求频率一直是稳定的</u>(见图11-7)。

![流处理 - 图1](https://static.sitestack.cn/projects/ddia/img/fig11-7.png)

#### * 了解什么时候准备就绪

​	<u>如果**基于事件发生时间**而定义窗口，面临一个棘手的问题是，你无法确定什么时候能收到特定窗口内的所有事件，或者是否还有一些事件尚未到来</u>。

​	例如，假设将事件分组成一分钟的窗口，以便可以统计每分钟的请求数。你已经计算了一些事件，这些事件的时间戳在一小时的第37分钟，时间继续向前移动，现在大部分事件都到了第38分钟和第39分钟。什么时候该宣布已经完成了第37分钟窗口的处理，可以输出计数器值？

​	在一段时间没有看到任何新的事件之后，可以认为超时并宣布关闭该窗口，但由于网络中断而延迟，仍然可能发生某些事件其实还缓存在另一台计算机上。**需要能够处理在窗口已经声明完成后才到达的这样的滞后事件**。大体上，有两个选择：

1. 忽略这些滞后的事件，因为他们在正常情况下可能只是一小部分事件。可以将丢弃事件的数量作为度量标准进行跟踪，当出现丢弃大量数据时发出警报。
2. <u>发布一个更正：针对滞后事件的一个更新值。可能还需要收回以前的输出</u>。

​	在某些情况下，可以使用一个特殊的消息来表明，“从现在开始，不会有比t更早的时间戳的消息”，消费者可以使用它来触发窗口处理。但是，如果不同机器上的多个生产者正在生成事件，每个事件都有自己的最小时间戳阈值，则消费者需要分别跟踪毎个生产者。在这种情况下添加和删除生产者变得比较麻烦。

#### * 你用谁的时钟？

​	当事件可能在系统中的多个点缓冲时，为事件分配时间戳就比较困难。例如，考虑向服务器报告使用率事件的移动应用。该应用可能会在设备处于离线状态时使用，在这种情况下，它会在设备本地缓存事件，并在下一次有可用的网络连接(可能是几小时甚至几天)时将其发送到服务器上。对于这个流的任何一个消费者来说，这些事件都是极其滞后的。

​	<u>这时，根据移动设备的本地时钟，事件的时间戳实际上指的是发生交互时的时间。然而，用户控制的设备上的时钟通常是不可信的，它可能会被意外或故意设置为错误的时间(请参阅第7章“时钟同步与精度”)。服务器收到事件的时间(根据服务器的时钟)更可能是准确的，因为服务器在你的控制之下，但是在描述用户交互方面意义就不大了</u>。

​	为了调整不正确的设备时钟，一种方法是记录三个时间戳：

+ 根据设备的时钟，记录事件发生的时间。
+ 根据设备的时钟，记录将事件发送到服务器的时间。
+ 根据服务器时钟，记录服务器收到事件的时间。

​	<u>通过从第三个时间戳中减去第二个时间戳，可以估计出设备时钟和服务器时钟之间的偏移量(假设网络延迟与所需的时间戳精度相比可以忽略不计)。然后，可以将该偏移量应用于事件时间戳，从而估计事件实际发生的真实时间(假设设备时钟偏移量在事件发生的时间与发送到服务器的时间之间没有发生变化)</u>。

​	这个问题并不是流处理独有的，批处理也会在时间推理方面遇到相同的问题。只不过在流处理环境中更加明显，因为我们更能注意到时间的变化。

#### 窗口类型

​	一旦明确了如何确定事件的时间戳，下一步就是决定如何定义时间段即窗口了。然后窗口就可用于聚合分析，例如计数事件，或计算窗口内的所有值的平均值。有以下几种常见的窗口类型：

+ 轮转窗口

  翻滚窗口的长度是固定的，每个事件都属于一个窗口。例如，如果有一个1分钟的翻滚窗口，则所有时间戳在10:03:00和10:03:59之间的事件都会被分组到一个窗口中，10:04:00和10:04:59之间的事件被分组到下一个窗口，依此类推。可以通过获取毎个事件时间戳并将其四舍五入到最接近的分钟来确定它所属的窗口，从而实现1分钟的轮转窗口。

+ 跳跃窗口

  跳跃窗口也具有固定长度，但允许窗口重叠以提供一些平滑过渡。例如，一个5分钟窗口，设定跳跃(hop)值为1分钟，则包含10:03:00至10:07:59之间的事件，而下一个窗口将包括10:04:00至10:08:59之间的事件，依此类推。可以通过首先计算1分钟滚动窗口，然后聚合几个相邻的窗口来实现跳跃窗口。

+ 滑动窗口

  滑动窗口包含在彼此的某个间隔内发生的所有事件。例如，一个5分钟的滑动窗口将包括10:03:39和10:08:12的事件，因为它们相距不到5分钟(注意，轮转和跳跃的5分钟窗口不会将这两个事件置于相同的窗口，因为它们都基于固定的边界)。<u>滑动窗口可以通过保留按时间排序的事件缓冲区并且在从窗口过期时移除旧事件来实现</u>。

+ 会话窗口

  与其他窗口类型不同，会话窗口没有固定的持续时间。相反，它是通过将同一用户在时间上紧密相关的所有事件分组在一起而定义的，一旦用户在一段时间内处于非活动状态(例如，如果30分钟内没有事件)，则窗口结束。会话分析是网站分析中常见的一种需求(参阅第10章“分组”)

### 11.3.3 流式join

​	在第10章中，我们讨论了批处理作业如何通过主键来join数据集，以及这种join如何构成数据管道。由于流处理将数据管道推广为对无界数据集的增量处理，因此对流进行join的需求也完全适用。

​	但是，新事件随时可能出现在流中，这个事实使得流式join比批处理作业更具挑战性。为了更好地理解，我们来区分三种不同类型的join：流和流join，流和表join和表和表join。在下面的章节中，我们将通过例子来逐一说明。

#### 流和流join （窗口join)

​	假设某网站支持搜索功能，并且想要检测网址搜索的最新趋势。每次有人输入搜索査询时，都会记录包含查询和返回结果的事件。每当有人单击其中一个搜索结果时，就会记录另一个单击的事件。为了计算搜索结果中每个网址的单击率，需要将搜索操作和单击操作的事件组合在一起，这些事件通过具有相同的会话ID进行join。广告系统也需要类似的分析。

​	如果用户并不相信搜索结果，可能永远不会发生单击事件，即使发生了这样的情况，搜索和单击之间的时间也可能存在很大间隔：在许多情况下可能是几秒钟，但是可能长达几天或几周(如果用户运行完搜索后忘记了浏览器的这个选项卡，过了些时间后又打开这个选项卡，再单击这个搜索结果)。由于网络延迟也是变化的，单击事件甚至可能在搜索事件之前到达。这时可以定义合适的join窗口，例如，如果搜索和单击的间隔不超过一小时，则可以join它们。

​	请注意，在单击事件中嵌入搜索的细节并不等同于join事件：这样做只会告诉你用户单击了什么搜索结果，而无法告诉你用户没单击时的情况。评估搜索质量需要准确的单击率，因此需要搜索事件和单击事件两个事件源。

​	<u>为了实现这种类型的join，流处理操作需要维护状态：例如，在最后一小时发生的所有事件(基于会话ID索引)。无论什么时候发生搜索事件或单击事件，都会将其添加到适当的索引，并且流处理还检查另一个索引，以查看同一个会话ID的另一个事件是否已到达。如果有匹配的事件，则发出一个派生事件来表明哪个搜索结果发生了单击。如果搜索事件过期而没有匹配到单击事件，则发出一个派生事件表明哪些搜索结果未被单击</u>。

#### 流和表join

​	在第10章的“示例:用户行为事件的分析”(见图10-2)中，我们介绍了join两个数据集的批处理任务例子：一组用户活动事件和一个用户资料。将用户活动事件视为流，并在流处理器中连续执行相同的join是比较自然的处理方法：输入是包含用户ID的活动事件流，输出是在用户ID上追加了用户的资料信息的活动事件流。这个过程有时被称为使用来自数据库的信息丰富活动事件。

​	要执行此join，流处理过程需要一次查看一个活动事件，在数据库中査找事件的用户ID，然后将该概要信息添加到活动事件中。数据库查询可以通过查询远程数据库来实现。但是，正如在第10章“示例:分析用户活动事件”中讨论的，此类远程查询可能很慢并且有可能导致数据库过载。	

​	另一种方法是将数据库副本加载到流处理器中，以便在本地进行查询而无需经过网络往返。这种技术与第10章“Map端的join操作”中所讨论的**哈希join**非常相似：本地副本可能是位于内存中的哈希表(如果数据比较小)，或者是本地磁盘上的索引。

​	**与批处理任务的区别在于，批处理任务使用数据库的时间点快照作为输入，而流处理器是长吋间运行的，并且数据库的内容可能随时间而改变，所以流处理器数据库的本地副本需要保持最新**。这个问题可以通过变更数据捕获手段来解决:流处理器可以订阅用户资料数据库的更新日志以及活动事件流。在创建或修改资料时，流处理器会更新其本地副本。因此，我们获得两个流之间的join。

​	流和表join实际上非常类似流和流join。它们最大的区别在于，对于表的更新日志流，join使用一个可以回溯到“开始时间”(一个在概念上是无限的窗口)的窗口，而新版本的记录会覆盖旧的记录。对于流输入，join可能根本无法维护这样一个窗口。

#### * 表和表join （物化视图维护）

​	考虑在第1章“描述负载”时 Twitter时间线的例子。当用户想要查看其主页时间线时，循环遍历用户关注的所有人、查找他们最近的推文并将其合并，这个操作代价很高。

​	相反，我们需要一个时间线缓存：这是一种基于每个用户的“收件箱”，在发送推文的时候就将其写入其中，因此读取时间线就只需要一次查询。实现和维护这个缓存需要以下事件处理：

+ 当用户u发送新的推文时，它被添加到每个关注你的用户的时间线上。
+ 当用户删除推文时，会从所有用户的时间线中删除。
+ 当用户u1开始关注用户u2，u2最近的推文被添加到u1的时间线上。
+ 当用户u1取消关注用户u2时，由u2发出的推文将从u1的时间线中移除。

​	要在流处理中实现这种缓存维护，需要用于推文(发送和删除)和关注关系(关注和取消关注)的事件流。流过程需要维护一个包含每个用户的关注者集合的数据库，以便知道当一个新的推文到达时需要更新哪些时间线。

​	理解这个流处理过程的另一种方法是，它维护一个两个表(推文和关注者)join查询的物化视图，如下所示：

```sql
SELECT follows.follower_id AS timeline_id, 
    array_agg(tweets.* ORDER BY tweets.timestamp DESC)
FROM tweets
JOIN follows ON follows.followee_id = tweets.sender_id 
GROUP BY follows.follower_id
```

​	<u>流的join直接对应于该查询中的表的join。时间线实际上是这个査询结果的缓存，每当底层表发生变化时就进行更新</u>。

#### * join 的时间依赖性

​	这里描述的三种类型的join(流-流，流-表和表-表)有很多相同之处：它们都需要流处理系统根据一个join输入来维护某些状态(例如搜索和单击事件，用户资料或关注列表)，并在来自另一个join输入的消息上查询该状态。

​	维持这个状态的事件的顺序非常重要(它关系到是先关注然后解除关注，或是相反)。在分区日志中，单个分区内的事件排序被保留，但**通常在不同的流或分区之间没有排序的保证**。

​	这就产生了一个问题：如果不同流中的事件发生在相近的时间里，它们按照何种顺序进行处理？在流和表join的示例中，如果用户更新了他们的资料，哪些活动事件会与旧资料(在资料更新之前处理)join，哪些又与新资料join(在资料更新之后处理)？换句话说：**如果状态随时间而改变，而join操作需要输入状态信息，那么应该使用什么时间点的状态来join呢?**

​	这种时间依赖性可能会在许多地方发生。比如，销售某些商品，需要对发票计算适当的税率，而税率取决于不同的国家或者州、产品类型和销售日期(因为税率会随时变化)。将销售情况与税率表join时，就希望join销售时所对应的税率，而如果正在重新处理历史数据，这可能就与当时的税率不同。	

​	如果跨流的事件排序是不确定的，那么join也变成非确定性的，这意味着你不能在相同的输入上重新运行同一作业来得到相同的结果：因为再次运行任务时，输入流上的事件可能以不同的方式交叉在一起。

​	在数据仓库中，这个问题被称为**缓慢变化的维度(Slowly Changing Dimension，SCD)**，通常通过对特定版本的join记录赋予唯一的标识符来解决：例如，毎当税率改变时，就獻予一个新的标识符，而发票也包括销售时的税率标识符。<u>这种方式使join操作具有确定性，但是由于表中所有不同版本的记录都需要保留，最后日志压缩几乎无法有效工作</u>。

### 11.3.4 流处理的容错

​	在本章的最后部分，我们来考虑一下流处理器如何容错。在第10章中看到，批处理框架可以比较容易实现容错：如果 MapReduce作业中的任务失败，可以简单地在另一台机器上重新启动，并丢弃失败任务的输出。这种透明的重试是可能的，因为输入文件是不可变的，每个任务都将其输出写到HDFS上的单独文件，并且输出仅在任务成功完成时可见。

​	<u>特别是，批处理容错方法可以确保批处理作业的输出与没有出错时的最终结果相同，即使事实上某些任务失败了</u>。看起来好像毎个输入记录都恰好被处理了一次，没有记录被遗漏，也没有记录被处理两次。尽管重新启动任务意味着实际上可能会多次处理记录，但在输出中的效果看起来和只处理过一次一样。这个原则被称为**恰好一次**语义，虽然有效一次应该更为准确。

​	在流处理过程中也会出现同样的容错问题，但是处理起来并不那么简单：<u>在使输出结果可见之前等待某个任务完成是不可行的，由于流是无限的，因此几乎永远无法完成这个任务</u>。

#### * 微批处理和校验点

​	<u>一种解决方案是将流分解成多个小块，并像小型批处理一样处理毎个块。这种方法被称为**微批处理**，它已经用于 Spark Streaming</u>。批处理大小通常约为1s，这是一个性能折中的考虑：较小的批处理会导致更大的调度和协调开销，而较大的批处理意味着流处理器的结果需要更长的延迟才能可见。

​	<u>微批处理隐含地设置了与批处理大小相等的轮转窗口(基于处理时间而不是事件时间)。任何需要更大窗口的作业都需要显式地将状态从一个微批处理保留至下一个微批处理</u>。

​	**Apache Flink中使用了该方法的一个变体，它<u>定期生成状态滚动检查点并将其写入持久化存储</u>**。如果流操作发生崩溃，它可以从最近的检查点重新启动，并丢弃在上一个检査点和崩溃之间生成的所有输出。检査点是由消息流中的barrier触发，类似于微批处理之间的边界，但并不强制特定的窗口大小。

​	<u>在流处理框架的范围内，微批处理和检查点的方法提供了与批处理一样的**恰好一次语义**</u>。<u>但是，一旦输出脱离了流处理系统(例如，通过写入数据库，向外部消息代理发送消息或发送电子邮件)，框架将无法丢弃失败批处理的输出了。在这种情况下，重启失败的任务将导致发生两次外部可见的副作用，单独使用微批处理或检查点都不足以防范此类问题</u>。

#### * 重新审视原子提交

​	<u>在岀现故障时，为了看起来实现**恰好处理了一次**，我们需要确保当且仅当处理成功时，所有输出和副作用才会生效</u>。这包括发送给下游的操作或外部消息传递系统(包括邮件或推送通知)的任何消息，所有数据库的写入，以及对操作状态的更改和任何对输入消息的确认(包括基于日志的消息代理中，向前移动消费者偏移量)。

​	这些事情要么原子的发生，要么都不发生，但不应该彼此不同步。这种方法听起来很熟悉，因为在第9章“恰好一次消息处理”中分布式事务和两阶段提交的上下文中已经有所讨论。

​	在第9章中，讨论了分布式事务的传统实现中的问题，比如XA。然而，在更受限制的环境中，有可能有效地实现这样的原子提交方法。 Google Cloud Dataflow和VoltDB中使用了这种方法， Apache Kafka也计划添加类似的功能。<u>与XA不同，这些实现不会尝试跨异构技术提供事务，而是通过在流处理框架中管理状态更改和消息传递来保持内部事务</u>。事务协议的开销可以通过在单个事务中处理多个输入消息来分摊。

#### * 幂等性

​	我们的目标是丢弃任何失败任务的部分输出，以便它们可以安全地重试而不会两次生效。分布式事务是实现这一目标的一种方式，而另一种方式则是依赖幂等性。

​	幂等操作是可以多次执行的操作，并且它与只执行一次操作具有相同的效果。例如，将键-值存储中的键设置为某个固定值是幂等的(再次写入该值会覆盖已有的相同值)，而递増计数器则不满足幂等(再次执行递増意味着该值递增两次)。

​	**即使操作不是天生具备幂等性，往往也可以使用一些额外的元数据使其变得幂等**。<u>例如，处理来自 Kafka的消息时，每条消息都有一个持久的、单调递增的偏移量。将值写入外部数据库时，可以在该值中包含触发最新写入消息所对应的偏移量。因此，根据该偏移量可以判断是否已经应用了该更新，从而避免重复执行相同的更新操作</u>。

​	Strom的Trident的状态处理也是基于类似的想法。依赖于幂等性意味着基于一些假设：重启失败的任务必须以相同的顺序重新处理相同的消息(基于日志的消息代理是这么做的)，处理必须是确定性的，并且没有其他节点并发更新相同的值。

​	<u>当从一个处理节点切换至另一个节点时，可能需要必要的**fencing措施**(请参阅第8章“主节点与锁”)，以防止标记为失败但实际上仍处于活动状态的节点的干扰</u>。<u>尽管存在这些禁忌，幂等操作依然是一种有效的方式来实现**恰好一次语义**，并且开销很小</u>。

#### * 故障后重建状态

​	任何需要状态的流处理，比如基于窗口的聚合(诸如计数器，平均值和直方图)以及表和索引的join操作，都必须确保在故障发生后状态可以恢复。

​	<u>一种选择是将状态保存在远程存储中并采取复制，然而为每个消息去查询远程数据库可能会很慢。另一种方法是将状态在本地保存，并定期进行复制。之后，当流处理器从故障中恢复时，新任务可以读取副本的状态并且在不丢失数据的情况下恢复处理</u>。

​	例如， <u>Flink定期对操作状态执行快照，并将他们写入HDFS等持久性存储中</u>。Samza和 Kafka Streams通过将状态更改发送到具有日志压缩功能的专用Kafka主题来保存状态的副本，这类似于**变更数据捕获**。 VoltDB则通过在多个节点上冗余处理毎个输入消息来复制状态(请参阅第7章“实际的串行执行”)。

​	在某些情况下，甚至可能不需要复制状态，而是从输入流开始重建。例如，如果状态由相当短的窗口中的聚合组成，简单地replay与该窗口相对应的输入事件就可能足够快。如果状态是数据库的本地副本(基于变更数据捕获方式来维护)，那么也可以从日志压缩的更改流那里重建数据库(请参阅本章前面“日志压缩”)。

​	但是，<u>所有这些权衡取决于底层基础架构的性能表现：在某些系统中，网络延迟可能低于磁盘访冋延迟，网络带宽可能与磁盘带宽相当。不存在适用于所有情况的理想的折中方案，随着存储和网络技术的发展，本地和远程状态的优点也可能会发生变化</u>。

## 11.4 小结

​	在本章中，我们讨论了事件流，他们的用途以及处理方法。在某些方面，流处理与第10章中讨论的批处理非常相似，但是它是对无限(永不停止)的流而不是固定大小的输入进行持续的处理。从这个角度讲，消息代理和事件日志可以视为流处理系统中文件系统。

​	我们花了一些时间比较两种类型的消息代理：

+ **AMQP/JMS风格的消息代理**

  代理将单个消息分配给消費者，消费者在成功处理后确认毎条消息。**消息被确认后从代理中删除**。这种方法适合作为一种**异步RPC**(另请参阅第4章“基于消息传递的数据流”)，<u>例如，在任务队列中，消息处理的确切顺序并不重要，并且不需要在他们处理完后返回并再次读取旧消息</u>。

+ **基于日志的消息代理**

  <u>代理将分区中的所有消息分配给相同的消费者节点，并始终以相同的顺序发送消息</u>。通过分区机制来实现并行，消费者通过检查他们处理的最后一条消息的偏移量来跟踪进度。<u>代理将消息保存在磁盘上，因此如果有必要，**可以回退**并重新读取旧消息</u>。

​	基于日志的方法与在数据库(请参阅第5章)和日志结构化存储引擎(请参阅第3章)中的复制日志相似。我们看到，这种方法特别适合流处理系统，即不断汲取输入流并生成派生状态或派生输出流。

​	就流的来源而言，我们讨论了几种可能性，包括用户活动事件，提供定期读数的传感器，以及可订阅数据源(如金融市场数据)等可以自然地表示为流。我们还看到，将对数据库的写入视为流也是有用的：可以**捕获变更日志**(即对数据库所做的所有更改的历史记录)，包括隐式地通过变更数据捕获或显式地通过**事件溯源**两种方法。通过日志压缩可以保留数据库内容的完整副本。

​	将数据库表示为流为高效集成系统提供了更多的机会。通过使用变更口志并将其应用到派生系统，可以不断更新搜索索引、缓存和分析系统等派生数据系统。甚至可以全新做起，将所有数据从始至终都视为变更日志，从而构建全新的视图。

​	**将状态保持为流并支持重做消息也是在各种流处理框架中实现基于流的join和容错的基础**。我们讨论了流处理的几个目标场景，包括<u>基于事件模式的查询(复杂事件处理)</u>，<u>计算窗口聚合(流分析)</u>以及<u>保持派生数据系统处于最新状态(物化视图)</u>等。

​	接下来讨论了在流处理中有关时间方面的些考虑，包括**处理时间**和**事件时间**之间的区别，以及**滞后事件**(在认为窗口已关闭之后才到达的事件)问题。

​	我们区分了流处理过程中可能出现的三种类型的join：

+ 流和流join

  两个输入流都由活动事件组成，采用join操作用来搜索在特定时间窗口内发生的相关事件。例如，它可以匹配相同用户在30min内采取的两个动作。如果想要在一个流中査找相关事件，则两个join输入可以是相同的流。

+ 流和表join

  一个输入流由活动事件组成，而另一个则是数据库变更日志。更新日志维护了数据库的本地最新副本。对于每个活动事件，join操作用来查询数据库并输出一个包含更多信息的事件。

+ 表和表join

  两个输入流都是数据库更新日志。在这种情况下，一方的每一个变化都与另一方的最新状态相join。结果是对**两个表之间join的物化视图进行持续的更新**。

​	最后，我们讨论了在流处理器中实现**容错**和**恰好一次语义**的技术。**与批处理一样，我们需要丢弃所有失败任务的部分输出**。<u>然而，由于流处理长时间运行并持续产生输出，所以不能把所有的输出都筒单地丟弃掉。相反，基于微批处理、检査点、事务或幂等性写入等，可以实现更细粒度的恢复机制</u>。

# 第12章 数据系统的未来

## 12.1 数据集成

​	本书反复强调了一个主题，对于任何给定的问题，都有多种解决方案，而这些解决方案都有各自优缺点和折中之处。例如，在第3章讨论存储引擎时，我们看到了日志结构存储，B-tree和列式存储。在第5章讨论复制时，我们介绍了主从复制，多主节点复制和无主节点复制。

​	如果给你一个问题，比如“需要把数据保存起来，可以支持查询”，这样的问题其实不存在唯一正确的解决方案，或者说不同具体情况下有不同的方案。而软件的实现通常必须选择一种特定的方法。通常一种实现很难同时兼具鲁棒与性能，如果试图兼顾所有方面，那唯一可以肯定的是最终的实现一定很糟糕。

​	因此，选择合适的软件组件也需要视情况而定。毎一个软件，即使是所谓的“通用”数据库，也都是针对特定的使用模式而设计的。

​	面对如此众多的备选方案，第一个挑战就是弄清楚软件产品与他们适合运行环境之间的对应关系。软件厂商是不愿意告诉你它们的软件不适合哪些负载，所以系统通过前面的章节可以帮助你准备一些有针对性的问题，例如产品字里行间的含义以及设计取舍之道。

​	然而，即使你完全理解了工具与使用环境之间的对应关系，还有一个挑战摆在那里在复杂的应用程序中，数据通常以多种不同的方式被使用。不太可能存在适用于所有不同环境的软件，因此你不可避免地要将几个不同的软件组合在一起，以提供应用程序的功能性。

### 12.1.1 采用派生数据来组合工具

​	例如，<u>经常需要将一个OLTP数据库和全文搜索索引集成起来以处理任意关键字查询</u>。虽然有些数据库(如 PostgreSQL)支持全文索引功能，可以满足一些简单应用但更复杂的查询则需要专业的信息检索工具。另一方面，搜索索引通常不太适合作为一个持久的记录系统，所以许多应用程序需要结合两种不同的工具来满足所有需求。

​	我们在第11章“保持系统同步”中提到了数据系统集成问题。随着不同类型的数据持续增加，集成问题会变得越来越困难。<u>除了数据库和搜索索引，你可能需要在分析系统中保存数据副本(例如数据仓库，批处理和流处理系统)；从派生数据中维护数据缓存或非规范化版本；把数据传递给机器学习系统，或者分类，排序以及推荐系统；还有发送数据更改的通知等</u>。

​	令人惊讶的是，我经常看到软件工程师煞有介事地说，“根据我的经验，99%的人只需要X”或“不需要X”(X可以代表各种值)。我认为这样的说法更多地在说他多么多么的有经验，而不是在谈论某项技术的实际用途。针对数据所做的事情千变万化，涉及的范围也是惊人的广泛。某些人认为是一个模糊而毫无意义的特性对其他人来说可能是核心需求。数据集成的需求通常只有在缩小并考虑整个组织框架内数据流时才会变得更加凸显。

#### 为何需要数据流

​	当同样数据的多个副本需要保存在不同的存储系统，以满足不同的访问模式需求时，你需要摸清楚数据输入和输岀：数据第一次是写在哪里？不同的数椐拷贝来自哪些数据源？如何以正确的格式将数据导入到所有正确的位置？

​	我们举个例子，比如可以将数据首先写入记录系统数据库，捕获对该数据库的更改(参见第11章“变更数据捕获”)，然后按相同顺序将更改应用到搜索索引中。如果变更数据捕获(CDC)是更新索引的唯一方法，那么你就可以确信索引完全来自记录系统，因此是一致的(软件bug除外)。写数据库是向系统提供新输入的唯一途径。

​	<u>允许应用程序同时向搜索索引和数据库写入会带来问题(见图11-4)，两个客户端同时发送冲突的写操作，而两个存储系统以不同的顺序来执行它们。在这种情况下，负责确定写操作顺序的既不是数据库也不是搜索索引，因此它们可能会做出相互矛盾的决定，以至于导致彼此之间永久的不一致</u>。

​	如果可以通过单个系统来决定所有输入的写入顺序，那么以相同的顺序处理写操作就可以更容易地派生岀数据的其他表示形式。这就是第9章“全序关系广播”中所介绍的状态机复制方法。无论是使用变更数据捕获还是事件获取日志，都不如**简化总体顺序**的原则重要。

​	根据事件日志来更新一个派生数据系统通常会比较好实现，并且可以实现**确定性**和**幂等性**(幂等概念参见第11章)，也因此使系统很容易从故障中恢复。

#### 派生数据与分布式事务

​	为了保持不同的数据系统彼此之间的一致性，经典的方法是通过分布式事务，我们在第9章讨论过原子提交和两阶段提交机制(2PC)。那么与分布式事务相比，派生数据系统的方法怎么样呢？

​	抽象点说，它们通过不同的方式达到类似的目标。分布式事务通过使用锁机制进行互斥来决定写操作的顺序(请参阅第7章“两阶段加锁”)，而<u>CDC和事件源使用**日志**进行排序</u>。分布式事务使用原子提交来确保更改只生效一次，而<u>基于日志的系统通常基于**确定性重试**和**幂等性**</u>。

​	最大的不同在于事务系统通常提供线性化(请参阅第9章“线性化”)，这意味着它可以保证读自己的写等一致性(请参阅第5章“读自己的写”)。另一方面，<u>派生的数据系统通常是**异步更新**的，所以默认情况下它们无法提供类似级别保证</u>。

​	在能够承受付出分布式事务所带来开销的特定环境中，分布式事物已有成功案例。但是，我认为XA的容错性和性能不尽如人意(请参阅第9章的“实践中的分布式事务”)，这严重限制了它的实用性。我相信为分布式事务未来可以有更好的协议，但是这样的协议被广泛采用而且能与现有工具集成将非常具有挑战性，并且不太可能在短期内发生。

​	考虑到好的分布式事务协议尚未得到广泛支持，我认为基于日志的派生数据是集成不同数据系统的最有前途的方法。然而，<u>一些保证如“读自己写”仍是非常有用，因此我不认为到处宣扬“最终的一致性是不可避免的，接受并学会处理它”有任何实际意义(至少在没有很好的指导来告诉人们如何处理它的情况下是不太可行的)</u>。

​	在本章后面“正确性目标”中，我们将讨论在异步派生系统之上实现更强保证的一些方法，并在分布式事务和基于日志的异步系统之外找到一条中间的路。

#### * 全序的局限

​	对于非常小的系统，构建一个完全有序的事件日志是完全可行的(流行的主从复制正是构造了这样的日志)。但是，随着系统越来越大，并且面对更为复杂的负载时，瓶颈就开始出现了:

+ <u>在大多数情况下，构建一个完全有序的日志需要所有事件都通过一个主节点来决定排序。如果事件吞吐量大于单台节点可处理的上限，则需要将其分区到多台节点上(请参见第11章的“分区日志”)，这就使得两个不同分区中的事件顺序变得不明确了</u>。
+ 如果服务器分布在多个不同地理位置的数据中心，为了避免整个数据中心不可用，且考虐到网络延迟使跨数据中心协调的同步效率很低，因此<u>通常在每个数据中心都有独立的主节点(参阅第5章“多主节点复制”)。这意味着来自两个不同数据中心的事件顺序不确定</u>。
+ 将应用程序部署为微服务时(请参阅第4章“基于服务的数据流：REST和RPC”)，常见的设计是将每个服务与其持久化的状态一起作为独立单元部署，而服务之间不共享持久化状态。当两个事件来自不同的服务时，这些事件没有清楚的顺序。
+ 某些应用程序在客户端维护一些状态，当用户输入时会立即更新(无需等待服务器的确认)，甚至可以继续离线工作(参阅第5章“客户端的离线操作”)。对于这样的应用程序，客户端和服务器很可能看到不同的事件顺序。

​	从形式上讲，决定事件的全序关系称为**全序关系广播**，它等价于**共识**(参见第9章“共识算法与全序关系广播”)。大多数共识算法是针对单节点吞吐量足以处理整个事件流而设计的，并且这些算法不提供支持多节点共享事件排序的机制。<u>设计突破单节点吞吐量甚至在广域地理环境分布的共识算法仍然是一个有待硏究的开放性问题</u>。

#### 排序事件以捕获因果关系

​	如果事件之间不存在因果关系，则不支持全序排序并不是一个大问题，因为并发事件可以任意排序。其他一些情况很容易处理：例如，当同一对象有多个更新时，可以通过将特定对象ID的所有更新都路由到同一个日志分区上。然而，因果依赖有时候会有一些更微妙的情况(详见第9章“排序与因果关系”)。

​	例如在一个社交网络服务里，有两个以前相恋，但现在分手了的用户。女孩子将这个分手的男朋友删除后，发送消息给她其余的朋友抱怨她的前男友。这个女孩子以为，他的前男友不应该看到这些抱怨信息，因为该消息是她删除男朋友的朋友状态之后发送的。

​	然而，如果朋友状态和消息分别存储在不同地方，那么可能会丢失“删除好友”事件和“抱怨”事件之间的顺序关系。如果没有正确捕获事件的因果关系，负责发送新消息通知的服务可能在“删除好友”之前就把“抱怨”消息广播给所有朋友，结果就是其前恋人还是会收到一条本不应该看到的消息。

​	在这个例子中，通知实际上是消息和朋友列表的联结操作，这和之前讨论的join与时间问题有关系了(请参阅第11章“join的时间依赖性”)。不幸的是，似乎这个问题并没有一个简单的答案。从头分析的话是这样的：

+ 逻辑时间戳可以在无协调者情况下提供的全序关系(参见第9章“序列号排序”)，所以当全序关系广播不可行时可以用得上。但是，它们仍然需要接收者去处理那些乱序事件，并且需要额外的元数据。
+ 如果可以记录一条事件来标记用户在做决定以前所看到系统状态，并给该事件个唯一的标识符，那么任何后续的事件都可以通过引用该事件标识符来记录因果关系。我们将在本章后面“读操作也是事件”中重谈这个概念。
+ 冲突解决算法(请参阅第5章的“自动冲突解决”)可以处理异常顺序的事件。他们可以帮助维护正确状态，但前提是行为不能外部可见的副作用(例如向用户发送通知)。

​	也许将来应用程序开发的模式将会出现可喜变化，使得因果关系被有效地捕获，派生状态得到正确维护，而不会强制所有事件必须通过<u>全序广播(系统瓶颈)</u>。

### 12.1.2 批处理和流处理集成

​	我认为数据整合的目标是确保数据在所有正确的地方以正确的形式结束。这样做涉及消费输入数据，转换，join，过滤，聚合，训练模型，评估并最终写入适当的输出。而批处理和流处理则是实现这一目标的有效工具。

​	批处理和流处理的输出是派生数据集，如搜索索引，实体化视图，向用户展现的建议，聚合度量标准等(请参阅第10章“批处理工作流的输出”和第11章的“流处理的适用场景”)。

​	正如我们在第10章和第11章中所介绍的，<u>批处理和流处理有许多共同的原则，而**根本区别在于流处理器运行在无界数据集上，而批处理的输入是已知的有限大小**</u>。处理引擎的实现方式也有很多细节上的差异，但是这些区别现在变得越来越模糊。

​	<u>Spark通过将流分解为微批处理来在批处理引擎之上执行流处理，而 Apache Flink则直接在流处理引擎上执行批处理</u>。原则上，一种类型的处理可以通过另一种类型来模拟，尽管性能特征会有所不同：例如，微批处理可能在跳跃或滑动窗口上表现不佳。

#### 保持派生状态

​	批处理具有相当强的功能特性(即使代码不是用函数式编程语言编写的)，包括倡导确定性、纯函数操作即输出仅依赖于输入，除了显式输出以外没有任何副作用，输入不可变，追加式输出结果等。流处理是类似的，但它扩展了操作来支持可管理的、容错的状态(请参阅第11章“故障后重建状态”)。

​	具有良好定义的输入和输出的确定性函数原理上不仅有利于容错(请参阅第11章“幂等性”)，而且还简化了组织中数据流的推理。无论派生数据是搜索索引，统计模型还是缓存，从数据管道的角度来看，对于从一个事物派生出另一个事物，通过功能应用程序代码推动一个系统中的状态更改以及将这种效果应用到派生系统，都是有帮助的。

​	原则上，派生数据系统可以同步维护，就像关系型数据库在同一个事务中同步地更新级索引，就像写入被索引的表一样。然而，<u>异步是使基于事件日志的系统健壮的原因：它允许系统的一部分在本地包含故障，而如果任何参与者失败，则分布式事务中止，因此他们倾向于通过将故障扩展到其余部分来放大故障(请参阅第9章“分布式事务的限制”)</u>。

​	我们在第6章“分区与二级索引”中看到，二级索引经常跨越分区边界。具有二级索引的分区系统需要将写入发送到多个分区(如果索引文件分区)或将读取发送到所有分区(如果索引是文档分区的话)。如果索引是异步维护的，这种交叉分区通信也是最可靠的和最易扩展的(另请参阅本章后面“多分区数据处理”)。

#### 为应用程序演化而重新处理数据

​	在需要维护派生数据时，批处理和流处理都会用得上。<u>流处理可以将输入的变化数据迅速反映在派生视图中，而批处理则可以反复处理大量的累积数据，以便将新视图导出到现有数据集上</u>。

​	特别是对现有数据进行重新处理，为维护系统提供了一个良好的机制，平滑支持新功能以及多变的需求(参见第4章)。如果不进行重新处理，则只能支持有限简单的模式变化，比如将新的可选字段添加到记录中，或者添加新的记录类型。这些简单的模式比较常见于写时模式和读时模式(参阅第2章“文档模型中的模式灵活性”)。另一方面，通过重新处理，可以将数据集重组为一个完全不同的模型，以便更好地满足新要求。

> 发生在铁路上的模式迁移
>
> 大规模的“模式迁移”并发仅限于计算机系统。例如，19世纪英国铁路建设的初期，轨距(两轨之间的距离)存在各种竟争标准。按照某一个尺寸标准而建的列车无法在另一个尺寸标准的軌道上运行，这严重限制了铁路网络大规模互连。
>
> 1846年最终确定了一个国家标准，其他标准的轨道因此必须更换，但是如何在不关闭火车线路的情况下进行数月甚至数年的调整呢？解决的办法是通过増加第三条铁轨的方法先把轨道换成双轨或混合轨距。这种转换可以逐步完成，当完成后，新老标准的列车都可以选择三条导轨的两条在线上运行。最后，一旦所有列车都支持标准轨距，则可以拆除提供非标准轨距的轨道。
>
> 以这种方式“再加工”现有的轨道，让新旧版本并存，可以在几年的时间内逐步改变轨距。然而，这是一个代价昂贵的项目，这就是今天仍然看到很多非标准轨道原因。例如，旧金山湾区的BART系统仍在使用与美国大部分地区不同的标准。

​	派生视图允许逐步演变。如果想重新构建数据集，无需采用高风险的陡然切换。而是可以在同一个基础数据上的两个独立派生视图来同时维护新老两种架构。然后，逐步开始将少量用户迁移到新视图中，以测试其性能并发现是否有错误，而大多数用户将继续路由到旧视图。之后，逐渐增加访问新视图的用户比例，最终放弃旧视图。

​	这种逐渐迁移的美妙之处在于，如果出现问题，每个阶段的过程都是可以轻易反转你总是有一个工作系统可以回退。通过减少不可逆损害的风险，可以更加自信的向前推进，从而更快地改善系统。

#### Lambda架构

​	如果使用批处理来重新处理历史数据，并且使用流处理来处理最近的更新，那么如何将这两者结合起来呢？备受关注的 Lambda结构是一个不错的解决方案。

​	Lambda体系结构的核心思想是进来的数据以不可变事件形式追加写到不断增长的数据集，类似于事件源(请参阅第11章的“事件溯源”)。基于这些总事件，可以派生出读优化的视图。 lambda结构建议并行运行两个不同的系统：一个批处理系统如Hadoop MapReduce，以及一个单独的流处理系统，如Storm。

​	在lambda方法中，流处理器处理事件并快速产生对视图的近似更新；批处理系统则处理同一组事件并产生派生视图的校正版本。这种设计背后的原因是，批处理更简单，不易岀现错误，而流处理器则相对太可靠的，难以实现容错(请参阅第11章“流处理的容错”)。而且流处理可以使用快速的近似算法，而批处理只能用较慢的精确算法。

​	Lambda架构师一个很有影响力的想法，它将数据系统的设计变得更好，特别是推广了将派生视图用于不可变事件流以及必要时重新处理事件的原则。但是，另一方面，我认为它有一些实际问题：

+ 不得不在批处理和流处理框架中运行相同的处理逻辑，这是一项重大的额外工作。虽然像Summingbird这样的库提供了一个可以在批处理或流处理环境下运行的计算抽象，但调试/优化和维护两个不同系统的操作复杂性依然存在。
+ 由于流处理和批处理各自产生单独的输出，因此需要合并二者结果再响应给用户请求。如果计算是通过滚动窗口的简单聚合，则此合并相当容易，但如果包含更复杂的操作(例如连接和会话流程)导出视图，或者输出不是时间序列，则合并非常困难。
+ 能够重新处理整个历史数据集思路是好，但是在大型数据集上这样做往往代价太高。因此，<u>批处理流水线通常需要设置为处理增量批处理(例如，在每小时结束时处理这一小时的数据)，而不是重新处理所有事情</u>。这会引发第11章“流的时可问题”中处理滞后消息和处理跨批量边界的窗口等问题。增加批量计算就增加了复杂性，使其更类似于流式，这与保持批处理层尽可能简单的目标背道而驰。

#### 统一批处理和流处理

​	<u>最近的发展使得 lambda结构可以充分发挥其优点而规避其缺点，那就是允许批处理计算(重新处理历史数据)和流计算(事件到达时即处理)在同一个系统中实现</u>。

​	在一个系统中统一批处理和流处理需要以下功能，而这些功能目前正越来越普及：

+ 支持以相同的处理引擎来处理最新事件和处理历史回放事件。例如，基于日志的消息代理可以重放消息(请参阅第11章“重新处理消息”)，某些流处理器可以从诸如HDFS等分布式文件系统读取输入。
+ 支持**只处理一次语义**。即使事实上发生了错误(请参阅第11章“流处理的容错”)，流处理器依然确保最终的输岀与未发生错误的输岀相同。<u>与批处理一样，这需要丢弃任何失败任务的部分输出</u>。
+ <u>支持依据事件发生时间而不是处理时间进行窗口化</u>。因为在重新处理历史事件时，处理时间毫无意义(请参阅第11章“流的时间问题”)。例如， Apache Beam提供了用于表示这种计算的API，支持在 Apache Flink或 Google CloudDataflow引擎上运行。

## 12.2 分拆数据库

​	从最抽象的层面上理解，数据库、 Hadoop和操作系统都提供了相同的功能：它们保存某些数据，并支持处理和査询数据。数据库将数据存储在某些数据模型的记录中(表中的行，文档，图中的点等)，而操作系统的文件系统将数据存储在文件中，但是核心都是“信息管理”系统。正如我们在第10章中介绍的， Hadoop生态系统有点像UNIX的分布式版本。

​	当然，有存在很大实际的区别。例如，许多文件系统不能很好地处理包含一千万个小文件的目录，而包含一千万条记录的数据库是完全正常的。尽管如此，操作系统和数据库之间的异同之处仍是值得探讨的。

​	UNIX和关系型数据库采用了大不一样的哲学思想看待信息管理问题。UNIX认为它的目的是为程序员提供一个逻辑的，但是相当低层次的硬件抽象，而关系型数据库则希望为应用程序员提供一个高层次的抽象，来隐藏磁盘上数据结构的复杂性、并发性、崩溃恢复等。UNⅠX开发的管道和文件只是字节序列，而数据库开发了SQL和事务。 

​	哪种方法更好？当然，这取决于你想要什么。UNIX是“简单的”，因为它是硬件资源的一层包装；关系型数据库也是“简单的”，因为一个很短的声明性查询可以利用很多强大的基础设施(査询优化、索引、join方法、并发控制、复制等)，而无需发起请求者详细理解实现细节。

​	这些哲学思想之间的对立已经持续了几十年(UNIX和关系模型都出现在20世纪70年代初)，目前仍然没有解决。例如，我宁愿将NoSQL的发展解释为一种将低级别抽象方法应用于分布式OLTP领域的诉求。

​	在这一部分，我试图辨证看待两种哲学，希望很好地将二者结合在一起，做到两全其美。

### 12.2.1 编排多种数据存储技术

​	在本书中，我们讨论了数据库提供的各种功能及其工作原理，其中包括：

+ 二级索引：根据字段值高效地搜索所有记录(请参阅第3章“其他索引结构”)。
+ **实体化视图：预先计算査询结果并将其缓存**(请参阅第3章“聚合:数据立方体与物化视图”)。
+ 复制日志：使多节点上数据副本保持最新(请参阅第5章“复制日志的实现”)。
+ 全文搜索索引：在文本中进行关键字搜索(请参阅第3章“全文搜索与模糊索引”)并且内置于某些关系型数据库。

​	在第10章和第11章中介绍了类似的主题。我们讨论了如何构建全文搜索索引(请参阅第10章“批处理工作流的输出”)，实体化视图维护(请参阅第11章“维护实体化视图”)，以及如何将变更数据从数据库复制到派生数据系统(请参阅第11章的“变更数据捕获”)。

​	由此看来，数据库中内置的这些功能与采用批处理和流处理器构建的派生数据系统似乎很多对应关系。

#### 创建一个索引

​	想想在关系型数据库中运行CREATE INDEX来创建新索引时会发生什么。数据库必须扫描表的一致性快照，挑选出所有被索引的字段值，对它们进行排序，然后得到索引。接下来，必须处理从一致性快照创建以来所累计的写入操作(假设表在创建索引时未被锁定，所以写操作可能会继续)。完成后，只要有事务写入表中，数据库就必须持续保持索引处于最新状态。

​	这个过程和配置新的从节点副本非常相似(请参阅第5章的“配置新的从节点”)也非常类似于流处理系统中的初始变更数据捕获(请参阅第11章“初始化快照”)。

​	无论何时运行 CREATE INDEX，数据库都会重新处理现有的数据集(正如我们在本章前面“为应用程序演化而重新处理数据”中所述)，并将索引作为新视图导出到现有数据上。现有数据可能是状态的快照，而不是所有发生变化的日志，但两者却密切相关(请参阅第11章“状态，流与不可变性”)。

#### * 元数据库

​	有鉴于此，我认为整个组织的数据流开始变得像一个巨大的数据库。每当批处理，流或ETL过程将数据从某个一个位置(和表单)传输到另一个位置(或表单)时，它就像数据库了系统一样需要保持索引或实体化视图至最新状态。

​	这样看来，批处理和流处理器就像触发器，存储过程和实体化视图维护相关实现。它们所维护的派生数据系统就像不同的索引类型。例如，关系型数据库可能支持B-Tree索引、哈希索引、空间索引(请参阅第3章“多列索引”)以及其他类型的索引。在新兴的派生数据系统体系结构中，这些特性不是靠单一集成数据产品来实现，而是由各种不同的软件所提供，他们运行在不同的机器上，并由不同的团队来管理。

​	这些发展在未来会我们引向何处？如果我们的前提是，没有一个统一的数据模型或存储格式适用于所有的访问模式，我推测，到时候会有两种途径，不同的存储和处理工具最终会组合成一个紧密结合的系统：

+ 联合数据库：统一读端

  可以为各种各样的底层存储引擎和处理方法提供一个统一的査询接口：一种称为联合数据库或聚合存储的方法。例如， PostgreSQL的外部数据包功能就符合这种模式。需要专门的数据模型或查询接口的应用程序仍然可以直接访问底层存储引擎，而想要组合来自不同位置的数据的用户可以通过联合接口轻松完成任务。

  联合査询接口遵循单一集成系统的关系模型，并具有高级查询语言和优雅的语义，但是内部实现非常复杂。

+ 分离式数据库：统一写端

  虽然上述联合式方案能解决跨系统的只读査询问题，但是跨系统的同步写入方面却没有一个好的答案。我们说对于一个数据库，创建一致的索引是其内置的功能。而在构建跨多个存储系统的数据库时，我们同样需要确保所有数据更改都会体现在所有正确的位置上，即使中间发生了某些故障。多个存储系统可以可靠地连接在一起(例如，通过变更数据捕获和事件日志)，类似思路，可以跨不同技术方案来同步写，从而实现分离式(或非捆绑式)数据库的索引维护功能。

  这种分离的方法正是遵循了UNIX传统：单一任务做好一件事，且内部通过统一的低级API(管道)进行通信，外部通过更高级别的语言(shell)进行组合。

#### * 分离式如何工作

​	联合方式与分离方式可以看出同一个硬币的两面：用不同的组件构成一个可靠、可扩展的和可维护的系统。联合的只读査询需要将一个数据模型映射到另一个数据模型，这里需要对模型匹配方面足够深入的考虑，但最终它还是一个可控问题。我认为同步写入多个存储系统是一个更难的工程问题，所以我将重点关注这个问题。

​	传统的同步写依赖于跨异构存储系统的分布式事务，我认为这是个错误的解决方案(请参阅本章前面“派生数据与分布式事务”)。<u>**在单个存储系统内或流处理系统内的事务是可行的，但是当数据跨越不同技术的边界时，我认为具有幂等写入的异步事件日志是一种更加健壮和可行的方法**</u>。

​	例如，在一些流处理器中使用分布式事务来实现 **exactly-once 语义**(请参阅第11章“重新思考原子提交”)，这是可行的。但是，当一个事务涉及到多个不同人群负责的系统时(例如，当数据从流系统写入到分布式键值存储或搜索索引时)，缺乏标准化的事务协议会使集成变得非常困难。<u>有序的事件日志结合幂等的事件处理(参见第11章“幂等性”)是一种更简单的抽象，因此在异构系统中实现时会更可行</u>。

​	基于日志的集成的一大优势是各个组件之间的松耦合，这体现在两个方面：

1. 在系统级别，异步事件流使整个系统在应对各个组件的中断或性能下降时表现更加稳健。如果消费者运行缓慢或失败，那么事件日志可以缓冲消息(请参阅第11章“磁盘空间使用情况”)，这样生产者和其他消费者可以继续运行不受影响。有问题的用户恢复正常后也可以继续运行，不会丢失任何数据，这样有效抑制了故障。**相比之下，分布式事务的同步交互往往会将本地故障升级为大规模故障(请参见第9章“分布式事务的限制”)**。
2. 在人员角度看，分离式数据系统使得不同的团队可以独立的开发、改进和维护不同的软件组件和服务。专业化使得每个团队都可以专注于做好一件事情，且与其他系统维护清晰明确的接口。事件日志提供了一个足够强大的接口，不但能捕获相当强的一致性(通过持久性和事件顺序)，同时也普遍适用于几乎任何类型的数据。

#### 分离式与集成式系统

​	即使分离式确实能成为未来的主流，它也不会取代目前形式的数据库，这些数据库仍然会像以前一样拥有巨大需求。维护流处理器中的状态仍然需要数据库，为批处理和流处理器的输出提供合并的査询(请参阅第10章“批处理工作流的输出”和第11章的“流处理”)也同样需要数据库。专门的査询引擎对于特定的工作负载仍然很重要：例如，MPP数据仓库中的查询引擎针对探索性分析査询进行了优化，非常适合处理这种类型的工作负载(请参阅第10章“对比 Hadoop与分布式数据库”)。

​	运行多个不同的基础架构所带来的复杂性可能确是一个问题：每一套软件都需要一个学习曲线，配置问题以及操作习惯等，因此需要尽可能地减少所部署的组件。与这种包含多个组件并依靠应用层代码组合在一起的系统相比，单个集成的软件产品有可能确实在其针对的负载上表现更好，性能更可预测。所以，正如我在序言中所讲的那样，<u>构建你并不需要的扩展规模是在浪费精力，可能会把你限制在一个不灵活的设计中。实际上，这是在做**过早优化**</u>。

​	分离的目标不是要与那些针对特定负载的单个数据库来竞争性能。目标是让你可以将多个不同的数据库组合起来，以便在更广泛的工作负载范围内实现比单一软件更好的性能。这里的关注点是广度而非深度。与我们在第10章的“对比 Hadoop与分布式数据库”中讨论存储和处理模型的多样性一脉相承。

​	因此，<u>如果有一种技术可以满足你的所有需求，那么最好使用该产品，而不是试图用基础组件来重新实现它。当没有单一的软件能满足所有需求时，分离和组合的优势才会显现出来</u>。

#### 遗漏了什么？

​	组合数据系统的工具正在变得越来越好，但是我认为还缺少一个主要部分：我们还没有与 UNIX shell媲美的分离型数据库(即，以简单和声明式构建存储和处理系统的高级语言)。

​	例如，如果可以简单地声明mysql| elasticsearch，类似UNIX管道)，我一定会非常喜欢这样的实现方式，因为这将是 CREATE INDEX的分离式等价物：它将使用MySQL数据库中的所有文档，并将其索引到 Elasticsearch集群中。然后，它会不断捕获对数据库所做的更改，并自动将其应用于搜索索引，而无需编写自定义应用程序代码。这种整合应该对几乎所有类型的存储或索引系统都是可行的。

​	同样，如果能够支持预先计算并方便地更新缓存就更好了。回想一下，实体化视图本质上是一个预计算的缓存，因此可以为复杂的查询以声明式创建实体化视图来缓存结果，包括图的递归査询(参阅第2章“类图数据模型”)以及一些应用处理逻辑。这方面有一些有趣的早期研究，比如差异化数据流，我希望这些想法能够投入到生产系统中去。

### 12.2.2 围绕数据流设计应用系统

​	通过应用层代码来组合多个专门的存储与处理系统并实现分离式数据库的方法也被称为“数据库由内向外”方法，这是我在2014年的一次会议报告中使用的标题。然而，把它称为“新架构”有点夸大了。我把它看作是一个设计模式，一个讨论的起点，名字的目的只是为了方便讨论。

​	这些想法不是我的；而是一些值得学习的他人的想法的融合。尤其是，它与数据流语言如Oz和Juttle，功能性反应式编程语言(Functional Reactive Programming，FRP)如Elm，逻辑编程语言如Bloom等存在很多共同点。在此背景下，由Jay Kreps提出了 unbundling这个术语。

​	甚至电子表格也比大多数主流编程语言更早支持数据流编程功能，在电子表格中，可以在一个单元格中放入公式(例如，另一列中的单元格的总和)，并且每当公式的任何输入发生更改时，都会自动重新计算公式的结果。这正是我们在数据系统级别所需要的：当数据库中的记录发生更改时，我们希望自动更新该记录的任何索引，并且自动刷新依赖于该记录的任何缓存的视图或聚合。你不必担心技术细节是如何实现这种刷新的，只需要能够确信它可以正常工作即可。

​	因此，我认为绝大多数的数据系统仍然可以从ⅤisiCalc在1979年已经具备的功能中学习。与电子表格的不同之处在于，今天的数据系统需要具备容错性、可扩展性以及数据持久存储能力。它们还需要能够集成不同团队编写的不同技术，并重用现有的库和服务：期望使用某种特定的语言、框架或工具来开发所有软件是不切实际的。

​	在本节中，我将对这些想法进行扩展，并围绕着分离式数据库和数据流思路来探索构建新型应用程序的一些方法。

#### 应用程序代码作为派生函数

​	当某个数据集从另一个数据集派生而来时，它一定会经历某种转换函数。例如:

+ 二级索引是一种派生的数据集，它具有一个简单的转换函数：对于主表中的每一行或者一个文档，挑选那些索引到的列或者字段值，并且按照值进行排序(假设个B-tree或者 SSTable索引，按主键排序，如第3章所述)。
+ 通过各种自然语言处理函数(如语言检测、自动分词、词干或词形识别、拼写检查和同文词识别)创建全文搜索索引，然后构建用于高效查找的数据结构(例如反向索引)。
+ 在机器学习系统中，可以考虑通过应用各种特征提取和统计分析功能从训练数据中导岀模型。当应用与新输入数据时，模型的输岀是基于输入数据和模型(因此间接地从训练数据)派生而来。
+ 缓存通常包含那些即将显式在用户界面(UI)的聚合数据。因此填充缓存需要知道在UI中引用哪些字段，UI更改可能也需要相应的调整缓存填充方式和重建方式。

​	为二级索引构造派生函数的需求非常普遍，作为核心特性，几乎都内置于很多数据库中，通过简单地调用 CREATE INDEX即可使用。对于全文索引，常见的语言方面的函数也会内置于数据库中，但更复杂的特性通常还需要针对特定领域进行调整。而对于机器学习，众所周知特征工程与特定应用紧密相关，此外，还需要集成很多用户交互和应用部署方面的很多详细知识。

​	如果创建派生数据集的函数并非创建二级索引那样标准函数时，就需要引入自定义的代码来处理特定应用相关。而这个自定义代码是让很多数据库纠结的地方。虽然关系型数据库通常都支持触发器、存储过程以及用户定义的函数，这些功能使得数据库可以执行应用程序代码，但它们在数据库设计中基本都是后来所添加的功能(请参阅第11章“发送事件流。

#### 应用程序代码与状态分离

​	理论上讲，数据库可以像操作系统那样成为任意应用程序代码的部署环境。然而，实际上它们并不太适合，主要是无法满足当今应用程序的很多开发要求，如依赖性和软件包管理，版本控制，痠动升级，可演化性，监控，指标，网络服务调用以及与外部系统的集成。

​	另一方面，诸如 Mesos，YARN， Docker， Kubernetes等用于部署和集群管理的工具是专门为运行应用程序代码而设计的。由于专注于做好一件事，它们能够做得比数据库更好，在众多特性中，都提供了用户定义函数的执行功能。

​	我认为系统的某部分专注于持久性数据存储，同时有另外一部分专门负责运行应用程序代码是有道理的。这两部会有交互，但是各自仍保持独立运行。

​	现在大多数web应用程序都被部署为**无状态服务**，用户请求都可以被路由到任意的应用程序服务器上，并且服务器发送响应之后就不会保留请求的任何状态信息。这种方式部署起来很方便，服务器可以随意添加或删除，然而有些状态势必需要保存在某个地方：通常是数据库。目前的趋势是将无状态应用程序逻辑与状态管理(数据库)分开:不把应用程序逻辑放在数据库中，也不把持久状态放在应用程序中36。正如函数程序社区的人喜欢开玩笑说，“我们相信可变状态与 Church的工作分离”。

​	<u>在这种典型的web应用模型中，数据库充当一种可以通过网络同步访问的可变共享变量。应用程序可以读取或更新变量，数据库负责持久性，提供一些并发控制和容错功能</u>。

​	但是，在大多数编程语言中，无法订阅可变变量的更改信息，而只能定期不断地读取它。与电子表格不同，如果变量的值发生了变化，变量的读取者将不会收到通知(你可以在你自己的代码中实现这样的通知，例如通过所谓的观察者模式，但是大多数编程语言级别没有内置该模式)。

​	数据库继承了这种被动方法来处理可变数据:如果想知道数据库的内容是否了变化，唯一的选择就是轮询(即定期地执行査询)。订阅更改只是最近才出现的新功能(请参阅第11章“更改数据流的API支持”)。

#### 数据流： 状态变化和应用程序代码之间的相互影晌

​	从数据流的角度思考应用意味着重新协调应用代码和状态管理之间的关系。我们不是将数据库简单地视为被应用程序所操纵的被动变量，而是更多地考虑状态、状态变化以及处理代码之间的相互作用和协作关系。应用程序代码在某个地方会触发状态变化，而在另一个地方又要对状态变化做出响应。

​	我们在第11章的“数捃库与数据流”中看到了这样的思路，当时讨论了将数据库更改日志作为可订阅的事件流来处理。消息传递系统(如 actors)(请参阅第4章“基于消息传递的0数据流”)也具有响应事件的概念。早在20世纪80年代，元组空间模型探索了将分布式计算的过程表示为一些进程负责观察状态的变化，同时一些进程作出响应。

​	如前所述，数据库内部也有类似情况，例如当触发器由于数据变化而触发时，或者数据发生变化而二级索引更新时。分离式数据库也采取这样的思路，并将其应用于在主数据库之外创建派生数据集，包括緩存、全文搜索索引、机器学习或分析系统。我们可以使用流处理结合消息传递系统来达到这个目的。

​	要记住的重要一点是，维护派生数据与异步作业执行不同，后者主要是传统消息系统所针对的目标场景(请参阅第1章“对比日志机制与传统消息系统”相关内容)。

+ <u>当维护派生数据时，状态更改的顺序通常很重要，如果从事件日志中派生出了多个视图，每个视图都需要按照相同的顺序来处理这些事件，以使它们互相保持一致</u>。如第11章“确认与重传”中所述，许多消息代理在重新发生未确认的消息时不文持该特性。而双重写入也被排除在外(请参阅第11章“保持系统同步”)。
+ 容错性是派生数据的关键：**丢失哪怕单个消息都会导致派生数据集永远无法与数据源同步**。消息传递和派生状态更新都必须可靠。例如，默认情况下，许多actor系统默认在内存中维护 actor状态和消息，所以如果机器崩溃，这些内存信息都会丢失。

​	对稳定的消息排序和可容错的消息处理的要求都非常严格，但是它们比分布式事务代价小很多，并且在操作上也更加稳健。现代流式处理可以对大规模环境提供排序和可靠性保证，并允许应用程序代码作为stream operator运行。

​	这个应用程序代码可以完成那些数据库内置函数所不支持的任意处理逻辑，就像管道链接的UNIX工具一样，多个stream operators可以组装在一起枃建大型数据流系统每个operator以变化的状态为输入，并产生其他状态变化流作为输出。

#### 流式处理与服务

​	当前流行的应用程序开发风格是将功能分解为一组通过同步网络请求(例如RESTAPI)进行通信的服务(请参阅第4章“基于服务的数据流：REST和RPC”)。这种面向服务的结构优于单体应用程序之处在于松耦合所带来的组织伸缩性：不同的团队可以在不同的服务上工作，这减少了团队之间的协调工作(只要服务可以独立部署和更新)。

​	<u>将stream operator组合成数据流系统与微服务理念有很多相似的特征。但是，底层的通信机制差异很大：前者是单向、异步的消息流，而不是同步的请求/响应交互</u>。

​	除了第4章“基于消息传递的数据流”中所列出的优点(比如更好的容错性)之外，数据流系统还可以获得更好的性能。例如，假设客户正在购头一种商品，这种商品以某一种货币定价，但需要以另一种货币支付。为了执行货币转换，需要知道当前的汇率。这个操作可以通过两种方式来实现：

1. 对于微服务方法，处理购汇的代码可能会查询汇率服务或数据库，以获取特定货币的当前汇率。
2. 对于数据流方法，处理购汇的代码会预先订阅汇率更新流，并在当地数据库发生更改时记录当前的汇率。当有购汇请求时，只需查询本地数据库即可。

​	第二种方法把向另一个服务发起的同步网络请求转换为本地数据库的查询(同一个进程或者同一台机器上)。数据流不仅方法更快，而且当另一个服务失败的话也更加健壮。最快和最可靠的网络请求就是根本没有网络诮求！我们现在在购汇请求和汇率更新事件之间有一个事件流join，而不是RPC，请参阋第11章“流和表 join操作“。

​	注意，这种join效果会随时间而变：如果购汇事件在稍后被重新处理，汇率已经发生改变。如果想重演当初的购买行为，则需要从原来购买的时间获得历史汇率。无论是查询服务还是订阋汇率更新流，都需要处理这种时间依赖性(请参阅第10章“ join的时间依赖性”)。

​	订阅变化的流，而不是在需要时去查询状态，使我们更接近类似电子表格那样的计算模型：当某些数据发生更改时，依赖于此的所有派生数据都可以快速更新。还有很多开放的问題，例如关于时间依赖join等，但我相信围绕数据流来构建应用程序是一个非常有前途的方向。

### 12.2.3 观察派生状态

​	概括来讲，上一节所讨论的数据流系统主要侧重如何创建派生数据集(如搜索索引，实体化视图和预测模型)并使其保持最新状态。我们可以把这个过程称为**写路径**：只要有信息写入系统，它就可能经历批处理和流处理的多个阶段，最终毎个派生数据集都会更新以包含新写入的数据。图12-1展示了更新搜索索引的示例。

![分拆数据库 - 图1](https://static.sitestack.cn/projects/ddia/img/fig12-1.png)

​	但是，为什么你首先创建派生数据集呢？很可能是因为想在以后多次査询它即**读路径**：当有用户访问请求时，从派生数据集中读取数据，做些必要的处理，然后返回处理后的结果以响应用户请求。

​	总而言之，写路径和读路径涵盖了数据的整个过程，从数据收集到数据使用(可能是由另一个人)。写路径可以看作是预计算的一部分，即一旦数据进入，即刻完成，无论是否有人要求访问它。而过程中的读路径则只有当明确有人要求访问时才会发生。如果你熟悉函数式编程语言，可能会注意到**写路径类似于尽早求值，读路径类似于惰性求值**。

​	写路径和读路径在派生数据集上交会，如图12-1所示。某种程度上，它是写入时需完成的工作量和读取时需完成的工作量之间的一种平衡。

#### * 实体化视图和缓存

​	全文搜索索引就是一个很好的例子：写路径更新索引，读路径搜索关键字索引。读写都需要做一些工作。写入主要更新文档中出现的所有术语的索引条目，而读则需要搜索输入的每个词，并应用布尔逻辑来查找包含所有词(AND运算符)的文档，或每个词(OR运算符)的任何同义词。

​	如果没有索引，则搜索査询将不得不扫描所有文档(如grep)，如果有大量文档，这样做的代价会很高。没有索引意味着写路径上的工作变少了(没有索引要更新)，但是在读路径上则需要更多的工作。

​	另一方面，可以想象一下为所有可能的查询预先计算搜索结果。此时，读路径上的工作量会减少：不需要布尔逻辑，只需要査找查询结果并返回即可。但是，写路径代价会非常高，主要是可能的查询组合是无限的，因此预先计算所有可能的搜索结果需要几乎无上限的时间和空间代价，这明显实际不可行。

​	<u>另一种方案是只对那些最常见的一组査询进行预计算，这样可以快速得到响应而不必去查询索引，而其他査询则仍然依靠索引。这就是通常所称的査询缓存，或者也可以称之为实体化视图，意味着当添加新文档时，如果其包含在那些最常见的查询结果之中，则视图(缓存)就需要随之更新</u>。

​	从这个例子中我们可以看到，索引并不是写路径与读路径之间链接的唯一边界。其他还可能包括对常见搜索结果的缓存；或者当文档数量有限时，无需索引而对所有文档进行grep方式的全文扫描。从这个角度来看，缓存、索引和实体化视图的角色就比较清楚了，它们主要是调整读、写路径之间的边界。通过预先计算结果，写路径上承担了更多的工作，而读路径则可以简化加速。

​	读写路径上工作量的转移或平衡实际上正是本书开始时讨论 Twitter例子的主题，详见第1章“描述负载”。在该例子中，我们还看到对于那些明星账户，读写路径可能与普通用户有很大差异。再次回顾，我们已经完成了本书近几百页的讨论。

#### 有状态，可离线客户端

​	我发现读写路径之间的这种边界非常有趣，可以继续讨论调整边界，以及实际场景中这种调整究竟意味着什么。接下来，让我们看看不同场景下的具体情况。

​	由于Web应用程序在过去二十年的普及流行，很多人由此对应用开发有了一些先入为主的假设。特别是，C/S架构即客户端/服务器端模型如此普遍(客户端大部分是无状态，服务器端拥有对数据的全部控制权)，以至于大家几乎都忽略了其他架构的存在。但是，技术永远在不断发展，我认为经常质疑现状是非常重要的。

​	传统上，Web浏览器是无状态的客户端，只有当互联网连接时才能触发后面的处理(离线时，唯一能做的事情就是在之前以打开的网页上滚动浏览)。然而，最近的“单页” JavaScript Web应用程序已经支持很多有状态的功能，包括基于客户端的可交互用户界面和数据持久保存在Web浏览器。移动应用程序类似地可以在设备上保存很多状态，并且很多交互不需要与服务器进行往返通信。

​	这些新特性重新引发了对离线优先应用程序的兴趣，它们尽可能地在同一设备上使用本地数据库，而不需要互联网连接，并在网络连接可用时在后台与远程服务器同步。由于移动设备的互联网连接通常比较慢，而且可靠性低，因此如果用户界面不需要等待同步网络请求，并且大多数应用程序可以离线工作，对用户来说是一个巨大优势(请参阅第5章“离线客户端操作”)。

​	当我们摆脱无状态客户端与核心数据库这样的假设，而转向在终端用户设备上维护状态时，就打开了一个全新机遇。特别是，<u>我们可以将设备上的状态视为服务器上的状态缓存。屏幕上的呈现是一种客户端对象模型的实体化视图；而客户端的对象模型则是远程数据中心在本地的状态副本</u>。

#### 状态更改推送至客户端

​	对于典型的Web网页，在Web浏览器加载页面之后，如果服务器端数据发生变化，则只能重新页面才能观察到新数据。浏览器只在某一个时间点读取数据，假设它是静态的，它不会订阅服务器端的更新。因此，除非显式地轮询更新，设备上的状态是旧的缓存(像RSS这样基于HTTP的订阅协议实际上只是一种轮询形式)。

​	之后更新的协议已经超越了基本的HTTP的请求/响应模式：服务器端发送事件(EventSource APl)和 WebSocket提供了一个通信通道，通过该通道，Web浏览器可以与服务器保持开放的TCP连接，只要处于连接状态，服务器可以主动推送消息至浏览器。这为服务器提供了一种新的更新机制，即主动通知并更新终端用户客户端本地存储状态，从而缩小与服务器状态的滞后程度。

​	就我们的写路径和读路径模型而言，主动推送状态至客户端设备意味着将写路径一直延伸到终端用户。当客户端首次初始化时，仍然需要使用读取路径来获得初始状态，但此后可能依赖于服务器发送的状态更改流。所以，流处理和消息传递方面的这些想法并不局限于仅在数据中心运行，我们可以进一步并将这些想法扩展到终端用户设备。

​	这些设备有时会离线，在此期间无法接收到服务器状态更改的任何通知。但是我们已经了解决方案，在第1章“消费者偏移量”中，我们讨论了基于日志的消息代理的消费者在失败或断开连接后可以重新连接，并确保它不会错过任何网络断开之后的新消息。同样的技术适用于个人用户，即每个设备都抽象为小型事件流的一个的订阅者。

#### * 端到端的事件流

​	最近的一些针对有状态客户端和用户接口的开发工具，如Elm语言和Facebook的React，Flux和Redux工具链，已经支持从服务器端订阅代表用户输入的事件流来管理内部的客户端状态，结构与事件溯源类似(请参阅第11章的“事件溯源”)。

​	自然地，可以将这种编程模型扩展为允许服务器将状态改变事件推送至客户端事件流水线中。因此，状态变化可以通过端到端的写路径流动：某个设备上交互行为触发了状态变化，通过事件日志、派生数据系统和流式处理等，一直到另一台设备上用户观察到状态。这些状态变化传播的延迟可以做到很低的水平，例如端到端只需一秒。

​	某些应用程序(例如即时消息系统和在线游戏)已经采用了这种“实时”结构(主要是从低延迟的交互意义上说，而不是第8章“响应时间保证”)。那么为什么并非所有应用都以这种方式构建呢？

​	主要挑战在于，目前的数据库、函数库、开发框架和交互协议等都有对无状态客户端和请求/响应交互根深蒂固的假设。许多数据存储支持的读写操作其实都是一个请求对应一个响应，但很少支持订阅更改，即一段时间内会主动返回一系列的响应(请参阅第11章“更改事件流的AP支持”)。

​	<u>为了将写路径扩展到最终用户，我们需要从根本上重新思考构建这些系统的方式：从请求/响应交互转向发布订阅数据流。我认为更具响应性的用户界面和更好的离线支持的优势是绝对值得尝试的。如果你正在设计数据系统，我希望你能够记住订阅更改这一方式，而不仅仅是查询当前的状态</u>。

#### 读也是事件

​	我们讨论了当流式处理将派生数据写入存储(数据库，缓存或索引)时，以及当用户请求査询存储时，存储充当了上述写路径与读路径之间的一种可调边界。当査询时，存储可以支持对数据进行随机快速访问(例如基于索引等)，或者扫描整个事件日志(没有索引时)。

​	很多情况下，数据存储与流处理系统是分开的。但流式处理自身还需维护一些状态以方便执行聚合和join(参阅第11章“流式join”)。这些状态通常隐藏在流处理系统内部，但是也有一些框架支持外部客户端查询，这样流处理系统本身也成为一种简单的数据库。

​	我想进一步来谈这个想法。正如前面所讨论的，对存储的写入是通过事件日志进行的，而读取则是即时的网络请求方式，查询直接路由到那些存储数据节点。这样的设计也很合理，但它并不是唯一可能的设计方案。<u>也可以将读请求表示为事件流，发送至流处理系统，流处理系统则将读结果发送至输出流来响应读取事件</u>。

​	当写入和读取都被表示为事件，并且被路由到相同的stream operator统一处理时，我们实际上是在查询流和数据库之间执行 stream-table join操作。读事件需要发送到保存数据的数据库分区节点上(参阅第6章“请求路由”)，这一点与批处理和流处理系统在join操作时需要在同一个主键上合并数据一样(参阅第10章“ reducer端的join与分组”)。

​	服务请求与执行join操作之间的对应关系非常重要。一次性读取请求只是通过join运算来传递请求，之后就再也不用；然后订阅请求则是对另一侧所有过去和未来事件的一种持续性的join操作。

​	<u>以日志方式记录读事件可能还可以帮助跟踪系统级别的**事件因果关系**和数据源：它可以**重建**用户在做出某个决定之前看到的内容</u>。例如，一个购物网站，向客户显示的预计出货日期和库存状态可能会影响他们选择购买某件物品。为了分析此间的关联，需要记录用户所查询到的相关快递和库存状态。

​	<u>将读取事件信息写入持久化存储，一方面可以更好地追踪因果关系(参阅本章前面“排序事件以捕获因果关系”)，同时会导致额外的存储和I/O开销</u>。对此进行优化以减少额外开销目前仍是一个开放的研究话题。但是，如果你已经出于运维目的而将读信息记录到日志，作为请求处理的副作用，附带支持将日志本身也作为请求的分析来源其实并没有改变太多。

#### 多分区数据处理

​	对于仅涉及单个分区的查询，通过流来发送查询并收集响应事件流可能显得有些大材小用。然而，这种方法却开启了一种分布式执行复杂查询的可能性，这需要合并来自多个分区的数据，并很好地借助底层流处理系统所提供的消息路由、分区和join功能。

​	Storm的分布式RPC功能支持这种使用模式(请参阅第1l章“消息传递和RPC”)例如，用来计算 Twitter有多少人观看了某个URL地址，即推送该条URL信息的所有用户的所有关注者的并集。由于 Twitter用户采用了分区机制，所以上述计算也需要合并多个分区上的查询结果。

​	这种模式的另一个例子是反欺诈：为了评估特定购买事件是否具有欺诈风险，可以检查用户的IP地址、电子邮件地址、账单地址、送货地址等等的信誉分数。这些信誉数据库每一个都是独立分区的，因此为特定购买事件收集最终分数需要一系列对不同分区数据的join。

​	MPP数据库的内部查询执行计划也具有相似的特征(请参阅第10章的“对比 Hadoop与分布式数据库”)。当需要执行这种跨分区的join操作时，数据库内置提供此功能可能比使用流处理系统实现起来更简单。然而，将查询也作为一种事件流则提供了一个新的选择，来实现大规模应用系统并摆脱现有传统方案的限制。

## 12.3 端到端的正确性

​	对于只读取数据的无状态服务，如果发生错误，这并不是什么大问题：可以修复错误并重新启动服务，然后一切恢复正常。但是像数据库这样的有状态系统就不是那么简单了：它们被设计成永远记住事物(有的多有的少)，所以一旦发生问题，影响可能会是永远性的，这就要求必须仔细对待。

​	我们总是希望构建可靠并且正确无误的应用程序(即使在面对各种故障的情况下，其语义依然保持凊晰定文和可理解)。<u>在最近四十年里，原子性，隔离性和持久性(第7章)的事务特性一直是保证正确性的首选方式。但是，这些基础特性实际上却比他们看起来要弱，例如我们已经见识了弱隔离级别的很多混淆不清的现状(请参阅第6章“弱隔离级别”)</u>。

​	在某些领域，事务处理被完全抛弃，被性能更好、扩展性更强的模型所取代，但是这些模型具有更复杂的语义(例如第5章“无主节点复制”)。<u>一致性虽然经常被关注，但始终缺乏明确的定义</u>(参见第7章和第9章的“一致性”)。业界还有声音主张，为了更好地可用性，我们应该“拥抱弱一致性”，然而实践中类似这样说法缺乏清晰的建议。

​	对于如此重要的话题，我们的理解和手头的工程方法确显得非常溥弱。例如，在特定事务隔离级别或复制模式下，想要确定某个特定应用程序是否安全就很有挑战性。简单的解决方案似乎能够低并发且没有错误的情况下正常工作，但是在要求更高的情况下会出现许多细微的错误。

​	例如， Kyle Kingsbury的 Jepsen实验已经表明，<u>当存在网终问题和节点崩溃时，某些产品所声称的安全保证与实际行为之间存在明显差异</u>。即使像数据库这样的基础架构产品没有问题，上层应用程序代码仍然需要正确地使用它们所提供的功能(包括弱隔离级别、 quorum配置以及其他)，如果配置难以理解，就很容易出错。

​	如果你的应用程序能够容忍偶尔发生一些不可预知的崩溃或丢失数据，那么情况就简单得多，或许你可以叉着手很轻松的置身事外。否则的话，只能需要更强的正确性保证，即**可串行化**和**原子提交**，但它们都是有代价的：通常只能用于单数据中心(因此排除了地域分布的部署架构)，有限的扩展规模和容错性。

​	传统的事务方法并不会消失，但我也相信，它也未必是保证应用程序正确和灵活处理错误的终极手段。本节我将提出一些基于数据流架构来思考、探索正确性的若干方法。

### 12.3.1 数据库的端到端争论

​	仅仅因为应用程序使用了具有较强安全属性的数据系统(例如可串行化的事务)，并不能意味着应用程序一定保证没有数据丢失或损坏。例如，如果应用程序的bug导致它写入不正确的数据，或者从数据库中删除数据，那么可串行化的事务也无济于事。

​	这个例子看似简单，但这些情况却值得重视，包括应用程序存在bug和人为操作存在失误。在第11章“状态，流和不可变性”中我使用了这个例子来阐述不可变数据和追加式更新，这样如果把错误的代码修复或移除掉，可以更容易从锴误中恢复。

​	尽管不可变性很有帮助，但它本身并不是一种万能的方法。让我们看看可能发生数据损坏的一个更微妙的例子。	

#### * Exactly-once执行操作

​	在第11章“流处理的容错”中，我们提到了称为“只执行一次”(exactly-once，或者effectively-once)语义。如果在处理消息过程中出现意外，可以选择放弃(丢弃消息，即导致数据丢失)或者再次尝试。**如果采用重试策略，而实际上第一次执行时已经事实成功，只是用户没有得到成功的确认，那么最终这个消息其实是处理了两次**。

​	处理两次是某种形式的数据损坏，例如相同的服务向客户收费两次(超额计费)或计数器增加两次(夸大了一些度量)都是不可取的。在这种情况下， exactly-once意味着合理安排计算，使得最终效果与没有发生错误的结果一样，即使操作实际上由于某种故障而被重试。我们以前讨论过实现这个目标的几种方法。

​	**最有效的方法之一是使operator满足幂等性(参阅第11章“幂等性”)**，即无论执行次还是多次，确保具有相同的效果。但是，如果操作本身并非幂等，而要改造使其具有幂等性则需要付出一定努力，例如可能需要维护一些额外的元数据(例如已成功完成的操作ID集合)，并确保在节点失效、切换过程中采取必要的fencing措施(参阅第8章“主节点与锁”)。

#### 重复消除

​	除了流处理之外，还有许多其他模式也需要消除重复。例如，TCP使用序列号来检测网络上是否有数据包丢失或重复，并最终确保数据包以正确的顺序接受。丢失的数据包都会被重新发送，并且在将数据交给应用程序之前，TCP堆栈将负责删除重复的数据包。

​	但是，**这种重复消除只能用于单个TCP连接上下文中**。假设客户端与数据库之间采用TCP连接，并且正在执行示例12-1中的事务。在许多数据库中，事务与某个客户端连接绑定(如果客户端发送了多个查询，且运行在同一个TCP连接上，则数据库就知道它们属于同一个事务)。如果客户端在发送 COMMIT之后，收到响应之前遭遇网络中断和连接超时等，它就不知道事务是否已被提交或中止(见图8-1)。

例12-1 资金从一个账户到另一个账户的非幂等转移：

```sql
BEGIN TRANSACTION;
    UPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234;
    UPDATE accounts SET balance = balance - 11.00 WHERE account_id = 4321;
COMMIT;
```

​	客户端可以重新连接到数据库并重试事务，但现在它不在TCP可重复消除的范围之内。由于示例12-1中的交易不是幂等的，可能会发生转了22美元，而不是实际希望的11美元。因此，尽管示例12-1是一个标准的交易原子性例子，但它实际上是有些问题，真实的银行不是像这样工作。

​	两阶段提交(请参阅第9章“原了提交与两阶段提交”)协议会破坏TCP连接和事务之间的1:1映射关系，因为在网络故障之后，事务协调者必须重新连接数据库，来通知事务是决定提交还是有问题因而中止。这是否能够确保交易只能执行一次吗？不幸的是，答案是否定的。

​	即使可以消除数据库客户端与服务器之间的重复事务，我们还得关注最终用户的设备和应用程序服务器之间的网络情况。比如下面这种情况，如果最终用户客户端是Web浏览器，则可能使用HTTP POST请求向服务器提交指令。也许用户在一个缓慢的移动蜂窝线路上，虽然成功地发送了POST，但是当服务器响应时网络信号太弱以至于无法成功接收。

​	在这种情况下，用户端很可能会显示一条错误消息，并可能不得不手动重试。Web浏览器警告说“确定要再次提交表单吗?”，用户单击是的，他希望看到操作成功的提示(Post/Redirect/Get模式可以避免正常操作中的类似警告消息，但是如果POST请求超时，这种模式也将无济于事)。<u>从Web服务器的角度来看，重试是一次独立的请求，而对于数据库来说重试请求变成一个另外全新的事务。通常的去重机制对此无能为力</u>。

#### 操作标识符

​	为了实现跨多次网络跳转请求而操作仍然具有幂等性，仅仅依靠数据库提供的事务机制是不够的，需要考虑请求的端到端过程。

​	例如，可以为操作生成一个唯一的标识符(如UUID)，并将其作为一个隐藏的表单字段包含在客户机应用程序中；或者对所有相关表单字段计算一个哈希值依次来代表操作ID。如果Web浏览器两次提交POST请求，则两个请求具有相同的操作ID。然后，可以将该操作ID一直传递到数据库，检査并确保对一个给定的ID只执行一个操作，参见示例12-2。

例12-2 **采用唯一ID来消除重复请求**

```sql
ALTER TABLE requests ADD UNIQUE (request_id);
BEGIN TRANSACTION;
    INSERT INTO requests(request_id, from_account, to_account, amount) 
    VALUES('0286FDB8-D7E1-423F-B40B-792B3608036C', 4321, 1234, 11.00);
    UPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234;
    UPDATE accounts SET balance = balance - 11.00 WHERE account_id = 4321;
COMMIT;
```

​	示例12-2依赖于 request_id列上的唯一性约束。如果一个事务试图插入一个已存在的ID，那么INSERT失败，事务被中止而退出，这样无法执行两次。<u>即使在弱隔离级别下，关系型数据库通常也可以正确地确保唯一性约束，而“应用程序采用先检査再插入”的方式可能会在不可串行化隔离下失败，参见第7章“写倾斜与幻读”的例子</u>。

​	除了消除重复请求之外，示例12-2中requests表作为一种事件日志，还可以表征事件源(请参阅第11章“事件溯源”)。账户余额的更新操作不一定要和插入处于同一个事务内，它们可以是冗余的，从下游消费者的请求事件中派生而来，而**<u>基于唯一请求ID可以确保事件被处理一次</u>**。

#### 端到端的争论

​	消除重复事务这种场景其实只是一种更为普遍的“端到端论点”原则的其中一例，该原则最早由Saltzer、Reed和Clark于1984年所阐述：

​	只有具备应用程序充分的知识，并且站在通信系统端点的角度的情况下，才能完全正确地实现所关注的功能。因此，以通信系统本身的特征来提供这种被质疑的功能是不可能的(有时，由通信系统提供的不完整版本则可能有助于提高性能)。

​	对于我们的例子，所关注的功能是**重复消除**。我们看到TCP协议可以在连接层消除重复的数据包，而一些流处理系统在消息处理级别提供了所谓的exactily-once语文，但是这还不足以防止用户在第一次超时后提交重复的请求。TCP，数据库事务和流处理器本身并不能完全排除这些重复。<u>解决这个问题需要一个**端到端**的解决方案：从终端用户客户端一直传递到数据库的唯一事务标识符</u>。	

​	端到端的参数也适用于检査数据的完整性：以太网，TCP和TLS中内置的校验码可以检测网络中数据包的损坏情况：但是由于网络收发两端的软件bug或存储在磁盘上的数据损坏则无法检测到。<u>如果想要捕获所有可能的数据源损坏，则还需要**端到端的校验和**</u>。

​	类似的讨论也适用于加密：家庭WiFi网络上的密码可以防止人们窃听WiFi流量，但无法防范联网过程中其他地方的攻击；而客户端和服务器之间的TLS/SSL加密则可以防止整个网络攻击，但不会保护服务器本身的安全。<u>只有端到端的加密和认证可以防止所有这些事情</u>。

​	尽管这些底层功能(TCP复制消除，以太网校验和，WiFi加密)无法单独提供所需的端到端功能，但它们仍然有用，因为它们可以降低更高层岀现问题的可能性。例如，如果TCP层不负责保证正确的数据包顺序，那么HTTP请求就可能出现混乱。我们只需要记住，**<u>底层的可靠性功能本身不足以确保端到端的正确性</u>**。

#### 在数据系统中采用端到端的思路

​	这里我要强调一下我原来的观点：即使应用程序所使用的数据系统提供了比较强的安全属性(如可串行化事务)，也并不意味着应用程序就一定没有数据丢失或损坏，应用程序本身也需要采取端到端的措施，例如重复消除。

​	这真是一个耻辱，时至今日，容错机制还是很难保证最后正确的结果。底层的可靠性机制(如TCP中的可靠性机制)没有问题，由此已经降低了高层故障的发生概率。因此，想办法把其余高层的容错机制封装成一个抽象层听起来是个不错的主意，这样就不必由应用程序代码担心了，然而我担心目前还没有找到这样正确的抽象层。

​	长期以来，事务被认为是一个非常好的抽象，我相信它的确非常有用。正如在第7章所讨论的那样，它把可能出现的诸多问题(并发写入，违反约東，崩溃，网络中断，磁盘故障等)归结为只有两种可能的结果：提交或中止。这对编程模型来讲是一个巨大的简化，但我担心这依然不够。

​	<u>事务处理的代价很高，特别是在涉及异构存储技术时(请参阅第9章“实践中的分布式事务”)</u>。当我们因为代价昂贵而拒绝采用分布式事务，最终不得不在应用代码中重新实现容错机制。正如本书中大量的例子所揭示的，关于并发性和部分错误的推理非常困难而且不够直观，所以我谨慎怀疑大多数应用程序级别的容错机制可能不能正常工作。最后结果是丢失或损坏数据。

​	基于以上种种考虑，我认为探索更好的容错抽象非常必要：它能够更容易地提供特定应用的端到端正确保证，并且在大规模的分布式环境中依然具有良好的性能和良好的操作性。

### 12.3.2 强制约束

​	让我们探讨一下分离式数据库背景下的正确性(本章前面的“分离式数据库”)。我们看到，使用从客户端一直传递到数据库的请求ID，可以实现端到端的重复消除。那么其他类型的限制怎么样呢？

​	我们特别关注唯一性约束，即示例12-2所依赖的约束。第9章“约東与唯一性保证”还有更多需要保证唯一性的应用示例，包括用户名或电子邮件地址必须唯一标识用户，文件存储服务不能有多个相同名称的文件，两个人不能预定同一个航班或剧院相同的座位等。

​	其他类型的约束条件非常类似，例如，确保账户余额永远不会变为负数，不能出售多于仓库库存的商品，或者会议室不能重复预订等。保证唯一性的技术通常也可以用于这类约束。

#### 唯一性约束需要达成共识

​	在第9章中我们介绍了在分布式环境中，保证唯一性约束需要达成共识：如果有多个具有相同值的并发请求，系统需要决定接受哪一个操作，并由于违法约束因此拒绝其他的冲突操作。

​	<u>达成这一共识的最常见的方式是将单一节点作为主节点，并负责作出所有的决定</u>。只要不介意通过单个节点发送所有请求(即使客户端来自地球另一边)，并且只要该节点不出问题，就能够达成共识。<u>如果需要容忍主节点出错，那么又重新回到了共识问题上(请参阅第9章“主从复制与共识”)</u>。

​	<u>采用分区方案可以提高唯一性检査的扩展性，即基于要求唯一性的字段进行分区</u>。例如，如果需要通过请求标识来确保唯一性，参见示例12-2，可以把相同请求标识的所有请求都路由到同一分区(请参阅第6章)。如果需要用户名唯一，可以通过用户名的哈希值进行分区。

​	但是，它无法支持异步的多主节点复制，因为可能会发生不同的主节点同时接受冲突写入，无法保证值的唯一(请参阅第9章“实现线性化系统”)。**如果想达到立即拒绝任何违反约束的写请求，则同步式的协调不可避免**。

#### * 基于日志的消息传递唯一性

​	<u>日志机制可以确保所有消费者以相同的顺序查看消息，这种保证在形式上被称为**全序关系广播**，它等价于共识问题(详见第9章“全序关系广播”)</u>。在基于日志的消息传递的分离式数据库系统中，我们可以采用非常类似的方法来保证**唯一性约束**。

​	流处理系统在单线程上严格按照顺序来消费处理日志分区中的所有消息(请参见第11章“对比日志机制与传统消息系统”)。因此，如果根椐需要保证唯一性的字段进行日志分区，则流处理系统可以清晰、明确地确定多个冲突操作哪一个先到达。我们来看一个典型的例子：多个用户尝试声明相同用户名：

1. 设置用户名的每个请求都编码为一条消息，并追加到根据用户名哈希值来确定的分区。
2. 一个流处理操作按顺序读取日志中的请求，基于本地数据库来跟踪记录已经使用了哪些用户名。对于用户名可用的每一个请求，它记录下所取得的用户名，并向输出流发出一条成功消息。对于那些用户名已被占用的请求，则向输出流发出拒绝消息。
3. 请求用户名的客户端观察输出流，并等待请求是否成功(或拒绝)。

​	<u>这个算法与第9章“采用全序关系广播实现线性化存储”基本相同。通过增加分区数量，可以轻松扩展来应对较大的请求吞吐量，这样每个分区都可以独立处理</u>。

​	**这个方法不仅适用于唯一性约束，而且适用于许多其他类型的约束**。<u>其基本原理是，任何可能冲突的写入都被路由到特定的分区并按顺序处理</u>。正如第5章“什么是冲突”和第7章“写倾斜与幻读”所述，冲突的定义可能取决于应用程序，但流处理系统可以使用任意逻辑来验证请求。这个想法与Bayou在20世纪90年代所开创的方法类似。

#### 多分区请求处理

​	当涉及多个分区时，确保操作原子执行且同时满足各种约束有件很有趣的事情。在例12-2中，可能有三个分区:一个包含请求ID，一个包含收款人账户，另一个包含付款人账户。没有理由一定要把他们三个放在一个分区里，因为本来它们就是相互独立的。

​	在传统的数据库方法中，执行这个事务需要樻跨所有三个分区进行原子提交，这实质上是对三个分区上的所有事物执行全序排列。由于要求跨分区协调，不同的分区不能再独立处理，吞吐量因此会收到显著影响。

​	但是，事实证明，使用分区日志是可以实现同等的正确性，并且不需要原子提交：

1. 账户A向账户B转账的请求由客户端赋予一个唯一的请求ID，基于该请求ID追加到对应的日志分区。
2. 流处理系统读取请求日志。对于每个请求消息，它发岀两条输出消息：到付款人账户A的付款指令(按照A进行分区)，以及到收款人账户B(按照B进行分区的信用指令。原始的请求ID都包含在这两条消息中。
3. 后续操作接收上述指令，并通过请求ID进行重复数据消除，将最后的更改应用于账户余额。

​	<u>步骤1和步骤2是必要的，因为如果客户端直接发送信用和付款指令，则需要在这两个分区之间进行原子提交以确保两者都成功(或者都不成功)。为了避免处理分布式事务，我们首先持久地将请求记录为单条消息，然后从第一条消息中派生出信用和付款两条指令。单一对象写入在几乎所有的数据系统中都是原子的(请参阅第7章“单对象写入”)，因此请求既可以通过日志消息，或者其他非日志方式，最终避免多分区的原子提交</u>。

​	如果步骤2的流处理操作发生崩溃，则从上一个快照检查点来恢复处理。这样做，就不会跳过任何请求消息，但它可能会多次处理请求，并产生重复的信用和付款指令。但是，由于它是确定性的，它只会再次产生相同的指令，步骤3中的处理可以基于端到端的请求ID来轻松实现重复数据消除。

​	如果还想确保付款人账户不会发生透支，可以添加另一个流处理操作符(按照付款人账号进行分区)维护账户余额并验证转账金额。在步骤1中，只有有效的事务才能放入到请求日志中。

​	通过将多分区事务划分为两个不同分区的处理阶段，并使用端到端的请求ID，我们实现了同样的正确性(每个转账请求只对付款人和收款人账户应用一次)，即使可能会发生故障，而且避免使用原子提交协议。<u>采用多个不同分区的处理阶段这一想法类似于我们在本章“多分区数据处理”一节所讨论的内容(另请参阅第11章“并发控制”)</u>。

### 12.3.3 时效性与完整性

​	事务的一个非常方便的属性是它们通常是可线性化的(请参阅第9章“线性化”)：即，**一个写入者等待直到事务提交，提交一旦完成，写入值立即对所有读者可**见。

​	而对于跨多个处理阶段的流操作处理，则情况并非如此；日志的消費模式是基于异步设计的，因此发送者不会等待消息处理。但是，客户端可能会等待消息出现在输出流中。例如，需要检査是否满足唯一性约束时，即本章前面“基于日志消息传递的唯性”所讨论的。

​	**在上述例子中，唯一性检査的正确性不取决于消息的发送者是否等待结果。等待只是为了同步的通知发送者唯一性检查是否成功，而该通知可以与处理消息剥离开。**

​	概括来讲，我认为一致性这个术语将两个值得分开考虑的不同的需求:时效性和完整性合二为一了：

+ 时效性

  <u>时效性意味着确保用户观察到系统的最新状态</u>。之前我们看到，如果用户从数据的旧副本中读取数据，他们可能会观察到不一致的状态(请参阅第5章的“复制滞后”)。但是，这种不一致是暂时的，最终通过等待和再次尝试来解决。CAP理论(请参阅第9章“线性化的代价”)提供基于线性化的一致性，这是实现时效性的强有力的方法。弱时效性，比如read-after-write一致性(请参阅第5章“读自己的写”)也很有用。

+ 完整性

  完整性意味着避免数据损坏，即没有数据丢失，也没有互相矛盾或错误的数据。尤其是，如果将某些派生数据集作为基础数据的视图来进行维护(请参阅第11章“从事件日志导出当前状态”)，派生必须做到正确。例如，数据库索引必须正确反映数据库的内容，否则部分缺失的索引实际用处大打折扣。**如果完整性受到破坏，这种不一致将是永久性的：在大多数情况下，等待并再次尝试不是来修复数据库损坏的**。相反，数据库需要专门的检査和修理。在ACID事务的上下文中(参见第7章“ACID的含义”)，一致性通常被理解为某种特定于应用的完整性概念。原子性和持久性则是保持完整性的重要手段。

​	简而言之，违反时效性导致“最终一致性”，而违反完整性则是“永久性不一致”。

​	我敢说对于大多数应用，完整性比时效性重要得多。违反时效性可能令人讨厌和混淆，但是对完整性的破坏则是灾难性的。

​	例如，在信用卡对账单上，如果你在过去24小时内完成的交易尚未出现，这并不奇怪，这些系统有一定的滞后性是正常的。我们知道银行都是异步的协调和解决结算交易的，时效性在这里并非头等重要。然而，如果报表余额不等于交易金额加上之前的报表余额(金额上出现的错误)，或者有些交易向你扣款完成但却没有支付给商家，后果将非常严重(资金凭空消失)。这样的问题属于严重破坏系统的完整性。

#### * 数据流系统的正确性

​	**ACⅠD事务通常既提供时效性(例如线性化)保证，又提供完整性(例如原子提交保证)**。因此，如果从ACID事务角度来看待应用程序的正确性，那么时效性和完整性之间的区别其实无关紧要。

​	另一方面，本章所讨论的基于事件的数据流系统一个有趣特性是它将时效性和完整性分开了。<u>在异步处理事件流时，除非在返回之前明确地创建了等待消息到达的消费者，否则不能保证**及时性**。但是，完整性仍上是流处理系统的核心</u>。

​	只执行一次(exactly-once或effectively-once)(请参阅第1章“流处理的容错”)是一种保证**完整性**的机制。如果事件丢失或者事件发生两次，数据系统的完整性可能被破坏。因此，容错的消息传递和重复消除(例如，幂等操作)对于面对故障时保持数据系统的完整性非常重要。

​	正如我们在上一节看到的那样，可靠的流处理系统可以在不需要分布式事务和原子提交协议的情况下保持完整性，这意味着它们可以实现等价的正确性，同时具有更好的性能和操作鲁棒性。我们主要是通过以下机制实现了这一完整性：

+ 将写入操作的内容表示为单条消息，可以轻松地采用原子方式编写，这种方法非常适合事件源(请参阅第1章“事件溯源”)。
+ 使用确定性派生函数从该条消息派生所有其他状态的更新操作(与存储过程类似)(请参见第7章“实际串行执行”和本章前面的“应用程序代码作为派生函数”)。
+ <u>通过所有这些级别的处理来传递客户端生成的请求ID，**实现端到端重复消除和幂等性**</u>。
+ 消息不可变，并支持多次重新处理派生数据，从而使错误恢复变得更容易(请参阅第11章“不可变事件的优势”)。

​	在我看来，上述多种机制的组合是构建未来容错应用的一个非常有前途的方向。

#### * 宽松的约束

​	**如前所述，保证唯一性约束需要达成共识，通常通过单个节点汇集特定分区的所有事件来实现**。如果想要达到传统的唯一性约東，上述处理限制对于流处理系统是不可避免的。

​	然而，另一个需要注意到的是，许多实际应用程序采用了较弱的唯一性因而可以摆脱该限制：

+ 如果万一两个人同时注册了相同的用户名或预订了同一个座位，则可以向其中一个发送道歉消息，并要求他们选择另一个。这种纠正错误的措施破称为**补偿性事务**。
+ 如果客户订购的商品超出当前库存，则可以追加补充库存，但需要为延误发货向客户道歉，并为他们提供折扣。实际上，如果叉车运走的仓库中的一些物品超过了限额，实际留下的货品也会比想象的还要少一些，这和用户超售最终效果是样的。因此，道歉的工作流程也需要成为业务流程的一部分，因此可能就没有必要对库存余量进行线性化约束了。
+ 同样地，许多航空公司会超售机票，因为它们预计有些乘客会错过航班，许多酒店也会超订房间，因为他们也预计有些客人会取消预定。在这种情况下，出于商业原因严格违反了“每个座位一人”的约束，并且在需求超过供应时要处理补偿过程(退款，升级，在邻近酒店提供免费房间等)。即使没有超额预订，为了应对由于恶劣天气而取消的航班或职员罢工等，也需要道和赔偿程序。出现这些问题而进行恢复属于业务的正常组成部分。
+ 如果万一有人提款额比他们账户中的钱还多，银行可以向他们收取透支费用，并要求他们偿还欠款。通过限制每天的提款总额，银行的风险可以控制在限定范围的。

​	因此，在许多商业环境中，实际上可以接受的是暂时违反约束，稍后通过道歉流程来修复。道歉(金钱或声誉)的成本各不相同，但通常很低：你不能取消已发送的电子邮件，但可以发送后续电子邮件并进行更正。如果不小心向信用卡收取了两次费用，可以退还其中一笔费用，而你的成本只是处理成本，或者客户投诉。如果一个账户透支了，客户也不还款，原则上银行可以通过收债员来追回款项，但是通过自动提款机超额取款，则没法直接把这笔钱收回来。

​	<u>道歉的成本是否可以接受是一个商业决策。如果可以接受的话，在写入数据之前检查所有约束的传统模型就是不必要的限制，并且不需要线性化的约東。继续写入操作，并在既成事实之后检査约朿，可能就成为一个合理的选择。你仍然可以在发生恢复代价昂贵的事情之前进行验证，但并不意味着在写入数据之前必须进行验证</u>。

​	而这些应用程序都非常需要完整性：你不会希望失去预留，或者由于错误匹配的信用和借记信息而使资金消失。但是他们并不需要时效性来执行这个约束：如果卖出的物品多于库存量，那么可以通过道歉来事后补救问题。这些做法与第5章“处理写入冲突”中讨论的冲突解决方法类似。

#### 无需协调的数据系统

​	现在我们有了两个有趣的观察：

1. 数据流系统可以保证派生数据的完整性，无需原子提交，线性化或跨分区的同步协调。
2. **虽然严格的唯一性约朿要求时效性和协调性，但是只要整体上保证完整性，即使发生暂时约束破坏，可以事后进行修复，因此许多应用实际上采用宽松式的约束并没有问题**。

​	总之，这些观察意味着数据流系统可以为许多应用程序提供数据管理服务，而不需要协调，同时仍然提供强大的完整性保证。这种邂免协调的数据系统具有很大的吸引力：与需要执行同步协调的系统相比，可以实现更好的性能和容错能力。

​	例如，它可以用于跨多个数据中心的多主节点复制系统，在区域之间异步复制。由于不需要跨区域同步协调，任何一个数据中心都可以独立运行。虽然时效性保证很弱如果不引入协调者就不可能实现线性化)，但是它仍然具有很强的完整性保证。

​	在这种情况下，可串行化事务作为维护派生状态的一部分还是有用处的，但是只能在一个限定的范围内工作很好。异构分布式事务(如XA事务)并不是必需的(请参阅第9章“实践中的分布式事务”)。同步协调可以仅在需要的地方才引入(例如，在不可能恢复的操作之前执行严格的限制)，但是如果只有一小部分应用程序需要它，就没有必要所有事务都进行协调。

​	另一种理解协调和约束的方法是：它们减少了由于不一致而引发的道歉数量，但是也可能降低系统的性能与可用性，并由此可能增加由于业务中断而引发的道歉数量。你不能将道歉减少到零，但是你可以根据自己的需求找到最佳的折中方案：选择一个合适点使得既不能有太多不一致，也不能出现太多可用性问题。

### 12.3.4 信任， 但要确认

​	我们所有关于正确性、完整性和容错性的讨论都是基于某些事情会出错，而其他事情不会出错的假定基础上。我们将这些假设称为我们的系统模型(请参阅第8章“理论系统模型与现实”)：例如，我们应该假设进程可能会崩溃，机器可能突然断电，网络可能会岀现不受控制的延迟或丟弃消息。我们也可以假设写入磁盘的数据在强制同步命令 fsync执行后不会丢失，内存中的数据没有被破坏，并且CPU的乘法指令总是返回正确的结果。

​	这些假设是相当合理的，因为大部分时间实际情况就是这样。如果我们不得不每分每秒都在担心计算机会出错，最终将无法完成任何工作。传统上，系统模型采用二元方法处理故障：假定有些事情可能发生，而其他事情是不会发生的。实际上，这更像是一个概率大小问题。关键问题是，违反我们假设的事情是否的确经常在现实中发生。

​	我们已经看到，即使未发生访问，磁盘上的数据也可能损坏(请参阅第7章“复制与持久性”)，并且网络上的数据损坏有时可能会影响TCP校验值(请参阅第8章的“弱的谎言形式”)。也许这才是我们应该更加关注的事情？

​	我过去曾经使用的一个应用程序收集了来自客户端的崩溃报告，其中一些报告只能通过设备内存发生了随机位翻转来解释。这听起来难以置信，但是如果有足够多的设备来运行你的软件，即使再不可能的事情也有发生的可能。<u>除了由于硬件故障或辐射导致的随机内存损坏之外，某些不合规的存储器访问模式可以在内存完好情况下导致位翻转，这种效应可以用来破坏操作系统中的安全机制(称为rowhammer)</u>。所以如果深入观察，硬件本身并不是它看起来那样完美正确的抽象。

​	要说明的是，随机位翻转在现代硬件上的确非常少见。我只是想指出，它们是可能会发生的，所以应该值得注意。

#### 软件缺陷时的完整性

​	除了硬件问题，总是存在软件错误的风险(bug)，这些错误无法通过底层的网络内存或文件系统校验和来捕获。即使是广泛使用的数据库软件也存在bug，例如尽管MySQL和PostgreSQL这种非常健壮且广为人知的数据库，多年来经受了很多验证，我个人还是曾经看到过MySQL没有正确维护唯一性约束和PostgreSQL的串行化隔离级别下所出现的写倾斜异常。而对于那些不太成熟的软件中，情况可能会更糟。

​	即使在软件设计、测试和代码检査方面投入相当多的努力，但bug仍无法彻底避免。虽然可能性低，且最终可以被发现然后修复，但此前总存在一段时间，可能会造成数据破坏。

​	当谈及应用程序代码时，我们往往不得不假定里面存在更多的bug，主要是大多数应用程序所做的代码检查和测试的量级无法和数据库相提并论。许多应用程序甚至不能正确使用数据库提供的完整性相关的功能，比如外键或唯一性约束。

​	ACID意义上的一致性(请参见第7章“一致性”)假定数据库以一致状态启动，而事务将其从某一个一致状态转换为另一个一致状态。所以，我们期望数据库始终处于某种一致状态。然而，前提是事务不存在bug。如果应用程序以一种错误地方式使用数据库，例如不安全地使用弱级别隔离，就不能保证数据库的完整性。

#### 不要盲目信任承诺

​	硬件和软件并不能总是处于理想状态，数据损坏迟似乎只是迟早的事情而无法避免。因此，我们至少需要有办法来査明数据是否已经损坏，以便之后修复这些数据，并试图找出错误的根源。检査数据的完整性也被称为**审计**。

​	正如第11章“不可变事件的优势”中所描述的，审计不仅适用于财务应用程序。但是，可审计性对于金融来说尤其重要，因为每个人都清楚总会有错误，而且都意识到检测和解决问题的必要性。

​	<u>成熟的系统同样会考虑不太可能的事情出错的可能性，并且主动管理这种风险。例如，HDFS和 Amazon S3等大型存储系统不完全信任磁盘：它们运行后台进程，不断读取文件，将其与其他副本进行比较，并将文件从一个磁盘移动到另一个磁盘，以减轻无提示数据损坏的风险</u>。

​	如果想确保你的数据仍然在那，只能不断地去读取和检査。大多数情况下，情况一起正常，但万一发现异常，则越早发现问题越好。基于此，今早尝试从备份来恢复数据，否则当你丢失数据，你会发现连备份也已经破坏，那时将为时已晚。**千万不要盲日地相信系统总是正常工作**。

#### * 验证的文化

​	像HDFS和AWS S3这样的系统也必须假定磁盘大部分时间都能正常工作，这是一个合理的假设，但这不同于假设它们总是正常工作。然而，目前还没有多少系统秉持这种“信任，但要确认”的理念来不断自我审计。许多人认为正确性的保证是绝对的，而没有为少见但可能的数据损坏而有所准备。我希望将来会有更多的自我验证或自我审计系统(self-validating或self-auditing)，不断的检查自身的完整性，而不是依赖盲目的信任。

​	我担心推崇ACID数据库的文化会导致我们在盲目信任底层技术(如事务机制)的基础上开发应用程序，而忽视了整个过程中的可审计性。由于我们所信任的这些技术在大多数情况下工作得很好，审计机制似乎并不值得投入。

​	**但之后数据库的格局发生了显著变化：在 NoSQL下，弱一致性成为新常态：底层存储技术并非那么成熟可靠但却普遍使用**。然而，数据审计机制尚未形成，我们仍然继续盲目信任底层技术而开发上层应用，这种做法已经变得更加危险。我们应该花些时间来思考一下关于可审计性的设计。

#### 可审计性的设计

​	如果某个事务改变了多个数据库对象，就很难说清楚事务究竟意味着什么了。即使捕获了事务日志(请参阅第11章的“变更数据捕获”)，相关表的插入、更新和删除操作等并不一定能清楚地说明为什么这样执行。决定这些变化的应用逻辑的调用总是千变万化，无法轻易复现。

​	相比之下，**基于事件的系统可以提供更好的可审计性**。在事件源方法中，用户对系统中的输入都被表示为一个单一的不可变事件，并且任何结果状态的更新都是依据该事件派生而来。派生可以很确定性的执行并且是可重复的，所以通过相同版本的派生代码来处理相同的事件日志将产生相同的状态更新。

​	<u>**凊楚地控制数据流(请參阅第10章“批处理输岀的晢学”)可以使数据的来源管理更加清晰，从而使完整性检査更加可行**。对于事件日志，我们可以使用哈希校验来检查存储层是否发生数据破坏。对于派生状态，我们可以重新运行对相同的事件日志执行的批处理和流处理，以检查是否得到相同的结果，甚至是并行运行一个冗余派生系</u>。

​	确定的和定义凊晰数据流也有助于系统调试和跟踪系统的操作，从而确定为什么发生了某些事情。如果中间发生了意外事件，可以提供诊断能力来重现导致意外事件的相同环境，这种精准复现历史时刻的调试能力将非常有价值。

#### 端到端论点的再讨论

​	如果我们不能完全相信系统中的每个组件都能免于损坏(每一个硬件永不出错，且毎一个软件都没有bug)，那么我们至少也要定期检査数据的完整性。如果不检查的话，就无法发现数据破坏直到为时已晚，并造成了一些相关损失，此时再去追踪这个问题就会更加困难和付出更多代价。

​	<u>检査数据系统的完整性最好以**端到端**的方式进行(请参阅本章前面的“数据库端到端的争论”)：在完整性检査中所包含的系统部件越多，则过程中某些阶段发生无告警的数据破坏的概率就越少</u>。如果我们可以检査整个派生系统流水线是端到端正确的，那么路径中的任何磁盘、网络、服务和算法等已经全部囊括在内了。

​	持续的端到端完整性检查可以提高你对系统正确性的信心，从而使你的发展速度更快。与自动化测试一样，审计增加了发现错误的可能性，从而降低了系统变更或新存储技术潜在造成损害的风险。如果你不害怕做出改变，就可以更好地演化应用使之满足不断变化的需求。

#### 审计数据系统的工具

​	目前，将可审计性列为高优先级别关注的数据系统并不多。有些应用程序实现了内部的审计机制，例如将所有更改记录到单独的审计表中，但是保证审计日志的完整性和数据库状态仍然有些困难。<u>事务日志可以通过定期使用硬件安全模块对事务日志进行签名来防止篡改，但这并不能保证正确的事务首先写入日志</u>。

​	**使用加密工具来证明系统的完整性**将是很有趣的，<u>这种方式对于广泛的硬件和软件问题甚至是潜在的恶意行为都是可靠的</u>。加密货币，区块链和分布式账本技术，比如比特币、以太坊、波纹、恒星等已经在这个领域崭露头角。

​	我没有资格评论这些技术作为货币或者共识合约的优缺点。但是，从数据系统的角度来看，它们包含了一些有趣的想法。本质上，它们都是分布式数据库，具有数据模型和事务机制，不同的副本可以由不信任的组织托管。副本不断检查彼此的完整性，并采用共识协议来就执行的事务达成一致。

​	我对这些技术中涉及的拜占庭式容错方面存有怀疑(参见第8章“拜占庭式故障”)，而且我认为发现工作证明(比如比特币挖矿)机制非常浪费资源。比特币的交易吞吐量是相当低的，尽管更多是处于出于政治和经济方面的原因。但是，其完整性检查方面很有意思。

​	密码审计和完整性检査通常依赖于**Merkle树**，这是一种哈希散列树，可以用来有效地证明某个数据集(和其他一些数据集)中出现的记录。除了加密货币的炒作之外，<u>证书透明度是一种依赖Merkle树来检查TLS/SSL证书有效性的安全技术</u>。

​	我可以想象完整性检査和审计算法，如证书透明度和分布式账本，在通用的数据系统中将得到越来越广泛的应用。使它们具有和没有加密审计的系统同样的可扩展性还需要更多一些工作，并且还要保持尽可能低的性能损失。但我认为这是一个值得关注的领域。

## * 12.4 做正确的事情

​	在本书的最后部分，我想退后一步再谈谈。本书中，我们已经考察了数据系统的各种不同架构，评估其优缺点，探讨了如何构建可靠、可扩展和可维护的应用系统。但是，我们特意留了一个非常重要的基础部分，现在，在结束之前我想重点讨论。

​	每个系统的都有其构建目的，我们所采取的每一个行动都会产生有意或无意的后果。目的可能像赚钱一样简单，但对世界来的影响可能远远超出我们的初衷。建立这些系统的工程师有责任仔细考虑这些后果，并有意识地决定我们想要生活在什么样的世界。

​	我们将数据作为一个抽象的东西来讨论，但要记住，许多数据集都是关于人的：他们的行为、他们的兴趣和他们的身份。我们必须以人性和尊重来对待这些数据。用户也是人，人的尊严是最重要的。

​	软件开发越来越牵扯到重要的道德选择。有一些指导方针可以帮助软件工程师解决这些问题，比如ACM的软件工程道德规范和专业实践，但是在实践中却很少被讨论、应用和实施。因此，工程师和产品经理们有时对隐私和产品潜在的负面后果采取了非常漫不经心的态度。

​	技术不合适或者技术本身有缺陷并不重要，重要的是如何使用它以及如何影响人们。对于像搜索引擎这样的软件系统来说，就像枪支武器一样重要。我认为软件工程师如果只专注于技术而忽视其后果是不够的，道德责任也是我们要担起的责任。评判道德总是困难的，但它太重要了以至无论如何不能被忽视。

### 12.4.1 预测性分析

​	预测分析是“大数据”炒作的主要部分。使用数据分析预测天气或疾病的传播是一回事。预测一个囚犯是否可能再次犯罪，贷款申请人是否有可能违约，或者一个保险客户是否有可能进行昂贵的索赔，则是另一回事。后者将直接影响到个人的生活。

​	在线支付网络需要防止欺诈性交易，银行希望避免不良贷款，航室公司希望避免劫持，公司希望避免雇用无用或不值得信任的人，这些都是很自然的事情，从它们的角度来看，错失商机的代价很低，但坏账或者有问题员工的成本要高得多，所以相关组织谨慎起见是很自然的。如果有疑问，他们最好说不。

​	然而，基于算法的决策变得越来越普遍，被这些算法(准确地或错误地)标记为有风险的人可能会由此面临大量“不”的遭遇。系统地被排除在包括工作、航空旅行、保险、物业租赁、金融服务以及其他社会关键方面之外，会对一个人的自由生活产生巨大的制约，因此也被称为“**算法囚笼**”。在很多国家，司法制度会在证明有罪之前推定当事人无罪。然而，自动化的系统则有可能系统地、任意地排除某个人参与社会活动，而且是在这个人没有任何犯罪证据的情况下，并且对他/她来说几乎没有上诉的机会。

#### 偏见与歧视

​	算法所做出的决定不一定比人类做得更好或更糟。每个人都可能有偏见，即使他们积极地试图消除自己的偏见，歧视性做法也可能在文化上形成制度化。希望根据数据而不是主观和本能的评估来做出决定可能会更加公平，而且给了那些在传统体制中经常被忽视的人以更好的机会。

​	当开发预测分析系统时，我们不仅仅是通过使用软件来指定什么时候说是或什么时侯说否的规则来自动化决策，甚至将规则本身从数据中推断出来。但是，<u>这些系统所学到的模式是不透明的：即使数据中存在一些相关性，我们也可能不知道为什么。如果在算法的输入中存在系统性偏见，那么系统很可能吸收塔并在最终输出中放大这种偏见</u>。

​	在许多国家，反歧视法律禁止根据种族、年龄、性别、性取向、残疾或信仰等受保护的特征点而区别对待不同的人。可以分析一个人的数据的其他特征，但是如果他们与受保护的特征相关，会发生什么？例如，在某些敏感的地区，一个人的邮政编码，甚至他们的IP地址，都是一个强有力的预测。<u>就像这样，相信一个算法可以以某种方式将偏倚的数据作为输入并产生公平和公正的输出，这似乎是荒谬的。然而，这种观点似乎常常被数据驱动型决策的支持者所采纳，这种态度被讽刺为“机器学习就像为偏见在洗钱</u>”。

​	预测分析系统只是基于过去而推断，如果过去是有偏见的，它们就会把这种偏见编码下来。如果我们希望未来比过去更好，那么就需要道德想象力，而这只有人类才具备。**数据和模型只应该是我们的工具，而不是我们的主人**。

#### 责任与问责

​	自动决策引发了责任与问责方面的问题。如果一个人犯了错误，他们可以被追究责任，受决定影响的人可以上诉。算法也会犯错误，但是如果出了问题谁会负责？当辆自动驾驶汽车引发事故时，谁来负责？如果自动信用评分算法系统地区分特定类型的人，是否有任何追索权？如果你的机器学习系统的决定受到司法审查，你能向法官解释算法是如何做出决定的吗？

​	信用评级机构是收集数据进并基于数据做出对人相关决策。不良的信用评分会让生活变得困难，但至少一个信用评分通常是基于一个人的实际借款历史等相关事实，并且记录中的任何错误都可以被纠正(虽然这些机构通常不会让这种修改变得那么容易)。然而，基于机器学习的评分算法通常使用更广泛的输入范围，而且更加不透明，更难理解某个特定决策是如何发生的，以及是否有人受到不公正的对待。

​	信用评分总结了“你以前的行为”，而预测分析则通常基于“谁与你类似，以及和你相似的人在过去的行为如何”。与他人行为相似的对比意味着刻板的来区分人们，例如基于他们居住的地方(这是一个接近社会经济阶层的信息)。那么那些被放错位置的人呢？而且，如果由于错误的数据而儆出的决定是不正确的，则追索权几乎是不可能的。

​	很多数据本质上是统计出来的，这意味着即使概率分布总体上是正确的，个别情况也可能是错的。例如，如果某个国家的平均寿命是80岁，并不意味着某个人过了80岁生日就会死亡。基于平均分布和概率分布无法预测某个人的寿命。同理，预测系统的输出是概率性的，在个别情况下也可能是错误的。

​	<u>盲目地相信数据至高无上不仅是误解的，而且是非常危险的。随着数据驱动的决策变得越来越普遍，我们需要弄清楚如何使算法更负责任和透明，如何免强化现有的偏见，以及如何在错误不可避免时加以修复</u>。

​	我们还需要弄清楚如何防止数据被滥用，并努力发挥数据的正面作用。例如，通过分析可以揭示人们生活中的财务状况和社会特征。一方面，可以利用它把援助和支持集中去帮助那些最需要援助的人。另一方面，它有时被不正当的企业用来识别弱势群体，并向他们销售高风险的产品，比如高成本的借贷和毫无价值的大学学位。

#### 反馈环路

​	即使预测性应用对人们所产生的立竿见影的长远影响较少，比如推荐系统，我们仍然要面对一些难以解决的问题。如果预测用户希望看到什么的服务做得很好，他们最终可能只会向人们展示系统认同的观点，从而容易产生陈旧观念、错误信息和两极分化的**回声室效应**。我们已经看到社交媒体在竞选活动上产生的类似影响。

​	当预测分析影响人们的生活时，特别是由于自我强化反馈环路而岀现一些有害问题。比如，考虑雇主使用信用评分来评估潜在雇员。你可能是一个有良好信用评分的员工，但是由于一些不可控情况，你会突然发现自己陷入了经济困境。由于错过了信用卡账单还款，你的信用评分会受损，并可能因此找工作会变得困难。失业使你陷入贫困，使信用评分进一步恶化，结果就是更难找到工作。由于不合适的假设，产生了这样一个隐藏在数学严谨性和数据的伪装背后的下降旋涡。

​	我们不是总能预测什么时候发生这样的反馈环路。然而，<u>通过思考整个系统(不仅是计算机化的部分，还有与之互动的人)可以预测许多后果，这是一种被称为**系统思维**的方法</u>。我们可以尝试理解一个数据分析系统是如何响应不同的行为、结构和特征。系统是否强化和扩大了人们之间存在的差异(例如，使富者变得更富或穷人变得更穷)？还是试图打击不公平性?即使有最好的意图，我们也必须小心意外的后果。

### 12.4.2 数据隐私与追踪

​	除了预测分析方面的问题(即使用数据来做出关于人的自动化决策)之外，数据收集本身也存在道德问题。收集数据的组织与正在收集数据的人之间的是什么关系呢？

​	当系统只保存用户所明确输入的数据(因为他们希望系统以某种方式进行存储和处理这些数据)，系统就是在为用户执行一项服务：用户就是客户。但是，处于其他目的而对用户的活动被跟踪并记录时，这种关系就不那么清晰了。服务不再仅仅是用户告诉它所做的事情，而是服务于自身的利益，这可能与用户的利益相冲突。

​	跟踪行为数据对于许多面向用户的在线服务变得越来越重要：跟踪哪些搜索结果被单击有助于提高搜索结果的排名；推荐“喜欢X的人也喜欢Y”，可以帮助用户发现有趣而有用的东西；A/B测试和用户流量分析可以帮助指出如何改进用户界面。这些功能需要一定量的用户行为跟踪，并且用户也可以从中受益。

​	但是，根据公司的商业模式，追踪往往不止于此。如果服务是通过广告获得资助的那么广告主就是实际的客户，而用户的利益则是次要的。跟踪数据会更加详细，分析变得更加深入，数据也会被保留很长一段时间，以便为营销目去建立毎个人的详细资料。

​	<u>现在，公司和被收集数据的用户之间的关系开始变得和以往大不一样了。用户得到免费的服务，并尽可能地被引诱参与到服务中。对用户的追踪不再是服务与个人，而是服务于资助广告客户的需求。我认为这种关系可以用一个更阴暗的词来描述：监视</u>。

#### 监控

​	我们做一个有意思的实验，尝试用监控一词来代替数据，观察常见的短语是否听起来还是那么美好。比如说：“在我们的监控驱动的组织，我们收集实时监控流，并将其存储在我们的监控仓库。我们的监测科学家使用先进的分析和监测处理，以获得新的见解。“

​	把这个观念实验用于本书书名“数据密集型应用系统设计”同样会引发巨大争议，但是我认为替换一个更强力的词汇更能说明其中的道理。在我们试图依靠软件“掌控世界”过程中，我们建立了世界上迄今为止最大规模的监视基础设施。我们正朝着物联网快速迈进，每个有人居住的空间至少包含一个联网的麦克风，很多设备包括智能手机，智能电视，语音助理设备，嬰儿监视器，甚至是儿童玩具等都在使用基于云的语音识别。这些设备中的很多都有可怕的安全记录。

​	即使是条件受限制的地方，也梦想着在每个房间放置一个麦克风，并强迫每个人不断地携带一个能跟踪他们位置和动作的设备。然而，我们今天显然自愿地、甚至迫不及待地把自己置身于一个全面监视的世界里。不同之处在于数据是由很多公司收集而不是政府机构。

​	**并非所有的数据收集都必须满足监控的要求，但是检查这些数据可以帮助理解我们与数据收集者的关系。为什么我们看起来乐意接受企业的监督？也许你觉得没有什么可隐瞒的东西，换句话说，你完全和现有的组织结构是一个阶层的，你不是被边缘化的少数派，不必害怕。但是不是每个人都如此幸运。也许这是因为你知道监控的目的似乎是好的，不是强制性和必须顺从的，而只是更好的建议和更个性化的营销。但是，结合上一节的预测分析的讨论，它们的差别似乎微乎其微**。

​	我们已经看到汽车保险费用已经和汽车内安装的追踪设备连在一起了，以及健康保险的覆盖范围也取决于人们身上佩戴的健身追踪设备。当监控被用来确定生活中重要的事情，例如保险或就业等方面的东西时，它就开始变得不那么亲切了。此外，数据分析可以揭示出令人惊讶的侵入性的事情：比如，智能手表或健身追踪器中的运动传感器可以相当准确地计算出正在输入的内容(例如密码)。况且现在的分析算法变得越来越强大。

#### 赞成与选择的自由

​	我们可能会宣称，用户自愿选择使用跟踪活动的服务，并且已经同意服务条款和隐私政策，因此他们同意被收集数据。我们甚至可以声称，用户正在用他们提供的数据来换取有价值的服务，而为了正常提供服务，跟踪是必要的。毫无疑问，社交网络、搜索引擎以及其他各种免费的在线服务对于用户来说的确是有价值，但是即使这样，上述说法也存在问题。

​	用户几乎不知道什么样的个人数据会进入到数据库，或者数据是如何保留和处理的，大多数隐私政策的条款也极尽所能地搞得含混不清。不清楚他们的数据会发生什么，用户就不能给予任何有意义的认同。通常，来自用户的数据还被用到了不是该服务的用户身上，并且该用户根本就没有同意数据收集的任何条款。我们在本书中讨论的派生数据集(其中来自整个用户群的数据可能是行为跟踪和外部数据源的结合体)恰恰是用户无法获得任何有意义的理解的那种。

​	而且，<u>数据是通过单向过程从用户提取而来，而不是通过真正的互惠关系，也不是公平的价值交换。没有对话，用户无法选择提供多少数据以及他们会收到什么样的服务：服务与用户之间的关系是非常不对称的：这些条款是由服务提供商所设置，而不是由用户</u>。

​	对于不同意被监视的用户，唯一真正的选择就是不使用服务。但是这个选择也不是免费的：如果一项服务非常受欢迎以至于“被大多数人认为是基本的社会参与所需要的“，那么指望人们选择退出这项服务是不合理的，使用它变成一种事实性强制约束。例如，在大多数社会群体中，携带智能手机，使用Facebook进行社交以及使用Google査找信息已成为常态。特别是当一个服务具有网络效应时，人们选择不使用它是有社会成本的。

​	由于担心服务跟踪用户而决定拒绝使用，这只对极少数拥有足够的时间和知识来充分了解隐私政策的人群可以称得上是一种选择，并且他们可以不需要担心由此可能会失去某些机会而被迫参与这些服务。然而，对于处境较差的人来说，选择自由没有意义：对他们来说，被监视变得不可避免。

#### 数据隐私和使用

​	有时人们会说“再无隐私”，理由是一些用户愿意将他们生活的各种事情发布到社交媒体上，有时是很稀松平常的事情，有时甚至是非常私密的。但我认为，这种说法是错误的，因为它基于了对隐私这个词的误解。

​	拥有隐私并不意味着一切事情都要保密；它意味着你可以自由选择向谁展示，并展示哪些东西，要公开什么，以及要保密什么。隐私权是一个决定权：每个人都能够决定在各种情况下如何在保密和透明之间取舍咧。这事关个人的自由和自主。

​	当通过监控基础设施从人们身上提取数据时，隐私权不一定被破坏，而是转移到了数据收集者。获取数据的公司基本上都会说：“相信我们对你的数据只做正确的事情”，这意味着决定要披露什么以及保密的权利是从个人转移到公司了。

​	这些公司最终选择对大部分数据继续保持私密，因为泄露数据会引起可怕的后果，并且会损害它们的商业模式(这样它们就可以比其他公司更了解用户)。关于用户的隐私信息通常是间接地被泄露，例如借助数据分析，将广告投放给特定人群(例如患有特定疾病的人)。

​	即使针对特定人群的广告并没有识别岀特定的个体，但他们也丧失了披露隐私信息的权利，例如是否患有某种疾病。现在不是用户根据自己的喜好决定向谁显示，显示哪些隐私内容：做决定的是公司，<u>公司通常会以最大化利润为目标來处置隐私权</u>。

​	许多公司都不希望人们将他们视为作恶者，例如想法避免被问及那些具有侵犯性的数据收集，而让人们感觉他们是服务于用户体验或感受。即使这些体验有时很糟糕：例如，有些事情虽然是事实，但它触发人们内心痛苦的回忆，用户可能并不希望它被反复提及。对于任何类型的数据，我们都应该期望在某种程度上会出现错误、不受欢迎或不适当的可能，并建立响应的处理机制。当然，何为“不受欢迎”或“不适当”，完全是由人来判断决定的。除非我们明确地设计程序使得它们尊重人类的需求，否则算法对此是没有任何概念的。作为这些系统的构建者，我们必须谦兼虚、乐于接受并为之做好准备。

​	所谓隐私设置，即允许在线服务的用户控制哪些数据对他人可见，只是将一些控制权还给用户的起点。但是，无论设置如何，服务本身仍然可以不受限制地访问数据，并且可以以隐私策略所允许的任何方式自由地使用它。即使服务承诺不会将数据岀售给第三方，它通常也授予自身无限制的权利，在内部处理和分析数据时往往远远超过用户公开可见的权利。

​	这种大规模地将隐私控制权从个人转移到公司在历史上是绝无仅有的。监视一直存在，但过去是昂贵和手动的，不可护展和自动化。信任关系一直存在，例如病人与医生之间，或被告与律师之间，但在这些情况下，数据的使用受到道德、法律和监管限制的严格管制。**互联网服务使得在没有用户同意的情况下积累大量敏感信息更加容易，并且在用户不知情相关后果的前提下大规模地使用它**。

#### 数据作为资产和权力

​	由于行为数据是用户与服务交互作用过程中产生的副产品，有时称为“废弃数据”表示这类数据是无价值的。通过这种方式，行为分析和预测分析可以被看作是某种回收方式，从那些本来可能被拋弃的数据中提取价值。

​	更准确的说是应该反过来看：从经济的角度来看，如果有针对性的广告是一种服务，那么关于人的行为数据就是服务的核心资产。在这种情况下，与用户交互的应用程序仅仅是一种诱使用户将越来越多的个人信息提供给监控基础设施的手段。在线服务中经常可以看到令人愉悦的人类创造力和社交关系被数据提取机器不负责任地利用。

​	数据中介公司的存在也印证了个人数据是宝贵资产的说法，这个数据中间商是一个秘密行业，从事采购、汇总、分析、推断和兜售侵入性个人数据，主要是为了营销目的。很多初创公司主要靠它们的用户量(或者“眼球”，即他们的监视渗透能力)来股价。

​	因为这些数据是有价值的，所以很多人都想要它。公司很明显是需要这些数据的，这就是为什么他们收集它的原因。当公司破产时，收集到的个人资料就是被出售的资产之一。而且，数据很难保证安全，所以令人不安的破坏事件时常发生。

​	这些观察使得某些批评者认为数据不仅仅是一种资产，而是一种“有毒资产”，或者至少是“有害物质”。即使我们认为有能力防止数据滥用，但是每当我们收集数据时，都需要平衡带来的好处和落入坏人手中的风险：计算机系统可能会被犯罪分子篡改，数据可能会被内部人员泄露，公司可能会落入不当价值观的无良管理层手中等。

​	收集数据时，一定要综合考量。

​	正如古老的格言所言，“知识就是力量”。此外，“审视他人但避免自我审查是最重要的权力形式之一”。尽管今天的科技公司并没有公开地寻求某些权力，但是它们所积累的数据和知识给了它们很大的控制权力，而且很多是在私下进行，不在公众监督之内。

#### 记住工业革命

​	数据是信息时代的关键性特征。互联网，数据存储，处理器和软件驱动的自动化正在对全球经济和人类社会产生重大影响。考虑到过去十余年我们的日常生活和社会组织所发生的巨大变化，有理由相信在未来几十年可能会继续发生根本性的变化，由此不由得联想到工业革命。

​	工业革命是伴随着重大技术和农业经济的大发展而来的，长远来看人们生活水平明显提高。然而，它也带来了一些重大问题：空气污染(由于烟雾和化学过程)以及水污染(由于工业废物和生活废物等)的后果是可怕的。工厂老板光鲜亮丽，工人住房条件差、工作时间长、工作条件恶劣。不合法的工人到处可见，包括在矿场的危险的，低薪的工作。

​	人类花了很长时间才制定了一系列的保护措施，如环境保护条例、工作场所安全协议、禁止使用童工和食品卫生检査规范等。毫无疑问，当工厂不能将废物倾倒入河中，不能销售受污染的食品，也不能再继续剥削工人时，成本就会增加。但是，整个社会获得了巨大的受益，所谓“绿水青山就是金山银山”，几乎没有人再愿意回到之前污染的年代。

​	正如工业革命存在需要被管理的黑暗面一样，向信息时代的过渡也有需要面对和解决的重大问题。我相信收集和使用数据就是其中一个。用 Bruce Schneier的话来说：

​	数据问题是信息时代的污染问题，保护隐私类似包含环境。几乎所有的电脑都在产生信息，数据持续不断然后又慢慢消逝。我们如何定义这个问题以及如何处置对信息经济的健康运行至关重要。正如我们今天回顾工业时代的早期几十年，惊呼为什么我们的祖先如此急于建立工业世界而忽视了污染问题，我们的子孙也将审视现在的信息时代初期，并评判我们如何来应对数据收集与滥用。

​	我们理应做到让后代们感到骄傲。

#### 立法与自律

​	数据保护法可能有助于维护个人的权利。例如，1995年的“欧洲数据保护指令”规定，个人数据必须“为特定的，明确的和合法的目的而收集，而不能与这些目的不相符的方式来处理”，而且数据必须“就其收集的目的而言充分，相关但不过分”。

​	但是，这个立法在今天的互联网环境下是否有效还是值得商榷。这些规则直接违背了大数据的理念，即最大化数据收集，将其与其他数据集合在一起，进行实验和探索，以产生新的见解。探索意味着将数据用于不可预见的目的，这与用户同意的“明确的和清晰的”目标相反(如果我们充分理解所认同内容)。目前更新版的规定仍在制定之中。

​	收集大量数据的公司反对监管，它们认为这是对创新的负担和阻碍。在某种程度上，这种反对是有道理的。例如，分享医疗数据时，隐私风险明显，但也有潜在的机会：如果数据分析能够帮助我们实现更好的诊断或找到更好的治疗方法，可以预防多少人死亡？过度管制可能会阻止这种突破。这种潜在的机会与风险之间平衡难以很好掌握。

​	从根本上说，我认为需要对针对个人数据的技术领域有观念上转变。我们应该停止过度以用户为衡量指标，牢记用户值得尊重。我们应该主动调整数据收集和处理流程，建立和维持与那些依赖我们软件的人们之间的信任关系。

​	我们应该主动向用户介绍他们的数据如何使用，而不是让他们蒙在鼓里全然不知。我们应该允许每个人维护自己的隐私，即控制自己的数据而不是通过监视来窃取他们的控制权。用户掌控自己数据的个人权利就像是国家公园中的优美环境：如果对环境没有明确的保护和关心，绿水青山将会被破坏。那将是所有公众的悲哀，我们所有人都会受到影响。无所不在的监视和数据滥用并非不可避免，我们仍然能够阻止它。

​	我们究竟该如何实现上述目标还是一个开放的问题。首先一点，我们不应该永远保留数据，一旦不再需要，就尽快清除它们。清除数据与不可变性的想法背道而驰请参阅第11章“不可变性的限制”)，但是可以解决这个问题。我所看到的一个很有前途的方法是通过加密协议来实施访问控制，而不仅仅是通过策略。总的来说，观念与态度的变化都是必要的。

## 12.5 小结

​	本章，我们讨论了设计数据系统的新方法，以及包括了我个人对未来的看法和猜测。我们从观察开始，即没有一种工具可以有效地服务于所有可能的场景，因此应用程序必须编排多个不同的软件来完成他们的目标。我们讨论了如何使用批处理和事件流来解决这个数据集成问题，以使数据在不同系统之间灵活流动。

​	基于这种方法，某些系统被指定为记录系统，而其他数据则是通过转换而来。通过这和方式，我们可以维护索引、实体化视图、机器学习模型、统计摘要等。通过使这些推导和变换异步松耦合，防止了局部问题可能扩散到系统的不相关部分，从而提高了整个系统的鲁棒性和容错性。

​	将数据流表示为从一个数据集到另一个数据集的转换也有助于演化应用程序：如果要更改其中一个处理步骤，例如更改索引或缓存的结构，则可以在整个输入数据集上重新运行新的转换代码以便重新输出。同样，如果出现问题，可以修复代码然后重新处理数据来执行恢复。

​	这些过程与内部数据库完成的过程非常相似，因此我们将数据流应用系统的概念重新分解为数据库组件，并通过组合这些松耦合的组件来构建应用程序。

​	<u>派生状态可以通过观察基础数据的变化来不断更新。而且，下游消费者可以进一步观察派生的状态。我们甚至可以将此数据流一直传送到显示数据的最终用户设备，从而构建动态更新的用户界面以反映数据更改并支持离线使用</u>。

​	接下来，我们讨论了在出现故障时如何确保处理正确。我们看到，强的完整性保证可以通过异步事件处理，通过使用**端到端**操作标识使操作最终满足**幂等性**，并通过异步检查约束来实现。客户可以等到检查通过或者不用等，但是如果万一违反约束则需要事后道歉。这种方法比使用分布式事务的传统方法更具可扩展性和可靠性，并且适合于实践中有多个业务流程同时工作的场景。

​	围绕数据流来构建应用程序并异步检查约東，我们可以避免大多数协调工作，系统保证完整性并性能良好，即使在地理位置分散的情况下或是岀现故障时也是如此。然后，我们简单介绍了使用审计来验证数据的完整性并检测数据是否发生破坏。

​	最后，我们又退后一步，审视了构建数据密集型应用系统在道德层面的一些问题。我们看到，虽然数据可以用来帮助人们，但是也可能造成重大的伤害：做出严重影响人们生活的貌似公平的决定，这种算法决定难以对其提起诉讼；导致歧视和剥削;使监视泛滥；暴露私密信息等。我们也面临着数据泄露的风险，而且即使善意的数据使用也可能会产生某些意想不到的后果。

​	由于软件和数据对世界的影响如此之大，我们的工程师们必须谨记，我们有责任为我们赖以生存的世界而努力：一个以人性和尊重来对待人的世界。我希望我们能够一起为实现这一目标而努力。
