# 数据密集型应用系统设计-学习笔记-03

# 第三部分 派生数据

# 第10章 批处理系统

​	本书前两部分讨论了很多有关请求和查询以及对应的响应或结果等方面的内容。许多现代数据系统都假设以这种方式来处理数据：用户请求某种信息，或者发送指令，一段时间后(希望)系统会返回结果。数据库、高速缓存、搜索索引、Web服务器和其他许多系统都是这样工作的。

​	对于这种在线系统，无论是请求页面的浏览器还是调用远程API的服务，往往都假定请求是由用户触发，而且用户正在等待响应。通常等待不应太久，所以我们非常重视这些系统的响应时间(参阅第1章“描述性能”)。

​	Web和越来越多基于HTTP/REST的API使得请求/响应的交互模式变得如此普遍，以至于很容易将其视为理所当然。但是我们应当记住，这并不是构建系统的唯一途径，其他方法也有其优点。下面我们来区分三种不同类型的系统：

+ 在线服务(或称在线系统)

  服务等待客户请求或指令的到达。当收到请求或指令时，服务试图尽可能快地处理它，并发回一个响应。响应时间通常是服务性能的主要衡量指标，而可用性同样非常重要(如果客户端无法访问服务，用户可能会收到一个报错消息)。

+ 批处理系统(或称离线系统)

  批处理系统接收大量的输入数据，运行一个作业来处理数据，并产生输出数据。作业往往需要执行一段时间(从几分钟到几天)，所以用户通常不会等待作业完成。相反，批量作业通常会定期运行(例如，每天一次)。批处理作业的主要性能衡量标准通常是吞吐量(处理一定大小的输入数据集所需的时间)。本章主要讨论批处理。

+ 流处理系统(或称近实时系统)

  <u>流处理介于在线与离线/批处理之间(所以有时称为近实时或近线处理)</u>。与批处理系统类似，流处理系统处理输入并产生输岀(而不是响应请求)。但是，流式作业在事件发生后不久即可对事件进行处理，而批处理作业则使用固定的组输入数据进行操作。这种差异使得流处理系统比批处理系统具有更低的延迟。由于流处理是在批处理的基础上进行的，我们将在第11章讨论流处理系统。

​	正如我们将在本章中所看到的，批处理是构建可靠、可扩展与可维护应用的重要组成部分。例如，2004年发表的著名批处理算法 MapReduce“使得 Google具有如此大规模的可扩展性能力”(虽然该说法有些夸大和简单化)。该算法随后在各科开源数据系统中被陆续实现，包括 Hadoop、 CouchDB和 MongoDB。

​	与多年前为数据仓库开发的并行处理系统相比， MapReduce是一个相当低级别的编程模型，但是对于运行在商用硬件上的处理规模来讲，它绝对是一个重大的进步。虽然 MapReduce的重要性现在有些下降，但它仍然值得深入理解，因为它清晰地解释了批处理为什么有用以及如何有用。

​	事实上，批处理是一种非常古老的计算形式。早在可编程数字计算机诞生之前，打孔卡制表机(例如1890年美国人口普查中使用的 Hollerith机器)就实现了半机械化的批量处理，以计算来自大量输入的汇总统计信息。而 MapReduce与1940年和950年泛应用于商业数据处理的IBM卡片分类机器有着惊人的相似之处。是的，历史总是在重演。

​	本章将介绍MapReduce和其他一些批处理算法和框架，并探讨它们在现代数据系统中的应用。但首先，我们从使用标准UNIX工具的数据处理开始。或许你已经很熟悉，但考虑到UNIX的思想和经验普遍见于大规模、异枃分布式数据系统，因此值得重温UNIX哲学。

## 10.1 使用UNIX工具进行批处理

​	我们从一个简单的例子开始。假设有一个Web服务器，每次响应请求时都会在日志文件中追加一行记录。例如，使用 nginx默认访问日志格式，日志中的一行记录如下所示：

```shell
216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] "GET /css/typography.css HTTP/1.1" 
200 3377 "http://martin.kleppmann.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) 
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36"
```

​	实际上这只是一行，分成多行只是为了方便阅读。这一行中包含很多信息。为了解释它，你需要了解日志格式的定义，如下所示：

```shell
 $remote_addr - $remote_user [$time_local] "$request"
 $status $body_bytes_sent "$http_referer" "$http_user_agent"
```

​	因此，这一行日志表示2015年2月27日17:55:11 UTC，服务器从IP地址为216.58.210.78的客户端接收到一个对文件/css/typography. cssl的请求。用户为非认证用户，所以$remote_addr被设置为连字符(-)。响应状态是200(即请求成功)，响应大小是3377字节。浏览器是 Chrome40，并且它加载了该文件，因为这个文件在URL为`http://martin.kleppmann.com`的页面中被引用。

### 10.1.1 简单日志分析

​	有很多工具可以用来处理日志文件，并生成关于网站流量的漂亮报告。但是出于实践的目的，我们从基本的UNIX工具做起。例如，假设想找出网站中前五个最受欢迎的网页，可以在 UNIX Shell执行下列操作：

```shell
cat /var/log/nginx/access.log | #1
    awk '{print $7}' | #2
    sort             | #3
    uniq -c          | #4
    sort -r -n       | #5
    head -n 5          #6
```

1. 读取日志文件。
2. 将毎一行按空格分割成不同的字段，每行只输出第七个字段，即请求的URL地址。在我们的示例行中，这个请求URL地址是/css/typography.css。
3. 按字母顺序排列URL地址列表。如果某个URL被请求过n次，那么排序后，结果中将包含连续n次的重复URL。
4. uniq命令通过检査两条相邻的行是否相同来过滤掉其输入中的重复行。-c选项为输出一个计数器：对于每个不同的URL，它会报告输入中出现该URL的次数。
5. 第二种排序按毎行起始处的数字(-n)排序，也就是URL的请求次数。然后以反向(-r)顺序输出结果，即结果中最大的数字首先返回。
6. 最后，head只输出输入中的前五行(-n 5)，并丢弃其他数据。

​	该命令序列的输出如下所示：

```shell
    4189 /favicon.ico
    3631 /2013/05/24/improving-security-of-ssh-private-keys.html
    2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
    1369 /
     915 /css/typography.css
```

​	如果你不熟悉UNIX工具，上面的命令行可能看起来令人费解，但是它的确非常强大。它将在几秒钟内处理千兆字节的日志文件，你可以轻松修改分析指令以满足自己的需求。例如，如果要省略CSS文件，将awk参数更改为`$7 !~ /\.css$/ {print $7}`即可。如果想得到请求次数最多的客户端IP地址而不是页面，那么就将awk参数更改为`{print $1}`。

​	本书并不打算详细探讨UNIX工具，但是这些工具非常值得学习。令人惊讶的是，使用awk，sed，grep，sort，uniq和 xargs的组合，可以在几分钟内完成许多数据分析任务，并且表现令人十分满意。

#### 命令链与自定义程序

​	你也可以写一个简单的程序来做同样的事情，而不是使用UNIX命令链。例如，Ruby代码的实现可能看起来像这样：

```ruby
counts = Hash.new(0)         # 1
File.open('/var/log/nginx/access.log') do |file| 
    file.each do |line|
        url = line.split[6]  # 2
        counts[url] += 1     # 3
    end
end
top5 = counts.map{|url, count| [count, url] }.sort.reverse[0...5] # 4
top5.each{|count, url| puts "#{count} #{url}" }                   # 5
```

1. counts是一个哈希表，保存了用于记录每个URL出现次数的计数器。计数器默认值为0。
2. 对日志的每一行，都把URL作为第七个被空格分隔的字段(这里的数组索引是6，因为Ruby数组是零索引的)。

3. 为日志当前行中URL地址的计数器加1
4. 按计数器值(降序)对哈希表内容进行排序，并取前五位。
5. 打印前五个条目。

​	这个程序并不像UNIX管道那样简洁，但是它的可读性很强，所以喜欢哪一个完全是个人爱好。除了表面上的语法差异之外，执行流程也存在很大差异。如果在大文件上运行此分析，差异会变得更加明显。

#### 排序与内存中聚合

​	Ruby脚本需要一个URL的内存哈希表，其中每个URL地址都会映射到被访问的次数。UNIX流水线例子中则没有这样的哈希表，而是依赖于对URL列表进行排序，在这个URL列表中多次出现的相同URL仅仅是简单重复。

​	哪种方法更好呢？这取决于有多少个不同的网址。对于大多数中小型网站，也许可以在内存中(比如说1GB内存)存储所有不同的URL，并且可以为每个URL提供一个计数器。在此示例中，作业的工作集(作业需要随机访问的内存量)仅取决于不同URL的数量：如果单个URL有一百万条日志条目，则哈希表中所需的空间表仍然只是一个URL加上计数器的大小。如果这个工作集足够小，那么内存哈希表工作正常，甚至在笔记本计算机上都能工作。

​	另一方面，如果作业的工作集大于可用内存，则排序方法的优点是可以高效地使用磁盘。这与第3章的“SSTables和LSM-Tree”中讨论的原理相同：<u>数据块可以在内存中排序并作为段文件写入磁盘，然后多个排序的段可以合并为一个更大的排序文件</u>。<u>归并排序在磁盘上有良好的顺序访问模式(请记住，优化为顺序IO是第3章反复出现的主题，这里则再次强调)</u>。

​	<u>GNU Coreutils(Linux)中的sort实用程序通过自动唤出到磁盘的方式支持处理大于内存的数据集，且排序可以自动并行化以充分利用多核CPU。这意味着之前简单的UNIX命令链可以很容易地扩展到大数据集，而不会耗尽内存。从磁盘读取输入文件倒可能会成为瓶颈</u>。

### * 10.1.2 UNIX 设计哲学

​	我们可以非常容易地使用类似例子中命令链来分析日志文件，这并不是巧合：事实上，这是UNIX的关键设计思想之一，而且时至今日依然令人惊奇。这里，我们更深入地硏究一下，目的是从中挖掘、借鉴一些不错的想法。

​	UNX管道的发明者Doug Mcllroy在1964年首先描述了这种用例：“当需要以另一种方式处理数据时，我们应该有一些连接程序的方法，就像花园软管互相拧在一起。这就是计算机的I/O。”管道类比一直沿用了下来，通过管道将程序连接起来的想法成为如今的UNIX哲学，在开发人员和UNIX用户中逐渐变得流行的一系列设计原则。1978年，对于这种哲学有了更为完整的描述：

1. 每个程序做好一件事。如果要做新的工作，则建立一个全新的程序，而不是通过增加新“特征”使旧程序变得更加复杂。
2. 期待每个程序的输出成为另一个尚未确定的程序的输入。不要将输出与无关信息混淆在一起。避免使用严格的表格状或二进制输入格式。不要使用交互式输入。
3. 尽早尝试设计和构建软件，甚至是操作系统，最好在几周内完成。需要扔掉那些笨拙的部分时不要犹豫，并立即进行重建。
4. 优先使用工具来减轻编程任务，即使你不得不额外花费时间去构建工具，并且预期在使用完成后会将其中一些工具扔掉。

​	这种方法(自动化、快速原型设计、增量式迭代、测试友好、将大型项目分解为可管理的模块等)听起来非常像今天的敏捷开发与 DevOps运动。令人惊讶的是，四十年来并没发生太大变化。

​	sort命令是一个很好的“一个程序只做好一件事情”的例了。可以说它比大多数编程语言标准库内置的排序算法实现的更好(后者通常不会自动唤出到磁盘，且不使用多线程，即使这样做会更加利于算法)。然而，单独使用sort工具几乎用处不大。只有将其与其他UNIX工具(如uniq)结合使用时，它才会变得无比强大。

​	像bash这样的 UNIX shell可以让我们轻松地将这些小程序组合成强大的数据处理作业。尽管这些程序中是由不同人所编写的，但它们可以灵活地结合在一起。那么，UNIX是如何实现这种可组合性的呢?

#### * 统一接口

​	如果希望某个程序的输出成为另一个程序的输入，也就意味着这些程序必须使用相同的数据格式，换句话说，需要兼容的接口。<u>如果你希望能够将任何程序的输岀连接到任何程序的输入，那意味着所有程序都必须使用相同的输入/输出接口</u>。

​	<u>在UNⅠX中，这个接口就是文件(更准确地说，是**文件描述符**)，文件只是一个有序的字节序列。这是一个非常简单的接口，因此可以使用相同的接口表示许多不同的东西：文件系统上的实际文件，到另一个进程(UNIX socket， stdin， stdout)的通信通道，设备驱动程序(比如/dev/audio或/dev/lp0)，表示TCP连接的套接字等。说起来容易，但实际上对于这些差异明显的不同的事物通过共享统一的接口，使得它们能够轻松连接在一起，确实不可思议</u>。

​	按照惯例，许多(但不是全部)UNIX程序将这个字节序列视为ASCII文本。我们的日志分析示例也基于这个事实：awk，sort，uniq和head都将它们的输入文件视为由\n(换行符，ASCII 0x0A)字符分隔的记录列表。其实ASCI|记录分隔符0x1E或许是一个更好的选择，因为它本来就是为了这个目的而设计的。但是无论如何，所有这些程序都使用标准相同的记录分隔符以支持交互操作。

​	对毎条记录(即一行输入)的解析则没有明确定义。UNIX工具通常使用空格或制表符将行分割成字段，但也使用CSV(逗号分隔)、管道符分隔或其他编码字符。即使像 xargs这样相当简单的工具也有六个命令行选项，用于指定如何解析输入。

​	以 ASCII文本作为统一接口的数据格式通常工作得很好，但这种方式并不见得很漂亮：我们的日志分析示例使用{print $7}来提取网址，但可读性并不好。理想情况下，它看起来应该类似于{print$ request_url}。我们稍后会再讨论这个想法。

​	尽管经过几十年的发展依然不够完美，但UNIX统一接口的表现仍然称得上卓越。不过，并没有太多软件能像UNIX工具一样实现交互操作与组合：不能通过自定义分析工具轻松地将电子邮件内容和在线购物历史记录传送到电子表格中，并将结果发布到社交网络或维基百科上。可以说，今天能像UNIX工具一样流畅地连接多个程序绝对是一个“异类”，而并不是常态。

​	即使是具有相同数据模型的数据库，从一个数据库中导出数据然后导入到另一个数据库也并非易事。集成性的缺失导致了数据的巴尔干化(或称碎片化)。

#### 逻辑与布线分离

​	UNIX工具的另一个特点是使用标准输入(stdin)和标准输出(stdout)。如果运行一个程序而不指定任何参数，那么标准输入来自键盘，标淮输出为屏幕。当然，也可以将文件作为输入和/或将输出重定向到文件。管道允许将一个进程的stdout附加到另一个进程的stdin(具有小的内存缓冲区，而不需要将全部中间数据流写入磁盘)。

​	程序仍然可以在需要时直接读取和写入文件。但如果程序不依赖特定的文件路径，只使用 stdin和 stdout，则UNIX方法的效果最好。这允许shell用户以任何他们想要的方式连接输入和输出；程序并不知道也不关心输入来自哪里以及输出到哪里。也可以说这是一种松耦合，后期绑定或控制反转)。将输入/输出的布线连接与程序逻辑分开，可以更容易地将小工具组合成更大的系统。

​	用户甚至可以编写自己的程序，并将它们与操作系统提供的工具组合在一起。程序只需要从 stdin读取输入并输出至 stdout，从而参与数捃处理流水线。在日志分析示例中，我们可以编写一个工具，将用户代理字、串转换为更为合理的浏览器标识符，或者将IP地址转换为国家代码，并将其插入流水线中。sort程序其实并不关心它正在与操作系统进行通信，还是与用户编写的程序进行通信。

​	但是，使用stdin和stdout也有其局限性。需要多个输入或输出的程序会变得很棘手。用户不能将程序的输出传输给一个网络连接。如果一个程序直接打开文件进行读写，或者将另一个程序作为子进程启动，或者打开一个网络连接，那么这个I/O就被程序本身连接起来。虽然支持一些可配置选项(例如通过命令行)，但是减少了在shell中连接输入和输出的灵活性。

#### 透明与测试

​	UNIX工具如此成功的部分原因在于，它可以非常轻松地观察事情的进展：

+ <u>UNIX命令的输入文件通常被视为是不可变的。这意味着可以随意运行命令，尝试各种命令行选项，而不会损坏输入文件</u>。
+ 可以在任何时候结束流水线，将输出管道输送到less，然后査看它是否具有预期的形式。这种检查能力对调试非常有用。
+ 可以将流水线某个阶段的输出写入文件，并将该文件用作下一阶段的输入。这使得用户可以重新启动后面的阶段，而无需重新运行整个流水线。

​	因此，与关系数据库的查询优化器相比，尽管UNIX工具相当简单，但是非常有用，特别是对于测试而言。

​	然而，UNIX工具的最大局限在于它们只能在一台机器上运行，而这正是像 Hadoop这样的工具的工作场景。

## 10.2 MapReduce 与分布式文件系统

​	MapReduce有点像分布在数千台机器上的UNIX工具。与UNIX工具类似，这是一个相当直接、蛮力，却又有效的神奇组件。 MapReduce作业可以和UNIX进程相媲美：需要一个或多个输入，并产生一个或多个输出。

​	<u>和大多数UNIX工具一样，运行MapReduce作业通常不会修改输入</u>，除了生成输出外没有任何副作用。输出文件以序列方式一次性写入<u>(在写文件时，不会修改任何现有的文件)</u>。

​	UNIX工具使用stdin和stdout作为输入和输出，而MapReduce作业在分布式文件系统上读写文件。在Hadoop的MapReduce实现中，该文件系统被称为HDFS [Hadoop Distributed File System，一个 Google文件系统(GFS)的开源实现版本]。

​	除HDFS外，还有其他各种分布式文件系统，包括 GlusterFS和 Quantcast File System(QFS)等。诸如 Amazon S3， Azure Blob存储和OpenStack Swift2)象存储服务也有很多相似之处。在这一章我们主要以HDFS为运行实例，不过这些原则也适用于其他分布式文件系统。

​	与网络连接存储(NAS)和存储区域网络(SAN)架构的共享磁盘方法相比，HDFS基于**无共享原则**(参见第二部分的介绍)。<u>共享磁盘存储由集中式存储设备实现，通常使用定制硬件和特殊网络基础设施(如光纤通道)。而无共享方法则不需要特殊硬件，只需要通过传统数据中心网络连接的计算机</u>。

​	<u>HDFS包含一个在每台机器上运行的守护进程，并会开放一个网络服务以允许其他节点访问存储在该机器上的文件(假设数据中心的每台节点都附带一些本地磁盘)。**名为 NameNode 的中央服务器会跟踪哪个文件块存储在哪台机器上**。因此，从概念上讲，HDFS创建了一个庞大的文件系统，来充分利用每个守护进程机器上的磁盘资源</u>。

​	<u>考虑到机器和磁盘的容错，文件块被复制到多台机器上</u>。复制意味着位于多个机器上的相同数据的多个副本(参阅第5章)；或者像Reed-Solomon代码这样的纠删码方案，相比于副本技术，**纠删码**可以以更低的存储开销来恢复数据。这些技术与RAID相似，它可以在同台机器的多个磁盘级别提供数据冗余；不同的是，在分布式文件系统中，文件访问和复制是在传统的数据中心网络上完成的，而不依赖特殊的硬件。

​	HDFS具有很好的扩展性：在撰写本书时，最大的HDFS集群运行在上万台机器上，总存储容量达到了几百PB。如此大规模的部署主要得益于商用硬件和开源软件的使用，使得HDFS上的数据存储与访问成本远低于同等容量的专用存储设备。

### 10.2.1 MapReduce 作业执行

​	MapReduce是一个编程框架，可以使用它编写代码来处理HDFS等分布式文件系统中的大型数据集。最简单的理解方法是参考本章前面的“简单目志分析”中的Web日志分析示例。 MaprReduce中的数据处理模式与此非常相似：

1. 读取一组输入文件，并将其分解成记录。在Web日志示例中，每个记录都是日志中的一行(即\n是记录分隔符)。
2. 调用 mapper函数从每个输入记录中提取一个键值对。在前面的例子中， mapper函数是`awk '{print $7}'`：它提取`URL($7)`作为关键字，并将相应的值留为空。
3. 按关键宇将所有的键值对排序。在日志示例中，这由第一个sort命令完成。
4. 调用 reducer函数遍历排序后的键值对。如果同一个键出现多次，排序会使它们在列表中相邻，所以很容易组合这些值，而不必在内存中保留过多状态。在前面的例子中， reducer是由`uniq -c`命令实现的，该命令对具有相同关键字的相邻记录进行计数。

​	这四个步骤可以由一个 MapReduce作业执行。步骤2(map)和4(reduce)是用户编写自定义数据处理的代码。步骤1(将文件分解成记录)由输入格式解析器处理。步骤3中的**排序步骤sort隐含在MapReduce中，无需用户编写， mapper的输出始终会在排序之后再传递给reducer**。

​	要创建 MapReduce作业，需要实现两个回调函数，即mapper和reducer，其行为如下(另请参阅第2章的“MapReduce查询”)。

+ Mapper

  <u>每个输入记录都会调用一次 mapper程序</u>，其任务是从输入记录中提取关键字和值。对于毎个输入，它可以生成任意数量的键值对(包括空记录)。<u>它不会保留从一个输入记录到下一个记录的任何状态，因此每个记录都是独立处理的</u>。

+ Reducer

  MapReduce框架使用由 mapper生成的键值对，**收集属于<u>同一个关键字</u>的所有值，并使用迭代器调用 reducer以使用该值的集合**。 Reducer可以生成输出记录(例如相同URL出现的次数)。

​	在web日志示例中，第5步由第二个sort命令按请求数对URL进行排序。**在MapReduce中，如果需要第二个排序阶段，则可以编写另一个 Mapreduce作业并将第个作业的输出用作第二个作业的输入。这样来看的话， mapper的作用是将数据放入一个适合排序的表单中，而 reducer的作用则是处理排序好的数据**。

#### * MapReduce的分布式执行

​	MapReduce与UNIX命令管道的主要区别在于它可以跨多台机器并行执行计算，其不必编写代码来指示如何并行化。 <u>**mapper和 reducer一次只能处理一条记录**，它们不需要知道输入来自哪里或者输岀到什么地方，所以框架可以处理复杂的跨机器移动数据的情形</u>。

​	在分布式计算中可以使用标准的UNIX工具作为 mapper和 reducer，但更为常见的是用传统编程语言实现的函数。在 Hadoop MapReduce中， mapper和reducer都是实现特定接口的Java类。在 MongoDB和 CouchDB中， mapper和 Reducer是 Javascript函数(参阅第2章的“ MapReduce查询”)。

​	图10-1展示了 Hadoop MapReduce作业中的数据流。其**并行化**基于**分区**(参阅第6章)实现：<u>作业的输入通常是HDFS中的一个目录，且输入目录中的**每个文件或文件块都被视为一个单独的分区，可以由一个单独的map任务来处理**</u>(在图10-1 标记为map任务1，map任务2和map任务3)。

​	一个输入文件的大小通常是几百兆字节。<u>只要有足够的空闲内存和CPU资源，MapReduce调度器(图中未显示)会尝试在输入文件副本的某台机器上运行mapper任务。这个原理被称为**将计算靠近数据：它避免将输入文件通过网络进行复制减少了网络负载，提高了访问局部性**</u>。

![MapReduce和分布式文件系统 - 图1](https://static.sitestack.cn/projects/ddia/img/fig10-1.png)

​	<u>大多数情况下，将要在map任务中所运行的应用程序代码在分配运行任务的节点上并不存在，所以MapReduce框架首先要复制代码(例如Java程序中的JAR文件)到该节点。然后启动map任务并开始读取输入文件，每次将一条记录传递给回调函数mapper。mapper的输出由键值对组成</u>。

​	<u>Reduce任务中的计算也被分割成块</u>。**Map任务的数量由输入文件块的数量决定，而reduce任务的数量则是由作业的作者来配置的(可以不同于map任务的数量)**。**<u>为了确保具有相同关键字的所有键值对都在相同的reducer任务中处理，框架使用关键字的哈希值来确定哪个reduce任务接收特定的键值对</u>**(请参阅第6章“基于关键字哈希值分区”)。

​	**键值对必须进行排序**。如果数据集太大，可能无法在单台机器上使用常规排序算法。事实上，**排序是分阶段进行的。首先，每个map任务都基于关键字哈希值，按照reducer对输出进行分块**。<u>每一个分区都被写入 mapper程序所在本地磁盘上的已排序文件</u>，使用的技术类似我们在第3章讨论的“SSTables和LSM-Trees”。

​	**当mapper完成读取输入文件并写入经过排序的输出文件， MapReduce调度器就会通知reducer开始从 mapper中获取输出文件**。 reducer与每个 mapper相连接，并按照其分区从 mapper下载<u>排序后的</u>键值对文件。**按照reducer分区，排序和将数据分区从 mapper复制到reducer，这样一个过程被称为shuffle**(这是一个容易令人困惑的术语，它并不完全与洗牌一样，<u>在MapReduce中其实没有随机性</u>)。

​	reduce任务从 mapper中获取文件并将它们合并在一起，同时保持数据的排序。因此，<u>如果不同的 mapper使用相同的关键字生成记录，则这些记录会在合并后的 reducer输入中位于相邻位置</u>。

​	reducer通过关键字和迭代器进行调用，而迭代器逐步扫描所有具有相同关键字的记录(某些情况下可能无法完全在内存中完成)。 reducer可以使用任意逻辑来处理这些记录，并且生成任意数量的输出记录。这些输出记录被写入分布式文件系统中的文件(通常是在运行 reducer机器的本地磁盘中的一个拷贝，在其他机器上存在副本)。

#### * MapReduce工作流

​	单个MapReduce作业可以解决的问题范围有限。回顾一下日志分析的例子，一个MapReduce作业可以确定每个URL页面的浏览次数，但不是最受欢迎的那些URL，因为这需要第二轮排序。

​	因此，将 Mapreduce作业链接到工作流中是非常普遍的，这样，<u>作业的输出将成为下个作业的输入</u>。 Hadoop mapReduce框架对工作流并没有任何特殊的支持，所以链接方式是通过目录名隐式完成的：第一个作业必须配置为将其输出写入HDFS中的指定目录，而第二个作业必须配置为读取相同的目录名作为输入。从 MapReduce框架的角度来看，它们仍然是两个独立的作业。

​	因此，<u>链接方式的MapReduce作业并不像UNIX命令流水线(它直接将一个进程的输出作为输入传递至另一个进程，只需要很小的内存缓冲区)，而更像是一系列命令，其中每个命令的输出被写入临时文件，下一个命令从临时文件中读取</u>。这种设计有利有弊，我们将在本章后面的“中间状态实体化”中讨论。	

​	**只有当作业成功完成时，批处理作业的输出才会被视为有效(MapReduce会丢弃失败作业的部分输出)**。<u>因此，工作流中的一个作业只有在先前的作业(即生成其输入目录的作业)成功完成时才能开始</u>。为了处理这些作业执行之间的依赖关系，已经开发了各种Hadoop的工作流调度器，包括Oozie， Azkaban，Luigi，Airflow和Pinball。

​	这些调度程序还具有管理功能，在维护大量批处理作业时非常有用。在构建推荐系统时，由50到100 MapReduce作业组成的工作流是非常常见的。而在大型组织中，许多不同的团队可能运行不同的作业来读取彼此的输出。良好的工具支持对于管理如此复杂的数据流非常重要。

​	Hadoop的各种高级工具(如Pig，Hive， Cascading， Crunch和FlumeJava)则支持设置多个MapReduce阶段的工作流，这些不同的阶段会被恰当地自动链接起来。

### 10.2.2 Reduce端的join与分组

​	我们在第2章中讨论了数据模型和查询语言的联结操作，但是我们还没有深入探讨join是如何实现的。现在我们将再次开始这个话题。

​	在许多数据集中，通常一条记录会与另一条记录存在关联，例如：关系模型中的外键，文档模型中的文档引用或图模型中的边。只要有代码需要访问该关联两边的记录(包含引用的记录和被引用的记录)，那么就需要join操作。正如第2章所讨论的那样，**反规范化可以减少对join的需求，但通常无法完全避免join**。

​	在数据库中，如果执行的杳询只涉及少量记录，那么数据库通常会使用索引来加速查找(请参阅第3章)。如果査询涉及到join臊作，则可能需要对多个索引进行査找。然而， MapReduce没有索引的概念，至少不是通常意义上的索引。

​	当给定一组文件作为MapReduce输入时，它读取所有文件的全部内容；数据库将其称作全表扫描。如果只想读取少量记录，则与索引查找相比，全表扫描的成本非常昂贵。但是，分析査询(参阅第3章“事务处理与分析处理”)通常需要计算大量记录的聚合。在这种情况下，扫描整个数据集是比较合理的，特别是如果可以在多台机器上并行处理。

​	在批处理的背景下讨论join时，我们主要是解决数据集内存在关联的所有事件。例如，假设一个作业是同时为所有用户处理数据，而不仅仅是为一个特定的用户查找数捃(这可以通过索引更高效地完成)。

#### 示例：分析用户活动事件

​	图10-2 给出了批处理作业中典型的join示例。图中左侧是事件日志，描述登录用户在网站上的活动(称为活动事件或单击流数据)，右侧是用户数据库。可以将此示例视为一种星型模式的一部分(请参阅第3章的“星型与雪花型分析模式”)：<u>事件日志是事实数据表，用户数据库是维度之一</u>。

![MapReduce和分布式文件系统 - 图2](https://static.sitestack.cn/projects/ddia/img/fig10-2.png)

​	分析任务可能需要将用户活动与用户描述信息相关联：例如，如果描述中包含用户年龄或出生日期，则系统可以确定哪些年龄组最受欢迎。但是，活动事件中仅包含用户标识，而不包含完整的用户描述信息。而在每个活动事件中嵌入这些描述信息又会太浪费。因此，活动事件需要与用户描述数据库进行join。

​	<u>join的最简单实现是逐个遍历活动事件，并在(远程服务器上的)用户数据库中查询每个遇到的用户ID。该方案首先是可行的，但性能会非常差：吞吐量将受到数据库服务器的往返时间的限制，本地缓存的有效性将很大程度上取决于数据的分布，并且同时运行的大量并行查询很容易使数据库不堪重负</u>。

​	**为了在批处理过程中实现良好的吞吐量，计算必须(尽可能)在一台机器上进行。如果通过网络对每条记录进行随机访问则请求太慢。而且，考虑到远程数据库中的数据可能会发生变化，查询远程数据库意味着会增加批处理作业的不确定性**。

​	因此，**更好的方法是获取用户数据库的副本(例如，使用ETL进程从数据库备份中提取数据，请参阅第3章的“数据仓库”)，并将其放入与用户活动事件日志相同的分布式文件系统。然后，可以将用户数据库放在HDFS中的一组文件中，并将用户活动记录放在另一组文件中，使用 MapReduce将所有相关记录集中到一起，从而有效地处理它们**。

#### * 排序-合并join

​	回想一下， mapper的目的是从每个输入记录中提取关键字和值。在图10-2的情况下这个关键字就是用户ID：一组mapper会扫描活动事件(提取用户ID作为关键字，而活动事件作为值)，而另一组 mapper将会遍历用户数据库(提取用户ID作为关键字，用户出生日期作为值)。过程如图10-3所示。

![MapReduce和分布式文件系统 - 图3](https://static.sitestack.cn/projects/ddia/img/fig10-3.png)

​	当MapReduce框架通过关键字对 mapper输出进行分区，然后对键值对进行排序时，结果是所有活动事件和用户ID相同的用户记录在reducer的输入中彼此相邻。 MapReduce作业甚至可以对记录进行排序，以便reducer会首先看到用户数据库中的记录，然后按时间戳顺序查看活动事件，这种技术称为**次级排序**。

​	<u>然后reducer可以很容易地执行真正的join逻辑：为每个用户ID调用一次 reducer函数</u>。由于次级排序，第一个值应该是来自用户数据库的出生日期记录。 Reducer将出生日期存储在局部变量中，然后使用相同的用户ⅠD遍历活动事件，输出相应的已观看网址和观看者年龄。随后的MapReduce作业可以计算每个URL的查看者年龄分布，并按年龄组进行聚类。

​	<u>由于 reducer每次处理一个特定用户ID的所有记录，因此只需要将用户记录在内存中保存一次，而不需要通过网络发出任何请求。这个算法被称为排序-合并join，因为mapper的输出是按关键字排序的，然后 reducer将来自join两侧的已排序记录列表合并在一起</u>。

#### 将相关数据放在一起

​	**在排序合并join中， mapper和排序过程确保将执行特定用户ID join操作的所有必要数据都放在一起，这样就只需要一次reducer调用**。<u>因为所有需要的数据已经预先排列好，所以 reducer是一段相当简单的单线程代码，以**高吞吐量**和**低内存开销**来处理记录</u>。

​	一种理解这种架构的方法是mapper发送“消息”给 reducer。当mapper发出一个键值对时，关键字就像传递值的目标地址一样。即使关键字只是一个任意字符串(不是那种像IP地址和端口一样的实际网络地址)，但是它的行为就像一个地址：所有具有相同关键字的键值对都将被传送到相同的目的地(即对reducer的调用)。

​	<u>**使用MapReduce编程模型将计算中的物理网络通信部分(从正确的机器获取数据)从应用逻辑(处理数据)中分离出来**</u>。这种分离与数据库的典型使用方式形成了鲜明对比：从数据库中获取数据的请求经常发生在应用程序代码的深处。<u>由于 MapReduce能够处理所有的网络通信，因此它也避免了在应用程序代码中处理局部故障，例如**某个节点的崩溃: MapReduce会在不影响应用程序逻辑的情况下透明地重试失败的任务**</u>。

#### 分组

​	**除了join之外，“将相关数据放在一起”模式的另一个常见用法是通过某个关键字(如SQL中的 GROUP BY子句)对记录进行分组。所有具有相同关键字的记录形成一个组，然后在每个组内执行某种聚合操作**，例如：

+ 计算每个组中记录的数量[例如，在统计页面视图的示例中，SQL将其表示为`COUNT(*)`聚合]。
+ 对SQL中的特定字段进行求和[`SUM(fieldname)`]。
+ 根据排名函数选择前k个记录。

​	<u>使用MapReduce实现这种分组操作的最简单方法是设置 mapper，使其生成的键值对使用所需的分组关键字</u>。然后，分区和排序过程将相同reducer中所有具有相同关键字的记录集合在一起。因此，在 MapReduce上实现的分组和join看起来非常相似。

​	<u>分组的另一个常见用途是收集特定用户会话的所有活动事件，以便发现用户的活动序列，称为**会话流程**</u>。例如，可以使用这种分析来确定选择网站新版本的用户是否比选择旧版本(A/B测试)的用户更有可能产生购买行为，或计算某个营销活动是否有效。

​	如果有多个Web服务器处理用户请求，则特定用户的活动事件很可能分散在各个不同服务器的日志文件中。可以通过使用会话 cookie、用户ID或类似的标识符作为分组关键字来实现访问流程，将特定用户的所有活动事件放在一起，同时将不同用户的事件分配到不同的分区。

#### * 处理数据倾斜

​	如果与单个关键字相关的数据量非常大，那么会破坏掉“将所有具有相同关键字的记录放在一起”的模式。例如，在社交网络中，大多数用户会有上百人关注者，但少数名人则可能有数百万的追随者。这种不成比例的活跃数据库记录被称为**关键对象**或**热键**。

​	在单个reducer中收集与名人相关的所有活动(例如对他们发布内容的回复)可能会导致严重的**数据倾斜**(也称为热点)。也就是说，某个reducer必须处理比其他 reducer更多的记录(请参阅第6章的“负载倾斜与热点”)。**由于 MapReduce作业只有在其所有mapper和reducer都完成时才能完成，因此<u>所有后续作业必须等待最慢的reducer完成之后才能开始</u>**。

​	<u>如果join输入中存在热键，则可以使用算法进行补偿。例如，Pig中的倾斜join方法首先运行一个抽样作业来确定哪些属于热键。**在真正开始执行join时， mapper将任何与热键有关的记录发送到随机选择的若干个reducer中的一个(传统 MapReduce基于关键字哈希来确定性地选择 reducer)**。对于join的其他输入，与热键相关的记录需要被复制到所有处理该关键字的reducer中</u>。

​	**这种技术将处理热键的工作分散到多个reducer上，可以更好地实现并行处理，代价是不得不将join的其他输入复制到多个reducer**。Crunch中的共亨join方法与此类似，但需要明确指定热键，而不是使用抽样作业。<u>这种技术也非常类似我们在第6章“负载倾斜与热点”所讨论的技术，**使用随机化来缓解分区数据库中的热点**</u>。

​	<u>Hive的倾斜join优化采取了另一种方法。它需要在表格元数据中明确指定热键，并将与这些键相关的记录与其余文件分开存放。在该表上执行join时，它将对热键使用map端join(请参阅下一节)</u>。

​	<u>使用热键对记录进行分组并汇总时，可以分两个阶段进行分组。第一个MapReduce阶段将记录随机发送到 reducer，以便毎个 reducer对热键的记录子集执行分组，并为毎个键输出更紧凑的聚合值。然后第二个 MapReduce作业将来自所有第一阶段reducer的值合并为每个键的单一值</u>。

### * 10.2.3 map端join操作

​	上一节描述的join算法在 reducer中执行实际的join逻辑，因此被称为**reducer端join**。mapper负责准备输入数据：从毎个输入记录中提取关键字和值，将键值对分配给reducer分区，并按关键字排序。

​	**Reduce端join方法的优点是不需要对输入数据做任何假设：无论其属性与结构如何，mapper都可以将数据处理好以谁备join**。然而，<u>**不利的一面是，所有这些排序，复制到reducer以及合并reducer输入可能会是非常昂贵的操作，这取决于可用的内存缓冲区，当数据通过MapReduce阶段时，数据可能需要写入磁盘若干次**</u>。

​	另一方面，如果可以对输入数据进行某些假设，则可以通过使用所谓的**map端join**来加快速度。这种方法使用了一个<u>缩减版本的 MapReduce作业，其中没有 reducer，也没有排序；相反，每个 mapper只需从分布式文件系统中读取输入文件块，然后将输出文件写入文件系统即可</u>。

#### * 广播哈希join

​	<u>实现map端join的最简单方法特别适合**大数据集与小数据集join**，尤其是小数据集能够全部加载到每个 mapper的内存中</u>。

​	例如，假设对于图10-2情况，用户数据库可以完全放入内存。在这种情况下，当mapper程序执行时，它可以首先将用户数据库从分布式文件系统读取到内存的哈希表中。然后， mapper程序扫描用户活动事件，并简单地查找哈希表中每个事件的用户ID。

​	<u>Map任务依然可以有多个：大数据集的每个文件块对应一个mapper(在图10.2的例子中，活动事件是大输入数据集)。**每个mapper还负责将小数据集全部加载到内存中**</u>。

​	<u>这种简单而有效的算法被称为**广播哈希join**：“**广播**”一词主要是指大数据集每个分区的 mapper还读取整个小数据集(即小数据集实际被“广播”给大数据集)，“**哈希**”意味着使用哈希表</u>。有多种系统都支持该方法，包括Pig(称为 replicated join)，Hive(称为 Map Join)， Cascading和Crunch等。它也用于数据仓库查询引擎，例如Impala。

​	<u>另一种方法并不需要将小数据集加载至内存哈希表中，而是将其保存在本地磁盘上的只读索引中。由于频繁访问，索引大部分内容其实是驻留在操作系统的页面缓存中，因此这种方法可以提供与内存哈希表几乎一样快的随机访问性能，而实际上并不要求整个数据集读入内存</u>。

#### * 分区哈希join

​	**如果以相同方式对map端join的输入进行分区，则哈希join方法可以独立作用于毎个分区**。在图10-2的情况下，可以根据用户ID的最后一位十进制数字(因此每一边都有10个分区)来分配活动事件和用户数据库中的记录。例如， Mapper 3首先将所有以3结尾的ID的用户加载到哈希表中，然后扫描ID以3结尾的每个用户的所有活动事件。

​	**如果分区操作正确完成，就可以确定所有要join的记录都位于相同编号的分区中，因此毎个mapper只需从毎个输入数据集中读取一个分区就足够了。这样的优点是每个mapper都可以将较少的数据加载到其哈希表中**。

​	**<u>这种方法只适用于两个join的输入具有相同数量的分区，根据相同的关键字和相同的哈希函数将记录分配至分区</u>**。如果输入是由之前已经执行过这个分组的MapReduce作业生成的，那么这是一个合理的假设。

​	分区哈希join在Hive中称为**bucketed map join**。

#### * map端合并join

​	如果输入数据集不仅以相同的方式进行分区，而且还基于相同的关键字进行了排序，则可以应用map端join的另一种变体。这时，输入是否足够小以载入内存并不重要，因为 **mapper可以执行通常由Reducer执行的合并操作：按关键宇升序增量读取两个输入文件，并且匹配具有相同关键字的记录**。

​	如果map端合并join是可能的，则意味着先前的 MapReduce作业会首先将输入数据集引入到这个经过分区和排序的表单中。<u>原则上，join可以在之前作业的 reduce阶段进行。但是，在独立的map作业中执行合并join更为合适，例如，除了特定的join操作之外，分区和排序后的数据集还可用于其他目的</u>。

#### * 具有map端join的MapReduce工作流

​	当下游作业使用 MapReduce join的输出时，map端或 reduce端 join的不同选择会影响到输出结构。 **reduce端join的输岀按join关键宇进行分区和排序，而map端join的输岀按照与大数据集相同的方式进行分区和排序**(因为对大数据集的每个文件块都会启动个map任务，无论是使用分区join还是广播join)。

​	正如所讨论的，map端join也存在对输入数据集的大小、排序和分区方面的假设。在优化join策略时，了解分布式文件系统中数据集的物理布局非常重要：仅仅知道编码格式和数据存储目录的名称是不够的；还必须知道数据分区数量，以及分区和排序的关键字。

​	**在Hadoop生态系统中，关于数据集分区的元数据经常在HCatalog和Hive metastore中维护**。

### 10.2.4 批处理工作流的输出

​	我们已经讨论了很多关于实现 MapReduce工作流的算法，但仍然忽略了一个重要问题：一旦作业完成，处理的最终结果是什么？为什么要把这些作业放在首位？

​	数据库査询中，根据分析目的我们区分了事务处理(OLTP)和分析型处理(参阅第3章“事务处理与分析处理”)。我们看到OLTP查询通常使用索引按关键字查找少量记录，然后将査询结果呈现给用户(例如在网页上)。另一方面，分析查询通常会扫描大量记录，执行分组与聚合，输出完整的报告，例如：显示度量随时间变化的图表，或某种排名中的前10项，或将某一数量分解成若干子类别。这种报告的消费者通常是需要做出商业决策的分析师或经理。

​	批处理即不是事务处理，也不是分析，那么，把它放在哪里更合适？批处理与分析更为接近，因为批处理过程通常会扫描大部分的输入数据集。但是， MapReduce作业的工作流与分析中SQL查询不同(请参阅本章后面的“对比 Hadoop与分布式数据库”)。批处理过程的输出通常不是报告，而是其他类型的数据结构。

#### * 生成搜索索引

​	<u>Google最初使用MapReduce的目的是为其搜索引擎建立索引，这个索引被实现为5到10个MapReduce作业的工作流</u>。尽管 Google后来不再使用 MapReduce，但是如果从构建搜索索引的角度来看 MapReduce，会更加有助于理解 MapReduce(<u>即使在今天， Hadoop MapReduce仍然是构建Lucene/Solr索引的好方法</u>)。

​	在第3章的“全文搜索与模糊索引”中简要讨论了像 Lucene这样的全文搜索索引是如何工作的：它是一个文件(术语字典)，可以在其中有效地查找特定关键字，并找到包含该关键字的所有文档ID列表(发布列表)。这是一个非常简单的搜索索引视图，实际上它还需要各种附加数据，以便按相关性对检索结果进行排序、拼写检査、解析同义词等等，但基本原则类似。

​	<u>如果需要对一组固定文档进行全文检索，则批处理是构建索引的有效方法：mapper根据需要对文档集进行分区，每个 reducer构建其分区索引，并将索引文件写入分布式文件系统。并行处理非常适用于构建这样的文档分区索引</u>(请参阅第5章的“分区与级索引”)。

​	**由于按关键字査询搜索索引是只读操作，因此这些索引文件一旦创建就是不可变的**。<u>如果索引的文档集合发生更改，则可以选择定期重新运行整个索引工作流，并在完成后用新的索引文件批量替换之前的索引文件。如果只有少量文档发生了变化，这种方法在计算上可能比较昂贵，但是它的优点是索引过程非常清晰合理：文档作为输入，索引作为输出</u>。

​	<u>增量建立索引是一种替代方法。如第3章所述，如果要添加、删除或更新索引中的文档， Lucene会生成新的段文件，并在后台异步合并和压缩段文件</u>。我们将在第11章中看到更多这样的增量处理。

#### * 批处理输出键值

​	搜索索引只是批处理工作流输出的一个示例。批处理的另一个常见用途是**构建机器学习系统**，如**分类器**(例如垃圾邮件过滤器，异常检测，图像识别)和**推荐系统**(例如你可能认识的人，你可能感兴趣的产品或相关搜索)。

​	这些批量作业的输出通常是某种数据库：例如，在用户数据库中通过用户ID进行查询以获取建议的好友，或者在产品数据库中通过产品ID查询以获取相关产品列表。

​	查询数据库需要在处理用户请求的Web应用中进行，而这些请求通常与 Hadoop基础架构是分离的。那么批处理过程的输出如何返回至数据库中以供Web应用杳询？

​	最明显的选择可能是直接在 mapper或 reducer中使用你最喜欢的数据库客户端软件包，而批处理作业则直接写入至数据库服务器，**一次写入一条记录**。这样的方法可行(假设防火墙规则允许从 Hadoop环境直接访问生产数据库)，但由于以下原因，这并不是一个好方案：

+ **正如前面讨论join时提到的，为每个记录发送一个网络请求比批处理任务的正常吞吐量要慢几个数量级。即使客户端软件包支持批处理，性能也可能很差**。
+ **MapReduce作业经常并行处理许多任务。如果所有的 mapper或 reducer都同时写入同一个输出数椐库，并以批处理期望的速率写入，那么数据库很容易过载，其查询性能会受到影响。这可能会进一步导致系统其他部分的操作问题**。
+ <u>**通常情况下， MapReduce为作业输出提供了一个干净的“全有或全无”的保证**：如果作业成功，则结果就是只运行一次任务的输出，即使中间发生了某些任务失败但最终重试成功。如果整个作业失败，则不会产生输出</u>。**然而，从作业内部写入外部系统会产生外部可见的副作用，而这种副作用无法彻底屏蔽。因此，<u>将不得不担心部分完成的作业产生对其他系统可见的结果</u>，以及 Hadoop任务尝试和推测性执行的复杂性**。

​	**更好的解决方案是，批处理作业创建一个全新的数据库，并将其作为文件写入分布式文件系统中的作业输出目录，就像上一节的搜索索引一样**。这些数据文件一旦写入就是不可变的，可以批量加载到处理只读查询的服务器中。各种键值存储都支持构建MapReduce作业中的数据库文件，包括Voldemort， Terrapin， ElephantDB和HBase批量加载。

​	<u>构建数据库文件是很好的MapReduce使用方法：使用 mapper提取关键字，然后使用该关键宇进行排序，这已经成为**构建索引**的必要步骤</u>。由于大多数键值存储是只读的(文件只能由批处理作业一次写入，而且是不可变的)，所以数据结构非常简单，例如，它们不需要预写日志(请参阅第3章的“可靠的B-tree”)。

​	将数据加载到Voldemort时，服务器将继续向旧数据文件发起请求，同时将新数据文件从分布式文件系统复制到服务器的本地磁盘。一旦复制完成，服务器会自动切换到新文件进行查询。如果在这个过程中出现任何问题，它可以很容易地再次切换回旧文件，因为这些旧文件依然存在，而且<u>不可变</u>。

#### * 批处理输出的哲学

​	本章前面讨论过的UNIX设计晢学倡导明确的数据流：一个程序读取输入并写回输出。在这个过程中，输入保持不变，任何以前的输出都被新输出完全替换，并且没有其他副作用。这意味着可以随心所欲地重新运行一个命令，进行调整或调试，而不会扰乱系统状态。

​	MapReduce作业的输出处理遵循相同的原理。将输入视为不可变，避免副作用(例如对外部数据库的写入)，批处理作业不仅实现了良好的性能，而且更容易维护：

+ **如果在代码中引入了漏洞，输出错误或者损坏，那么可以简单地回滚到先前版本，然后重新运行该作业，将再次生成正确的输出；或者更简单的办法是将旧的输出保存在不同的目录中，然后切换回原来的目录**。相比之下，具有读写事务的数据库就不具有这样的属性：如果部署了将错误数据写入数据库的错误代码，回滚代码并不能修复数据库中的数据。能够从错误代码中恢复的思想被称为**人为容错性**。
+  与发生错误即意味着不可挽回的损害相比，易于回滚的特性更有利于快速开发新功能。这种使不可逆性最小化的原则对于敏捷开发是有益的。
+ 如果map或reduce任务失败， MapReduce框架会自动重新安排作业并在同一个输入上再次运行。如果失败是由于代码漏洞造成的，那么它会一直崩溃，最终导致作业在数次尝试之后失败。但是如果故障是由于暂时问题引起的，则可以实现容错。**由于输入总是不可变，这种自动重试是安全的，而失败任务的输出则被MapReduce框架丢弃**。
+ **相同的文件可用作各种不同作业的输入**，其中包括监控作业，它可以搜集相关运行指标，并评估其他作业的输出是否满足预期特性(例如，将其与前一次运行的输出进行比较并测量差异)。
+ **与UNIX工具类似， MapReduce作业将逻辑与连线(配置输入和输出目录)分开，从而可以更好地隔离问题**，重用代码：一个团队可以专注于实现“做好一件事”的工作，而其他团队可以决定何时何地运行该作业。

​	在这些方面，那些UNIX验证过的设计原则似乎用在 Hadoop上也同样效果很好，但UNIX和 Hadoop在某些方面存在不同。例如，因为大多数UNIX工具都假定输入为没有类型的文本文件，所以必须做大量的输入解析工作(本章开头的日志分析示例使用`{print $7}`来提取URL)。在 Hadoop中，通过使用更多结构化的文件格式，可以消除一些低价值的语法转换：Avro(请参阅第4章的“Avro”)和 Parquet(请参阅第3章的“列式存储”)通常用于提供高效的基于模式的编码，并支持模式的不断演变(参见第4章)。

### 10.2.5 对比Hadoop与分布式数据库

​	正如我们所看到的， Hadoop有点像UNIX的分布式版本，其中HDFS是文件系统，而MapReduce则是UNIX进程的特殊实现(<u>它总是在map阶段和reduce阶段之间运行sort工具</u>)。我们讨论了如何在这些原语之上实现各种join和分组操作。

​	MapReduce论文发表时，在某种意义上来说它并不新鲜。我们在前几节讨论的所有处理和并行join算法都已经在十多年前所谓的大规模并行处理(MPP)数据库中实现了。 Gamma数据库机器， Teradata和 Tandem NonStop SQL是这方面的先驱。

​	**这其中最大的区别在于：**

+ **MPP数据库专注于在一个机器集群上并行执行SQL查询分析，**
+ **而 MapReduce和分布式文件系统的结合则更像是一个可以运行任意程序的通用操作系统**。

#### * 存储多样性

​	**数据库要求根据特定的模型(例如，关系或文档)来构造数据，而分布式文件系统中的文件只是字节序列，可以使用任何数据模型和编码来编写**。它们可能是数据库记录的集合，也同样可以是文本、图像、视频、传感器读数、稀疏矩阵、特征向量、基因组序列或任何其他类型的数据。

​	<u>更直接地讲， Hadoop开放了将数据不加区分地转存到HDFS的可能性，之后再去考虑如何进一步处理数据。相比之下，在将数据导入数据库的专有存储格式之前，MPP数据库通常需要对欻椐和査询模式进行仔细的前期建模</u>。

​	从纯粹主义者的角度来看，这种仔细的建模和导入方式似乎是可取的，因为这意味着数据库用户拥有质量更好的数据。**<u>然而在实践中，看起来只是简单地使数据可用；而即使是一个古怪的，难以使用的原始格式往往也比预先决定理想的数据模型更有价值</u>**。

​	这个想法与数据仓库类似(请参阅第3章的“数据仓库”)：只将大型组织中各个部分的数据集中在一起是非常有价值的，因为它可以在之前完全分离的数据集执行join操作。<u>MPP数据库所秉持的谨慎设计模式减慢了集中式数据的收集速度</u>；**因此，仅仅以原始形式收集数据，之后再考虑模式设计，从而使收集数据的速度加快(**这种形式有时也被称为“**数据湖**”或“企业数据中心”)。

​	<u>不加区分地数据转储也转移了**数据解释**的负担：不是强迫数据集的**生产者**将其转化为标准化格式，而是将解释数据变为**消费者**的问题(**读时模式**，请参阅第2章的“文档模型中的模式灵活性”)</u>。如果生产者和消费者分属不同优先级的团队，这会是个优势。甚至也可能并不存在一个理想的数据模型，而是针对不同的目的产生对数据不同的看法。<u>以原始形式简单地转储数据可以进行多次这样的转换。这种方法被称为**寿司原则**</u>。

​	因此， <u>**Hadoop经常被用于实现ETL过程**(参见第3章“数据仓库”)：来自事务处理系统的数据以某种原始形式转储到分布式文件系统中，然后编写MapReduce作业进行数据清理，将其转换为关系表单，并将其导入MPP数据仓库以进行分析</u>。**数据建模仍然会发生，但它位于一个单独步骤中，与数据收集是分离的。由于分布式文件系统支詩以任何格式编码的数据，所以这种解耦是可行的**。

#### 处理模型的多样性

​	<u>MPP数据库属于一体化、紧密集成的软件系统，包括磁盘存储布局、查询计划、调度和执行</u>。由于这些组件都可以针对数据库的特定需求进行调整和优化，因此整个系统可以在其设计的査询类型上实现非常好的性能。此外，SQL查询语言支持表达式查询，以及优雅的语义，而无需编写代码，因此，业务分析人员使用的图形化工具(如Tableau)即可执行查询。

​	另一方面，并非所有类型的处理都可以合理地表达为SQL查询。例如，如果正在构建机器学习和推荐系统，或具有相关性排名模型的全文搜索索引，或执行图像分析，则很可能需要更具一般性的数据处理模型。这些类型的处理通常特定于专门的应用程序(例如机器学习的特征工程，机器翻译的自然语言模型，欺诈预测的风险评估功能)，所以不可避免地需要编写代码，而不仅仅是查询。

​	MapReduce使工程师能够轻松地在大型数据集上运行自己的代码。<u>如果你有HDFS和MapReduce，可以在它上面建立一个SQL查询执行引擎，事实上这就是Hive项目所做的事情</u>。也可以编写许多其他形式的不适合用SQL查询表示的批处理。

​	随后，人们发现 MapReduce对于某些类型的处理局限性太大，而且执行得太差，因此在 Hadoop之上开发了各种其他的处理模型(我们将在本章后面的“超越 MapReduce中展开讨论)。仅有SQL和 MapReduce两种处理模型还不够，我们需要更多不同的模型！而且由于 Hadoop平台的开放性，实施一整套处理方法也是可行的，但对于一体化的MPP数据库来说则力所不及。

​	<u>至关重要的是，这些不同的处理模型都可以在一个共享的机器集群中运行，所有机器都可以访问分布式文件系统上的相同文件。在 Hadoop方法中，不需要将数据导入到几个不同的专用系统中进行不同类型的处理：系统已经足够灵活，可以支持同一集群中不同的工作负载。无需移动数据使得从数据中获取价值变得容易得多，而且更容易使用新的处理模型进行测试</u>。

​	Hadoop生态系统包括可以随机访问的OLTP数据库，如HBase(请参阅第3章的“ SSTables和 LSM-tree”)以及MPP模式的分析数据库，如Impala。 HBase和Impala都不使用MapReduce，但都使用HDFS进行存储。尽管它们访问和处理数据的方法差异很大，但是可以共存并被集成到同一个系统中。

#### * 针对频繁故障的设计

​	在比较MapReduce和MPP数据库时，设计方法的另外两个不同点是：如何处理故障和如何使用内存和磁盘。<u>与在线系统相比，批处理对故障的敏感度较低，因为如果遇到失败的任务，它们不会立即影响用户，而且总是可以重新运行</u>。

​	<u>如果一个节点在执行查询时崩溃，大多数MPP数据库会中止整个查询，并让用户重新提交査询或自动重新运行査询</u>。由于查询通常最多运行几秒钟或几分钟，所以这种处理错误的方式是可以接受的，因为重试的代价不是太大。<u>MPP数据库还倾向于在内存中保留尽可能多的数据(例如使用**哈希join**)以避免磁盘读取的成本</u>。

​	另一方面， **MapReduce可以容忍map或reduce任务的失败，通过以单个任务的粒度重试工作来避免影响整体作业。它也会期望将数据写入磁盘，一方面是为了容错，另一方面是假设数据集太大而不能载入内存**。

​	**MapReduce方法更适用于较大的作业**：处理大量的数据并运行很长时间的作业，以至于在此过程中可能至少会遇到一次任务故障。在这种情况下，<u>由于单个任务失败而重新运行整个工作将是巨大的浪费。虽然以单个任务粒度进行恢复带来的开销会使无故障处理变得更慢，但如果任务失败率足够高，这仍然是一种合理的权衡方式</u>。

​	但是这些假设的现实性究竟如何呢？在大多数集群中，机器故障确实在发生，但并不是很频繁，可能很少发生，大多数作业都不会遇到机器故障。为了容错，真的值得引入这么大的开销吗?

​	要了解MapReduce节省使用内存和任务级恢复的背后原因，查看最初设计MapReduce的环境会很有帮助。 Google拥有混合使用的数据中心，在线生产服务和离线批处理作业在同一台机器上运行。<u>任务通过使用容器来实现资源分配(CPU，内有，磁盘空间等)。毎个任务都具有优先级，如果优先级较高的任务需要更多的资源，则可以终止(抢占)同一机器上较低优先级的任务以释放资源。优先级还决定了计算资源的定价：团队必须为他们使用的资源付费，而优先级更高的任务花费也会更多</u>。

​	这种架构允许非生产(低优先级)计算资源被过度使用，因为系统知道可以在必要时回收资源。与那些隔离生产和非生产任务的系统相比，过度使用资源反过来可以更好地利用机器和提高效率。<u>但是，由于MapReduce作业以低优先级运行，当优先级较高的进程需要其资源时，随时都可以发生抢占。**批处理任何工作可以有效地利用高优先级进程剩下的任何可用计算资源**</u>。

​	在Google，运行一个小时的MapReduce任务大约有5%被终止的风险，<u>为更高优先级的进程腾出空间</u>。这个比例要比由于硬件问题、机器重启或其他原因导致的故障率高出一个数量级。以这样的抢占率，如果一个作业有100个任务，每个任务运行10分钟，那么至少有一个任务在完成之前将被终止的风险大于50%。

​	**<u>这就是为什么 MapReduce被设计为容忍意外任务终止的原因：不是因为硬件特别不可靠，而是因为任意终止进程的灵活性能够更好地利用集群资源</u>**。

​	而**在开源集群调度器中，抢占的使用情况则相对较少**。YARN的Capacity Scheduler支持抢占以平衡不同队列的资源分配。但在编写本书时，YARN， Mesos或Kubernetes不支持通用优先级抢占。**在任务不经常被终止的环境中， MapReduce的设计决策没有多少意义**。在下一节中，我们将看看 MapReduce的一些替代方法，这些替代方案做出了不同的设计决定。

## 10.3 超越MapReduce

​	尽管MapReduce在20世纪末变得非常流行并被大量炒作，但它只是分布式系统的许多可能的编程模型之一。取决于具体的数据量、数据结构以及处理类型，其他工具可能更适合特定的计算。

​	尽管如此，我们仍然在讨论MapReduce上花了很多篇幅，因为它是一个有用的学习工具，因为它是分布式文件系统的一个相当清晰和简单的抽象。在这里，简单是指从理解它在做什么的角度来说，而不是容易使用的角度。事实上与容易使用恰恰相反：使用原始的 MapReduce API来实现一个复杂的处理任务是相当困难和费力的，例如，你需要从头开始实现全部join算法。

​	针对直接使用的困难，在 MapReduce上创建了各种高级编程模型(Pig，Hive，Cascading， Crunch)进一步封装抽象。如果了解 MapReduce的工作原理，那么学习这些模型也相当容易，而且它们的高级构造使许多常见的批处理任务更加容易实现。

​	然而，MapReduce执行模型本身也存在一些问题，这些问题并没有通过增加另一个抽象层次而得到解决，而且在某些类型的处理中表现出糟糕的性能。一方面，MapReduce非常强大：可以使用它来处理那些运行在不可靠的多租户系统上、任务频繁终止的超大规模数据，并且仍然可以完成工作(虽然速度很慢)。另一方面，对于某些类型的处理，其他工具则可能要快几个数量级。

​	在本章的剩余部分中，我们将看到一些批处理的替代方案。第11章将转向流处理，这可以看作是加速批处理的另一种方法。

### 10.3.1 中间状态实体化

​	如前所述，<u>每个 MapReduce作业都独立于其他任何作业。作业与其他任务的主要联系点是分布式文件系统上的输入和输出目录</u>。**如果希望一个作业的输出成为第二个作业的输入，则需要将第二个作业的输入目录配置为与第一个作业的输出目录相同，并且外部工作流调度程序必须仅在第一个作业已经完成后才开始第二个作业**。

​	如果第一个作业的输出是要在组织内部广泛分发的数据集，则此设置是合理的。在这种情况下，需要能够通过名称来引用它，并将其用作多个不同作业(包括由其他团队开发的作业)的输入。将数据发布到分布式文件系统中众所周知的位置可以实现松耦合，这样作业就不需要知道谁在生成输出或者消耗输出(请参见本章前面的“逻辑与布线分离”)。

​	<u>但是，在很多情况下，我们知道一个作业的输出只能用作另一个作业的输入，这个作业由同一个团队维护。在这种情况下，分布式文件系统上的文件只是中间状态：一种将数据从一个作业传递到下一个作业的方式。在用于构建由50或100个 MapReduce作业组成的推荐系统的复杂工作流中，存在很多这样的中间状态</u>。

​	<u>将这个中间状态写入文件的过程称为**实体化(或物化)**</u>。我们在第3章“聚合：数据立方体与物化视图”中巳经在物化视图的背景下遇到了这个术语，它主要是指<u>提前计算某些操作的结果并将其写入磁盘，而不是在需要时才进行计算</u>。

​	相比之下，本章开头的日志分析示例使用UNIX管道将一个命令的输出与另一个命令的输入连接起来。管道并不完全实现中间状态，而是只使用一个小的内存缓冲区，逐渐将输出流式传输到输入。

​	与UNIX管道相比， MapReduce完全实体化中间状态的方法有一些不利之处：

+ **MapReduce作业只有在前面作业(生成其输入)中的所有任务都完成时才能启动**，而通过UNIX管道连接的进程同时启动，输出一旦生成就会被使用。<u>不同机器的差异或不同的负载意味着作业中往往会有一些任务需要比其他任务花費更长的时间。必须等待前面作业中所有任务的完成必然会减慢整个工作流的执行</u>。
+ **Mapper通常是冗余的：它们只是读取刚刚由 reducer写入的同一个文件，并为下个分区和排序阶段做准备**。<u>在许多情况下， mapper代码可能是之前 reducer的一部分：如果 reducer的输出被分区和排序的方式与 mapper输出相同，那么不同阶段的 reducer可以直接链接在一起，而不需要与 mapper阶段交错</u>。
+ **将中间状态存储在分布式文件系统中意味着这些文件被复制到多个节点，对于这样的临时数据来说通常是大材小用了**。

#### * 数据流引擎

​	为了解决 MapReduce的这些问题，开发了用于分布式批处理的新的执行引擎，其中最著名的是 Spark，Tez，和 Flink。它们的设计方式有很多不同之处，但有一个共同点：**它们把整个工作流作为一个作业来处理，而不是把它分解成独立的子作业**。

​	<u>由于通过若干个处理阶段明确地建模数据流，所以这些系统被称为数据流引擎。**像MapReduce一样，它们通过反复调用用户定义的函数来在单个线程上一次处理一条记录。它们通过对输入进行分区来并行工作，并将一个功能的输出复制到网络上，成为另一个功能的输入**</u>。

​	与 MapReduce不同，这些功能不需要严格交替map和reduce的角色，而是以更灵活的方式进行组合。我们称为**函数运算符**，数据流引擎提供了多种不同的选项来连接一个运算符的输出到另一个的输入：

+ 一个选项是通过关键字对记录进行重新分区和排序，就像在MapReduce的shuffle阶段一样(请参阅本章前面的“ MapReduce的分布式执行”)。此功能可以像在MapReduce I中一样进行排序-合并join和分组。
+ 另一个可能性是读取若干个输入，并以相同的方式进行分区，但忽略排序。这节省了分区哈希join的工作，其中记录的分区是重要的，但顺序不相关，因为构建哈希表实现了顺序的随机化。
+ 对于广播哈希join，可以将一个运算符的输出发送到join运算符的所有分区。

​	这种处理引擎的风格源于Dryad和Nephele这样的研究系统，与MapReduce模型相比，它有几个优点：

+ **排序等计算代价昂贵的任务只在实际需要的地方进行，而不是在每个map和reduce阶段之间默认发生**。
+ 没有不必要的map任务，因为mapper所做的工作通常可以合并到前面的reduce运算符中(mapper不会更改数据集的分区)。
+ **由于工作流中的所有join和数据依赖性都是明确声明的，因此调度器知道哪些数据在哪里是必需的，因此它可以进行本地优化**。<u>例如，可以尝试将占用某些数据的任务放在与生成它的任务相同的机器上，以便可以通过**共享内存缓冲区**来交换数据，而不必通过网络复制数据。</u>
+ **将运算符之间的中间状态保存在内存中或写入本地磁盘通常就足够了，这比将内容写入HDFS(必须将其复制到多个节点并写入到每个副本所在的磁盘)需要更少的IO**。 MapReduce已经将这种优化用于mapper的输出，但是数据流引擎将该思想推广到了所有的中间状态。
+ **运算符可以在输入准备就绪后立即开始执行，<u>在下一个开始之前不需要等待前个阶段全部完成</u>**。
+ 与 **MapReduce(为每个任务启动一个新的JVM)**相比，现有的Java虚拟机(JVM)进程可以被重用来运行新的运算符，从而减少启动开销。

​	可以使用数据流引擎来执行与MapReduce工作流相同的计算，并且由于这些优化，通常执行速度会明显加快。**<u>由于运算符是map和reduce的一个泛化，相同的处理代码可以在任一执行引擎上运行：在Pig，Hive或 Cascading中所实现的工作流可以通过简单的配置更改从MapReduce切换到Tez或 Spark，而无需修改代码</u>**。

​	<u>Tez是一个相当轻量的库，它依靠YARN的 shuffle服务来实现节点之间的数据复制，而Spark和Flink则是包含网络通信层、调度器和面向用户API的大型框架</u>。我们将很快开始讨论这些高级API。

#### * 容错

​	将中间状态完全实体化到分布式文件系统的一个优点是持久化，这使得在 MapReduce中实现容错变得相当容易：如果一个任务失败了，它可以在另一台机器上重新启动并从文件系统重新读取相同的输入。

​	<u>Spark， Flink和Tez避免将中间状态写入HDFS，所以它们采用不同的方法来容忍错误：如果机器发生故障，并且该机器上的中间状态丢失，则利用其他可用的数据重新计算(例如之前的中间阶段，如果可能的话；或原始输入数据，通常在HDFS上)</u>。

​	**为了实现重新计算，框架必须追踪给定数据是如何计算的，使用了哪个输入分区以及应用了哪个运算符。 Spark使用<u>弹性分布式数据集(Resilient Distributed Dataset，RDD)抽象</u>来追踪数据的祖先，而Flink对运算符状态建立<u>检查点</u>，从而允许将执行过程中遇到故障的运算符恢复运行**。

​	<u>在重新计算数据时，知道计算是否具有**确定性**非常重要</u>：也就是说，如果给定相同的输入数据，那么运算符是否始终产生相同的输出？如果部分丢失的数据已经发送给下游运算符，这个问题就非常关键。如果运算符重新启动，重新计算的数据与原有的丟失数据不一致，下游运算符会很难解决新旧数据之间的冲突。<u>如果出现非确定性运算符，解决方案通常是将下游运算符也终止掉，并在新数据上再次运行它们</u>。

​	**为了避免这种级联故障，<u>最好让运算符具有确定性</u>**。**<u>但是请注意，非确定性行为很容易意外发生</u>**：例如，许多编程语言在迭代哈希表的元素时不能保证任何特定顺序，许多概率和统计算法明确依赖于使用随机数，以及使用系统时钟或外部数据源都是不确定的。<u>为了可靠地从故障中恢复，就需要消除这些不确定性因素，例如通过使用固定的种子产生伪随机数</u>。

​	**<u>通过重新计算数据以实现从故障中恢复并不总是能得到正确的答案：如果中间数据比源数据小得多，或者如果计算量非常大，那么将中间数据转化为文件比重新计算文件的代价要小</u>**。

#### * 关于实体化的讨论

​	回到UNIX的类比，我们看到MapReduce就像是将每个命令的输出写入临时文件，而数据流引擎看起来更像是UNIX管道。尤其Flink是围绕流水线执行的思想而建立起来的，也就是将运算符的输岀递增地传递给其他运算符，并且<u>在开始处理之前不等待输入完成</u>。

​	<u>排序操作不可避免地需要消耗其全部输入，才可以产生输出，因为有可能最后的输入记录具有最小的关键字，因此它必须成为第一个输出记录。**任何需要分类的运算符都需要至少暂时地累积状态。但是工作流的许多其他部分可以以流水线方式执行**</u>。

​	<u>当作业完成时，它的输出需要在某个地方实现持久化，以便用户可以找到并使用它，很可能它会再次写入分布式文件系统</u>。**因此，在使用数据流引擎时，HDFS上的实体化数据集通常仍是作业的输入和最终输出**。<u>**和 MapReduce一样，输入是不可变的，输出被完全替换**</u>。

​	**<u>简单总结一下，数据流对MapReduce的改进是，不需要自己将所有中间状态写入文件系统</u>**。

### 10.3.2 图与迭代处理

​	在第2章中的“类图数据模型”中，我们讨论了使用图来建模数据，并使用图查询语言遍历图中的边和顶点。第2章的讨论集中在OLTP的使用方式上：快速査询某些少量符合特定条件的顶点。

​	在批处理环境中査看图也很有趣，其目标是<u>在整个图上执行某种离线处理或分析</u>。这种需求经常出现在机器学习应用程序(如推荐引擎)或排名系统中。<u>例如，最著名的图分析算法之一是 PageRank，它试图根据链接至某网页的其他网页来评估该网页的受欢迎程度。它是确定网络搜索引擎结果呈现顺序的标准之一</u>。

> 像 Spark， Flink和Tez这样的数据流引擎(参见本章前面的“中间状态实体化”)通常将运算符作为**有向无环图(DAG)**排列在作业中。这与图处理并不一样：**在数据流引擎中，运算符之间的数据流被构造成一个图，数据本身通常由关系式元组构成；而在图处理中，数据本身具有图的形式**。真不幸，又一个由命名引起的混淆！

​	**许多图算法通过一次遍历一个边，将一个顶点与相邻顶点join起来以便传递某种信息，重复该过程直到满足某种条件为止**。例如，直到没有更多的边需要遍历，或者直到某些度量值收敛。我们在图2-6中看过一个例子，它通过迭代跟踪那些能够表明某个位置位于哪些其他位置之内(这种算法被称为传递闭包)的边，列出了数据库中所有位于北美的位置。

​	可以在分布式文件系统(包含顶点和边的列表文件)中存储图，但是这种“重复直到完成”的想法不能用普通的 MapReduce来表示，因为它只执行一次数据传递。这种算法通常需要以**迭代**方式实现：

1. 外部调度程序运行批处理来执行算法的一个步骤。
2. 当批处理过程完成时，调度器检查遍历是否完成(基于特定完成条件，例如没有更多的边要遍历，或者与最后一次迭代相比的变化低于某个阈值)。
3. 如果尚未完成，则调度程序返回到步骤1并运行另一轮批处理。

​	这种方法有效，但是<u>用MapReduce来实现却非常低效，主要是因为**MapReduce没有考虑算法的迭代性质**：即使与上一次迭代相比，只有图的一小部分发生了改变，它也总是读取整个输入数据集并产生一个全新的输出数据集</u>。

#### * Pregel处理模型

​	<u>作为对图数据的批处理优化，计算的批量同步并行(bulk synchronous parallel，BSP)模型已经流行起来，典型系统包括Apache Giraph， Sparke的GraphX API和Flink的 Gelly API。由于最早是 Google的 Pregel论文将这种处理图的方法普及，因此它也被称为 Pregel模型</u>。

​	回想一下在MapReduce中， mapper在概念上“发送消息”给 reducer调用，因为框架将所有具有相同关键字的 mapper输出集中在一起。 **Pregel中的想法与此类似：一个顶点可以“发送消息”到另一个顶点，通常这些消息沿着图的边被发送**。

​	**在每次迭代中，<u>为毎个顶点调用函数</u>，将所有发送至该顶点的消息传递给它，就像调用reducer一样**。<u>与 MapReduce不同之处在于， **Pregel模型的迭代过程中，顶点保存它在内存中的状态，所以函数只需要处理新输入的消息**</u>。如果图的某个部分没有收到发送消息，则不需要做任何工作。

​	这与参与者模型有些相似(请参阅第4章的“分布式Actor model”)，可以将每个节点视为参与者，不同之处在于，<u>顶点状态和顶点之间的消息具有**容错性**和**持久性**，并且通信以固定的方式进行：毎一轮迭代中，框架会将前一次迭代中的所有消息都发送出去</u>。 Actor model通常没有这样的时序保证。

#### * 容错

​	**事实上，顶点只能通过消息传递进行通信(而不是通过彼此间的直接查询)有助于提高 Pregel作业的性能，因为消息可以被批量处理，并且等待通信的次数会减少**。<u>唯一的等待是在迭代之间：由于 **Pregel模型保证在一次迭代中发送的所有消息都会在下次迭代中被发送，所以先前的迭代必须全部完成，而且所有的消息必须在下一次迭代开始之前复制到网络中**</u>。

​	即使底层网络可能会丢弃、重复或任意延迟消息(请参阅第8章的“不可靠的网络”)，但 Pregel的实现可以保证在后续迭代中消息在目标顶点**<u>只会被处理一次</u>**。像MapReduce一样，该框架透明地从故障中恢复，以简化Pregel顶层算法的编程模型。

​	这种容错方式是通过在**<u>迭代结束时定期快照所有顶点的状态</u>**来实现的，即将其全部状态写入持久存储。<u>如果某个节点发生故障并且其内存中状态丢失，则最简单的解决方法是将整个图计算回滚到上一个检査点，然后重新开始计算。如果算法是确定性的，并且记录了消息，那么也可以选择性地只恢复丟失的分区(就像我们之前讨论过的数据流引擎)</u>。

#### * 并行执行

​	顶点不需要知道它运行在哪台物理机器上。当它发送消息到其他顶点时，只需要将消息发送至一个顶点ID。框架对图进行分区，即确定哪个顶点运行在哪个机器上，以及如何通过网络路由消息，以便它们都能到达正确的位置。

​	由于**编程模型一次仅处理一个顶点(有时也称为“像顶点一样思考”)，所以框架能够以任意方式划分图**。<u>理想情况下，如果顶点之间需要进行大量的通信，那么它们会被分区至同一台机器。但是，**找到这样的优化分区是很困难的，通常按照任意分配的顶点ID对图进行分区，而不会尝试将相关的顶点分组在一**起</u>。

​	因此，图算法往往会有很多跨机器通信的开销，**中间状态(节点之间发送的消息)往往比原始图大**。通<u>**过网络发送消息的开销会显著减慢分布式图算法**</u>。

​	出于这样的原因，**<u>如果图可以载入到计算机内存中，那么单机(甚至可能是单线程)算法的性能可能要比分布式批处理的性能更好</u>**。即使图大于内存，也可以放在单个计算机的磁盘中，使用GraphChi等框架进行单机处理也是一个可行的选择。如果图太大而不适合单个机器，则需要采用像Pregel这样的分布式方法。<u>有效的并行化图算法是一个正在研究中的领域</u>。

### 10.3.3 高级API和语言

​	自 MapReduce流行以来，分布式批处理的执行引擎已经逐渐成熟。到目前为止，基础设施已经足够强大，能够存储和处理超过10000台机器集群中的PB级数据。由于在这种规模下物理操作批处理过程的问题已经或多或少得到了解决，所以问题已经转向其他领域：<u>改进编程模型，提高处理效率，扩大解决的问题域</u>等。

​	如前所述，由于手工编写 MapReduce作业太过耗时费力，因此Hive，Pig， Cascading和 Crunch等高级语言API变得非常流行。<u>随着Tez的出现，这些高级语言还能够移植到新的数据流执行引擎，而无需重写作业代码</u>。 Spark和 Flink也包含他们自己的高级数据流API，其中很多灵感来自FlumeJava。

​	这些数据流API通常使用关系式构建块来表示计算：将数据集join到某个字段的值上；按关键宇对元组进行分组；按条件进行过滤；并通过计数、求和或其他函数来聚合元组。在内部，这些运算使用本章前面讨论过的各种join和分组算法来实现。

​	除了减少代码的明显优势之外，这些高级接口还允许交互式使用。在这种交互式使用中，可以在 shell中逐步编写分析代码，并随时运行以观察计算结果。这种开发方式在探索数据集和测试处理方法时非常有用。这一点很像前面讨论过的UNIX设计哲学中。

​	此外，这些高级接口不仅使提高了系统利用率，而且提高了机器级别的作业执行效率。

#### 转向声明式查询语言

​	**与编写执行join的代码相比，指定join作为关系运算符的优点在于，框架可以分析join輸入的属性，并自动决定哪个join算法最适合当前的任务**。<u>Hive， Spark和Flink利用基于成本的查询优化器来实现这样的功能，甚至还可以改变join顺序，使中间状态数量最小化</u>。

​	**join算法的选择会对批处理作业性能产生很大的影响**，当然也不需要理解和记住本章讨论过的所有join算法。如果以声明方式指定join，那么可以在应用程序中简单地说明哪些join是必需的，由查询优化器决定如何最佳执行。之前在第2章“数据查询语言”中讨论过这个想法。

​	但是，在其他方面， MapReduce及后续的数据流引擎与SQL的完全声明式查询模型有很大不同。 MapReduce是围绕函数回调的思想构建的：对于每个记录或者一组记录，调用由用户定义的函数( mapper或 reducer)，共且该函数可以灵活调用任意代码来决定输出。这种方法的优点是，可以利用现有库的大型生态系统来执行数据解析、自然语言分析、图像分析以及运行数值或统计算法等。

​	轻松运行任意代码是 MapReduce之类的批处理系统与MPP数据库的区别所在(参见本章前面的“对比 Hadoop与分布式数据库”)。尽管数据库具有编写用户定义函数的功能，但是使用起来通常会很麻烦，而且与大多数编程语言中广泛使川的程序包管理器和依赖管理系统(例如而向Java的 Maven， JavaScript的npm和Ruby的Rubygems)无法很好地集成。

​	<u>而且，除了join之外，数据流引擎已经证实在很多其他情形中，声明性特征也同样具有优势</u>。例如，如果回调函数只包含一个简单的过滤条件，或者只是从一条记录中选择了部分宇段，那么在调用每条记录的函数时会有相当多的CPU开销。**如果以声明式表示简单的过滤和map操作，那么查询优化器可以利用面向列的存储格式(请参阅第3章的“列式存储”)，从磁盘仅读取所需的列**。Hive， Spark DataFrames和 Impala也使用**向量化执行**(请参阅第3章的“内存带宽与向量化处理”)：在对CPU高速缓存很友好的内部循环中迭代数据，并避免函数调用。 Spark生成JVM字节码， Impala使用LLVM为这些内部循环生成本机代码。

​	<u>通过将声明式特征与高级API结合，使查询优化器在执行期间可以利用这些优化方法，批处理框架看起来就更像MPP数据库了(并且能够实现性能相当)。同时，通过具有运行任意代码和读取任意格式数据的可扩展性，它们依然保持了灵活性的优势</u>。

#### 不同领域的专业化

​	尽管能够运行任意代码的可扩展性是有用的，但是标准处理模式不断反复运行的情形也极其常见，因比有必要实现可重用的通用构建模块。传统上，MPP数据库满足商业智能分析和商业报告的需求，但这只是批处理的诸多应用领域之一。

​	另一个越来越重要的领域是统计和数值算法，这是诸如分类和推荐系统等机器学习应用所需要的。已经出现了一些**可重复使用的实现**：例如， **Mahout在 MapReduce、Spark和 Flink之上实现了用于机器学习的各种算法，而 MADliB在关系型MPP数据库(Apache HAWQ)中实现了类似的功能**。

​	<u>可重用实现对例如k-近邻这样的空间算法也是有用的，它在多维空间中搜索与给定项接近的数据项，是一种相似性搜索。近似搜索对于基因组分析算法也很重要，它们需要找到相似但不相同的字符串</u>。

​	批处理引擎正被用于日益广泛的算法领域的分布式执行。<u>随着批处理系统获得更丰富的内置功能和高级声明式运算符，而MPP数据库也变得更具可编程性和灵活性，两者开始变得更加相似：最终，它们都是用于存储和处理数据的系统</u>。

## 10.4 小结

​	本章探讨了批处理这一主题。我们首先查看了诸如awk，grep和sort等UNIX工具，然后讨论了这些工具的设计理念是如何运用到 MapReduce和最新的数据流引擎中。这些设计原则包括：**输入是不可变的**，**输出是为了成为另一个(还未知的)程序的输入**，而复杂的问题通过编写“做好一件事”的小工具来解决。

​	**在UNIX世界中，允许多个程序组合在一起的统一接口是文件和管道；在 MapReduce中，该接口是分布式文件系统**。<u>我们看到数据流引擎添加了自己的**管道式数据传输机制**，**以避免在分布式文件系统中将中间状态实体化**，而**作业的初始输入和最终输出通常仍然是HDFS**</u>。

​	分布式批处理框架需要解决的两个主要问题是：

+ 分区

  在 MapReduce中， mapper根据输入文件块进行分区。 mapper的输出被重新分区、排序，合并成一个可配置数量的 reducer分区。这个过程的目的是把所有的相关数据——例如，具有相同关键字的所有记录都放在同一个地方。

  <u>除非必需，后MapReduce的数据流引擎都**尽量避免排序**，但它们釆取了大致类似的分区方法</u>。

+ 容错

  **MapReduce需要频繁写入磁盘**，这使得可以从单个失败任务中轻松恢复，而无需重新启动整个作业，但**在无故障情况下则会减慢执行速度**。<u>**数据流引擎执行较少的中间状态实体化并保留更多的内存，这意味着如果节点出现故障，他们需要重新计算更多的数据。确定性运算符减少了需要重新计算的数据量**</u>。

​	我们讨论了几种 MapReduce的join算法，其中大部分也在MPP数据库和数据流引擎中使用。分区算法的示例包括：

+ **排序-合并join**

  毎个将要join的输入都会由一个join关键字的 mapper来处理。通过分区、排序与合并，具有相同关键字的所有记录最终都会进入对 reducer的同一个调用。然后这个函数输出join记录。

+ **广播哈希join**

  两个join输入中的某一个数据集很小，所以不需要分区，而完全加载到哈希表中。因此，<u>可以为大数据集的每个分区启动一个 mapper，将小数据集的哈希表加载到每个 mapper中，然后一次扫描大数据集的一条记录，对每条记录进行哈希表查询</u>。

+ **分区哈希join**

  如果两个join的输入以相同的方式分区(使用相同的关键字，相同的哈希函数和相同数量的分区)，则哈希表方法可以**独立**用于每个分区。

​	<u>分布式批处理引擎有一个有意限制的编程模型：回调函数(如 mapper和 reducer)被设定为**无状态**，并且除了指定输出之外没有外部可见的任何副作用</u>。这个限制使得框架隐藏了抽象背后的一些困难的分布式系统问题，从而在面对崩溃和网络问题时，可以安全地重试任务，并丢弃任何失败任务的输出。**如果针对某个分区的多个任务都成功了，则实际上只会有其中一个任务使其输出可见**。

​	得益于这样的框架，在批处理作业中的代码无需考虑容错机制的实现：框架可以保证作业的最终输出与没有发生错误的情况相同(即使实际上各种任务可能不得不重试)。<u>而在线服务在处理用户请求时，将写入数据库作为处理请求的副作用，与之相比，批处理的可靠性语义要强大得多</u>。

​	批处理作业的显著特点是它读取一些输入数据并产生一些输出数据，而**不修改输入**。换句话说，输出是从输入派生而来。至关重要的是，输入数据是有界的：数椐大小固定已知(例如，它包含一些时间点的日志文件或数据库内容的快照)。<u>因为它是有界的，所以一个作业总是可以知道何时完成了对整个输入的读取，何时作业最终完成</u>。

​	在下一章中，我们将转向**流处理，其中输入是无界的**。也就是说，作业的输入是永无止境的数据流。在这种情况下，作业永远不会完成，因为在任何时候都可能有更多的输入进来。我们将看到流处理和批处理在某些方面是相似的，但流数据无界的假设也会深刻改变我们设计系统的方法。

# 第11章 流处理系统

P410

