# 数据密集型应用系统设计-学习笔记-02

# 第二部分 分布式数据系统

​	本书第一部分主要讨论了单台机器存储系统设计的主要技术。在第二部分，我们将继续向前迈进，当需要多台机器提供数据存储和检索服务时，又会有哪些挑战和方案呢？

​	主要出于以下目的，我们需要在多台机器上分布数据：

+ 扩展性

  当数据量或者读写负载巨大，严重超出了单台机器的处理上限，需要将负载分散到多台机器上。

+ 容错与高可用性

  当单台机器(或者多台，以及网络甚至整个数据中心)出现故障，还希望应用系统可以继续工作，这时需要采用多台机器提供冗余。这样某些组件失效之后，冗余组件可以迅速接管。

+ **延迟考虑**

  如果客户遍布世界各地，通常需要考虑在全球范围内部署服务，以方便用户就近访问最近数据中心所提供的服务，从而避免数据请求跨越了半个地球才能到达目标。

## **系统扩展能力**

​	**当负载增加需要更强的处理能力时，最简单的办法就是购买更强大的机器(有时称为垂直扩展)**。<u>由一个操作系统管理更多的CPU，内存和磁盘，通过**高速内部总线**使每个CPU都可以访问所有的存储器或磁盘。在这样一个共享内存架构中，所有这些组件的集合可看作一台大机器</u>。

​	共享内存架构的问题在于，成本增长过快甚至超过了线性：即如果把一台机器内的CPU数量增加一倍，内存扩容一倍，磁盘容量加大一倍，则最终总成本增加不止一倍。<u>并且由于性能瓶颈因素，这样一台机器尽管拥有了两倍的硬件指标但却不一定能处理两倍的负载</u>。

​	<u>共享内存架构能够提供有限的容错能力，例如高端的服务器可以热插拔很多组件(在不关闭机器的情况下更换磁盘，内存模块，甚至是CPU)</u>。但很显然，它仍局限于某个特定的地理位置，无法提供异地容错能力。

​	另一种方法是共享磁盘架构，它拥有多台服务器，每个服务器各自拥有独立的CPU和内存，然后将数据存储在可共享访问的磁盘阵列上，服务器与磁盘阵列之间往往通过高速网络连接。<u>这种架构多适用于数据仓库等负载，然而通常由于资源竞争以及锁的开销等限制了其进一步的扩展能力</u>。

## 无共享结构

​	相比之下，无共享架构(也称为水平扩展)则获得了很大的关注度。当采用这种架构时，运行数据库软件的机器或者虚拟机称为节点。每个节点独立使用本地的CPU，内存和磁盘。<u>节点之间的所有协调通信等任务全部运行在传统网络(以太网)之上且核心逻辑主要依靠软件来实现</u>。

​	无共享系统不需要专门的硬件，具有较高的性价比。它可以跨多个地理区域分发数据，从而减少用户的访问延迟，甚至当整个数据中心发生灾难时仍能继续工作。通过云计算虛拟机的部署方式，即便是没有Google级别规模的小公司，也可以轻松拥有跨区域的分布式架构和服务能力。

​	本部分内容将重点放在无共享体系架构上，并不是因为它一定是所有应用的最佳选择，而是因为它需要应用开发者更多的关注和深入理解。例如把数据分布在多节点上，就需要了解在这样一个分布式系统下，背后的权衡设计和隐含限制，数据库并不能魔法般地把所有复杂性都屏蔽起来。

​	虽然分布式无共享体系架构具有很多优点，但也会给应用程序带来更多的复杂性，有时甚至会限制实际可用的数据模型。例如在某些极端情况下，一个简单的单线程程序可能比一个拥有100多个CPU核的集群性能更好。而另一方面，无共享系统也可以做到性能非常强大。接下来的几章里我们将详细讨论数据分布时所面临的主要问题。

## 复制与分区

​	将数据分布在多节点时有两种常见的方式：

+ 复制

  **在多个节点上保存相同数据的副本**，每个副本具体的存储位置可能不尽相同。复制方法可以提供冗余：如果某些节点发生不可用，则可以通过其他节点继续提供数据访问服务。复制也可以帮助提高系统性能。我们在第5章将主要讨论复制技术。

+ 分区

  **将一个大块头的数据库拆分成多个较小的子集即分区，不同的分区分配给不同的节点(也称为分片)**。我们在第6章主要介绍分区技术。

​	这些是不同的数据分布机制，然而它们经常被放在一起组合使用。参见图II-1的示例。

​	在了解以上概念之后，我们会讨论分布式环境中错综复杂的权衡之道，很可能你会在实际设计系统时不得不面对这些艰难选择。我们将在第7章介绍事务，帮助理解数据系统中各种可能岀错的情况以及处理方法，而第8章和第9章将深入分析分布式系统内在的局限性，之后结束本部分。

![第二部分：分布式数据 - 图1](https://static.sitestack.cn/projects/ddia/img/figii-1.png)

​	在之后本书的第三部分，我们将讨论如何把多个(可能每一个都是分布式)数据存储组件集成到一个更大的系统中，以满足更复杂的应用需求。但在那之前，我们首先来谈谈分布式数据系统。

# 第5章 数据复制

​	复制主要指通过互联网络在多台机器上保存相同数据的副本。正如第二部分开头所介绍的，通过数据复制方案，人们通常希望达到以下目的：

+ 使数据在地理位置上更接近用户，从而**降低访问延迟**。
+ 当部分组件出现故障，系统依然可以继续工作，从而**提高可用性**。
+ 扩展至多台机器以同时提供数据访问服务，从而**提高读吞吐量**。

​	本章我们将假设数据规模比较小，集群的每一台机器都可以保存数据集的完整副本。在接下来的第6章中，我们放宽这一假设，讨论单台机器无法容纳整个数据集的情况(即必须分区)。在后面的章节中，我们还将讨论复制过程中可能出现的各种故障，以及该如何处理这些故障。

​	如果复制的数据一成不变，那么复制就非常容易：只需将数据复制到每个节点，一次即可搞定。然而所有的技术挑战都在于处理那些持续更改的数据，而这正是本章讨论的核心。我们将讨论三种流行的复制数据变化的方法：**主从复制、多主节点复制和无主节点复制**。几乎所有的分布式数据库都使用上述方法中的某一种，而三种方法各有优缺点，我们稍后会详细解读。

​	复制技术存在许多需要折中考虑的地方，例如采用同步复制还是异步复制，以及如何处理失败的副本等。数据库通常采用可配置选项来调整这些处理策略，虽然在处理细节方面因数据库实现而异，但存在一些通用的一般性原则。本章我们还将在讨论不同选项可能出现的后果。

​	<u>数据库复制其实是个很古老的话题。因为网络的基本约束条件没有发生本质的改变，可以说自1970年所研究的基本复制原则，时至今日也没有发生太大的变化</u>。然而，除了学术研究，实践中很多开发人员仍然假定数据库只运行在单节点上，分布式数据库成为主流也只是最近发生的事情。由于许多应用开发人员在这方面经验还略显不足，对诸如“最终一致性”等问题存在一些误解。因此，在“复制滞后问题”中，我们会详细讨论最终一致性，包括读自己的写和单调读等。

## * 5.1 主节点与从节点

​	**每个保存数据库完整数据集的节点称之为副本**。当有了多副本，不可避免地会引入个问题：<u>如何确保所有副本之间的数据是一致的</u>？

​	**对于每一笔数据写入，所有副本都需要随之更新；否则，某些副本将出现不一致**。最常见的解决方案是基于主节点的复制(也称为主动被动，或主从复制)，如图5-1所示。主从复制的工作原理如下：

1. 指定某一个副本为主副本(或称为主节点)。当客户写数据库时，必须将写请求首先发送给主副本，主副本首先将新数据写入本地存储。
2. **其他副本则全部称为从副本(或称为从节点)。主副本把新数据写入本地存储后，然后将数据更改作为复制的日志或更改流发送给所有从副本。每个从副本获得更改日志之后将其应用到本地，且严格保持与主副本相同的写入顺序**。
3. 客户端从数据库中读数据时，可以在主副本或者从副本上执行査询。再次强调，**只有主副本才可以接受写请求；从客户端的角度来看，从副本都是只读的**。

​	许多关系型数据库都内置支持主从复制，例如 PostgreSQL(9.0版本以后)、MYSQL、 Oracle Data Guard和 SQL Server的AlwaysOn Availability Groups。而些非关系数据库如 MongoDB、 RethinkDB和 Espresso也支持主从复制。另外，主从复制技术也不仅限于数据库，还广泛用于分布式消息队列如 Kafka 和 RabbitMQ可以及一些网络文件系统和复制块设备(如DRBD)。

![领导者与追随者 - 图1](https://static.sitestack.cn/projects/ddia/img/fig5-1.png)

### * 5.1.1 同步复制与异步复制

​	复制非常重要的一个设计选项是同步复制还是异步复制。<u>对于关系数据库系统，同步或异步通常是一个可配置的选项；而其他系统则可能是硬性指定或者只能二选一</u>。

​	结合图5-1的例子，网站用户需要更新首页的头像图片。其基本流程是，客户将更新请求发送给主节点，主节点接收到请求，接下来将数据更新转发给从节点。最后，由主节点来通知客户更新完成。

​	图5-2则进一步描述了系统各个模块间的通信情况，包括客户端，主节点和两个从节点。时间从左到右。请求或响应标记为粗箭头。

![领导者与追随者 - 图2](https://static.sitestack.cn/projects/ddia/img/fig5-2.png)

​	图5-2中，从节点1的复制是同步的，即主节点需等待直到从节点1确认完成了写入，然后才会向用户报告完成，并且将最新的写入对其他客户端可见。而从节点2的复制是异步的：主节点发送完消息之后立即返回，不用等待从节点2的完成确认。

​	图5-2中，从节点2在接收复制日志之前有一段很长的延迟。通常情况下，复制速度会非常快，例如多数数据库系统可以在一秒之内完成所有从节点的更新。但是，系统其实并没有保证一定会在多长时间内完成复制。有些情况下，从节点可能落后主节点几分钟甚至更长时间，例如，由于从节点刚从故障中恢复，或者系统已经接近最大设计上限，或者节点之间的网络出现问题。

​	**同步复制的优点是，一旦向用户确认，从节点可以明确保证完成了与主节点的更新同步，数据已经处于最新版本。万一主节点发生故障，总是可以在从节点继续访问最新数据。缺点则是，如果同步的从节点无法完成确认(例如由于从节点发生崩溃，或者网络故障，或任何其他原因)，写入就不能视为成功。主节点会阻塞其后所有的写操作，直到同步副本确认完成**。

​	因此，把所有从节点都配置为同步复制有些不切实际。因为这样的话，任何一个同步节点的中断都会导致整个系统更新停滞不前。**<u>实践中，如果数据库启用了同步复制，通常意味着其中某一个从节点是同步的，而其他节点则是异步模式</u>**。**万一同步的从节点变得不可用或性能下降，则将另一个异步的从节点提升为同步模式。这样可以保证至少有两个节点(即主节点和一个同步从节点)拥有最新的数据副本。这种配置有时也称为<u>半同步</u>**。

​	<u>主从复制还经常会被配置为**全异步**模式</u>。此时如果主节点发生失败且不可恢复，则所有尚未复制到从节点的写请求都会丟失。这意味着即使向客户端确认了写操作，却无法保证数据的持久化。<u>但全异步配置的优点则是，不管从节点上数据多么滞后，主节点总是可以继续响应写请求，系统的吞吐性能更好</u>。

​	**异步模式这种弱化的持久性听起来是一个非常不靠谱的折中设计，但是异步复制还是被广泛使用，特别是那些从节点数量巨大或者分布于广域地理环境**。我们将在本章后面的“复制滞后问题”继续这个话题。

> 复制问题研究
>
> 主节点发生故障时异步复制系统可能会丢失数据，这是一个非常严重的问题，因此在保证数据不丢失的前提下，研究人员尝试了各种办法来提高复制性能与系统可用性。例如，链式复制是同步复制的一种变体，已经在一些系统(如Microsoft Azure存储)中得以实现。
>
> 多副本一致性与共识之间有着密切的联系(即让多个节点对数据状态达成一致)，我们将在第9章详细探讨这一点。本章主要集中于数据库实践中常用的、相对简单的复制技术。

#### 配置新的从节点

​	当如果岀现以下情况时，如需要增加副本数以提高容错能力，或者替换失败的副本，就需要考虑增加新的从节点。但如何确保新的从节点和主节点保持数据一致呢?

​	简单地将数据文件从一个节点复制到另一个节点通常是不够的。主要是因为客户端仍在不断向数据库写入新数据，数据始终处于不断变化之中，因此常规的文件拷贝方式将会导致不同节点上呈现出不同时间点的数据，这不是我们所期待的。

​	或许应该考虑锁定数据库(使其不可写)来使磁盘上的文件保持一致，但这会违反高可用的设计目标。好在我们可以做到在不停机、数据服务不中断的前提下完成从节点的设置。逻辑上的主要操作步骤如下:

1. 在某个时间点对主节点的数据副本产生一个**一致性快照**，这样避免长时间锁定整个数据库。目前大多数数据库都支持此功能，快照也是系统备份所必需的。而在某些情况下，可能需要第三方工具，如MySQL的 innobackupex。
2. **将此快照拷贝到新的从节点**。
3. **从节点连接到主节点并请求快照点之后所发生的数据更改日志**。因为在第一步创建快照时，快照与系统复制日志的某个确定位置相关联，这个位置信息在不同的系统有不同的称呼，如PostgreSQL将其称为“log sequence number”(日志序列号)，而 MySQL将其称为“ binlog coordinates”。
4. 获得日志之后，从节点来应用这些快照点之后所有数据变更，这个过程称之为追赶。接下来，它可以继续处理主节点上新的数据变化。并重复步骤1~步骤4。

​	建立新的从副本具体操作步骤可能因数据库系统而异。某些系统中，这个过程是全自动化的，而其他系统中所涉及的步骤、流程可能会比较复杂，甚至需要管理员手动介入。

#### * 处理节点失效

​	系统中的任何节点都可能因故障或者计划内的维护(例如重启节点以安装内核安全补丁)而导致中断甚至停机。如果能够在不停机的情况下重启某个节点，这会对运维带来巨大的便利。我们的目标是，<u>尽管个别节点会出现中断，但要保持系统总体的持续运行，并尽可能减小节点中断带来的影响</u>。

​	那么如何通过主从复制技术来实现系统高可用呢?

+ 从节点失效：追赶式恢复

  从节点的本地磁盘上都保存了副本收到的数据变更日志。如果从节点发生崩溃，然后顺利重启，或者主从节点之间的网络发生暂时中断(闪断)，则恢复比较容易，**根据副本的复制日志，从节点可以知道在发生故障之前所处理的最后一笔事务，然后连接到主节点，并请求自那笔事务之后中断期间内所有的数据变更**。在收到这些数据变更日志之后，将其应用到本地来追赶主节点。之后就和正常情况一样持续接收来自主节点数据流的变化。

+ 主节点失效：节点切换

  **处理主节点故障的情况则比较棘手：选择某个从节点将其提升为主节点；客户端也需要更新，这样之后的写请求会发送给新的主节点，然后其他从节点要接受来自新的主节点上的变更数据**，这一过程称之为**切换**。故障切换可以手动进行，例如通知管理员主节点发生失效，采取必要的步骤来创建新的主节点；或者以自动方式进行。自动切换的步骤通常如下：

  1. **确认主节点失效**。有很多种岀错可能性，例如由于系统崩溃，停电，网络问题等。没有万无一失的方法能够确切地检测到究竟问题出在哪里，所以**大多数系统都采用了基于超时的机制：节点间频蘩地互相发生发送心跳存活消息，如果发现某一个节点在一段比较长时间内(例如30s)没有响应，即认为该节点发生失效**(如果主节点在计划内出于维护目的而故意下线，则不在此讨论范围)。
  2. **选举新的主节点**。可以通过选举的方式(超过多数的节点达成共识)来选举新的主节点，或者由之前选定的某控制节点来指定新的主节点。候选节点最好与原主节点的数据差异最小，这样可以最小化数据丢失的凤险。让所有节点同意新的主节点是个典型的**共识问题**，会在第9章详细讨论。
  3. 重新配置系统使新主节点生效。客户端现在需要将写请求发送给新的主节点(细节将在第6章的“请求路由”中讨论)。<u>如果原主节点之后重新上线，可能仍然自认为是主节点，而没有意识到其他节点已经达成共识迫使其下台。**这时系统要确保原主节点降级为从节点，并认可新的主节点**</u>。

  ​	然而，上述切换过程依然充满了很多变数：

  + <u>如果使用了异步复制，且失效之前，新的主节点并未收到原主节点的所有数据；在选举之后，原主节点很快又重新上线并加入到集群，接下来的写操作会发生什么？新的主节点很可能会收到冲突的写请求，这是因为原主节点未意识的角色变化，还会尝试同步其他从节点，但其中的一个现在已经接管成为现任主节点</u>。**常见的解决方案是，原主节点上未完成复制的写请求就此丢弃，但这可能会违背数据更新持久化的承诺**。
  + **如果在数据库之外有其他系统依赖于数据库的内容并在一起协同使用，丟弃数据的方案就特别危险**。例如，<u>在 Github的一个事故中，某个数据并非完全同步的MySQL从节点被提升为主副本，数据库使用了自增计数器将主键分配给新创建的行，但是因为新的主节点计数器落后于原主节点(即二者并非完全同步)，它重新使用了已被原主节点分配出去的某些主键，而恰好这些主键已被外部Redis所引用，结果出现 MYSQL和 Redis之间的不一致，最后导致了某些私有数据被错误地泄露给了其他用户</u>。
  + **在某些故障情况下(参见第8章)，可能会发生两个节点同时都自认为是主节点。这种情况被称为脑裂**，它非常危险：<u>两个主节点都可能接受写请求，并且没有很好解决冲突的办法(参阅本章后面的“多主节点复制技术”)，最后数据可能会丟失或者破坏</u>。作为一种安全应急方案，有些系统会采取措施来强制关闭其中一个节点。然而，如果设计或者实现考虑不周，可能会出现两个节点都被关闭的情况。
  + 如何设置合适的超时来检测主节点失效呢？<u>主节点失效后，超时时间设置得越长也意味着总体恢复时间就越长</u>。<u>但如果超时设置太短，可能会导致很多不必要的切换</u>。例如，突发的负载峰值会导致节点的响应时间变长甚至超时，或者由于网络故障导致延迟增加。**如果系统此时已经处于高负载压力或网络已经出现严重拥塞，不必要的切换操作只会使总体情况变得更糟**。

​	**坦白讲，对于这些问题没有简单的解决方案。因此，即使系统可能支持自动故障切换，有些运维团队仍然更愿意以手动方式来控制整个切换过程**。

​	上述这些问题，包括**节点失效、网络不可靠、副本一致性、持久性、可用性与延迟**之间各种细微的权衡，实际上正是分布式系统核心的基本问题。在第8章和第9章中，我们还会进一步讨论。

### * 5.1.2 复制日志的实现

​	主从复制技术到底是如何工作呢？实践中有多种不同的实现方法，此处我们逐一做些介绍。

#### 基于语句的复制

​	<u>最简单的情况，主节点记录所执行的每个**写请求(操作语句)**并将该操作语句作为日志发送给从节点</u>。对于关系数据库，这意味着每个 INSERT、 UPDATE或 DELETE语句都会转发给从节点，并且每个从节点都会分析并执行这些SQL语句，如同它们是来自客户端那样。

​	听起来很合理也不复杂，但这种复制方式有一些不适用的场景：

+ <u>任何调用非确定性函数的话句，如NOW()获取当前时间，或RAND()获取一个随机数等，可能会在不同的副本上产生不同的值</u>。

+ 如果语句中使用了自增列，或者依赖于数据库的现有数据(例如， `UPDATE ... WHERE <某些条件>`)，则所有副本必须按照完全相同的顺序执行，否则可能会带来不同的结果。进而，<u>如果有多个同时并发执行的事务时，会有很大的限制</u>。
+ <u>有副作用的语句(例如，触发器、存储过程、用户定义的函数等)，可能会在每个副本上产生不同的副作用</u>。

​	<u>有可能采取一些特殊措施来解决这些问题，例如，主节点可以在记录操作语句时将非确定性函数替换为执行之后的确定的结果，这样所有节点直接使用相同的结果值。但是，这里面存在太多边界条件需要考虑，因此目前通常首选的是其他复制实现方案</u>。

​	<u>**MySQL5.1版本之前采用基于操作语句的复制**。现在由于逻辑紧凑，依然在用，**但是默认情况下，如果语句中存在一些不确定性操作，则 MySQL会切换到基于行的复制(稍后讨论)**。 VoltDB使用基于语句的复制，它通过事务级别的确定性来保证复制的安全</u>。

#### 基于预写日志(WAL)传输

​	在第3章中，我们讨论了存储引擎的磁盘数据结构，<u>通常毎个**写操作**都是以**追加写**的方式写入到日志中</u>：

+ 对于日志结构存储引擎(参阅第3章的“SSTables和LSM-trees”)，日志是主要的存储方式。日志段在后台压缩并支持垃圾回收。
+ 对于采用覆盖写磁盘的B-tree(参阅第3章的“B-tree”)结构，<u>**每次修改会预先写入日志**，如系统发生崩溃，通过索引更新的方式迅速恢复到此前一致状态</u>。

​	**不管哪种情况，所有对数据库写入的字节序列都被记入日志。因此可以使用完全相同的日志在另一个节点上构建副本：除了将日志写入磁盘之外，主节点还可以通过网络将其发送给从节点**。

​	从节点收到日志进行处理，建立和主节点内容完全相同的数据副本。

​	PostgreSQL、 Oracle以及其他系统等支持这种复制方式。其**主要缺点是日志描述的数据结果非常底层**：**一个WAL包含了哪些磁盘块的哪些字节发生改变，诸如此类的细节**。**这使得复制方案和存储引擎紧密耦合**。<u>如果数据库的存储格式从一个版本改为另一个版本，那么系统通常无法支持主从节点上运行不同版本的软件</u>。

​	看起来这似乎只是个有关实现方面的小细节，但可能对运营产生巨大的影响。<u>如果复制协议允许从节点的软件版本比主节点更新，则可以实现数据库软件的不停机升级：首先升级从节点，然后执行主节点切换，使升级后的从节点成为新的主节点</u>。相反，**<u>复制协议如果要求版本必须严格一致(例如WAL传输)，那么就势必以停机为代价</u>**。

#### 基于行的逻辑日志复制

​	另一种方法是**<u>复制和存储引擎采用不同的日志格式，这样复制与存储逻辑剥离</u>**。<u>这种复制日志称为**逻辑日志**，以区分物理存储引擎的数据表示</u>。

​	<u>关系数据库的**逻辑日志**通常是指一系列记录来描述数据表**行级别**的**写请求**</u>：

+ 对于行插入，日志包含所有相关列的新值。
+ <u>对于行删除，日志里有足够的信息来唯一标识已删除的行，通常是靠主键，但如果表上没有定义主键，就需要记录所有列的旧值</u>。
+ 对于行更新，日志包含足够的信息来唯一标识更新的行，以及所有列的新值(或至少包含所有已更新列的新值)。

​	**如果一条事务涉及多行的修改，则会产生多个这样的日志记录，并在后面跟着一条记录，指出该事务已经提交**。 MySQL的二进制日志binlog(当配置为**基于行的复制**时)使用该方式。

​	**由于逻辑日志与存储引擎逻辑解耦，因此可以更容易地保持向后兼容，从而使主从节点能够运行不同版本的软件甚至是不同的存储引擎**。

​	对于外部应用程序来说，逻辑日志格式也更容易解析。如果要将数据库的內容发送到外部系统(如用于离线分析的数据仓库)，或构建自定义索引和缓存等，基于逻辑日志的复制更有优势。该技术也被称为**变更数据捕获**，我们将在第11章中继续讨论。

#### 基于触发器的复制

​	到目前为止所描述的复制方法都是由数据库系统来实现的，不涉及任何应用程序代码。通常这是大家所渴望的，不过，在某些情况下，我们可能需要更高的灵活性。例如，只想复制数据的一部分，或者想从一种数据库复制到另一种数据库，或者需要订制、管理冲突解决逻辑(参阅本章后面的“处理写冲突”)，则需要将复制控制交给应用程序层。

​	有一些工具，例如Oracle Golden Gate，可以通过读取数据库日志让应用程序获取数据变更。另一种方法则是借助许多关系数据库都支持的功能：<u>触发器</u>和<u>存储过程</u>。

​	触发器支持注册自己的应用层代码，使得当数据库系统发生数据更改(写事务)时自动执行上述自定义代码。通过触发器技术，可以将数据更改记录到一个单独的表中，然后外部处理逻辑访问该表，实施必要的自定义应用层逻辑，例如将数据更改复制到另一个系统。 Oracle的Databus和Postgres的Bucardo就是这种技术的典型代表。

​	<u>基于触发器的复制通常比其他复制方式开销更高，也比数据库内置复制更容易出错，或者暴露一些限制。然而，其高度灵活性仍有用武之地</u>。

## * 5.2 复制滞后问题

​	<u>容忍节点故障</u>只是使用复制其中的一个原因。正如第二部分开头所介绍的，其他原因包括<u>可扩展性</u>(采用多节点来处理更多的请求)和<u>低延迟</u>(将副本部署在地理上距离用户更近的地方)。

​	**主从复制要求所有写请求都经由主节点，而任何副本只能接受只读査询**。对于读操作密集的负载(如Web)，这是一个不错的选择：创建多个从副本，将读请求分发给这些从副本，从而减轻主节点负载并允许读取请求就近满足。

​	在这种扩展体系下，只需添加更多的从副本，就可以提高读请求的服务吞吐量。<u>但是，这种方法实际上只能用于**异步复制**，如果试图同步复制所有的从副本，则单个节点故障或网络中断将使整个系统无法写入</u>。而且**节点越多，发生故障的概率越高，所以完全同步的配置现实中反而非常不可靠**。

​	不幸的是，<u>如果一个应用正好从一个**异步**的从节点读取数据，而该副本落后于主节点，则应用可能会读到过期的信息。这会导致数据库中出现明显的不一致：由于并非所有的写入都反映在从副本上，如果同时对主节点和从节点发起相同的査询，可能会得到不同的结果</u>。这种不一致只是一个暂时的状态，如果停止写数据库，经过一段时间之后，从节点最终会赶上并与主节点保持一致。这种效应也被称为**最终一致性**。

​	"最终"一词有些含糊不清，总的来说，副本落后的程度理论上并没有上限。正常情况下，主节点和从节点上完成写操作之间的时间延迟(复制滞后)可能不足1秒，这样的滞后，在实践中通常不会导致太大影响。但是，<u>如果系统已接近设计上限，或者网络存在问题，则滞后可能轻松增加到几秒甚至几分钟不等</u>。

​	当滞后时间太长时，导致的不一致性不仅仅是一个理论存在的问题，而是个实实在在的现实问题。在本节中，我们将重点介绍三个复制滞后可能出现的问题，并给出相应的解决思路。

### 5.2.1 读自己的写

​	许多应用让用户提交一些数据，接下来查看他们自己所提交的内容。例如客户数据库中的记录，亦或者是讨论主题的评论等。**提交新数据须发送到主节点，但是当用户读取数据时，数据可能来自从节点**。这对于**读密集**和偶尔写入的负载是个非常合适的方案。

​	然而对于异步复制存在这样一个问题，如图5-3所示，**用户在写入不久即查看数据，则新数据可能尚未到达从节点**。对用户来讲，看起来似乎是刚刚提交的数据丢失了，显然用户不会高兴。

![复制延迟问题 - 图1](https://static.sitestack.cn/projects/ddia/img/fig5-3.png)

​	对于这种情况，我们需要“**写后读一致性**”，也称为**读写一致性**。该机制保证如果用户重新加载页面，他们总能看到自己最近提交的更新。但对其他用户则没有任何保证，这些用户的更新可能会在稍后才能刷新看到。

​	基于主从复制的系统该如何实现写后读一致性呢？有多种可行的方案，以下例举一二：

+ **如果用户访问可能会被修改的内容，从主节点读取；否则，在从节点读取**。这背后就要求有一些方法在实际执行查询之前，就已经知道内容是否可能会被修改。例如，社交网络上的用户首页信息通常只能由所有者编辑，而其他人无法编辑。因此，这就形成一个简单的规则：总是从主节点读取用户自己的首页配置文件，而在从节点读取其他用户的配置文件。
+ **<u>如果应用的大部分内容都可能被所有用户修改，那么上述方法将不太有效，它会导致大部分内容都必须经由主节点，这就丧失了读操作的扩展性</u>**。此时需要其他方案来判断是否从主节点读取。例如，<u>跟踪最近更新的时间，如果更新后一分钟之內，则总是在主节点读取；并监控从节点的复制滞后程度，避免从那些滞后时间超过一分钟的从节点读取</u>。
+ <u>客户端还可以记住最近更新时的时间戳，并附带在读请求中，据此信息，系统可以确保对该用户提供读服务吋都应该至少包含了该时间戳的更新。如果不够新要么交由另一个副本来处理，要么等待直到副本接收到了最近的更新</u>。时间戳可以是**逻辑时间戳**(例如用来指示写入顺序的日志序列号)或实际系统时钟(在这种情况下，时钟同步又称为一个关键点，请参阅第8章“不可靠的时钟”)。
+ 如果副本分布在多数据中心(例如考虑与用户的地理接近，以及高可用性)，情况会更复杂些。<u>必须先把请求路由到主节点所在的数据中心</u>(该数据中心可能离用户很远)。

​	如果同一用户可能会从多个设备访问数据，例如一个桌面Web浏览器和一个移动端的应用，情况会变得更加复杂。此时，要提供跨设备的写后读一致性，即如果用户在某个设备上输入了一些信息然后在另一台设备上查看，也应该看到刚刚所输入的内容。

​	在这种情况下，还有一些需要考虑的问题：

+ 记住用户上次更新时间戳的方法实现起来会比较困难，因为在一台设备上运行的代码完全无法知道在其他设备上发生了什么。此时，<u>元数据必须做到全局共享</u>。
+ 如果副本分布在多数据中心，无法保证来自不同设备的连接经过路由之后都到达同一个数据中心。例如，用户的台式计算机使用了家庭宽带连接，而移动设备则使用蜂窝数据网络，不同设备的网络连接线路可能完全不同。<u>如果方案要求必须从主节点读取，则首先需要想办法确保将来自不同设备的请求路由到同一个数据中心</u>。

### 5.2.2 单调读

​	在前面异步复制读异常的第二个例子里，出现了用户数据向后回滚的奇怪情况。

​	假定用户从不同副本进行了多次读取，如图5-4所示，用户刷新一个网页，读请求可能被随机路由到某个从节点。用户2345先后在两个从节点上执行了两次完全相同的查询(先是少量滞后的节点，然后是滞后很大的从节点)，则很有可能出现以下情况。第一个査询返回了最近用户1234所添加的评论，但第二个查询因为滞后的原因，还没有收到更新因而返回结果是空。实际上，第二个查询结果代表了更早时间点的状态。如果第一个查询没有返回任何内容，用户2345并不知道用户1234最近的评论，情况还不算太糟糕，但当用户2345看到了用户1234的评论之后，紧接着评论又消失了，他就会感觉很困惑。

![复制延迟问题 - 图2](https://static.sitestack.cn/projects/ddia/img/fig5-4.png)

​	单调读一致性可以确保不会发生这种异常。这是一个比强一致性弱，但比最终一致性强的保证。当读取数据时，**<u>单调读保证，如果某个用户依次进行多次读取，则他绝不会看到回滚现象，即在读取较新值之后又发生读旧值的情况</u>**。

​	<u>实现单调读的一种方式是，确保每个用户总是从**固定的同一副本执行读取**(而不同的用户可以从不同的副本读取)</u>。例如，<u>基于用户ID的哈希的方法而不是随机选择副本。但如果该副本发生失效，则用户的查询必须重新路由到另一个副本</u>。

### 5.2.3 前缀一致读

​	第三个由于复制滞后导致因果反常的例子。例如Poons先生与Cake夫人之间的以下对话：

+ Poons先生

  Cake夫人，您能看到多远的未来？

+ Cake夫人

  通常约10s， Poons先生。

​	这两句话之间存在因果关系：Cake夫人首先是听到了Poons先生的问题，然后再去回答该问题。

​	现在，想象第三个人正在通过从节点收听上述对话。Cake夫人所说的话经历了短暂的滞后到达该从节点，但 Poons先生所说的经历了更长的滞后才到达(见图5-5)。观察者听到的对话变成这样:

+ Cake夫人

  通常约10s， Poons先生。

+ Poons先生

  Cake夫人，您能看到多远的未来?

​	对于观察者来说，似乎在Poon先生提出问题之前，Cake夫人就开始了回答问题。首先，这种超自然的力量确认令人印象深刻，但逻辑上却是混乱的。

​	<u>防止这种异常需要引入另一种保证：**前缀一致读**。该保证是说，对于一系列按照某个顺序发生的写请求，那么读取这些内容时也会按照当时写入的顺序</u>。

​	这是分区(分片)数据库中出现的一个特殊问题，细节将在第6章中讨论。**如果数据库总是以相同的顺序写入，则读取总是看到一致的序列，不会发生这种反常**。<u>**然而，在许多分布式数据库中，不同的分区独立运行，因此不存在全局写入顺序。这就导致当用户从数据库中读数据时，可能会看到数据库的某部分旧值和另一部分新值**</u>。

![复制延迟问题 - 图3](https://static.sitestack.cn/projects/ddia/img/fig5-5.png)

​	**一个解决方案是确保任何具有因果顺序关系的写入都交给一个分区来完成，但该方案真实实现效率会大打折扣**。现在有一些新的算法来显式地追踪事件因果关系，在本章稍后的"**Happened-before关系与并发**"继续该问题的探讨。

### 5.2.4 复制滞后的解决方案

​	使用最终一致性系统时，最好事先就思考这样的问题：如果复制延迟增加到几分钟甚至几小时，那么应用层的行为会是什么样子？如果答案是"没问题"，那没得说。但是，如果带来糟糕的用户体验，那么在设计系统时，就要考虑提供一个更强的一致性保证，比如**写后读**；<u>如果系统设计时假定是同步复制，但最终它事实上成为了异步复制，就可能会导致灾难性后果</u>。

​	正如前面所讨论的，<u>在应用层可以提供比底层数据库更强有力的保证。例如只在主节点上进行特定类型的读取，而代价则是，应用层代码中处理这些问题通常会非常复杂，且容易出错</u>。

​	如果应用程序开发人员不必担心这么多底层的复制问题，而是假定数据库"做正确的事情"，情况就变得很简单。而这也是事务存在的原因，事务是数据库提供更强保证的一种方式。

​	**单节点上支持事务已经非常成熟**。<u>然而，在转向分布式数据库(即支持复制和分区)的过程中，有许多系统却选择放弃支持事务，并声称事务在性能与可用性方面代价过高，然后断言在可扩展的分布式系统中最终的一致性是无法避免的终极选择</u>。关于这样的表述，首先它有一定道理，但情况远不是所说的那么简单，我们将在本书其余部分展开讨论，尝试形成一个更为深入的观点。例如在第7章和第9章将理解事务，然后在第三部分再介绍其他一些替代机制。

## 5.3 多主节点复制

​	到目前为止，我们只考虑了单个主节点的主从复制架构。主从复制方法较为常见，但也存在其他一些有趣的方案。

​	首先，**主从复制存在一个明显的缺点：系统只有一个主节点，而所有写入都必须经由主节点**。如果由于某种原因，例如与主节点之间的网络中断而导致主节点无法连接，主从复制方案就会影响所有的写入操作。

​	<u>对主从复制模型进行自然的扩展，则可以配置多个主节点，每个主节点都可以接受写操作，后面复制的流程类似：处理写的每个主节点都必须将该数据更改转发到所有其他节点。这就是多主节点(也称为主-主，或主动/主动)复制</u>。此时，**每个主节点还同时扮演其他主节点的从节点**。

### 5.3.1 适用场景

​	**在一个数据中心内部使用多主节点基本没有太大意义，其复杂性已经超过所能带来的好处**。但是，在以下场景这种配置则是合理的。

#### 多数据中心

​	为了容忍整个数据中心级别故障或者更接近用户，可以把数据库的副本横跨多个数据中心。而如果使用常规的基于主从的复制模型，主节点势必只能放在其中的某一个数据中心，而所有写请求都必须经过该数据中心。

​	有了多主节点复制模型，则可以在每个数据中心都配置主节点，如图5-6所示的基本架构。**在每个数据中心内，采用常规的主从复制方案；而在数据中心之间，由各个数据中心的主节点来负责同其他数据中心的主节点进行数据的交换、更新**。

![多主复制 - 图1](https://static.sitestack.cn/projects/ddia/img/fig5-6.png)

​	可以对比一下在多数据中心环境下，部署单主节点的主从复制方案与多主复制方案之间的差异：

+ 性能

  **对于主从复制，每个写请求都必须经由广域网传送至主节点所在的数据中心。这会大大增加写入延迟，并基本偏离了采用多数据中心的初衷(即就近访问)**。<u>**而在多主节点模型中，每个写操作都可以在本地数据中心快速响应，然后采用异步复制方式将变化同步到其他数据中心**</u>。因此，对上层应用有效屏蔽了数据中心之间的网络延迟，使得终端用户所体验到的性能更好。

+ 容忍数据中心失效

  对于主从复制，如果主节点所在的数据中心发生故障，必须切换至另一个数据中将其中的一个从节点被提升为主节点。在多主节点模型中，每个数据中心则可以独立于其他数据中心继续运行，发生故障的数据中心在恢复之后更新到最新状态。

+ 容忍网络问题

  数据中心之间的通信通常经由广域网，它往往不如数据中心内的本地网络可靠。对于主从复制模型，由于写请求是同步操作，对数据中心之间的网络性能和稳定性等更加依赖。多主节点模型则通常采用异步复制，可以更好地容忍此类问题，例如临时网络闪断不会妨碍写请求最终成功。

​	有些数据库已内嵌支持了多主复制，但有些则借助外部工具来实现，例如 MySQL的Tungsten Replicator， PostgreSQL的BDR以及Oracle的Golden Gate。

​	尽管多主复制具有上述优势，但也存在一个很大的缺点：**<u>不同的数据中心可能会同时修改相同的数据，因而必须解决潜在的写冲突(如图5-6中的“冲突解决”)</u>**。我们会在本章稍后的“处理写入冲突”详细介绍。

​	由于多主复制在许多数据库中还只是新增的高级功能，所以可能存在配置方面的细小缺陷；在与其他数据库功能(例如自增主键，触发器和完整性约束等)交互时有时会出现意想不到的副作用。出于这个原因，一些人认为多主复制比较危险，应该谨慎使用或者避免使用。

#### 离线客户端操作

​	另一种多主复制比较适合的场景是，应用在与网络断开后还需要继续工作。

​	比如手机，笔记本电脑和其他设备上的日历应用程序。无论设备当前是否联网，都需要能够随时查看当前的会议安排(对应于读请求)或者添加新的会议(对应于写请求)。在离线状态下进行的任何更改，会在下次设备上线时，与服务器以及其他设备同步。

​	这种情况下，<u>每个设备都有一个充当主节点的本地数据库(用来接受写请求)</u>，然后在所有设备之间采用异步方式同步这些多主节点上的副本，同步滞后可能是几小时或者数天，具体时间取决于设备何时可以再次联网。

​	从架构层面来看，上述设置基本上等同于数据中心之间的多主复制，只不过是个极端情况，即一个设备就是数据中心，而且它们之间的网络连接非常不可靠。多个设备同步日历的例子表明，多主节点可以得到想要的结果，但中间过程依然有很多的未知数。

​	有一些工具可以使多主配置更为容易，如CouchDB就是为这种操作模式而设计的。

#### 协作编辑

​	实时协作编辑应用程序允许多个用户同时编辑文档。例如， Etherpad 和 Google Docs允许多人同时编辑文本文档或电子表格(算法简要会在本章后面的“自动冲突解决”中讨论)。

​	我们通常不会将协作编辑完全等价于数据库复制问题，但二者确实有很多相似之处。当一个用户编辑文档时，所做的更改会立即应用到本地副本(Web浏览器或客户端应用程序)，然后异步复制到服务器以及编辑同一文档的其他用户。

​	如果要确保不会发生编辑冲突，则应用程序必须先将文档锁定，然后才能对其进行编辑。如<u>果另一个用户想要编辑同一个文档，首先必须等到第一个用户提交修改并释放锁。这种协作模式相当于主从复制模型下在主节点上执行事务操作</u>。

​	为了加快协作编辑的效率，可编辑的粒度需要非常小。例如，单个按键甚至是全程无锁。然而另一方面，也会面临所有多主复制都存在的挑战，即如何解决冲突。

### 5.3.2 处理写冲突

​	多主复制的最大冋题是可能发生写冲突，这意味着必须有方案来解决冲突。

​	例如，两个用户同时编辑Wiki页面，如图5-7所示。用户1将页面的标题从A更改为B，与此同时用户2却将标题从A改为C。每个用户的更改都顺利地提交到本地主节点。但是，当更改被异步复制到对方时，却发现存在冲突。注意，正常情况下的主从复制则不会出现这种情况。

![多主复制 - 图2](https://static.sitestack.cn/projects/ddia/img/fig5-7.png)

#### 同步与异步冲突检测

​	**如果是主从复制数据库，第二个写请求要么会被阻塞直到第一个写完成，要么被中止(用户必须重试)**。<u>然而在多主节点的复制模型下，这两个写请求都是成功的，并且只能在稍后的时间点上才能异步检测到冲突，那时再要求用户层来解决冲突为时已晚</u>。

​	理论上，也可以做到同步冲突检测，即等待写请求完成对所有副本的同步，然后再通知用户写入成功。但是，这样做将会失去多主节点的主要优势：允许每个主节点独立接受写请求。<u>如果确实想要同步方式冲突检测，或许应该考虑采用**单主节点**的主从复制模型</u>。

#### 避免冲突

​	处理冲突最理想的策略是避免发生冲突，即如果应用层可以保证对特定记录的写请求总是通过同一个主节点，这样就不会发生写冲突。**现实中，由于不少多主节点复制模型所实现的冲突解决方案存在瑕疵，因此，<u>避免冲突反而成为大家普遍推荐的首选方案</u>**。

​	<u>例如，一个应用系统中，用户需要更新自己的数据，那么我们确保**特定用户的更新请求总是路由到特定的数据中心**，并在该数据中心的主节点上进行读/写。不同的用户则可能对应不同的主数据中心(例如根据用户的地理位置来选择)。从用户的角度来看，这基本等价于主从复制模型</u>。

​	但是，有时可能需要改变事先指定的主节点，例如由于该数据中心发生故障，不得不将流量重新路由到其他数据中心，或者是因为用户已经漫游到另一个位置，因而更靠近新数据中心。此时，冲突避免方式不再有效，必须有措施来处理同时写入冲突的可能性。

#### * 收敛于一致状态

​	**对于主从复制模型，数据更新符合顺序性原则**，即<u>**如果同一个字段有多个更新，则最后一个写操作将决定该字段的最终值**</u>。

​	对于多主节点复制模型，由于不存在这样的写入顺序，所以最终值也会变得不确定。在图5-7中，主节点1接受到请求把标题更新为B，然后更新为C；而在主节点2，则是相反的更新顺序。两者都无法辩驳谁更正确。

​	**<u>如果每个副本都只是按照它所看到写入的顺序执行，那么数据库最终将处于不一致状态</u>**。例如主节点1看到最终值C，而主节点2看到的是B，这绝对是不可接受的，**<u>所有的复制模型至少应该确保数据在所有副本中最终状态一定是一致的</u>**。因此，数据库必须以一种收敛趋同的方式来解决冲突，这也意味着当所有更改最终被复制、同步之后，所有副本的最终值是相同的。

​	实现收敛的冲突解决有以下可能的方式：

+ 给每个写入分配唯一的ID，例如，一个时间戳，一个足够长的随机数，一个UUID或者一个基于键-值的哈希，挑选最高ID的写入作为胜利者，并将其他写入丢弃。**如果基于时间戳，这种技术被称为最后写入者获胜**。虽然这种方法很流行但是很容易造成数据丢失。我们将在本章最后部分来详细解释。
+ 为每个副本分配一个唯一的ID，并制定规则，例如<u>序号高的副本写入始终优先于序号低的副本</u>。这种方法也可能会导致数据丢失。
+ 以某种方式将这些值合并在一起。例如，按字母顺序排序，然后拼接在一起(图5-7中，合并的标题可能类似于“B/C”)。
+ 利用预定义好的格式来记录和保留冲突相关的所有信息，然后依靠应用层的逻辑，事后解决冲突(可能会提示用户)。

#### * 自定义冲突解决逻辑

​	<u>解决冲突最合适的方式可能还是依靠应用层</u>，所以大多数多主节点复制模型都有工具来让用户编写应用代码来解决冲突。可以在写入时或在读取时执行这些代码逻辑：

+ 在写入时执行

  **只要数据库系统在复制变更日志时检测到冲突，就会调用应用层的冲突处理程序**。例如，Bucardo支持编写一段Perl代码。这个处理程序通常不能在线提示用户，而只能在后台运行，这样速度更快。

+ 在读取时执行

  **<u>当检测到冲突时，所有冲突写入值都会暂时保存下来</u>。下一次读取数据时，会将数据的多个版本读返回给应用层。应用层可能会提示用户或自动解决冲突，并将最后的结果返回到数据库**。 CouchDB采用了这样的处理方式。

​	**<u>注意，冲突解决通常用于单个行或文档，而不是整个事务</u>**。因此，**<u>如果有一个原子事务包含多个不同写请求(如第7章)，每个写请求仍然是分开考虑来解决冲突</u>**。

#### 什么是冲突？

​	有些冲突是显而易见的。在图5-7的例子中，两个写操作同时修改同一个记录中的同一个字段，并将其设置为不同的值。毫无疑问，这就是一个冲突。

​	而其他类型的冲突可能会非常微妙，更难以发现。例如一个会议室预订系统，它主要记录哪个房间由哪个人在哪个时间段所预订。这个应用程序需要确保每个房间只能有一组人同时预定(即不得有相同房间的重复预订)。如果为同一个房间创建两个不同的预订，可能会发生冲突。<u>尽管应用在预订时会检查房间是否可用，但如果两个预订是在两个不同的主节点上进行，则还是存在冲突的可能</u>。

> **自动冲突解决**
>
> ​	冲突解决的规则可能会变得越来越复杂，且自定义代码很容易出错。亚马逊是个经常被引用的反面例子：有一段时间，购物的冲突解决逻辑依靠用户的购物车页面，后者保存了所有的物品，但顾客有时候会发现之前已经被拿掉的商品，再次出现在他们的购物车中。
>
> ​	有一些有意思的研究尝试自动解决并发修改所引起的冲突。下面这些方法值得一看：
>
> 1. **无冲突的复制数据类型(Conflict-free Replicated Datatypes，CRDT)**。CRDT是可以由多个用户同时编辑的数据结构，包括map、 ordered list、计数器等，并且以内置的合理方式自动地解决冲突。一些CRDT已经在Riak2.0中得以具体实现。
> 2. **可合并的持久数据结构(Mergeable persistent data)。它跟踪变更历史，类似于Git版本控制系统，并提出三向合并功能(three-way merge function，CRDT采用双向合并)**。
> 3. **操作转换(Operational transformation)** 。它是Etherpad和Google Docs等协作编辑应用背后的冲突解决算法。专为可同时编辑的有序列表而设计，如文本文档的字符列表。
>
> ​	这些算法总体来讲还处于早期阶段，但将来它们可能会被整合到更多的数据系统中。这些自动冲突解决方案可以使主复制模型更简单、更容易被应用程序来集成。

​	很遗憾，这里没有现成的答案。通过接下来的几章，我们将对这个问题进行深入的剖析和讲解。我们将在第7章看到更多的冲突示例，在第12章中讨论检测和解决冲突的可扩展方法。

### 5.3.3 拓扑结构

​	**复制的拓扑结构描述了写请求从一个节点的传播到其他节点的通信路径**。如果有两个主节点，如图5-7所示，则只存在一个合理的拓扑结构：主节点1必须把所有的写同步到主节点2，反之亦然。但如果存在两个以上的主节点，则会有多个可能的同步拓扑结构，如图5-8所示。

![多主复制 - 图3](https://static.sitestack.cn/projects/ddia/img/fig5-8.png)

​	**最常见的拓扑结构是全部-至-全部，见图5-8(c)，每个主节点将其写入同步到其他所有主节点**。而其他一些拓扑结构也有普遍使用，例如，<u>**默认情况下MySQL只支持环形拓扑结构**，其中的每个节点接收来自前序节点的写入，并将这些写入(加上自己的写入)转发给后序节点</u>。另一种流行的拓扑是星形结构：一个指定的根节点将写入转发给所有其他节点。星形拓扑还可以推广到树状结构。

​	在环形和星形拓扑中，写请求需要通过多个节点才能到达所有的副本，即中间节点需要转发从其他节点收到的数据变更。<u>为防止无限循环，每个节点需要赋予一个唯一的标识符，在复制日志中的每个写请求都标记了已通过的节点标识符</u>。如果某个节点收到了包含自身标识符的数据更改，表明该请求已经被处理过，因此会忽略此变更请求，避免重复转发。

​	**环形和星形拓扑的问题是，如果某一个节点发生了故障，在修复之前，会影响其他节点之间复制日志的转发**。可以采用重新配置拓扑结构的方法暂时排除掉故障节点。在大多数部署中，这种重新配置必须手动完成。而对于链接更密集的拓扑(如全部到全部)，消息可以沿着不同的路径传播，避免了单点故障，因而有更好的容错性。

​	但另一方面，**<u>全链接拓扑也存在一些自身的问题。主要是存在某些网络链路比其他链路更快的情况(例如由于不同网络拥塞)，从而导致复制日志之间的覆盖</u>**，如图5-9所示。

​	在图5-9中，客户端A向主节点1的表中首先插入一行，然后客户端B在主节点3上对该行记录进行更新。而在主节点2上，由于网络原因可能出现意外的写日志复制顺序，例如它先接收到了主节点3的更新日志(从主节点2的角度来看，这是对数据库中不存在行的更新操作)，之后才接收到主节点1的插入日志(按道理应该在更新日志之前到达)。

![多主复制 - 图4](https://static.sitestack.cn/projects/ddia/img/fig5-9.png)

​	<u>这里涉及到一个因果关系问题，类似于在本章前面“**前缀一致读**”所看到的：更新操乍一定是依赖于先前完成的插入，因此我们要确保所有节点上一定先接收插入日志，然后再处理更新。在每笔写日志里简单地添加时间戳还不够，**主要因为无法确保时钟完全同步**，因而无法在主节点2上正确地排序所收到日志(参见第8章)</u>。

​	为了使得日志消息正确有序，可以使用一种称为**版本向量**的技术，本章稍后将讨论这种技术(参见本章后面的“检测并发写入”)。需要指出，<u>冲突检测技术在许多多主节点复制系统中的实现还不够完善</u>，例如在撰写本书时， PostgreSQL BDR尚不支持写操作的因果排序，而 MySQL的Tungsten Replicator甚至还没有基本的冲突检测功能。

​	如果正在使用支持多主节点复制的系统，这些问题都值得注意，仔细查阅相关文档，详细测试这些数据库，以确保它确实提供所期望的功能。

## 5.4 无主节点复制

​	到目前为止本章所讨论的复制方法，包括单主节点和多主节点复制，都是基于这样种核心思路，<u>即客户端先向某个节点(主节点)发送写请求，然后数据库系统负责将写请求复制到其他副本。由主节点决定写操作的顺序，从节点按照相同的顺序来应用主节点所发送的写日志</u>。

​	**一些数据存储系统则采用了不同的设计思路：选择放弃主节点，允许任何副本直接接受来自客户端的写请求**。其实最早的数据复制系统就是无主节点的(或称为**去中心复制**，**无中心复制**)，但后来到了关系数据库主导的时代，这个想法被大家选择性遗忘了。当亚马逊内部采用了Dynamo系统之后，无主复制又再次成为一种时髦的数据库架构。Riak、Cassandra和Voldemort都是受Dynamo启发而设计的无主节点、开源数据库系统，这类数据库也被称为Dynamo风格数据库。

​	**对于某些无主节点系统实现，客户端直接将其写请求发送到多副本，而在其他一些实现中，由一个协调者节点代表客户端进行写入，但与主节点的数据库不同，协调者并不负责写入顺序的维护**。我们很快就会看到，这种设计上的差异对数据库的使用方式有着深刻的影响。

### * 5.4.1 节点失效时写入数据库

​	<u>**假设一个三副本数据库，其中一个副本当前不可用(例如正在重启以安装系统更新)。在基于主节点复制模型下，如果要继续处理写操作，则需要执行切换操作**(参阅本章前面的“处理节点失效”)</u>。

​	**对于无主节点配置，则不存在这样的切换操作**。图5-10展示了所发生的情况：用户1234将写请求并行发送到三个副本，有两个可用副本接受写请求，而不可用的副本无法处理该写请求。如果假定三个副本中有两个成功确认写操作，用户1234收到两个确认的回复之后，即可认为写入成功。客户完全可以忽略其中一个副本无法写入的情况。

​	<u>现在设想一下，失效的节点之后重新上线，而客户端又开始从中读取內容。由于节点失效期间发生的任何写入在该节点上都尚未同步，因此读取可能会得到过期的数据</u>。

​	<u>为了解决这个问题，当一个客户端从数据库中读取数据时，它**不是向一个副本发送请求，而是并行地发送到多个副本**。客户端可能会得到不同节点的不同响应，包括某些节点的新值和某些节点的旧值。可以采用**版本号**技术确定哪个值更新</u>(参见本章后面的“检测并发写入”)。

#### * 读修复和反熵

​	**复制模型应确保所有数据最终复制到所有的副本**。当一个失效的节点重新上线之后，它如何赶上中间错过的那些写请求呢?

![无主复制 - 图1](https://static.sitestack.cn/projects/ddia/img/fig5-10.png)

Dynamo风格的数据存储系统经常使用以下两种机制：

+ 读修复

  当客户端并行读取多个副本时，可以检测到过期的返回值。例如，在图5-10中用户2345从副本3获得的是版本6，而从副本1和2得到的是版本7。客户端可以判断副本3一个过期值，然后将新值写入到该副本。这种方法主要**适合那些被频繁读取的场景**。

+ **反熵过程**

  此外，一些数据存储有后台进程不断查找副本之间数据的差异，将任何缺少的数据从一个副本复制到另一个副本。**与基于主节点复制的复制日志不同，此反熵过程并不保证以特定的顺序复制写入，并且会引入明显的同步滞后**。

​	并不是所有的系统都实现了上述两种方案。例如， Voldemort目前没有反熵过程。<u>请注意，当缺少反熵过程的支持时，由于**读时修复只在发生读取时才可能执行修复，那些很少访问的数据有可能在某些副本中已经丢失而无法检测到，从而降低了写的持久性**</u>。

#### * 读写quorum

​	图5-10的例子中，三个副本中如果有两个以上完成处理，写入即可认为成功。如果三个副本中只有一个完成了写请求，会怎样呢？依次类推，究竟多少个副本完成才可以认为写成功?

​	我们知道，<u>成功的写操作要求三个副本中至少两个完成，这意味着至多有一个副本可能包含旧值</u>。<u>因此，**在读取时需要至少向两个副本发起读请求，通过版本号可以确定定至少有一个包含新值**。如果第三个副本出现停机或响应缓慢，则读取仍可以继续并返回最新值</u>。

​	把上述道理推广到一般情况，**<u>如果有n个副本，写入需要w个节点确认，读取必须至少查询r个节点，则只要w+r>n，读取的节点中一定会包含最新值</u>**。例如在前面的例子中，n=3，w=2，r=2。满足上述这些r、w值的读/写操作称之为法定票数读(或仲裁读)或法定票数写(或仲裁写)。**也可以认为r和w是用于判定读、写是否有效的最低票数**。

​	**在Dynamo风格的数据库中，参数n、w和r通常是可配置的**。一个常见的选择是设置n为某奇数(通常为3或5)，w=r=(n+1)/2(向上舍入)。也可以根据自己的需求灵活调整这些配置。<u>例如，对于读多写少的负载，设置w=n和r=1比较合适，这样读取速度更快，但是一个失效的节点就会使得数据库所有写入因无法完成quorum而失败</u>。

> ​	集群中可能存在多于n个节点，但是数据只会保存在所设定的n个节点上。我们可以对数据集进行**分区**，从而支持比节点容纳上限更大的数据集，第6章我们将讨论分区技术。

​	**仲裁条件w+r>n定义了系统可容忍的失效节点数**，如下所示：

+ 当w<n，如果一个节点不可用，仍然可以处理写入。
+ 当r<n，如果一个节点不可用，仍然可以处理读取。
+ 假定n=3，w=2，r=2，则可以容忍一个不可用的节点。
+ 假定n=5，w=3，r=3，则可以容忍两个不可用的节点。如图5-11所示。
+ **<u>通常，读取和写入操作总是并行发送到所有的n个副本。参数w和参数r只是决定要等待的节点数</u>**。即有多少个节点需要返回结果，我们才能判断出结果的正确性。

​	**<u>如果可用节点数小于所需的w或r，则写入或读取就会返回错误</u>**。不可用的原因可能有很多种，包括节点崩溃或者断电而关机，执行操作时岀错(例如磁盘已满而无法写入)，客户端和节点之间的网络中断等。这里，我们只需关心节点是否有返回值，而不需区分出错的具体原因。

![无主复制 - 图2](https://static.sitestack.cn/projects/ddia/img/fig5-11.png)

### * 5.4.2 Quorum 一致性的局限性

​	<u>**如果有n个副本，并且配置w和r，使得w+r>n，可以预期可以读取到一个最新值。之所以这样，是因为成功写入的节点集合和读取的节点集合必然有重合，这样读取的节点中至少有一个具有最新值**</u>(见图5-11)。

​	<u>**通常，设定r和w为简单多数(多于n/2)节点，即可确保w+r>n，且同时容忍多达n/2个节点故障**</u>。但是， quorum不一定非得是多数，**<u>读和写的节点集中有一个重叠的节点才是最关键的</u>**。设定其他的 quorum分配数也是可行的。

​	<u>也可以将w和r设置为较小的数字，从而w+r<=n(即不满足仲裁条件)。此时，读取和写入操作仍会被发送到n个节点，但只需等待更少的节点回应即可返回</u>。

​	由于w和r配置的节点数较小，读取请求当中可能恰好没有包含新值的节点，因此最终可能会返回一个过期的旧值。好的一方面是，这种配置可以获得更低的延迟和更高的可用性，例如网络中断，许多副本变得无法访问，相比而言有更高的概率继续处理读取和写入。只有当可用的副本数已经低于w或r时，数据库才会变得无法读/写，即处于不可用状态。

​	**即使在w+r>n的情况下，也可能存在返回旧值的边界条件**。这主要取决于具体实现，可能的情况包括:

+ 如果采用了sloppy quorum(参阅本章后面的"宽松的quorum与数据回传")，写操作的w节点和读取的r节点可能完全不同，因此无法保证读写请求一定存在重叠的节点。

+ 如果两个写操作同时发生，则无法明确先后顺序。这种情况下，唯一安全的解决方案是合并并发写入(参见本章前面的“处理写冲突”)。如果根据时间戳(最后写入获胜)挑选胜者，则由于时钟偏差问题，某些写入可能会被错误地抛弃。
+ **如果写操作与读操作同时发生，写操作可能仅在一部分副本上完成。此时，读取时返回旧值还是新值存在不确定性**。
+ **如果某些副本上已经写入成功，而其他一些副本发生写入失败(例如磁盘巳满)，且总的成功副本数少于w，那些已成功的副木上不会做回滚**。这意味着尽管这样的写操作被视为失败，后续的读操作仍可能返回新值。
+ **<u>如果具有新值的节点后来发生失效，但恢复数据来自某个旧值，则总的新值副本数会低于w，这就打破了之前的判定条件</u>**。
+ 即使一切工作正常，也会出现一些边界情况，如第9章所介绍的“可线性化与quorun“。

​	**<u>因此，虽然quorum设计上似乎可以保证读取最新值，但现实情况却往往更加复杂</u>**。<u>Dynamo风格的数据库通常是针对最终一致性场景而优化的。我们建议最妤不要把参数w和r视为绝对的保证，而是一种灵活可调的读取新值的概率。</u>

​	例如，<u>这里通常无法得到本章前面的“复制滞后问题”中所罗列的一致性保证，包括**写后读**、**单调读**、**前缀一致读**等，因此前面讨论种种异常同样会发生在这里</u>。**如果确实需要更强的保证，需要考虑事务与共识问题**，接下来的第7章和第9章将对此展开讨论。

#### * 监控旧值

​	从运维角度来看，监视数据库是否返回最新结果非常重要。即使应用程序可以容忍读取旧值，也需要仔细了解复制的当前运行状态。<u>如果已经出现了明显的滞后，它就是个重要的信号提醒我们需要采取必要措施来排查原因</u>(例如网络问题或节点超负荷)。

​	对于主从复制的系统，数据库通常会导出复制滞后的相关指标，可以将其集成到统一监控模块。原理大概是这样，**由于主节点和从节点上写入都遵从相同的顺序，而毎个节点都维护了复制日志执行的当前偏移量。<u>通过对比主节点和从节点当前偏移量的差值，即可衡量该从节点落后于主节点的程度</u>**。

​	然而，<u>对于无主节点复制的系统，并**没有固定的写入顺序**，因而监控就变得更加困难</u>。而且，<u>如果数据库只支持读时修复(不支持反熵)，那么旧值的落后就没有一个上限。例如如果一个值很少被访问，那么所返回的旧值可能非常之古老</u>。

​	目前针对无主节点复制系统已经有一些研究，根据参数n，w和r来预测读到旧值的期望百分比。不过，总体讲还不是很普及。即便如此，**将旧值监控纳入到数据库标准指标集中还是很有必要**。要知道，**最终一致性其实是个非常模糊的保证，从可操作性上讲，量化究竟何为“最终”很有实际价值**。

#### * 宽松的quorum与数据回传

​	<u>配置适当quorum的数据库系统可以容忍某些节点故障，也不需要执行故障切换。它们还可以容忍某些节点变慢，这是因为请求并不需要等待所有n个节点的响应，只需w或r节点响应即可。对于需要高可用和低延迟的场景来说，还可以容忍偶尔读取旧值，所有这些特性使之具有很高的吸引力</u>。

​	但是， quorum并不总如期待的那样提供高容错能力。一个网络中断可以很容易切断一个客户端到多数数据库节点的链接。尽管这些集群节点是活着的，而且其他客户端也确实可以正常链接，但是**对于断掉链接的客户端来讲，情况无疑等价于集群整体失效**。这种情况下，很可能无法满足最低的w和r所要求的节点数，因此导致客户端无法满足quorum要求。

​	在一个大规模集群中(节点数远大于n个)，<u>客户可能在网络中断期间还能连接到某些数据库节点，但这些节点又不是能够满足数据仲裁的那些节点</u>。此时，数据库设计者就面临着一个选择：

+ 如果无法达到w或r所要求quorum，将错误明确地返回给客户端?
+ 或者，我们是否应该接受该写请求，只是将它们暂时写入一些可访问的节点中？注意，<u>这些节点并不在n个节点集合中</u>。

​	**后一种方案称之为放松的仲裁：写入和读取仍然需要w和r个成功的响应，但包含了那些并不在先前指定的n个节点**。打个比方，如果你把不小心把自己锁在房子外面，可能会敲开邻居家的门，请求否可以坐在沙发上暂时休息一下。

​	**一旦网络问题得到解决，临时节点需要把接收到的写入全部发送到原始主节点上**。<u>这就是所谓的数据回传(或暗示移交)</u>。即一旦你找到了房子的钥匙，你的邻居会礼貌地请你离开沙发回到自己的家中。

​	可以看出， **<u>sloppy quorum对于提高写入可用性特别有用：只要有任何w个节点可用，数据库就可以接受新的写入</u>**。<u>然而这意味着，即使满足w+r>n，也不能保证在读取某个键时，一定能读到最新值，因为**新值可能被临时写入n之外的某些节点且尚未回传过来**</u>。

​	因此，**sloppy quorum并非传统意义上quorum。而更像是为了数据持久性而设计的一个保证措施，除非回传结束，否则它无法保证客户端一定能从r个节点读到新值**。

​	**<u>目前，所有Dynamo风格的系统都已经支持sloppy quorum</u>**。在Riak中，默认启用，而在 Cassandra和 Voldemort中则默认关闭。

#### 多数据中心操作

​	我们之前以多数据中心为例介绍了多主节点复制(参见本章前面的“多主节点复制”)。而<u>无主节点复制由于旨在更好地容忍并发写入冲突</u>，网络中断和延迟尖峰等，因此也可适用于多数据中心操作。

​	Cassandra和Voldemort在其默认配置的无主节点模型中都支持跨数据中心操作：<u>副本的数量n是包含所有数据中心的节点总数</u>。配置时，可以指定每个数据中心各有多少副本。每个客户端的写入都会发送到所有副本，但客户端通常只会等待来自本地数据中心内的quorum节点数的确认，这样避免了高延迟和跨数据中心可能的网络异常。<u>尽管可以灵活配置，但对远程数据中心的写入由于延迟很高，通常都被配置为**异步方式**</u>。

​	Riak则将客户端与数据库节点之间的通信限制在一个数据中心内，因此<u>n描述的是以个数据中心内的副本数量</u>。<u>集群之间跨数据中心的复制则在后台异步运行，类似于多主节点复制风格</u>。

### * 5.4.3 检测并发写

​	Dynamo风格的数据库允许多个客户端对相同的主键同时发起写操作，即使采用严格的quorum机制也可能会发生写冲突。这与多主节复制类似(参见本章前面的“处理写冲突”)，此外，由于读时修复或者数据回传也会导致并发写冲突。

​	一个核心问题是，由于网络延迟不稳定或者局部失效，请求在不同的节点上可能会呈现不同的顺序。如图5-12所示，对于包含三个节点的数据系统，客户端A和B同时向主键X发起写请求：

+ 节点1收到来自客户端A的写请求，但由于节点失效，没有收到客户端B的写请求。

+ 节点2首先收到A的写请求，然后是B的写请求。
+ 与节点2相反，节点3首先收到B的写请求，然后是A的写请求

![无主复制 - 图3](https://static.sitestack.cn/projects/ddia/img/fig5-12.png)

​	**如果节点每当收到新的写请求时就简单地覆盖原有的主键，那么这些节点将永久无法达成一致**，如图5-12中的所示，节点2认为X的最终值是B，而其他节点认为值是A。

​	我们知道副本应该收敛于相同的内容，这样才能达成最终一致。但如何才能做到呢？有人可能希望数据副本之间能自动处理，然而非常不幸，<u>目前大多数的系统实现都无法令人满意，如果你不想丢失数据，应用开发者必须了解很多关于数据库内部冲突处理的机制</u>。

​	我们已经在本章前面的“处理写冲突”简要介绍了一些解决冲突的技巧。在总结本章之前，我们来更详细地探讨这个问题。

#### 最后写入者获胜(丢弃并发写入)

​	**一种实现最终收敛的方法是，每个副本总是保存最新值，允许覆盖并丢弃旧值**。那么，<u>假定每个写请求都最终同步到所有副本，只要我们有一个明确的方法来确定哪个写入是最新的，则副本可以最终收敛到相同的值</u>。

​	这个想法其实有些争议，**关键点在于前面所提到关于如何定义“最新”**。在图5-12的例子中，当客户端向数据库节点发送写请求时，一个客户端无法意识到另一个客户端，也不清楚哪一个先发生。其实，争辩哪个先发生没有太大意义，当我们说支持写入并发，也就意味着它们的顺序是不确定的。

​	**即使无法确定写请求的“自然顺序”，我们可以强制对其排序**。例如，<u>为每个写请求附加一个时间戳，然后选择最新即最大的时间戳，丢弃较早时间戳的写入。这个冲突解决算法被称为**最后写入者获胜(last write wins，LWW)**</u>，它是Cassandra仅有的冲突解决方法，而在Riak中，它是可选方案之一。

​	**LWW可以实现最终收敛的目标，但是以牺牲数据持久性为代价**。<u>如果同一个主键有多个并发写，即使这些并发写都向客户端报告成功(因为完成了写入w个副本)，但**最后只有一个写入值会存活下来**，其他的将被系统默默丢弃。此外，LWW甚至可能会删除那些非并发写，我们将在第8章“时间戳与事件顺序”中举例说明</u>。

​	在一些场景如缓存系统，覆盖写是可以接受的。如果覆盖、丟失数据不可接受，则LWW并不是解决冲突很好的选择。

​	**要确保LWW安全无副作用的唯一方法是，<u>只写入一次然后写入值视为不可变</u>，这样就避免了对同一个主键的并发(覆盖)写**。例如， Cassandra的一个推荐使用方法就是采用UUID作为主键，这样每个写操作都针对的不同的、系统唯一的主键。

#### * Happens-before关系和并发

​	如何判断两个操作是否是并发呢？首先为了建立起一个快速的直觉判断，我们先来看一些例子：

+ 图5-9中，两个写入不是并发的：A的插入操作发生在B的增量修改之前，B的递增是基于A插入的行。换句话说，B后发生，其操作建立在A基础之上。A和B属于**因果依赖**关系。
+ 另一个例子，图5-12中的两个写入则是并发的:每个客户端启动写操作时，并不知道另一个客户端是否也在同一个主键上执行操作。因此，操作之间不存在**因果关系**。

​	如果B知道A，或者依赖于A，或者以某种方式在A基础上构建，则称操作A在操作B之前发生。这是定义何为并发的关键。事实上，我们也可以简单地说，**如果两个操作都不在另一个之前发生，那么操作是并发的(或者两者都不知道对方)**。

​	因此，对于两个操作A和B，一共存在三种可能性：A在B之前发生，或者B在A之前发生，或者A和B并发。我们需要的是一个算法来判定两个操作是否并发。<u>如果一个操作发生在另一个操作之前，则后面的操作可以覆盖较早的操作。如果属于并发，就需要解决潜在的冲突问题</u>。

> **并发性、时间和相对性**
>
> ​	<u>通常如果两个操作“同时”发生，则称之为并发，然而事实上，操作是否在时间上重叠并不重要。**由于分布式系统中复杂的时钟同步问题(第8章将会详细讨论)，现实当中，我们很难严格确定它们是否同时发生**</u>。
>
> ​	**<u>为更好地定义并发性，我们并不依赖确切的发生时间，即不管物理的时机如何，如果两个操作并不需要意识到对方，我们即可声称它们是并发操作</u>**。一些人尝试把这个思路与物理学中狭义相对论联系起来，后者引入了“信息传递不能超越光速”的假定，如果两个事件发生的间隔短于光在它们之间的折返，那么这两个事件不可能有相互影响，因此就是并发。
>
> ​	**在计算机系统中，即使光速快到允许一个操作影响到另一个操作，但两个操作仍可能被定义为并发。例如，发生了网络拥塞或中断，可能就会出现两个操作由于网络问题导致一个操作无法感知另一个，因此二者成为并发**。

#### * 确定前后关系

​	我们来看一个确定操作并发性的算法，即两个操作究竟属于并发还是一个发生在另个之前(依赖关系)。简单起见，我们先从只有一个副本的数据库开始，在阐明其原理之后，将其推广到有多个副本的无主节点数据库。

​	图5-13的例子是两个客户端同时向购物篮车加商品(如果觉得这个例子太微不足道，可以类比为，两个空中交管员同时把飞机添加到他们所管理的追踪目标里)。初始时购物车为空。然后两个客户端向数据库共发出五次写入操作:

1. 客户端1首先将牛奶加入购物车。这是第一次写入该主键的值，服务器保存成功然后分配版本1，服务器将值与版本号一起返回给该客户端1。
2. 客户端2将鸡蛋加入购物车，此时它并不知道客户端1已添加了牛奶，而是认为鸡蛋是购物车中的唯一物品。服务器为此写入并分配版本2，然后将鸡蛋和牛奶存储为两个单独的值，最好将这两个值与版本号2返回给客户端2。
3. 同理，客户端1也并不意识上述步骤2，想要将面粉加入购物车，且以为购物车的内容应该是[牛奶，面粉]，将此值与版本号1一起发送到服务器。服务器可以从版本号中知道[牛奶，面粉]的新值要取代先前值[牛奶]，但值[鸡蛋]则是新的并发操作。因此，服务器将版本3分配给[牛奶，面粉]并覆盖版本1的[牛奶]，同时保留版本2的值[鸡蛋]，将二者同时返回给客户端1。

![无主复制 - 图4](https://static.sitestack.cn/projects/ddia/img/fig5-13.png)

4. 同时，客户端2想要加入火腿，也不知道客户端1刚刚加了面粉。其在最后一个响应中从服务器收到的两个值是[牛奶]和[蛋]，现在合并这些值，并添加火腿形成一个新的值[鸡蛋，牛奶，火腿]。它将该值与前一个版本号2一起发送到服务器。服务器检测到版本2会覆盖[鸡蛋]，但与[牛奶，面粉]是同时发生，所以设置为版本4并将所有这些值发送给客户端2。
5. 最后，客户端1想要加培根。它以前在版本3中从服务器接收[牛奶，面粉]和[鸡蛋]，所以合并这些值，添加培根，并将最终值[牛奶，面粉，鸡蛋，培根]连同版本号3来覆盖[牛奶，面粉]，但与[鸡蛋，牛奶，火腿]并发，所以服务器会保留这些并发值。

​	图5-13操作之间的数据流可以通过图5-14形象展示。箭头表示某个操作发生在另一个操作之前，即后面的操作“知道”或是“依赖”于前面的操作。<u>在这个例子中，因为总有另一个操作同时进行，所以每个客户端都没有时时刻刻和服务器上的数据保持同步。但是，新版本值最终会覆盖旧值，且不会发生已写入值的丢失</u>。

![无主复制 - 图5](https://static.sitestack.cn/projects/ddia/img/fig5-14.png)

​	需要注意的是，服务器判断操作是否并发的依据主要依靠**对比版本号**，而并不需要解释新旧值本身(值可以是任何数据结构)。算法的工作流程如下：

+ 服务器为每个主键维护一个版本号，每当主键新值写入时递增版本号，并将新版本号与写入的值一起保存。
+ <u>当客户端读取主键时，服务器将返回所有(未被覆盖的)当前值以及最新的版本号。**且要求写之前，客户必须先发送读请求**</u>。
+ **客户端写主键，写请求必须包含之前读到的版本号、读到的值和新值合并后的集合**。写请求的响应可以像读操作一样，会返回所有当前值，这样就可以像购物车例子那样一步步链接起多个写入的值。
+ **当服务器收到带有特定版本号的写入时，覆盖该版本号或更低版本的所有值(因为知道这些值已经被合并到新传入的值集合中)，但<u>必须保存更高版本号的所有值(因为这些值与当前的写操作属于并发)</u>**。

​	**<u>当写请求包含了前一次读取的版本号时，意味着修改的是基于以前的状态。如果一个写请求没有包含版本号，它将与所有其他写入同时进行，不会覆盖任何已有值，其传入的值将包含在后续读请求的返回值列表当中</u>**。

#### * 合并同时写入的值

​	上述算法可以保证不会发生数据丢弃，但不幸的是，客户端需要做一些额外的工作：即如果多个操作并发发生，则<u>客户端必须通过**合并并发写入的值**来继承旧值</u>。Riak称这些并发值为**兄弟关系**。

​	合并本质上与先前讨论的多节点复制冲突解决类似(参阅本章前面的“处理写冲突”)。一个简单的方法是基于版本号或时间戳(即最后写入获胜)来选择其中的一个值，但这意味着会丢失部分数据。所以，需要在应用程序代码中额外些工作。

​	以购物车为例，合并并发值的合理方式是包含新值和旧值(union操作)。图5-14中，两个客户端最后的值分别是[牛奶，面粉，鸡蛋，熏肉]和[鸡蛋，牛奶，火腿]。注意，虽然牛奶和鸡蛋只是写入了一次，但它在两个客户端中均有出现。合并的最终值应该是[牛奶，面粉，鸡蛋，培根，火腿]，其中去掉了重复值。

​	<u>然而，设想一下人们也可以在购物车中删除商品，此时把并发值都合并起来可能会导致错误的结果：**如果合并了两个客户端的值，且其中有一个商品被某客户端删除掉，则被删除的项目会再次出现在合并的终值中**</u>。<u>为了防止该问题，项目在删除时不能简单地从数据库中删除，**系统必须保留一个对应的版本号以恰当的标记该项目需要在合并时被剔除。这种删除标记被称为墓碑**(之前我们在第3章“哈希索引”日志压缩时提到过)</u>。

​	<u>考虑到在应用代码中合并非常复杂且容易出错，因此可以设计一些专门的数据结构来自动执行合并，例如，Riak支持称为CRDT一系列数据结构(具体参见本章前面的“自动冲突解决”)，以合理的方式高效自动合并，包括支持删除标记</u>。

#### * 版本矢量

​	图5-13中的示例只有一个副本。如果存在多个副本但没有主节点，算法又该如何呢?

​	图5-13使用单个版本号来捕获操作之间的依赖关系，<u>当多个副本同时接受写入时，这是不够的。因此我们需要为每个副本和每个主键均定义一个版本号。每个副本在处理写入时增加自己的版本号，并且跟踪从其他副本看到的版本号。通过这些信息来指示要覆盖哪些值、该保留哪些并发值</u>。

​	<u>所有副本的版本号集合称为**版本矢量**</u>。这种思路还有一些变体，但最有趣的可能是在Riak 2.0中使用的虚线版本矢量。我们无法在此深入其细节，但是它的工作方式与购物车例子所展示的非常相似。

​	与图5-13中版本号类似，当读取数据时，数据库副本会返回版本矢量给客户端，而在随后写入时需要将将版本信息包含在请求当中一起发送到数据库。Riak将版本矢量编码为一个称之为因果上下文的字符串。**版本矢量技术使数据库可以区分究竟应该覆盖写还是保留并发值**。

​	另外，就像单副本的例子一样，应用程序仍然需要执行合并操作。版本矢量可以保证从某一个副本读取值然后写入到另一个副本，而这些值可能会导致在其他副本上衍生出来新的“兄弟”值，但至少不会发生数据丢失且可以正确合并所有并发值。

> **版本矢量和矢量时钟**
>
> 版本矢量有时也被称为矢量时钟，然而两者并不完全相同，细微差别可参阅文献。**<u>简而言之，当需要比较副本状态时，应当采用版本矢量</u>**。

## 5.5 小结

​	本章，我们详细探讨了复制相关的话题。复制或者多副本技术主要服务于以下目的：

+ 高可用性：即使某台机器(或多台机器，或整个数据中心)出现故障，系统也能保持正常运行。
+ 连接断开与容错：允许应用程序在出现网络中断时继续工作。
+ 低延迟：将数据放置在距离用户较近的地方，从而实现更快地交互。
+ 可扩展性：釆用多副本读取，大幅提高系统读操作的吞吐量。

​	在多台机器上保存多份相同的数据副本，看似只是个很简单的目标，但事实上复制技术是一个非常烧脑的问题。需要仔细考虑并发以及所有可能出错的环节，并小心处理故障之后的各种情形。<u>最最基本的，要处理好节点不可用与网络中断问题，这里甚至还没考虑一些更隐蔽的失效场景，例如由于软件bug而导致的无提示的数据损坏</u>。

我们主要讨论了三种多副本方案：

+ 主从复制

  所有的客户端写入操作都发送到某一个节点(主节点)，由该节点负责将数据更改事件发送到其他副本(从节点)。每个副本都可以接收读请求，但内容可能是过期值。

+ 多主节点复制

  系统存在多个主节点，每个都可以接收写请求，客户端将写请求发送到其中的一个主节点上，由该主节点负责将数据更改事件同步到其他主节点和自己的从节点。

+ **无主节点复制**

  <u>客户端将写请求发送到多个节点上，读取时从多个节点上并行读取，以此检测和纠正某些过期数据</u>。

​	每种方法都有其优点和缺点。<u>**主从复制非常流行，主要是因为它很容易理解，也不需要担心冲突问题**。而万一出现节点失效、网络中断或者延迟抖动等情况，多主节点和无主节点复制方案会更加可靠，不过背后的代价则是系统的复杂性和弱一致性保证</u>。

​	复制可以是同步的，也可以是异步的，而一旦发生故障，二者的表现差异会对系统行为产生深远的影响。在系统稳定状态下异步复制性能优秀，但仍须认真考虑一旦岀现复制滞后和节点失效两种场景会导致何种影响。**万一某个主节点发生故障，而一个异步更新的从节点被提升为新的主节点，要意识到最新确认的数据可能有丢失的风险**。

​	我们还分析了由于复制滞后所引起的一些奇怪效应，并讨论了以下一致性模型，来帮助应用程序处理复制滞后：

+ 写后读一致性

  保证用户总能看到自己所提交的最新数据。

+ **单调读**

  **用户在某个时间点读到数据之后，保证此后不会出现比该时间点更早的数据**。

+ **前缀一致读**

  **保证数据之间的因果关系，例如，总是以正确的顺序先读取问题，然后看到回答**。

​	最后，我们讨论了多主节点和无主节点复制方案所引入的并发问题。即由于多个写可能同时发生，继而可能产生冲突。为此，我们研究了一个算法使得数据库系统可以判定某操作是否发生在另一个操作之前，或者是同时发生。接下来，探讨采用合并并发更新值的方法来解决冲突。

​	下一章我们继续研究多节点上数据的分布问题，与本章不同的是，它是针对一个大型数据集而采用**分区技术**。

# 第6章 数据分区

​	第5章讨论了复制技术，即在不同节点上保存相同数据的多个副本。然而，<u>面对一些海量数据集或非常高的査询压力，复制技术还不够，我们还需要将数据拆分成为分区，也称为分片</u>。

> 术语澄清
>
> 这里我们所讨论的分区，在不同系统有着不同的称呼，例如它对应于MongoDB，Elasticsearch和Solrcloud中的shard，Hbase的region，Bigtable中的tablet，Cassandra和Riak中的vnode，以及Couchbase中的 vBucket。总体而言，分区是最普遍的术语。

​	**分区通常是这样定义的，即每一条数据(或者每条记录，每行或每个文档)只属于某个特定分区**。实现的方法有多种，稍后将逐一介绍。<u>实际上，每个分区都可以视为一个完整的小型数据库，虽然数据库可能存在一些跨分区的操作</u>。

​	<u>采用数据分区的主要目的是**提高可扩展性**</u>。不同的分区可以放在一个无共享集群(参阅第二部分关于无共享架构的定义)的不同节点上。<u>这样一个大数据集可以分散在更多的磁盘上，査询负载也随之分布到更多的处理器上</u>。

​	<u>对单个分区进行查询时，每个节点对自己所在分区可以独立执行查询操作，因此**添加更多的节点可以提高査询吞吐量**</u>。超大而复杂的査询尽管比较困难，但也可能做到跨节点的并行处理。

​	分区数据库最初在20世纪80年代由Teradata和Tandem NonStop SQL等率先推出，最近又被一些NoSQL数据库和基于Hadoop的数据仓库重视起来。这些系统有些是为事务型负载设计的，有些是分析型(参阅第3章的“事务处理与分析处理”)。二者的差异会显著影响系统的优化策略，然而分区技术的基本原理则可以普遍适用。

​	本章我们将首先介绍切分大型数据集的若干方法，讨论数据索引如何影响分区。接下来讨论**分区的再平衡**，这对动态添加或删除节点非常重要。最后，我们将介绍数据库如何将请求路由到正确的分区并执行查询。

## 6.1 数据分区与数据复制

​	<u>**分区通常与复制结合使用，即每个分区在多个节点都存有副本**</u>。<u>这意味着某条记录属于特定的分区，而同样的内容会保存在不同的节点上以提高系统的容错性</u>。

​	一个节点上可能存储了多个分区。图6-1展示了主从复制模型与分区组合使用时数据的分布情况。由图可知，每个分区都有自己的主副本，例如被分配给某节点，而从副本则分配在其他一些节点。<u>一个节点可能即是某些分区的主副本，同时又是其他分区的从副本</u>。

​	**第5章所讨论的所有复制相关的原理同样适用于对分区数据的复制**。考虑到分区方案的选择通常独立于复制，因此本章将力求简洁，而省略与复制相关的内容。

## 6.2 键-值数据的分区

​	假设面临海量数据，现在需要切分它们，那么该如何决定哪些记录放在哪些节点上呢？

​	**分区的主要目标是将数据和査询负载均匀分布在所有节点上**。如果节点平均分担负载，那么理论上10个节点应该能够处理10倍的数据量和10倍于单个节点的读写吞吐量(忽略复制)。

![分区与复制 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-1.png)

​	而如果<u>分区不均匀，则会出现某些分区节点比其他分区承担更多的数据量或查询负载，称之为**倾斜**</u>。倾斜会导致分区效率严重下降，在极端情况下，所有的负载可能会集中在一个分区节点上，这就意味着10个节点9个空闲，系统的瓶颈在最繁忙的那个节点上。这种负载严重不成比例的分区即成为**系统热点**。	

​	避免热点最简单的方法是将记录随机分配给所有节点上。这种方法可以比较均匀地分布数据，但是有一个很大的缺点：当试图读取特定的数据时，没有办法知道数据保存在哪个节点上，所以不得不并行查询所有节点。

​	可以改进上述方法。现在我们假设数据是简单的键-值数据模型，这意味着总是可以通过关键字来访问记录。例如，像一个纸质百科全书，可以通过标题来查找某一个条目；而所有的条目按字母序排序，因此可以做到快速查找条目。

### 6.2.1 基于关键字区间分区

​	一种分区方式是为每个分区分配一段连续的关键宇或者关键字区间范围(以最小值和最大值来指示)，如图6-2所示的纸质百科全书的卷目录。如果知道关键字区间的上下限，就可以轻松确定哪个分区包含这些关键字。如果还知道哪个分区分配在哪个节点，就可以直接向该节点发出请求(对于百科全书的例子，就是从书架上直接取到所要的书籍)。

![键值数据的分区 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-2.png)

​	关键字的区间段不一定非要均匀分布，这主要是因为数据本身可能就不均匀。例如，在图6-2中，卷1只包含以A和B开头的单词，但是卷12则包含了T、U、V、Ⅹ、Y和Z开始的单词。如果只是简单地规定每个分区包含两个字母，则可能会导致一些卷比其他卷要大很多。<u>为了更均匀地分布数据，分区边界理应适配数据本身的分布特征</u>。

​	分区边界可以由管理员手动确定，或者由数据库自动选择(我们将在本章后面的“分区再平衡”中更详细地讨论)。采用这种分区策略的系统包括Bigtable， Bigtable的开源版本Hbase，RethinkDB和2.4版本之前MongoDB。

​	<u>毎个分区内可以按照关键字排序保存(参阅第3章的“ SSTables和LSM-Trees”)。这样可以轻松支持区间查询，即将关键字作为一个拼接起来的索引项从而一次查询得到多个相关记录(参阅第3章的“多列索引”)</u>。例如，对于一个保存网络传感器数据的应用系统，选择测量的时间戳(年-月-日-时-分-秒)作为关键字，此时区间查询会非常有用，它可以快速获得某个月份内的所有数据。

​	然而，**基于关键字的区间分区的缺点是某些访问模式会导致热点**。如果关键字是时间戳，则分区对应于一个时间范围，例如每天一个分区。然而，当测量数据从传感器写入数据库时，所有的写入操作都集中在同一个分区(即当天的分区)，这会导致该分区在写入时负载过高，而其他分区始终处于空闲状态的。

​	为了避免上述问题，需要使用时间戳以外的其他内容作为关键字的第一项。例如，可以在时间戳前面加上传感器名称作为前缀，这样首先由传感器名称，然后按时间进行分区。假设同时有许多传感器处于活动状态，则写入负载最终会比较均匀地分布在多个节点上。接下来，当需要获取一个时间范围内、多个传感器的数据时，可以根据传感器名称，各自执行区间查询。

### * 6.2.2 基于关键字晗希值分区

​	对于上述数据倾斜与热点问题，许多分布式系统采用了基于关键字哈希函数的方式来分区。

​	**一个好的哈希函数可以处理数据倾斜并使其均匀分布**。例如一个处理字符串的32位哈希函数，当输入某个宇符串，它会返回一个0和2<sup>32</sup>~1之间近似随机分布的数值。即使输入的字符串非常相似，返回的哈希值也会在上述数字范围内均匀分布。

​	<u>用于数据分区目的的哈希函数不需要在加密方面很强</u>：例如，Cassandra和 MongoDB使用MD5， Voldemort使用 Fowler-Noll-Vo函数。许多编程语言也有内置的简单哈希函数(主要用于哈希表)，但是要注意这些内置的哈希函数可能并不适合分区，例如，Java的`Object.hashcode`和Ruby的`object#hash`，同一个键在不同的进程中可能返回不同的哈希值。

​	一旦找到合适的关键字哈希函数，就可以**为每个分区分配一个哈希范围(而不是直接作用于关键字范围)**，关键字根据其哈希值的范围划分到不同的分区中。如图6-3所示。

![键值数据的分区 - 图2](https://static.sitestack.cn/projects/ddia/img/fig6-3.png)

​	这种方法可以很好地将关键字均匀地分配到多个分区中。<u>分区边界可以是均匀间隔，也可以是伪随机选择(在这种情况下，该技术有时被称为**一致性哈希**)</u>。

​	然而，**通过关键字哈希进行分区，我们丧失了良好的区间查询特性**。<u>即使关键字相邻，但经过哈希之后会分散在不同的分区中，区间查询就失去了原有的有序相邻的特性</u>。<u>在 MongoDB中，如果启用了基于哈希的分片模式，则区间查询会发送到所有的分区上，而Riak、 Couchbase和Voldemort干脆就不支持关键字上的区间查询</u>。

> 一致性哈希
>
> 一致性哈希，由Karger等人首先提出，是一种平均分配负载的方法，最初用于内容分发网络(CDN)等互联网缓存系统。它采用随机选择的分区边界来规避中央控制或分布式共识。请注意，此处的一致性与副本一致性(第5章)或ACID致性(第7章)没有任何关联，它只描述了数据动态平衡的一种方法。
>
> 正如后面“分区再平衡”一节将要介绍的，**这种特殊的分区方法对于数据库实际效果并不是很好，所以目前很少使用(虽然某些数据库的文档仍采用一致性哈希的术语，但其实并不准确)**。为避免混淆，我们此处只用术语哈希分区。

​	<u>Cassandra则在两种分区策略之间做了一个折中。 Cassandra中的表可以声明为由多个列组成的复合主键。复合主键只有第一部分可用于哈希分区，而其他列则用作组合索引来对 Cassandra SSTable中的数据进行排序。因此，它不支持在第一列上进行区间査询，但如果为第一列指定好了固定值，可以对其他列执行高效的区间查询</u>。

​	**组合索引为一对多的关系提供了一个优雅的数据模型**。例如，在社交网站上，一个用户可能会发布很多消息更新。如果更新的关键字设置为`(user_id，update_timestamp)`的组合，那么可以有效地检索由某用户在一段时间内所做的所有更新，且按时间戳排序。<u>不同的用户可以存储在不同的分区上，但是对于某一用户，消息按时间戳顺序存储在一个分区上</u>。

### * 6.2.3 负载倾斜与热点

​	如前所述，基于哈希的分区方法可以减轻热点，但无法做到完全避免。<u>一个极端情况是，所有的读/写操作都是针对同一个关键字，则最终所有请求都将被路由到同一个分区</u>。

​	这种负载或许并不普遍，但也并非不可能：例如，社交媒体网站上，一些名人用户有数百万的粉丝，当其发布一些热点事件时可能会引发一场访问风暴，出现大量的对相同关键字的写操作(其中关键字可能是名人的用户ID，或者人们正在评论的事件ID)。此时，哈希起不到任何帮助作用，因为两个相同ID的哈希值仍然相同。

​	**大多数的系统今天仍然无法自动消除这种高度倾斜的负载，而只能通过应用层来减轻倾斜程**度。例如，如果某个关键字被确认为热点，一个简单的技术就是在关键字的开头或结尾处添加一个随机数。只需一个两位数的十进制随机数就可以将关键字的写操作分布到100个不同的关键字上，从而分配到不同的分区上。

​	<u>但是，随之而来的问题是，之后的任何读取都需要些额外的工作，必须从所有100个关键字中读取数据然后进行合并。因此通常只对少量的热点关键字附加随机数才有意义；而对于写入吞吐量低的绝大多数关键字，这些都意味着不必要的开销。此外，还需要额外的元数据来标记哪些关键字进行了特殊处理</u>。

​	也许将来某一天，数据系统能够自动检测负载倾斜情况，然后自动处理这些倾斜的负载。但截至目前，仍然需要开发者自己结合应用来综合权衡。

## * 6.3 分区与二级索引

​	我们之前所讨论的分区方案都依赖于键-值数据模型。键-值模型相对简单，即都是通过关键字来访冋记录，自然可以根据关键字来确定分区，并将读写请求路由到负责该关键字的分区上。

​	但是，如果涉及二级索引，情况会变得复杂(参阅第3章的“其他索引结构”)。**二级索引通常不能唯一标识一条记录，而是用来加速特定值的查询**，例如查找用户123的所有操作，找到所有含有hogwash的文章，查找所有颜色为红色的汽车等。

​	<u>二级索引是关系数据库的必备特性，在文档数据库中应用也非常普遍</u>。但考虑到其复杂性，许多键-值存储(如 Hbase和Voldemort)并不支持二级索引；但其他一些如Riak则开始增加对二级索引的支持。此外，二级索引技术也是Solr和Elasticsearch等全文索引服务器存在之根本。

​	**二级索引带来的主要挑战是它们不能规整的地映射到分区中**。有两种主要的方法来支持对二级索引进行分区：**基于文档的分区**和**基于词条的分区**。

### * 6.3.1 基于文档分区的二级索引

​	假设一个销售二手车的网站(见图6-4)。每个列表都有一个唯一的文档ID，用此ID对数据库进行分区，例如，ID0到499归分区0，ID500到999划为分区1。

​	现在用户需要搜索汽车，可以持按汽车颜色和厂商进行过滤，所以需要在颜色和制造商上设定二级索引(在文档数据库中这些都是字段；在关系数据库中则是列)。声明这些索引之后，数据库会自动创建索引。例如，每当一辆红色汽车添加到数据库中，数据库分区会自动将其添加到索引条目为“ color:red”的文档列表中。

![分片与次级索引 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-4.png)

​	**<u>在这种索引方法中，每个分区完全独立，各自维护自己的二级索引，且只负责自己分区内的文档而不关心其他分区中数据</u>**。<u>每当需要写数据库时，包括添加，删除或更新文档等，只需要处理包含目标文档ID的那一个分区</u>。因此**文档分区索引也被称为<u>本地索引</u>，而不是全局索引**，后者将在本章后面介绍。

​	<u>但读取时需要注意：除非对文档ID做了特别的处理，否则不太可能所有特定颜色或特定品牌的汽车都放在一个分区中，例如图6-4中，红色汽车就出现在分区0和分区1中。因此，如果想要搜索红色汽车，就**需要将查询发送到所有的分区，然后合并所有返回的结果**</u>。

​	这种査询分区数据库的方法有时也称为**分散/聚集**，显然这种二级索引的查询代价高昂。即使采用了并行査询，也容易导致读延迟显著放大(参阅第1章的“实践中的百分位数”)。尽管如此，它还是广泛用于实践：MongoDB、Riak、 Cassandra、ElasticSearch、 SolrCloud 和 VoltDB都支持基于文档分区二级索引。大多数数据库供应商都建议用户自己来构建合适的分区方案，尽量由单个分区满足二级索引查询，但现实往往难以如愿，尤其是当查询中可能引用多个二级索引时(例如同时指定颜色和制造商两个条件)。

### * 6.3.2 基于词条的二级索引分区

​	另一种方法，我们可以**对所有的数据构建全局索引，而不是每个分区维护自己的本地索引**。而且，为避免成为瓶颈，不能将全局索引存储在一个节点上，否则就破坏了设计分区均衡的目标。所以，**全局索引也必须进行分区，且可以与数据关键字采用不同的分区策略**。

![分片与次级索引 - 图2](https://static.sitestack.cn/projects/ddia/img/fig6-5.png)

​	以图65为例：所有数据分区中的颜色为红色的汽车被收录到在索引`color:red`中，而索引本身也是分区的，例如从a到r开始的颜色放在分区0中，从s到的颜色放在分区1中。类似的，汽车制造商的索引也被分区(两个分区的边界分别是字母f和字母h)。

​	我们将<u>这种索引方案称为**词条分区**，它**以待查找的关键字本身作为索引**</u>。例如颜色`color:red`。<u>名字词条源于全文索引(一种特定类型的二级索引)，term指的是文档中出现的所有单词的集合</u>。

​	<u>和前面讨论的方法一样，可以直接通过关键词来全局划分索引，或者对其取哈希值。直接分区的好处是可以支持高效的区间查询(例如，查询汽车报价在某个值以上)而采用哈希的方式则可以更均匀的划分分区</u>。

​	这种全局的词条分区相比于文档分区索引的主要优点是，**它的读取更为高效，即它不需要釆用 scatter/gather对所有的分区都执行一遍查询，相反，客户端只需要向包含词条的那一个分区发出读请求**。<u>然而全局索引的不利之处在于，**写入速度较慢且非常复杂**，主要因为单个文档的更新时，里面可能会涉及多个二级索引，而二级索引的分区又可能完全不同甚至在不同的节点上，由此势必引入显著的**写放大**</u>。

​	<u>理想情况下，索引应该时刻保持最新，即写入的数据要立即反映在最新的索引上。但是，对词条分区来讲，这需要个跨多个相关分区的分布式事务支持，写入速度会受到极大的影响，所以**现有的数据库都不支持同步更新二级索引**(参阅第7章和第9章)</u>。

​	**实践中，<u>对全局二级索引的更新往往都是异步的(也就意味着，如果在写入之后马上去读索引，那么刚刚发生的更新可能还没有反映在索引中)</u>**。例如， AmazonDynamoDB的二级索引通常可以在1秒之内完成更新，但当底层设施出现故障时，也有可能需要等待很长的时间。其他使用全局索引的系统还包括Riak的搜索功能和Oracle数据仓库，后者允许用户来选择是使用本地还是全局索引。在第12章我们会重新讨论如何实现全局二级索引。

## 6.4 分区再平衡

​	随着时间的推移，数据库可能总会出现某些变化：

+ 查询压力增加，因此需要更多的CPU来处理负载。
+ 数据规模增加，因此需要更多的磁盘和内存来存储数据。
+ 节点可能出现故障，因此需要其他机器来接管失效的节点。

​	<u>所有这些变化都要求数据和请求可以从一个节点转移到另一个节点</u>。这样一个迁移负载的过程称为**再平衡(或者动态平衡)**。无论对于哪种分区方案，分区再平衡通常至少要满足：

+ 平衡之后，负载、数据存储、读写请求等应该在集群范围更均匀地分布。
+ **再平衡执行过程中，数据库应该可以继续正常提供读写服务**。
+ 避免不必要的负载迁移，以加快动态再平衡，并尽量减少网络和磁盘IO影响。

### 6.4.1 动态再平衡的策略

​	将分区对应到节点上存在多种不同的分配策略，这里逐一介绍：

#### * 为什么不用取模？

​	我们在前面提到(见图6-3)，最好将哈希值划分为不同的区间范围，然后将每个区间分配给一个分区。例如，区间[0，b0)对应于分区0，[b0，b1)对应分区1等。

​	也许你会问为什么不直接使用mod(许多编程语言里的取模运算符%)。例如，`hash(key) mod 10`会返回一个介于0和9之间的数字，如果有10个节点，则依次对应节点0到9，这似乎是将每个关键字分配到节点的最简单方法。

​	**对节点数取模方法的问题是，如果节点数N发生了变化，会导致很多关键字需要从现有的节点迁移到另一个节点**。例如，假设hash(key)=123456，假定最初是10个节点，那么这个关键字应该放在节点6(123456 mod 10=6)；当节点数增加到11时，它需要移动到节点3(123456 mod 11=3)；当继续增长到12个节点时，又需要移动到节点0(123456 mod 12=0)。<u>这种频繁的迁移操作大大增加了再平衡的成本</u>。

​	因此我们需要一种减少迁移数据的方法。

#### * 固定数量的分区

​	幸运的是，有一个相当简单的解决方案：**首先，创建远超实际节点数的分区数，然后为每个节点分配多个分区**。例如，对于一个10节点的集群，数据库可以从一开始就逻辑划分为1000个分区，这样大约每个节点承担100个分区。

​	<u>接下来，如果集群中添加了一个新节点，该新节点可以从每个现有的节点上匀走几个分区，直到分区再次达到全局平衡。该过程如图6-6所示。如果从集群中删除节点，则采取相反的均衡措施</u>。

​	<u>选中的整个分区会在节点之间迁移，但分区的总数量仍维持不变，也不会改变关键字到分区的映射关系。**这里唯一要调整的是分区与节点的对应关系**</u>。考虑到节点间通过网络传输数据总是需要些时间，这样调整可以逐步完成，在此期间，旧的分区仍然可以接收读写请求。

![分区再平衡 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-6.png)

​	<u>原则上，也可以将集群中的不同的硬件配置因素考虑进来，即性能更强大的节点将分配更多的分区，从而分担更多的负载</u>。

​	目前，Riak、 Elasticsearch、 couchbase和Voldemort都支持这种动态平衡方法。

​	**使用该策略时，分区的数量往往在数据库创建时就确定好，之后不会改变**。<u>原则上也可以拆分和合并分区(稍后介绍)，但固定数量的分区使得相关操作非常简单，因此许多采用固定分区策略的数据库决定不支持分区拆分功能</u>。所以，在初始化时，已经充分考虑将来扩容增长的需求(未来可能拥有的最大节点数)，设置一个足够大的分区数。而每个分区也有些额外的管理开销，选择过高的数字可能会有副作用。

​	如果数据集的总规模高度不确定或可变(例如，开始非常小，但随着时间的推移可能会变得异常庞大)，此时如何选择合适的分区数就有些困难。每个分区包含的数据量的上限是固定的，实际大小应该与集群中的数据总量成正比。如果分区里的数据量非常大，则每次再平衡和节点故障恢复的代价就很大；但是如果一个分区太小，就会产生太多的开销。分区大小应该“恰到好处”，不要太大，也不能过小，**如果分区数量固定了但总数据量却高度不确定，就难以达到一个最佳取舍点**。

#### * 动态分区

​	对于采用关键字区间分区的数据库，如果边界设置有问题，最终可能会出现所有数据都挤在一个分区而其他分区基本为空，那么设定固定边界、固定数量的分区将非常不便：而手动去重新配置分区边界又非常繁琐。

​	因此，一些数据库如 HBase和 RethinkDB等采用了**动态创建分区**。<u>当分区的数据增长超过一个可配的参数阈值(Hbase上默认值是10GB)，它就拆分为两个分区，每个承担一半的数据量。相反，如果大量数据被删除，并且分区缩小到某个阈值以下，则将其与相邻分区进行合并</u>。该过程类似于B树的分裂操作(参阅第3章的“B-tree”)。

​	**<u>每个分区总是分配给一个节点</u>，而每个节点可以承载多个分区，这点与固定数量的分区一样。当一个大的分区发生分裂之后，可以将其中的一半转移到其他某节点以平衡负载**。<u>对于 HBase，分区文件的传输需要借助HDFS(底层分布式文件系统)</u>。

​	**动态分区的一个优点是分区数量可以自动适配数据总量**。<u>如果只有少量的数据，少量的分区就足够了，这样系统开销很小；如果有大量的数据，每个分区的大小则被限制在一个可配的最大值</u>。

​	但是，需要注意的是，<u>对于一个空的数据库，因为没有任何先验知识可以帮助确定分区的边界，所以会从一个分区开始。可能数据集很小，但直到达到第一个分裂点之前，所有的写入操作都必须由单个节点来处理，而其他节点则处于空闲状态</u>。为了缓解这个问题， HBase和 MongoDB允许在一个空的数据库上配置一组初始分区(这被称为**预分裂**)。对于关键字区间分区，预分裂要求已经知道一些关键字的分布情况。

​	动态分区不仅适用于关键字区间分区，也适用于基于哈希的分区策略。 MongoDB从版本2.4开始，同时支持二者，并且都可以动态分裂分区。

#### 按节点比例分区

​	采用动态分区策略，拆分和合并操作使每个分区的大小维持在设定的最小值和最大值之间，因此分区的数量与数据集的大小成正比关系。另一方面，对于固定数量的分区方式，其每个分区的大小也与数据集的大小成正比。两种情况，分区的数量都与节点数无关。

​	Cassandra和Ketama则采用了第三种方式，<u>使分区数与集群节点数成正比关系</u>。换句话说，**每个节点具有固定数量的分区**。此时，<u>当节点数不变时，每个分区的大小与数据集大小保持正比的增长关系</u>；当节点数增加时，分区则会调整变得更小。较大的数据量通常需要大量的节点来存储，因此这种方法也使每个分区大小保持稳定。

​	**当一个新节点加入集群时，它随机选择固定数量的现有分区进行分裂，然后拿走这些分区的一半数据量，将另一半数据留在原节点**。随机选择可能会带来不太公平的分区分裂，但是当平均分区数量较大时(Cassandra默认情况下，每个节点有256个分区)，新节点最终会从现有节点中拿走相当数量的负载。 Cassandra在3.0时推出了改进算法，可以避免上述不公平的分裂。

​	**随机选择分区边界的前提要求采用基于哈希分区(可以从哈希函数产生的数字范围里设置边界)**。这种方法也最符合本章开头所定义一致性哈希。一些新设计的哈希函数也可以以较低的元数据开销达到类似的效果。

### 6.4.2 自动与手动再平衡操作

​	动态平衡另一个重要问题我们还没有考虑：它是自动执行还是手动方式执行？

​	全自动式的再平衡(即由系统自动决定何时将分区从一个节点迁移到另一个节点，不需要任何管理员的介入)与纯手动方式(即分区到节点的映射由管理员来显式配置)之间，可能还有一个过渡阶段。例如， Couchbase，Riak和 Voldemort会自动生成一个分区分配的建议方案，但需要管理员的确认才能生效。

​	<u>全自动式再平衡会更加方便，它在正常维护之外所增加的操作很少。但是，也有可能出现结果难以预测的情况。再平衡总体讲是个比较昂贵的操作，它需要重新路由请求并将大量数据从一个节点迁移到另一个节点。万一执行过程中间出现异常，会使网络或节点的负载过重，并影响其他请求的性能</u>。

​	**将自动平衡与自动故障检测相结合也可能存在一些风险**。<u>例如，假设某个节点负载过重，对请求的响应暂时受到影响，而其他节点可能会得到结论：该节点已经失效；接下来激活自动平衡来转移其负载。客观上这会加重该节点、其他节点以及网络的负荷，可能会使总体情况变得更糟，**甚至导致级联式的失效扩散**</u>。

​	出于这样的考虑，**让管理员介入到再平衡可能是个更好的选择**。它的确比全自动过程响应慢一些，但它可以有效防止意外发生。

## 6.5 请求路由

​	现在我们已经将数据集分布到多个节点上，但是仍然有一个悬而未决的问题：当客户端需要发送请求时，如何知道应该连接哪个节点？如果发生了分区再平衡，分区与节点的对应关系随之还会变化。为了回答该问题，我们需要一段处理逻辑来感知这些变化，并负责处理客户端的连接，例如想要读/写关键字“foo”，需要连接哪个IP地址和哪个端口号。

​	这其实属于一类典型的**服务发现问题**，服务发现并不限于数据库，任何通过网络访问的系统都有这样的需求，尤其是当服务目标支持高可用时(在多台机器上有冗余配置)。许多公司已经开发了自己的内部服务发现工具，其中很多已经开源。

​	概括来讲，这个问题有以下几种不同的处理策略(分别如图6-7所示的三种情况)：

1. 允许客户端链接任意的节点(例如，采用**循环式的负载均衡器**)。如果某节点恰好拥有所请求的分区，则直接处理该请求；否则，将请求转发到下一个合适的节点，接收答复，并将答复返回给客户端。
2. 将所有客户端的**请求都发送到一个路由层，由后者负责将请求转发到对应的分区节点上**。路由层本身不处理任何请求，它仅充一个分区感知的负载均衡器。
3. 客户端**感知分区和节点分配关系**。此时，客户端可以直接连接到目标节点，而不需要任何中介。

​	<u>**不管哪种方法，核心问题是：作出路由决策的组件(可能是某个节点，路由层或客户端)如何知道分区与节点的对应关系以及其变化情况**</u>?

![请求路由 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-7.png)

​	这其实是一个很有挑战性的问题，所有参与者都要达成**共识**这一点很重要。否则请求可能被发送到错误的节点，而没有得到正确处理。分布式系统中有专门的共识协议算法，但通常难以正确实现(详见第9章)。

​	许多分布式数据系统依靠独立的协调服务(如 ZooKeeper)跟踪集群范围内的元数据，如图6-8所示。每个节点都向 ZooKeeper中注册自己， ZooKeeper维护了分区到节点的最终映射关系。其他参与者(如路由层或分区感知的客户端)可以向 ZooKeeper订阅此信息。一旦分区发生了改变，或者添加、删除节点，ZooKeeper就会主动通知路由层，这样使路由信息保持最新状态。

​	例如， LinkedIn的 Espresso使 Helix进行集群管理(底层是 ZooKeeper)，实现了图6-8所示的请求路由层。 HBase， SolrCloud和Kafka也使用 ZooKeeper来跟踪分区分配情况。 MongoDB有类似的设计，但它依赖于自己的配置服务器和mongos守护进程来充当路由层。

​	<u>Cassandra和Riak则采用了不同的方法，它们在节点之间使用gossip协议来同步群集状态的变化。请求可以发送到任何节点，由该节点负责将其转发到目标分区节点(图6-7中的方法1)。这种方式增加了数据库节点的复杂性，但是避免了对ZooKeeper之类的外部协调服务的依赖</u>。

![请求路由 - 图2](https://static.sitestack.cn/projects/ddia/img/fig6-8.png)

​	Couchbase并不支持自动再平衡功能，这简化了设计。它通过配置一个名为moxi的路由选择层，向集群节点学习最新的路由变化。

​	<u>当使用路由层或随机选择节点发送请求时，客户端仍然需要知道目标节点的IP地址。IP地址的变化往往没有分区节点变化那么频繁，采用DNS通常就足够了</u>。

### 6.5.1 并行查询执行

​	到目前为止，我们只关注了读取或写入单个关键字这样简单的查询(对于文档分区的二级索引，里面要求分散/聚集查询)。这基本上也是大多数NoSQL分布式数据存储所支持的访问类型。

​	然而对于**大规模并行处理(massively parallel processin，MPP)**这一类主要用于数据分析的关系数据库，在查询类型方面要复杂得多。**典型的数据仓库查询包含多个联合、过滤、分组和聚合操作。<u>MPP査询优化器会将复杂的査询分解成许多执行阶段和分区，以便在集群的不同节点上并行执行</u>。尤其是涉及全表扫描这样的查询操作，可以通过并行执行获益颇多**。

​	数据仓库中快速并行执行查询可以作为单独的话题。考虑到分析业务的重要性，目前它已得到了广泛的商业关注。我们将在第10章中讨论并行查询执行所需的一些技术。有关并行数据库更多相关技术细节请参考文献。

## 6.6 小结

​	本章，我们探讨了将大规模数据集划分成更小子集的多种方法。数据量如果太大，单台机器进行存储和处理就会成为瓶颈，因此需要引入数据分区机制。**分区的目地是通过多台机器均匀分布数据和查询负载，避免出现热点**。这需要选择合适的数据分区方案，**在节点添加或删除时重新动态平衡分区**。

​	我们讨论了两种主要的分区方法：

+ **基于关键字区间的分区**。先对关键字进行排序，每个分区只负责一段包含最小到最大关键字范围的一段关键字。<u>对关键字排序的优点是可以支持高效的区间查询，但是如果应用程序经常访问与排序一致的某段关键字，就会存在热点的风险</u>。釆用这种方法，当分区太大时，通常将其分裂为两个子区间，从而动态地再平衡分区。
+ **哈希分区**。将哈希函数作用于每个关键字，每个分区负责一定范围的哈希值。这种方法打破了原关键字的顺序关系，<u>它的区间查询效率比较低，但可以更均匀地分配负载</u>。采用哈希分区时，通常事先创建好足够多(但固定数量)的分区，让每个节点承担多个分区，当添加或删除节点时将某些分区从一个节点迁移到另个节点，也可以支持动态分区。

​	混合上述两种基本方法也是可行的，例如使用复合键：键的一部分来标识分区，而另部分来记录排序后的顺序。

​	我们还讨论了分区与二级索引，二级索引也需要进行分区，有两种方法：

+ **基于文档来分区二级索引(本地索引)**。二级索引存储在与关键字相同的分区中，这意味着写入时我们只需要更新一个分区，但<u>缺点是读取二级索引时需要在所有分区上执行 scatter/gather</u>。
+ **基于词条来分区二级索引(全局索引)**。它是基于索引的值而进行的独立分区。二级索引中的条目可能包含来自关键字的多个分区里的记录。<u>在写入时，不得不更新二级索引的多个分区；但读取时，则可以从单个分区直接快速提取数据</u>。

​	最后，我们讨论了如何将查询请求路由到正确的分区，包括简单的分区感知负载均衡器，以及复杂的并行査询执行引擎。

​	理论上，毎个分区基本保持独立运行，这也是为什么我们试图将分区数据库分布、扩展到多台机器上。但是，如果写入需要跨多个分区，情况就会格外复杂，例如，如果其中一个分区写入成功，但另一个发生失败，接下来会发生什么？我们将在下面的章节中讨论类似这样的技术挑战。

# 第7章 事务

P211
