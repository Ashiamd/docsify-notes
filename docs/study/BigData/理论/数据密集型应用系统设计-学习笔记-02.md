# 数据密集型应用系统设计-学习笔记-02

# 第二部分 分布式数据系统

​	本书第一部分主要讨论了单台机器存储系统设计的主要技术。在第二部分，我们将继续向前迈进，当需要多台机器提供数据存储和检索服务时，又会有哪些挑战和方案呢？

​	主要出于以下目的，我们需要在多台机器上分布数据：

+ 扩展性

  当数据量或者读写负载巨大，严重超出了单台机器的处理上限，需要将负载分散到多台机器上。

+ 容错与高可用性

  当单台机器(或者多台，以及网络甚至整个数据中心)出现故障，还希望应用系统可以继续工作，这时需要采用多台机器提供冗余。这样某些组件失效之后，冗余组件可以迅速接管。

+ **延迟考虑**

  如果客户遍布世界各地，通常需要考虑在全球范围内部署服务，以方便用户就近访问最近数据中心所提供的服务，从而避免数据请求跨越了半个地球才能到达目标。

## **系统扩展能力**

​	**当负载增加需要更强的处理能力时，最简单的办法就是购买更强大的机器(有时称为垂直扩展)**。<u>由一个操作系统管理更多的CPU，内存和磁盘，通过**高速内部总线**使每个CPU都可以访问所有的存储器或磁盘。在这样一个共享内存架构中，所有这些组件的集合可看作一台大机器</u>。

​	共享内存架构的问题在于，成本增长过快甚至超过了线性：即如果把一台机器内的CPU数量增加一倍，内存扩容一倍，磁盘容量加大一倍，则最终总成本增加不止一倍。<u>并且由于性能瓶颈因素，这样一台机器尽管拥有了两倍的硬件指标但却不一定能处理两倍的负载</u>。

​	<u>共享内存架构能够提供有限的容错能力，例如高端的服务器可以热插拔很多组件(在不关闭机器的情况下更换磁盘，内存模块，甚至是CPU)</u>。但很显然，它仍局限于某个特定的地理位置，无法提供异地容错能力。

​	另一种方法是共享磁盘架构，它拥有多台服务器，每个服务器各自拥有独立的CPU和内存，然后将数据存储在可共享访问的磁盘阵列上，服务器与磁盘阵列之间往往通过高速网络连接。<u>这种架构多适用于数据仓库等负载，然而通常由于资源竞争以及锁的开销等限制了其进一步的扩展能力</u>。

## 无共享结构

​	相比之下，无共享架构(也称为水平扩展)则获得了很大的关注度。当采用这种架构时，运行数据库软件的机器或者虚拟机称为节点。每个节点独立使用本地的CPU，内存和磁盘。<u>节点之间的所有协调通信等任务全部运行在传统网络(以太网)之上且核心逻辑主要依靠软件来实现</u>。

​	无共享系统不需要专门的硬件，具有较高的性价比。它可以跨多个地理区域分发数据，从而减少用户的访问延迟，甚至当整个数据中心发生灾难时仍能继续工作。通过云计算虛拟机的部署方式，即便是没有Google级别规模的小公司，也可以轻松拥有跨区域的分布式架构和服务能力。

​	本部分内容将重点放在无共享体系架构上，并不是因为它一定是所有应用的最佳选择，而是因为它需要应用开发者更多的关注和深入理解。例如把数据分布在多节点上，就需要了解在这样一个分布式系统下，背后的权衡设计和隐含限制，数据库并不能魔法般地把所有复杂性都屏蔽起来。

​	虽然分布式无共享体系架构具有很多优点，但也会给应用程序带来更多的复杂性，有时甚至会限制实际可用的数据模型。例如在某些极端情况下，一个简单的单线程程序可能比一个拥有100多个CPU核的集群性能更好。而另一方面，无共享系统也可以做到性能非常强大。接下来的几章里我们将详细讨论数据分布时所面临的主要问题。

## 复制与分区

​	将数据分布在多节点时有两种常见的方式：

+ 复制

  **在多个节点上保存相同数据的副本**，每个副本具体的存储位置可能不尽相同。复制方法可以提供冗余：如果某些节点发生不可用，则可以通过其他节点继续提供数据访问服务。复制也可以帮助提高系统性能。我们在第5章将主要讨论复制技术。

+ 分区

  **将一个大块头的数据库拆分成多个较小的子集即分区，不同的分区分配给不同的节点(也称为分片)**。我们在第6章主要介绍分区技术。

​	这些是不同的数据分布机制，然而它们经常被放在一起组合使用。参见图II-1的示例。

​	在了解以上概念之后，我们会讨论分布式环境中错综复杂的权衡之道，很可能你会在实际设计系统时不得不面对这些艰难选择。我们将在第7章介绍事务，帮助理解数据系统中各种可能岀错的情况以及处理方法，而第8章和第9章将深入分析分布式系统内在的局限性，之后结束本部分。

![第二部分：分布式数据 - 图1](https://static.sitestack.cn/projects/ddia/img/figii-1.png)

​	在之后本书的第三部分，我们将讨论如何把多个(可能每一个都是分布式)数据存储组件集成到一个更大的系统中，以满足更复杂的应用需求。但在那之前，我们首先来谈谈分布式数据系统。

# 第5章 数据复制

​	复制主要指通过互联网络在多台机器上保存相同数据的副本。正如第二部分开头所介绍的，通过数据复制方案，人们通常希望达到以下目的：

+ 使数据在地理位置上更接近用户，从而**降低访问延迟**。
+ 当部分组件出现故障，系统依然可以继续工作，从而**提高可用性**。
+ 扩展至多台机器以同时提供数据访问服务，从而**提高读吞吐量**。

​	本章我们将假设数据规模比较小，集群的每一台机器都可以保存数据集的完整副本。在接下来的第6章中，我们放宽这一假设，讨论单台机器无法容纳整个数据集的情况(即必须分区)。在后面的章节中，我们还将讨论复制过程中可能出现的各种故障，以及该如何处理这些故障。

​	如果复制的数据一成不变，那么复制就非常容易：只需将数据复制到每个节点，一次即可搞定。然而所有的技术挑战都在于处理那些持续更改的数据，而这正是本章讨论的核心。我们将讨论三种流行的复制数据变化的方法：**主从复制、多主节点复制和无主节点复制**。几乎所有的分布式数据库都使用上述方法中的某一种，而三种方法各有优缺点，我们稍后会详细解读。

​	复制技术存在许多需要折中考虑的地方，例如采用同步复制还是异步复制，以及如何处理失败的副本等。数据库通常采用可配置选项来调整这些处理策略，虽然在处理细节方面因数据库实现而异，但存在一些通用的一般性原则。本章我们还将在讨论不同选项可能出现的后果。

​	<u>数据库复制其实是个很古老的话题。因为网络的基本约束条件没有发生本质的改变，可以说自1970年所研究的基本复制原则，时至今日也没有发生太大的变化</u>。然而，除了学术研究，实践中很多开发人员仍然假定数据库只运行在单节点上，分布式数据库成为主流也只是最近发生的事情。由于许多应用开发人员在这方面经验还略显不足，对诸如“最终一致性”等问题存在一些误解。因此，在“复制滞后问题”中，我们会详细讨论最终一致性，包括读自己的写和单调读等。

## * 5.1 主节点与从节点

​	**每个保存数据库完整数据集的节点称之为副本**。当有了多副本，不可避免地会引入个问题：<u>如何确保所有副本之间的数据是一致的</u>？

​	**对于每一笔数据写入，所有副本都需要随之更新；否则，某些副本将出现不一致**。最常见的解决方案是基于主节点的复制(也称为主动被动，或主从复制)，如图5-1所示。主从复制的工作原理如下：

1. 指定某一个副本为主副本(或称为主节点)。当客户写数据库时，必须将写请求首先发送给主副本，主副本首先将新数据写入本地存储。
2. **其他副本则全部称为从副本(或称为从节点)。主副本把新数据写入本地存储后，然后将数据更改作为复制的日志或更改流发送给所有从副本。每个从副本获得更改日志之后将其应用到本地，且严格保持与主副本相同的写入顺序**。
3. 客户端从数据库中读数据时，可以在主副本或者从副本上执行査询。再次强调，**只有主副本才可以接受写请求；从客户端的角度来看，从副本都是只读的**。

​	许多关系型数据库都内置支持主从复制，例如 PostgreSQL(9.0版本以后)、MYSQL、 Oracle Data Guard和 SQL Server的AlwaysOn Availability Groups。而些非关系数据库如 MongoDB、 RethinkDB和 Espresso也支持主从复制。另外，主从复制技术也不仅限于数据库，还广泛用于分布式消息队列如 Kafka 和 RabbitMQ可以及一些网络文件系统和复制块设备(如DRBD)。

![领导者与追随者 - 图1](https://static.sitestack.cn/projects/ddia/img/fig5-1.png)

### * 5.1.1 同步复制与异步复制

​	复制非常重要的一个设计选项是同步复制还是异步复制。<u>对于关系数据库系统，同步或异步通常是一个可配置的选项；而其他系统则可能是硬性指定或者只能二选一</u>。

​	结合图5-1的例子，网站用户需要更新首页的头像图片。其基本流程是，客户将更新请求发送给主节点，主节点接收到请求，接下来将数据更新转发给从节点。最后，由主节点来通知客户更新完成。

​	图5-2则进一步描述了系统各个模块间的通信情况，包括客户端，主节点和两个从节点。时间从左到右。请求或响应标记为粗箭头。

![领导者与追随者 - 图2](https://static.sitestack.cn/projects/ddia/img/fig5-2.png)

​	图5-2中，从节点1的复制是同步的，即主节点需等待直到从节点1确认完成了写入，然后才会向用户报告完成，并且将最新的写入对其他客户端可见。而从节点2的复制是异步的：主节点发送完消息之后立即返回，不用等待从节点2的完成确认。

​	图5-2中，从节点2在接收复制日志之前有一段很长的延迟。通常情况下，复制速度会非常快，例如多数数据库系统可以在一秒之内完成所有从节点的更新。但是，系统其实并没有保证一定会在多长时间内完成复制。有些情况下，从节点可能落后主节点几分钟甚至更长时间，例如，由于从节点刚从故障中恢复，或者系统已经接近最大设计上限，或者节点之间的网络出现问题。

​	**同步复制的优点是，一旦向用户确认，从节点可以明确保证完成了与主节点的更新同步，数据已经处于最新版本。万一主节点发生故障，总是可以在从节点继续访问最新数据。缺点则是，如果同步的从节点无法完成确认(例如由于从节点发生崩溃，或者网络故障，或任何其他原因)，写入就不能视为成功。主节点会阻塞其后所有的写操作，直到同步副本确认完成**。

​	因此，把所有从节点都配置为同步复制有些不切实际。因为这样的话，任何一个同步节点的中断都会导致整个系统更新停滞不前。**<u>实践中，如果数据库启用了同步复制，通常意味着其中某一个从节点是同步的，而其他节点则是异步模式</u>**。**万一同步的从节点变得不可用或性能下降，则将另一个异步的从节点提升为同步模式。这样可以保证至少有两个节点(即主节点和一个同步从节点)拥有最新的数据副本。这种配置有时也称为<u>半同步</u>**。

​	<u>主从复制还经常会被配置为**全异步**模式</u>。此时如果主节点发生失败且不可恢复，则所有尚未复制到从节点的写请求都会丟失。这意味着即使向客户端确认了写操作，却无法保证数据的持久化。<u>但全异步配置的优点则是，不管从节点上数据多么滞后，主节点总是可以继续响应写请求，系统的吞吐性能更好</u>。

​	**异步模式这种弱化的持久性听起来是一个非常不靠谱的折中设计，但是异步复制还是被广泛使用，特别是那些从节点数量巨大或者分布于广域地理环境**。我们将在本章后面的“复制滞后问题”继续这个话题。

> 复制问题研究
>
> 主节点发生故障时异步复制系统可能会丢失数据，这是一个非常严重的问题，因此在保证数据不丢失的前提下，研究人员尝试了各种办法来提高复制性能与系统可用性。例如，链式复制是同步复制的一种变体，已经在一些系统(如Microsoft Azure存储)中得以实现。
>
> 多副本一致性与共识之间有着密切的联系(即让多个节点对数据状态达成一致)，我们将在第9章详细探讨这一点。本章主要集中于数据库实践中常用的、相对简单的复制技术。

#### 配置新的从节点

​	当如果岀现以下情况时，如需要增加副本数以提高容错能力，或者替换失败的副本，就需要考虑增加新的从节点。但如何确保新的从节点和主节点保持数据一致呢?

​	简单地将数据文件从一个节点复制到另一个节点通常是不够的。主要是因为客户端仍在不断向数据库写入新数据，数据始终处于不断变化之中，因此常规的文件拷贝方式将会导致不同节点上呈现出不同时间点的数据，这不是我们所期待的。

​	或许应该考虑锁定数据库(使其不可写)来使磁盘上的文件保持一致，但这会违反高可用的设计目标。好在我们可以做到在不停机、数据服务不中断的前提下完成从节点的设置。逻辑上的主要操作步骤如下:

1. 在某个时间点对主节点的数据副本产生一个**一致性快照**，这样避免长时间锁定整个数据库。目前大多数数据库都支持此功能，快照也是系统备份所必需的。而在某些情况下，可能需要第三方工具，如MySQL的 innobackupex。
2. **将此快照拷贝到新的从节点**。
3. **从节点连接到主节点并请求快照点之后所发生的数据更改日志**。因为在第一步创建快照时，快照与系统复制日志的某个确定位置相关联，这个位置信息在不同的系统有不同的称呼，如PostgreSQL将其称为“log sequence number”(日志序列号)，而 MySQL将其称为“ binlog coordinates”。
4. 获得日志之后，从节点来应用这些快照点之后所有数据变更，这个过程称之为追赶。接下来，它可以继续处理主节点上新的数据变化。并重复步骤1~步骤4。

​	建立新的从副本具体操作步骤可能因数据库系统而异。某些系统中，这个过程是全自动化的，而其他系统中所涉及的步骤、流程可能会比较复杂，甚至需要管理员手动介入。

#### * 处理节点失效

​	系统中的任何节点都可能因故障或者计划内的维护(例如重启节点以安装内核安全补丁)而导致中断甚至停机。如果能够在不停机的情况下重启某个节点，这会对运维带来巨大的便利。我们的目标是，<u>尽管个别节点会出现中断，但要保持系统总体的持续运行，并尽可能减小节点中断带来的影响</u>。

​	那么如何通过主从复制技术来实现系统高可用呢?

+ 从节点失效：追赶式恢复

  从节点的本地磁盘上都保存了副本收到的数据变更日志。如果从节点发生崩溃，然后顺利重启，或者主从节点之间的网络发生暂时中断(闪断)，则恢复比较容易，**根据副本的复制日志，从节点可以知道在发生故障之前所处理的最后一笔事务，然后连接到主节点，并请求自那笔事务之后中断期间内所有的数据变更**。在收到这些数据变更日志之后，将其应用到本地来追赶主节点。之后就和正常情况一样持续接收来自主节点数据流的变化。

+ 主节点失效：节点切换

  **处理主节点故障的情况则比较棘手：选择某个从节点将其提升为主节点；客户端也需要更新，这样之后的写请求会发送给新的主节点，然后其他从节点要接受来自新的主节点上的变更数据**，这一过程称之为**切换**。故障切换可以手动进行，例如通知管理员主节点发生失效，采取必要的步骤来创建新的主节点；或者以自动方式进行。自动切换的步骤通常如下：

  1. **确认主节点失效**。有很多种岀错可能性，例如由于系统崩溃，停电，网络问题等。没有万无一失的方法能够确切地检测到究竟问题出在哪里，所以**大多数系统都采用了基于超时的机制：节点间频繁地互相发生发送心跳存活消息，如果发现某一个节点在一段比较长时间内(例如30s)没有响应，即认为该节点发生失效**(如果主节点在计划内出于维护目的而故意下线，则不在此讨论范围)。
  2. **选举新的主节点**。可以通过选举的方式(超过多数的节点达成共识)来选举新的主节点，或者由之前选定的某控制节点来指定新的主节点。候选节点最好与原主节点的数据差异最小，这样可以最小化数据丢失的凤险。让所有节点同意新的主节点是个典型的**共识问题**，会在第9章详细讨论。
  3. 重新配置系统使新主节点生效。客户端现在需要将写请求发送给新的主节点(细节将在第6章的“请求路由”中讨论)。<u>如果原主节点之后重新上线，可能仍然自认为是主节点，而没有意识到其他节点已经达成共识迫使其下台。**这时系统要确保原主节点降级为从节点，并认可新的主节点**</u>。

  ​	然而，上述切换过程依然充满了很多变数：

  + <u>如果使用了异步复制，且失效之前，新的主节点并未收到原主节点的所有数据；在选举之后，原主节点很快又重新上线并加入到集群，接下来的写操作会发生什么？新的主节点很可能会收到冲突的写请求，这是因为原主节点未意识的角色变化，还会尝试同步其他从节点，但其中的一个现在已经接管成为现任主节点</u>。**常见的解决方案是，原主节点上未完成复制的写请求就此丢弃，但这可能会违背数据更新持久化的承诺**。
  + **如果在数据库之外有其他系统依赖于数据库的内容并在一起协同使用，丟弃数据的方案就特别危险**。例如，<u>在 Github的一个事故中，某个数据并非完全同步的MySQL从节点被提升为主副本，数据库使用了自增计数器将主键分配给新创建的行，但是因为新的主节点计数器落后于原主节点(即二者并非完全同步)，它重新使用了已被原主节点分配出去的某些主键，而恰好这些主键已被外部Redis所引用，结果出现 MySQL和 Redis之间的不一致，最后导致了某些私有数据被错误地泄露给了其他用户</u>。
  + **在某些故障情况下(参见第8章)，可能会发生两个节点同时都自认为是主节点。这种情况被称为脑裂**，它非常危险：<u>两个主节点都可能接受写请求，并且没有很好解决冲突的办法(参阅本章后面的“多主节点复制技术”)，最后数据可能会丟失或者破坏</u>。作为一种安全应急方案，有些系统会采取措施来强制关闭其中一个节点。然而，如果设计或者实现考虑不周，可能会出现两个节点都被关闭的情况。
  + 如何设置合适的超时来检测主节点失效呢？<u>主节点失效后，超时时间设置得越长也意味着总体恢复时间就越长</u>。<u>但如果超时设置太短，可能会导致很多不必要的切换</u>。例如，突发的负载峰值会导致节点的响应时间变长甚至超时，或者由于网络故障导致延迟增加。**如果系统此时已经处于高负载压力或网络已经出现严重拥塞，不必要的切换操作只会使总体情况变得更糟**。

​	**坦白讲，对于这些问题没有简单的解决方案。因此，即使系统可能支持自动故障切换，有些运维团队仍然更愿意以手动方式来控制整个切换过程**。

​	上述这些问题，包括**节点失效、网络不可靠、副本一致性、持久性、可用性与延迟**之间各种细微的权衡，实际上正是分布式系统核心的基本问题。在第8章和第9章中，我们还会进一步讨论。

### * 5.1.2 复制日志的实现

​	主从复制技术到底是如何工作呢？实践中有多种不同的实现方法，此处我们逐一做些介绍。

#### 基于语句的复制

​	<u>最简单的情况，主节点记录所执行的每个**写请求(操作语句)**并将该操作语句作为日志发送给从节点</u>。对于关系数据库，这意味着每个 INSERT、 UPDATE或 DELETE语句都会转发给从节点，并且每个从节点都会分析并执行这些SQL语句，如同它们是来自客户端那样。

​	听起来很合理也不复杂，但这种复制方式有一些不适用的场景：

+ <u>任何调用非确定性函数的话句，如NOW()获取当前时间，或RAND()获取一个随机数等，可能会在不同的副本上产生不同的值</u>。

+ 如果语句中使用了自增列，或者依赖于数据库的现有数据(例如， `UPDATE ... WHERE <某些条件>`)，则所有副本必须按照完全相同的顺序执行，否则可能会带来不同的结果。进而，<u>如果有多个同时并发执行的事务时，会有很大的限制</u>。
+ <u>有副作用的语句(例如，触发器、存储过程、用户定义的函数等)，可能会在每个副本上产生不同的副作用</u>。

​	<u>有可能采取一些特殊措施来解决这些问题，例如，主节点可以在记录操作语句时将非确定性函数替换为执行之后的确定的结果，这样所有节点直接使用相同的结果值。但是，这里面存在太多边界条件需要考虑，因此目前通常首选的是其他复制实现方案</u>。

​	<u>**MySQL5.1版本之前采用基于操作语句的复制**。现在由于逻辑紧凑，依然在用，**但是默认情况下，如果语句中存在一些不确定性操作，则 MySQL会切换到基于行的复制(稍后讨论)**。 VoltDB使用基于语句的复制，它通过事务级别的确定性来保证复制的安全</u>。

#### 基于预写日志(WAL)传输

​	在第3章中，我们讨论了存储引擎的磁盘数据结构，<u>通常毎个**写操作**都是以**追加写**的方式写入到日志中</u>：

+ 对于日志结构存储引擎(参阅第3章的“SSTables和LSM-trees”)，日志是主要的存储方式。日志段在后台压缩并支持垃圾回收。
+ 对于采用覆盖写磁盘的B-tree(参阅第3章的“B-tree”)结构，<u>**每次修改会预先写入日志**，如系统发生崩溃，通过索引更新的方式迅速恢复到此前一致状态</u>。

​	**不管哪种情况，所有对数据库写入的字节序列都被记入日志。因此可以使用完全相同的日志在另一个节点上构建副本：除了将日志写入磁盘之外，主节点还可以通过网络将其发送给从节点**。

​	从节点收到日志进行处理，建立和主节点内容完全相同的数据副本。

​	PostgreSQL、 Oracle以及其他系统等支持这种复制方式。其**主要缺点是日志描述的数据结果非常底层**：**一个WAL包含了哪些磁盘块的哪些字节发生改变，诸如此类的细节**。**这使得复制方案和存储引擎紧密耦合**。<u>如果数据库的存储格式从一个版本改为另一个版本，那么系统通常无法支持主从节点上运行不同版本的软件</u>。

​	看起来这似乎只是个有关实现方面的小细节，但可能对运营产生巨大的影响。<u>如果复制协议允许从节点的软件版本比主节点更新，则可以实现数据库软件的不停机升级：首先升级从节点，然后执行主节点切换，使升级后的从节点成为新的主节点</u>。相反，**<u>复制协议如果要求版本必须严格一致(例如WAL传输)，那么就势必以停机为代价</u>**。

#### 基于行的逻辑日志复制

​	另一种方法是**<u>复制和存储引擎采用不同的日志格式，这样复制与存储逻辑剥离</u>**。<u>这种复制日志称为**逻辑日志**，以区分物理存储引擎的数据表示</u>。

​	<u>关系数据库的**逻辑日志**通常是指一系列记录来描述数据表**行级别**的**写请求**</u>：

+ 对于行插入，日志包含所有相关列的新值。
+ <u>对于行删除，日志里有足够的信息来唯一标识已删除的行，通常是靠主键，但如果表上没有定义主键，就需要记录所有列的旧值</u>。
+ 对于行更新，日志包含足够的信息来唯一标识更新的行，以及所有列的新值(或至少包含所有已更新列的新值)。

​	**如果一条事务涉及多行的修改，则会产生多个这样的日志记录，并在后面跟着一条记录，指出该事务已经提交**。 MySQL的二进制日志binlog(当配置为**基于行的复制**时)使用该方式。

​	**由于逻辑日志与存储引擎逻辑解耦，因此可以更容易地保持向后兼容，从而使主从节点能够运行不同版本的软件甚至是不同的存储引擎**。

​	对于外部应用程序来说，逻辑日志格式也更容易解析。如果要将数据库的內容发送到外部系统(如用于离线分析的数据仓库)，或构建自定义索引和缓存等，基于逻辑日志的复制更有优势。该技术也被称为**变更数据捕获**，我们将在第11章中继续讨论。

#### 基于触发器的复制

​	到目前为止所描述的复制方法都是由数据库系统来实现的，不涉及任何应用程序代码。通常这是大家所渴望的，不过，在某些情况下，我们可能需要更高的灵活性。例如，只想复制数据的一部分，或者想从一种数据库复制到另一种数据库，或者需要订制、管理冲突解决逻辑(参阅本章后面的“处理写冲突”)，则需要将复制控制交给应用程序层。

​	有一些工具，例如Oracle Golden Gate，可以通过读取数据库日志让应用程序获取数据变更。另一种方法则是借助许多关系数据库都支持的功能：<u>触发器</u>和<u>存储过程</u>。

​	触发器支持注册自己的应用层代码，使得当数据库系统发生数据更改(写事务)时自动执行上述自定义代码。通过触发器技术，可以将数据更改记录到一个单独的表中，然后外部处理逻辑访问该表，实施必要的自定义应用层逻辑，例如将数据更改复制到另一个系统。 Oracle的Databus和Postgres的Bucardo就是这种技术的典型代表。

​	<u>基于触发器的复制通常比其他复制方式开销更高，也比数据库内置复制更容易出错，或者暴露一些限制。然而，其高度灵活性仍有用武之地</u>。

## * 5.2 复制滞后问题

​	<u>容忍节点故障</u>只是使用复制其中的一个原因。正如第二部分开头所介绍的，其他原因包括<u>可扩展性</u>(采用多节点来处理更多的请求)和<u>低延迟</u>(将副本部署在地理上距离用户更近的地方)。

​	**主从复制要求所有写请求都经由主节点，而任何副本只能接受只读査询**。对于读操作密集的负载(如Web)，这是一个不错的选择：创建多个从副本，将读请求分发给这些从副本，从而减轻主节点负载并允许读取请求就近满足。

​	在这种扩展体系下，只需添加更多的从副本，就可以提高读请求的服务吞吐量。<u>但是，这种方法实际上只能用于**异步复制**，如果试图同步复制所有的从副本，则单个节点故障或网络中断将使整个系统无法写入</u>。而且**节点越多，发生故障的概率越高，所以完全同步的配置现实中反而非常不可靠**。

​	不幸的是，<u>如果一个应用正好从一个**异步**的从节点读取数据，而该副本落后于主节点，则应用可能会读到过期的信息。这会导致数据库中出现明显的不一致：由于并非所有的写入都反映在从副本上，如果同时对主节点和从节点发起相同的査询，可能会得到不同的结果</u>。这种不一致只是一个暂时的状态，如果停止写数据库，经过一段时间之后，从节点最终会赶上并与主节点保持一致。这种效应也被称为**最终一致性**。

​	"最终"一词有些含糊不清，总的来说，副本落后的程度理论上并没有上限。正常情况下，主节点和从节点上完成写操作之间的时间延迟(复制滞后)可能不足1秒，这样的滞后，在实践中通常不会导致太大影响。但是，<u>如果系统已接近设计上限，或者网络存在问题，则滞后可能轻松增加到几秒甚至几分钟不等</u>。

​	当滞后时间太长时，导致的不一致性不仅仅是一个理论存在的问题，而是个实实在在的现实问题。在本节中，我们将重点介绍三个复制滞后可能出现的问题，并给出相应的解决思路。

### 5.2.1 读自己的写

​	许多应用让用户提交一些数据，接下来查看他们自己所提交的内容。例如客户数据库中的记录，亦或者是讨论主题的评论等。**提交新数据须发送到主节点，但是当用户读取数据时，数据可能来自从节点**。这对于**读密集**和偶尔写入的负载是个非常合适的方案。

​	然而对于异步复制存在这样一个问题，如图5-3所示，**用户在写入不久即查看数据，则新数据可能尚未到达从节点**。对用户来讲，看起来似乎是刚刚提交的数据丢失了，显然用户不会高兴。

![复制延迟问题 - 图1](https://static.sitestack.cn/projects/ddia/img/fig5-3.png)

​	对于这种情况，我们需要“**写后读一致性**”，也称为**读写一致性**。该机制保证如果用户重新加载页面，他们总能看到自己最近提交的更新。但对其他用户则没有任何保证，这些用户的更新可能会在稍后才能刷新看到。

​	基于主从复制的系统该如何实现写后读一致性呢？有多种可行的方案，以下例举一二：

+ **如果用户访问可能会被修改的内容，从主节点读取；否则，在从节点读取**。这背后就要求有一些方法在实际执行查询之前，就已经知道内容是否可能会被修改。例如，社交网络上的用户首页信息通常只能由所有者编辑，而其他人无法编辑。因此，这就形成一个简单的规则：总是从主节点读取用户自己的首页配置文件，而在从节点读取其他用户的配置文件。
+ **<u>如果应用的大部分内容都可能被所有用户修改，那么上述方法将不太有效，它会导致大部分内容都必须经由主节点，这就丧失了读操作的扩展性</u>**。此时需要其他方案来判断是否从主节点读取。例如，<u>跟踪最近更新的时间，如果更新后一分钟之內，则总是在主节点读取；并监控从节点的复制滞后程度，避免从那些滞后时间超过一分钟的从节点读取</u>。
+ <u>客户端还可以记住最近更新时的时间戳，并附带在读请求中，据此信息，系统可以确保对该用户提供读服务吋都应该至少包含了该时间戳的更新。如果不够新要么交由另一个副本来处理，要么等待直到副本接收到了最近的更新</u>。时间戳可以是**逻辑时间戳**(例如用来指示写入顺序的日志序列号)或实际系统时钟(在这种情况下，时钟同步又称为一个关键点，请参阅第8章“不可靠的时钟”)。
+ 如果副本分布在多数据中心(例如考虑与用户的地理接近，以及高可用性)，情况会更复杂些。<u>必须先把请求路由到主节点所在的数据中心</u>(该数据中心可能离用户很远)。

​	如果同一用户可能会从多个设备访问数据，例如一个桌面Web浏览器和一个移动端的应用，情况会变得更加复杂。此时，要提供跨设备的写后读一致性，即如果用户在某个设备上输入了一些信息然后在另一台设备上查看，也应该看到刚刚所输入的内容。

​	在这种情况下，还有一些需要考虑的问题：

+ 记住用户上次更新时间戳的方法实现起来会比较困难，因为在一台设备上运行的代码完全无法知道在其他设备上发生了什么。此时，<u>元数据必须做到全局共享</u>。
+ 如果副本分布在多数据中心，无法保证来自不同设备的连接经过路由之后都到达同一个数据中心。例如，用户的台式计算机使用了家庭宽带连接，而移动设备则使用蜂窝数据网络，不同设备的网络连接线路可能完全不同。<u>如果方案要求必须从主节点读取，则首先需要想办法确保将来自不同设备的请求路由到同一个数据中心</u>。

### 5.2.2 单调读

​	在前面异步复制读异常的第二个例子里，出现了用户数据向后回滚的奇怪情况。

​	假定用户从不同副本进行了多次读取，如图5-4所示，用户刷新一个网页，读请求可能被随机路由到某个从节点。用户2345先后在两个从节点上执行了两次完全相同的查询(先是少量滞后的节点，然后是滞后很大的从节点)，则很有可能出现以下情况。第一个査询返回了最近用户1234所添加的评论，但第二个查询因为滞后的原因，还没有收到更新因而返回结果是空。实际上，第二个查询结果代表了更早时间点的状态。如果第一个查询没有返回任何内容，用户2345并不知道用户1234最近的评论，情况还不算太糟糕，但当用户2345看到了用户1234的评论之后，紧接着评论又消失了，他就会感觉很困惑。

![复制延迟问题 - 图2](https://static.sitestack.cn/projects/ddia/img/fig5-4.png)

​	单调读一致性可以确保不会发生这种异常。这是一个比强一致性弱，但比最终一致性强的保证。当读取数据时，**<u>单调读保证，如果某个用户依次进行多次读取，则他绝不会看到回滚现象，即在读取较新值之后又发生读旧值的情况</u>**。

​	<u>实现单调读的一种方式是，确保每个用户总是从**固定的同一副本执行读取**(而不同的用户可以从不同的副本读取)</u>。例如，<u>基于用户ID的哈希的方法而不是随机选择副本。但如果该副本发生失效，则用户的查询必须重新路由到另一个副本</u>。

### 5.2.3 前缀一致读

​	第三个由于复制滞后导致因果反常的例子。例如Poons先生与Cake夫人之间的以下对话：

+ Poons先生

  Cake夫人，您能看到多远的未来？

+ Cake夫人

  通常约10s， Poons先生。

​	这两句话之间存在因果关系：Cake夫人首先是听到了Poons先生的问题，然后再去回答该问题。

​	现在，想象第三个人正在通过从节点收听上述对话。Cake夫人所说的话经历了短暂的滞后到达该从节点，但 Poons先生所说的经历了更长的滞后才到达(见图5-5)。观察者听到的对话变成这样:

+ Cake夫人

  通常约10s， Poons先生。

+ Poons先生

  Cake夫人，您能看到多远的未来?

​	对于观察者来说，似乎在Poon先生提出问题之前，Cake夫人就开始了回答问题。首先，这种超自然的力量确认令人印象深刻，但逻辑上却是混乱的。

​	<u>防止这种异常需要引入另一种保证：**前缀一致读**。该保证是说，对于一系列按照某个顺序发生的写请求，那么读取这些内容时也会按照当时写入的顺序</u>。

​	这是分区(分片)数据库中出现的一个特殊问题，细节将在第6章中讨论。**如果数据库总是以相同的顺序写入，则读取总是看到一致的序列，不会发生这种反常**。<u>**然而，在许多分布式数据库中，不同的分区独立运行，因此不存在全局写入顺序。这就导致当用户从数据库中读数据时，可能会看到数据库的某部分旧值和另一部分新值**</u>。

![复制延迟问题 - 图3](https://static.sitestack.cn/projects/ddia/img/fig5-5.png)

​	**一个解决方案是确保任何具有因果顺序关系的写入都交给一个分区来完成，但该方案真实实现效率会大打折扣**。现在有一些新的算法来显式地追踪事件因果关系，在本章稍后的"**Happened-before关系与并发**"继续该问题的探讨。

### 5.2.4 复制滞后的解决方案

​	使用最终一致性系统时，最好事先就思考这样的问题：如果复制延迟增加到几分钟甚至几小时，那么应用层的行为会是什么样子？如果答案是"没问题"，那没得说。但是，如果带来糟糕的用户体验，那么在设计系统时，就要考虑提供一个更强的一致性保证，比如**写后读**；<u>如果系统设计时假定是同步复制，但最终它事实上成为了异步复制，就可能会导致灾难性后果</u>。

​	正如前面所讨论的，<u>在应用层可以提供比底层数据库更强有力的保证。例如只在主节点上进行特定类型的读取，而代价则是，应用层代码中处理这些问题通常会非常复杂，且容易出错</u>。

​	如果应用程序开发人员不必担心这么多底层的复制问题，而是假定数据库"做正确的事情"，情况就变得很简单。而这也是事务存在的原因，事务是数据库提供更强保证的一种方式。

​	**单节点上支持事务已经非常成熟**。<u>然而，在转向分布式数据库(即支持复制和分区)的过程中，有许多系统却选择放弃支持事务，并声称事务在性能与可用性方面代价过高，然后断言在可扩展的分布式系统中最终的一致性是无法避免的终极选择</u>。关于这样的表述，首先它有一定道理，但情况远不是所说的那么简单，我们将在本书其余部分展开讨论，尝试形成一个更为深入的观点。例如在第7章和第9章将理解事务，然后在第三部分再介绍其他一些替代机制。

## 5.3 多主节点复制

​	到目前为止，我们只考虑了单个主节点的主从复制架构。主从复制方法较为常见，但也存在其他一些有趣的方案。

​	首先，**主从复制存在一个明显的缺点：系统只有一个主节点，而所有写入都必须经由主节点**。如果由于某种原因，例如与主节点之间的网络中断而导致主节点无法连接，主从复制方案就会影响所有的写入操作。

​	<u>对主从复制模型进行自然的扩展，则可以配置多个主节点，每个主节点都可以接受写操作，后面复制的流程类似：处理写的每个主节点都必须将该数据更改转发到所有其他节点。这就是多主节点(也称为主-主，或主动/主动)复制</u>。此时，**每个主节点还同时扮演其他主节点的从节点**。

### 5.3.1 适用场景

​	**在一个数据中心内部使用多主节点基本没有太大意义，其复杂性已经超过所能带来的好处**。但是，在以下场景这种配置则是合理的。

#### 多数据中心

​	为了容忍整个数据中心级别故障或者更接近用户，可以把数据库的副本横跨多个数据中心。而如果使用常规的基于主从的复制模型，主节点势必只能放在其中的某一个数据中心，而所有写请求都必须经过该数据中心。

​	有了多主节点复制模型，则可以在每个数据中心都配置主节点，如图5-6所示的基本架构。**在每个数据中心内，采用常规的主从复制方案；而在数据中心之间，由各个数据中心的主节点来负责同其他数据中心的主节点进行数据的交换、更新**。

![多主复制 - 图1](https://static.sitestack.cn/projects/ddia/img/fig5-6.png)

​	可以对比一下在多数据中心环境下，部署单主节点的主从复制方案与多主复制方案之间的差异：

+ 性能

  **对于主从复制，每个写请求都必须经由广域网传送至主节点所在的数据中心。这会大大增加写入延迟，并基本偏离了采用多数据中心的初衷(即就近访问)**。<u>**而在多主节点模型中，每个写操作都可以在本地数据中心快速响应，然后采用异步复制方式将变化同步到其他数据中心**</u>。因此，对上层应用有效屏蔽了数据中心之间的网络延迟，使得终端用户所体验到的性能更好。

+ 容忍数据中心失效

  对于主从复制，如果主节点所在的数据中心发生故障，必须切换至另一个数据中将其中的一个从节点被提升为主节点。在多主节点模型中，每个数据中心则可以独立于其他数据中心继续运行，发生故障的数据中心在恢复之后更新到最新状态。

+ 容忍网络问题

  数据中心之间的通信通常经由广域网，它往往不如数据中心内的本地网络可靠。对于主从复制模型，由于写请求是同步操作，对数据中心之间的网络性能和稳定性等更加依赖。多主节点模型则通常采用异步复制，可以更好地容忍此类问题，例如临时网络闪断不会妨碍写请求最终成功。

​	有些数据库已内嵌支持了多主复制，但有些则借助外部工具来实现，例如 MySQL的Tungsten Replicator， PostgreSQL的BDR以及Oracle的Golden Gate。

​	尽管多主复制具有上述优势，但也存在一个很大的缺点：**<u>不同的数据中心可能会同时修改相同的数据，因而必须解决潜在的写冲突(如图5-6中的“冲突解决”)</u>**。我们会在本章稍后的“处理写入冲突”详细介绍。

​	由于多主复制在许多数据库中还只是新增的高级功能，所以可能存在配置方面的细小缺陷；在与其他数据库功能(例如自增主键，触发器和完整性约束等)交互时有时会出现意想不到的副作用。出于这个原因，一些人认为多主复制比较危险，应该谨慎使用或者避免使用。

#### 离线客户端操作

​	另一种多主复制比较适合的场景是，应用在与网络断开后还需要继续工作。

​	比如手机，笔记本电脑和其他设备上的日历应用程序。无论设备当前是否联网，都需要能够随时查看当前的会议安排(对应于读请求)或者添加新的会议(对应于写请求)。在离线状态下进行的任何更改，会在下次设备上线时，与服务器以及其他设备同步。

​	这种情况下，<u>每个设备都有一个充当主节点的本地数据库(用来接受写请求)</u>，然后在所有设备之间采用异步方式同步这些多主节点上的副本，同步滞后可能是几小时或者数天，具体时间取决于设备何时可以再次联网。

​	从架构层面来看，上述设置基本上等同于数据中心之间的多主复制，只不过是个极端情况，即一个设备就是数据中心，而且它们之间的网络连接非常不可靠。多个设备同步日历的例子表明，多主节点可以得到想要的结果，但中间过程依然有很多的未知数。

​	有一些工具可以使多主配置更为容易，如CouchDB就是为这种操作模式而设计的。

#### 协作编辑

​	实时协作编辑应用程序允许多个用户同时编辑文档。例如， Etherpad 和 Google Docs允许多人同时编辑文本文档或电子表格(算法简要会在本章后面的“自动冲突解决”中讨论)。

​	我们通常不会将协作编辑完全等价于数据库复制问题，但二者确实有很多相似之处。当一个用户编辑文档时，所做的更改会立即应用到本地副本(Web浏览器或客户端应用程序)，然后异步复制到服务器以及编辑同一文档的其他用户。

​	如果要确保不会发生编辑冲突，则应用程序必须先将文档锁定，然后才能对其进行编辑。如<u>果另一个用户想要编辑同一个文档，首先必须等到第一个用户提交修改并释放锁。这种协作模式相当于主从复制模型下在主节点上执行事务操作</u>。

​	为了加快协作编辑的效率，可编辑的粒度需要非常小。例如，单个按键甚至是全程无锁。然而另一方面，也会面临所有多主复制都存在的挑战，即如何解决冲突。

### 5.3.2 处理写冲突

​	多主复制的最大冋题是可能发生写冲突，这意味着必须有方案来解决冲突。

​	例如，两个用户同时编辑Wiki页面，如图5-7所示。用户1将页面的标题从A更改为B，与此同时用户2却将标题从A改为C。每个用户的更改都顺利地提交到本地主节点。但是，当更改被异步复制到对方时，却发现存在冲突。注意，正常情况下的主从复制则不会出现这种情况。

![多主复制 - 图2](https://static.sitestack.cn/projects/ddia/img/fig5-7.png)

#### 同步与异步冲突检测

​	**如果是主从复制数据库，第二个写请求要么会被阻塞直到第一个写完成，要么被中止(用户必须重试)**。<u>然而在多主节点的复制模型下，这两个写请求都是成功的，并且只能在稍后的时间点上才能异步检测到冲突，那时再要求用户层来解决冲突为时已晚</u>。

​	理论上，也可以做到同步冲突检测，即等待写请求完成对所有副本的同步，然后再通知用户写入成功。但是，这样做将会失去多主节点的主要优势：允许每个主节点独立接受写请求。<u>如果确实想要同步方式冲突检测，或许应该考虑采用**单主节点**的主从复制模型</u>。

#### 避免冲突

​	处理冲突最理想的策略是避免发生冲突，即如果应用层可以保证对特定记录的写请求总是通过同一个主节点，这样就不会发生写冲突。**现实中，由于不少多主节点复制模型所实现的冲突解决方案存在瑕疵，因此，<u>避免冲突反而成为大家普遍推荐的首选方案</u>**。

​	<u>例如，一个应用系统中，用户需要更新自己的数据，那么我们确保**特定用户的更新请求总是路由到特定的数据中心**，并在该数据中心的主节点上进行读/写。不同的用户则可能对应不同的主数据中心(例如根据用户的地理位置来选择)。从用户的角度来看，这基本等价于主从复制模型</u>。

​	但是，有时可能需要改变事先指定的主节点，例如由于该数据中心发生故障，不得不将流量重新路由到其他数据中心，或者是因为用户已经漫游到另一个位置，因而更靠近新数据中心。此时，冲突避免方式不再有效，必须有措施来处理同时写入冲突的可能性。

#### * 收敛于一致状态

​	**对于主从复制模型，数据更新符合顺序性原则**，即<u>**如果同一个字段有多个更新，则最后一个写操作将决定该字段的最终值**</u>。

​	对于多主节点复制模型，由于不存在这样的写入顺序，所以最终值也会变得不确定。在图5-7中，主节点1接受到请求把标题更新为B，然后更新为C；而在主节点2，则是相反的更新顺序。两者都无法辩驳谁更正确。

​	**<u>如果每个副本都只是按照它所看到写入的顺序执行，那么数据库最终将处于不一致状态</u>**。例如主节点1看到最终值C，而主节点2看到的是B，这绝对是不可接受的，**<u>所有的复制模型至少应该确保数据在所有副本中最终状态一定是一致的</u>**。因此，数据库必须以一种收敛趋同的方式来解决冲突，这也意味着当所有更改最终被复制、同步之后，所有副本的最终值是相同的。

​	实现收敛的冲突解决有以下可能的方式：

+ 给每个写入分配唯一的ID，例如，一个时间戳，一个足够长的随机数，一个UUID或者一个基于键-值的哈希，挑选最高ID的写入作为胜利者，并将其他写入丢弃。**如果基于时间戳，这种技术被称为最后写入者获胜**。虽然这种方法很流行但是很容易造成数据丢失。我们将在本章最后部分来详细解释。
+ 为每个副本分配一个唯一的ID，并制定规则，例如<u>序号高的副本写入始终优先于序号低的副本</u>。这种方法也可能会导致数据丢失。
+ 以某种方式将这些值合并在一起。例如，按字母顺序排序，然后拼接在一起(图5-7中，合并的标题可能类似于“B/C”)。
+ 利用预定义好的格式来记录和保留冲突相关的所有信息，然后依靠应用层的逻辑，事后解决冲突(可能会提示用户)。

#### * 自定义冲突解决逻辑

​	<u>解决冲突最合适的方式可能还是依靠应用层</u>，所以大多数多主节点复制模型都有工具来让用户编写应用代码来解决冲突。可以在写入时或在读取时执行这些代码逻辑：

+ 在写入时执行

  **只要数据库系统在复制变更日志时检测到冲突，就会调用应用层的冲突处理程序**。例如，Bucardo支持编写一段Perl代码。这个处理程序通常不能在线提示用户，而只能在后台运行，这样速度更快。

+ 在读取时执行

  **<u>当检测到冲突时，所有冲突写入值都会暂时保存下来</u>。下一次读取数据时，会将数据的多个版本读返回给应用层。应用层可能会提示用户或自动解决冲突，并将最后的结果返回到数据库**。 CouchDB采用了这样的处理方式。

​	**<u>注意，冲突解决通常用于单个行或文档，而不是整个事务</u>**。因此，**<u>如果有一个原子事务包含多个不同写请求(如第7章)，每个写请求仍然是分开考虑来解决冲突</u>**。

#### 什么是冲突？

​	有些冲突是显而易见的。在图5-7的例子中，两个写操作同时修改同一个记录中的同一个字段，并将其设置为不同的值。毫无疑问，这就是一个冲突。

​	而其他类型的冲突可能会非常微妙，更难以发现。例如一个会议室预订系统，它主要记录哪个房间由哪个人在哪个时间段所预订。这个应用程序需要确保每个房间只能有一组人同时预定(即不得有相同房间的重复预订)。如果为同一个房间创建两个不同的预订，可能会发生冲突。<u>尽管应用在预订时会检查房间是否可用，但如果两个预订是在两个不同的主节点上进行，则还是存在冲突的可能</u>。

> **自动冲突解决**
>
> ​	冲突解决的规则可能会变得越来越复杂，且自定义代码很容易出错。亚马逊是个经常被引用的反面例子：有一段时间，购物的冲突解决逻辑依靠用户的购物车页面，后者保存了所有的物品，但顾客有时候会发现之前已经被拿掉的商品，再次出现在他们的购物车中。
>
> ​	有一些有意思的研究尝试自动解决并发修改所引起的冲突。下面这些方法值得一看：
>
> 1. **无冲突的复制数据类型(Conflict-free Replicated Datatypes，CRDT)**。CRDT是可以由多个用户同时编辑的数据结构，包括map、 ordered list、计数器等，并且以内置的合理方式自动地解决冲突。一些CRDT已经在Riak2.0中得以具体实现。
> 2. **可合并的持久数据结构(Mergeable persistent data)。它跟踪变更历史，类似于Git版本控制系统，并提出三向合并功能(three-way merge function，CRDT采用双向合并)**。
> 3. **操作转换(Operational transformation)** 。它是Etherpad和Google Docs等协作编辑应用背后的冲突解决算法。专为可同时编辑的有序列表而设计，如文本文档的字符列表。
>
> ​	这些算法总体来讲还处于早期阶段，但将来它们可能会被整合到更多的数据系统中。这些自动冲突解决方案可以使主复制模型更简单、更容易被应用程序来集成。

​	很遗憾，这里没有现成的答案。通过接下来的几章，我们将对这个问题进行深入的剖析和讲解。我们将在第7章看到更多的冲突示例，在第12章中讨论检测和解决冲突的可扩展方法。

### 5.3.3 拓扑结构

​	**复制的拓扑结构描述了写请求从一个节点的传播到其他节点的通信路径**。如果有两个主节点，如图5-7所示，则只存在一个合理的拓扑结构：主节点1必须把所有的写同步到主节点2，反之亦然。但如果存在两个以上的主节点，则会有多个可能的同步拓扑结构，如图5-8所示。

![多主复制 - 图3](https://static.sitestack.cn/projects/ddia/img/fig5-8.png)

​	**最常见的拓扑结构是全部-至-全部，见图5-8(c)，每个主节点将其写入同步到其他所有主节点**。而其他一些拓扑结构也有普遍使用，例如，<u>**默认情况下MySQL只支持环形拓扑结构**，其中的每个节点接收来自前序节点的写入，并将这些写入(加上自己的写入)转发给后序节点</u>。另一种流行的拓扑是星形结构：一个指定的根节点将写入转发给所有其他节点。星形拓扑还可以推广到树状结构。

​	在环形和星形拓扑中，写请求需要通过多个节点才能到达所有的副本，即中间节点需要转发从其他节点收到的数据变更。<u>为防止无限循环，每个节点需要赋予一个唯一的标识符，在复制日志中的每个写请求都标记了已通过的节点标识符</u>。如果某个节点收到了包含自身标识符的数据更改，表明该请求已经被处理过，因此会忽略此变更请求，避免重复转发。

​	**环形和星形拓扑的问题是，如果某一个节点发生了故障，在修复之前，会影响其他节点之间复制日志的转发**。可以采用重新配置拓扑结构的方法暂时排除掉故障节点。在大多数部署中，这种重新配置必须手动完成。而对于链接更密集的拓扑(如全部到全部)，消息可以沿着不同的路径传播，避免了单点故障，因而有更好的容错性。

​	但另一方面，**<u>全链接拓扑也存在一些自身的问题。主要是存在某些网络链路比其他链路更快的情况(例如由于不同网络拥塞)，从而导致复制日志之间的覆盖</u>**，如图5-9所示。

​	在图5-9中，客户端A向主节点1的表中首先插入一行，然后客户端B在主节点3上对该行记录进行更新。而在主节点2上，由于网络原因可能出现意外的写日志复制顺序，例如它先接收到了主节点3的更新日志(从主节点2的角度来看，这是对数据库中不存在行的更新操作)，之后才接收到主节点1的插入日志(按道理应该在更新日志之前到达)。

![多主复制 - 图4](https://static.sitestack.cn/projects/ddia/img/fig5-9.png)

​	<u>这里涉及到一个因果关系问题，类似于在本章前面“**前缀一致读**”所看到的：更新操作一定是依赖于先前完成的插入，因此我们要确保所有节点上一定先接收插入日志，然后再处理更新。在每笔写日志里简单地添加时间戳还不够，**主要因为无法确保时钟完全同步**，因而无法在主节点2上正确地排序所收到日志(参见第8章)</u>。

​	为了使得日志消息正确有序，可以使用一种称为**版本向量**的技术，本章稍后将讨论这种技术(参见本章后面的“检测并发写入”)。需要指出，<u>冲突检测技术在许多多主节点复制系统中的实现还不够完善</u>，例如在撰写本书时， PostgreSQL BDR尚不支持写操作的因果排序，而 MySQL的Tungsten Replicator甚至还没有基本的冲突检测功能。

​	如果正在使用支持多主节点复制的系统，这些问题都值得注意，仔细查阅相关文档，详细测试这些数据库，以确保它确实提供所期望的功能。

## 5.4 无主节点复制

​	到目前为止本章所讨论的复制方法，包括单主节点和多主节点复制，都是基于这样种核心思路，<u>即客户端先向某个节点(主节点)发送写请求，然后数据库系统负责将写请求复制到其他副本。由主节点决定写操作的顺序，从节点按照相同的顺序来应用主节点所发送的写日志</u>。

​	**一些数据存储系统则采用了不同的设计思路：选择放弃主节点，允许任何副本直接接受来自客户端的写请求**。其实最早的数据复制系统就是无主节点的(或称为**去中心复制**，**无中心复制**)，但后来到了关系数据库主导的时代，这个想法被大家选择性遗忘了。当亚马逊内部采用了Dynamo系统之后，无主复制又再次成为一种时髦的数据库架构。Riak、Cassandra和Voldemort都是受Dynamo启发而设计的无主节点、开源数据库系统，这类数据库也被称为Dynamo风格数据库。

​	**对于某些无主节点系统实现，客户端直接将其写请求发送到多副本，而在其他一些实现中，由一个协调者节点代表客户端进行写入，但与主节点的数据库不同，协调者并不负责写入顺序的维护**。我们很快就会看到，这种设计上的差异对数据库的使用方式有着深刻的影响。

### * 5.4.1 节点失效时写入数据库

​	<u>**假设一个三副本数据库，其中一个副本当前不可用(例如正在重启以安装系统更新)。在基于主节点复制模型下，如果要继续处理写操作，则需要执行切换操作**(参阅本章前面的“处理节点失效”)</u>。

​	**对于无主节点配置，则不存在这样的切换操作**。图5-10展示了所发生的情况：用户1234将写请求并行发送到三个副本，有两个可用副本接受写请求，而不可用的副本无法处理该写请求。如果假定三个副本中有两个成功确认写操作，用户1234收到两个确认的回复之后，即可认为写入成功。客户完全可以忽略其中一个副本无法写入的情况。

​	<u>现在设想一下，失效的节点之后重新上线，而客户端又开始从中读取內容。由于节点失效期间发生的任何写入在该节点上都尚未同步，因此读取可能会得到过期的数据</u>。

​	<u>为了解决这个问题，当一个客户端从数据库中读取数据时，它**不是向一个副本发送请求，而是并行地发送到多个副本**。客户端可能会得到不同节点的不同响应，包括某些节点的新值和某些节点的旧值。可以采用**版本号**技术确定哪个值更新</u>(参见本章后面的“检测并发写入”)。

#### * 读修复和反熵

​	**复制模型应确保所有数据最终复制到所有的副本**。当一个失效的节点重新上线之后，它如何赶上中间错过的那些写请求呢?

![无主复制 - 图1](https://static.sitestack.cn/projects/ddia/img/fig5-10.png)

Dynamo风格的数据存储系统经常使用以下两种机制：

+ 读修复

  当客户端并行读取多个副本时，可以检测到过期的返回值。例如，在图5-10中用户2345从副本3获得的是版本6，而从副本1和2得到的是版本7。客户端可以判断副本3一个过期值，然后将新值写入到该副本。这种方法主要**适合那些被频繁读取的场景**。

+ **反熵过程**

  此外，一些数据存储有后台进程不断查找副本之间数据的差异，将任何缺少的数据从一个副本复制到另一个副本。**与基于主节点复制的复制日志不同，此反熵过程并不保证以特定的顺序复制写入，并且会引入明显的同步滞后**。

​	并不是所有的系统都实现了上述两种方案。例如， Voldemort目前没有反熵过程。<u>请注意，当缺少反熵过程的支持时，由于**读时修复只在发生读取时才可能执行修复，那些很少访问的数据有可能在某些副本中已经丢失而无法检测到，从而降低了写的持久性**</u>。

#### * 读写quorum

​	图5-10的例子中，三个副本中如果有两个以上完成处理，写入即可认为成功。如果三个副本中只有一个完成了写请求，会怎样呢？依次类推，究竟多少个副本完成才可以认为写成功?

​	我们知道，<u>成功的写操作要求三个副本中至少两个完成，这意味着至多有一个副本可能包含旧值</u>。<u>因此，**在读取时需要至少向两个副本发起读请求，通过版本号可以确定定至少有一个包含新值**。如果第三个副本出现停机或响应缓慢，则读取仍可以继续并返回最新值</u>。

​	把上述道理推广到一般情况，**<u>如果有n个副本，写入需要w个节点确认，读取必须至少查询r个节点，则只要w+r>n，读取的节点中一定会包含最新值</u>**。例如在前面的例子中，n=3，w=2，r=2。满足上述这些r、w值的读/写操作称之为法定票数读(或仲裁读)或法定票数写(或仲裁写)。**也可以认为r和w是用于判定读、写是否有效的最低票数**。

​	**在Dynamo风格的数据库中，参数n、w和r通常是可配置的**。一个常见的选择是设置n为某奇数(通常为3或5)，w=r=(n+1)/2(向上舍入)。也可以根据自己的需求灵活调整这些配置。<u>例如，对于读多写少的负载，设置w=n和r=1比较合适，这样读取速度更快，但是一个失效的节点就会使得数据库所有写入因无法完成quorum而失败</u>。

> ​	集群中可能存在多于n个节点，但是数据只会保存在所设定的n个节点上。我们可以对数据集进行**分区**，从而支持比节点容纳上限更大的数据集，第6章我们将讨论分区技术。

​	**仲裁条件w+r>n定义了系统可容忍的失效节点数**，如下所示：

+ 当w<n，如果一个节点不可用，仍然可以处理写入。
+ 当r<n，如果一个节点不可用，仍然可以处理读取。
+ 假定n=3，w=2，r=2，则可以容忍一个不可用的节点。
+ 假定n=5，w=3，r=3，则可以容忍两个不可用的节点。如图5-11所示。
+ **<u>通常，读取和写入操作总是并行发送到所有的n个副本。参数w和参数r只是决定要等待的节点数</u>**。即有多少个节点需要返回结果，我们才能判断出结果的正确性。

​	**<u>如果可用节点数小于所需的w或r，则写入或读取就会返回错误</u>**。不可用的原因可能有很多种，包括节点崩溃或者断电而关机，执行操作时岀错(例如磁盘已满而无法写入)，客户端和节点之间的网络中断等。这里，我们只需关心节点是否有返回值，而不需区分出错的具体原因。

![无主复制 - 图2](https://static.sitestack.cn/projects/ddia/img/fig5-11.png)

### * 5.4.2 Quorum 一致性的局限性

​	<u>**如果有n个副本，并且配置w和r，使得w+r>n，可以预期可以读取到一个最新值。之所以这样，是因为成功写入的节点集合和读取的节点集合必然有重合，这样读取的节点中至少有一个具有最新值**</u>(见图5-11)。

​	<u>**通常，设定r和w为简单多数(多于n/2)节点，即可确保w+r>n，且同时容忍多达n/2个节点故障**</u>。但是， quorum不一定非得是多数，**<u>读和写的节点集中有一个重叠的节点才是最关键的</u>**。设定其他的 quorum分配数也是可行的。

​	<u>也可以将w和r设置为较小的数字，从而w+r<=n(即不满足仲裁条件)。此时，读取和写入操作仍会被发送到n个节点，但只需等待更少的节点回应即可返回</u>。

​	由于w和r配置的节点数较小，读取请求当中可能恰好没有包含新值的节点，因此最终可能会返回一个过期的旧值。好的一方面是，这种配置可以获得更低的延迟和更高的可用性，例如网络中断，许多副本变得无法访问，相比而言有更高的概率继续处理读取和写入。只有当可用的副本数已经低于w或r时，数据库才会变得无法读/写，即处于不可用状态。

​	**即使在w+r>n的情况下，也可能存在返回旧值的边界条件**。这主要取决于具体实现，可能的情况包括:

+ 如果采用了sloppy quorum(参阅本章后面的"宽松的quorum与数据回传")，写操作的w节点和读取的r节点可能完全不同，因此无法保证读写请求一定存在重叠的节点。

+ 如果两个写操作同时发生，则无法明确先后顺序。这种情况下，唯一安全的解决方案是合并并发写入(参见本章前面的“处理写冲突”)。如果根据时间戳(最后写入获胜)挑选胜者，则由于时钟偏差问题，某些写入可能会被错误地抛弃。
+ **如果写操作与读操作同时发生，写操作可能仅在一部分副本上完成。此时，读取时返回旧值还是新值存在不确定性**。
+ **如果某些副本上已经写入成功，而其他一些副本发生写入失败(例如磁盘巳满)，且总的成功副本数少于w，那些已成功的副木上不会做回滚**。这意味着尽管这样的写操作被视为失败，后续的读操作仍可能返回新值。
+ **<u>如果具有新值的节点后来发生失效，但恢复数据来自某个旧值，则总的新值副本数会低于w，这就打破了之前的判定条件</u>**。
+ 即使一切工作正常，也会出现一些边界情况，如第9章所介绍的“可线性化与quorun“。

​	**<u>因此，虽然quorum设计上似乎可以保证读取最新值，但现实情况却往往更加复杂</u>**。<u>Dynamo风格的数据库通常是针对最终一致性场景而优化的。我们建议最妤不要把参数w和r视为绝对的保证，而是一种灵活可调的读取新值的概率。</u>

​	例如，<u>这里通常无法得到本章前面的“复制滞后问题”中所罗列的一致性保证，包括**写后读**、**单调读**、**前缀一致读**等，因此前面讨论种种异常同样会发生在这里</u>。**如果确实需要更强的保证，需要考虑事务与共识问题**，接下来的第7章和第9章将对此展开讨论。

#### * 监控旧值

​	从运维角度来看，监视数据库是否返回最新结果非常重要。即使应用程序可以容忍读取旧值，也需要仔细了解复制的当前运行状态。<u>如果已经出现了明显的滞后，它就是个重要的信号提醒我们需要采取必要措施来排查原因</u>(例如网络问题或节点超负荷)。

​	对于主从复制的系统，数据库通常会导出复制滞后的相关指标，可以将其集成到统一监控模块。原理大概是这样，**由于主节点和从节点上写入都遵从相同的顺序，而毎个节点都维护了复制日志执行的当前偏移量。<u>通过对比主节点和从节点当前偏移量的差值，即可衡量该从节点落后于主节点的程度</u>**。

​	然而，<u>对于无主节点复制的系统，并**没有固定的写入顺序**，因而监控就变得更加困难</u>。而且，<u>如果数据库只支持读时修复(不支持反熵)，那么旧值的落后就没有一个上限。例如如果一个值很少被访问，那么所返回的旧值可能非常之古老</u>。

​	目前针对无主节点复制系统已经有一些研究，根据参数n，w和r来预测读到旧值的期望百分比。不过，总体讲还不是很普及。即便如此，**将旧值监控纳入到数据库标准指标集中还是很有必要**。要知道，**最终一致性其实是个非常模糊的保证，从可操作性上讲，量化究竟何为“最终”很有实际价值**。

#### * 宽松的quorum与数据回传

​	<u>配置适当quorum的数据库系统可以容忍某些节点故障，也不需要执行故障切换。它们还可以容忍某些节点变慢，这是因为请求并不需要等待所有n个节点的响应，只需w或r节点响应即可。对于需要高可用和低延迟的场景来说，还可以容忍偶尔读取旧值，所有这些特性使之具有很高的吸引力</u>。

​	但是， quorum并不总如期待的那样提供高容错能力。一个网络中断可以很容易切断一个客户端到多数数据库节点的链接。尽管这些集群节点是活着的，而且其他客户端也确实可以正常链接，但是**对于断掉链接的客户端来讲，情况无疑等价于集群整体失效**。这种情况下，很可能无法满足最低的w和r所要求的节点数，因此导致客户端无法满足quorum要求。

​	在一个大规模集群中(节点数远大于n个)，<u>客户可能在网络中断期间还能连接到某些数据库节点，但这些节点又不是能够满足数据仲裁的那些节点</u>。此时，数据库设计者就面临着一个选择：

+ 如果无法达到w或r所要求quorum，将错误明确地返回给客户端?
+ 或者，我们是否应该接受该写请求，只是将它们暂时写入一些可访问的节点中？注意，<u>这些节点并不在n个节点集合中</u>。

​	**后一种方案称之为放松的仲裁：写入和读取仍然需要w和r个成功的响应，但包含了那些并不在先前指定的n个节点**。打个比方，如果你把不小心把自己锁在房子外面，可能会敲开邻居家的门，请求否可以坐在沙发上暂时休息一下。

​	**一旦网络问题得到解决，临时节点需要把接收到的写入全部发送到原始主节点上**。<u>这就是所谓的数据回传(或暗示移交)</u>。即一旦你找到了房子的钥匙，你的邻居会礼貌地请你离开沙发回到自己的家中。

​	可以看出， **<u>sloppy quorum对于提高写入可用性特别有用：只要有任何w个节点可用，数据库就可以接受新的写入</u>**。<u>然而这意味着，即使满足w+r>n，也不能保证在读取某个键时，一定能读到最新值，因为**新值可能被临时写入n之外的某些节点且尚未回传过来**</u>。

​	因此，**sloppy quorum并非传统意义上quorum。而更像是为了数据持久性而设计的一个保证措施，除非回传结束，否则它无法保证客户端一定能从r个节点读到新值**。

​	**<u>目前，所有Dynamo风格的系统都已经支持sloppy quorum</u>**。在Riak中，默认启用，而在 Cassandra和 Voldemort中则默认关闭。

#### 多数据中心操作

​	我们之前以多数据中心为例介绍了多主节点复制(参见本章前面的“多主节点复制”)。而<u>无主节点复制由于旨在更好地容忍并发写入冲突</u>，网络中断和延迟尖峰等，因此也可适用于多数据中心操作。

​	Cassandra和Voldemort在其默认配置的无主节点模型中都支持跨数据中心操作：<u>副本的数量n是包含所有数据中心的节点总数</u>。配置时，可以指定每个数据中心各有多少副本。每个客户端的写入都会发送到所有副本，但客户端通常只会等待来自本地数据中心内的quorum节点数的确认，这样避免了高延迟和跨数据中心可能的网络异常。<u>尽管可以灵活配置，但对远程数据中心的写入由于延迟很高，通常都被配置为**异步方式**</u>。

​	Riak则将客户端与数据库节点之间的通信限制在一个数据中心内，因此<u>n描述的是以个数据中心内的副本数量</u>。<u>集群之间跨数据中心的复制则在后台异步运行，类似于多主节点复制风格</u>。

### * 5.4.3 检测并发写

​	Dynamo风格的数据库允许多个客户端对相同的主键同时发起写操作，即使采用严格的quorum机制也可能会发生写冲突。这与多主节复制类似(参见本章前面的“处理写冲突”)，此外，由于读时修复或者数据回传也会导致并发写冲突。

​	一个核心问题是，由于网络延迟不稳定或者局部失效，请求在不同的节点上可能会呈现不同的顺序。如图5-12所示，对于包含三个节点的数据系统，客户端A和B同时向主键X发起写请求：

+ 节点1收到来自客户端A的写请求，但由于节点失效，没有收到客户端B的写请求。

+ 节点2首先收到A的写请求，然后是B的写请求。
+ 与节点2相反，节点3首先收到B的写请求，然后是A的写请求。

![无主复制 - 图3](https://static.sitestack.cn/projects/ddia/img/fig5-12.png)

​	**如果节点每当收到新的写请求时就简单地覆盖原有的主键，那么这些节点将永久无法达成一致**，如图5-12中的所示，节点2认为X的最终值是B，而其他节点认为值是A。

​	我们知道副本应该收敛于相同的内容，这样才能达成最终一致。但如何才能做到呢？有人可能希望数据副本之间能自动处理，然而非常不幸，<u>目前大多数的系统实现都无法令人满意，如果你不想丢失数据，应用开发者必须了解很多关于数据库内部冲突处理的机制</u>。

​	我们已经在本章前面的“处理写冲突”简要介绍了一些解决冲突的技巧。在总结本章之前，我们来更详细地探讨这个问题。

#### 最后写入者获胜(丢弃并发写入)

​	**一种实现最终收敛的方法是，每个副本总是保存最新值，允许覆盖并丢弃旧值**。那么，<u>假定每个写请求都最终同步到所有副本，只要我们有一个明确的方法来确定哪个写入是最新的，则副本可以最终收敛到相同的值</u>。

​	这个想法其实有些争议，**关键点在于前面所提到关于如何定义“最新”**。在图5-12的例子中，当客户端向数据库节点发送写请求时，一个客户端无法意识到另一个客户端，也不清楚哪一个先发生。其实，争辩哪个先发生没有太大意义，当我们说支持写入并发，也就意味着它们的顺序是不确定的。

​	**即使无法确定写请求的“自然顺序”，我们可以强制对其排序**。例如，<u>为每个写请求附加一个时间戳，然后选择最新即最大的时间戳，丢弃较早时间戳的写入。这个冲突解决算法被称为**最后写入者获胜(last write wins，LWW)**</u>，它是Cassandra仅有的冲突解决方法，而在Riak中，它是可选方案之一。

​	**LWW可以实现最终收敛的目标，但是以牺牲数据持久性为代价**。<u>如果同一个主键有多个并发写，即使这些并发写都向客户端报告成功(因为完成了写入w个副本)，但**最后只有一个写入值会存活下来**，其他的将被系统默默丢弃。此外，LWW甚至可能会删除那些非并发写，我们将在第8章“时间戳与事件顺序”中举例说明</u>。

​	在一些场景如缓存系统，覆盖写是可以接受的。如果覆盖、丟失数据不可接受，则LWW并不是解决冲突很好的选择。

​	**要确保LWW安全无副作用的唯一方法是，<u>只写入一次然后写入值视为不可变</u>，这样就避免了对同一个主键的并发(覆盖)写**。例如， Cassandra的一个推荐使用方法就是采用UUID作为主键，这样每个写操作都针对的不同的、系统唯一的主键。

#### * Happens-before关系和并发

​	如何判断两个操作是否是并发呢？首先为了建立起一个快速的直觉判断，我们先来看一些例子：

+ 图5-9中，两个写入不是并发的：A的插入操作发生在B的增量修改之前，B的递增是基于A插入的行。换句话说，B后发生，其操作建立在A基础之上。A和B属于**因果依赖**关系。
+ 另一个例子，图5-12中的两个写入则是并发的:每个客户端启动写操作时，并不知道另一个客户端是否也在同一个主键上执行操作。因此，操作之间不存在**因果关系**。

​	如果B知道A，或者依赖于A，或者以某种方式在A基础上构建，则称操作A在操作B之前发生。这是定义何为并发的关键。事实上，我们也可以简单地说，**如果两个操作都不在另一个之前发生，那么操作是并发的(或者两者都不知道对方)**。

​	因此，对于两个操作A和B，一共存在三种可能性：A在B之前发生，或者B在A之前发生，或者A和B并发。我们需要的是一个算法来判定两个操作是否并发。<u>如果一个操作发生在另一个操作之前，则后面的操作可以覆盖较早的操作。如果属于并发，就需要解决潜在的冲突问题</u>。

> **并发性、时间和相对性**
>
> ​	<u>通常如果两个操作“同时”发生，则称之为并发，然而事实上，操作是否在时间上重叠并不重要。**由于分布式系统中复杂的时钟同步问题(第8章将会详细讨论)，现实当中，我们很难严格确定它们是否同时发生**</u>。
>
> ​	**<u>为更好地定义并发性，我们并不依赖确切的发生时间，即不管物理的时机如何，如果两个操作并不需要意识到对方，我们即可声称它们是并发操作</u>**。一些人尝试把这个思路与物理学中狭义相对论联系起来，后者引入了“信息传递不能超越光速”的假定，如果两个事件发生的间隔短于光在它们之间的折返，那么这两个事件不可能有相互影响，因此就是并发。
>
> ​	**在计算机系统中，即使光速快到允许一个操作影响到另一个操作，但两个操作仍可能被定义为并发。例如，发生了网络拥塞或中断，可能就会出现两个操作由于网络问题导致一个操作无法感知另一个，因此二者成为并发**。

#### * 确定前后关系

​	我们来看一个确定操作并发性的算法，即两个操作究竟属于并发还是一个发生在另个之前(依赖关系)。简单起见，我们先从只有一个副本的数据库开始，在阐明其原理之后，将其推广到有多个副本的无主节点数据库。

​	图5-13的例子是两个客户端同时向购物篮车加商品(如果觉得这个例子太微不足道，可以类比为，两个空中交管员同时把飞机添加到他们所管理的追踪目标里)。初始时购物车为空。然后两个客户端向数据库共发出五次写入操作:

1. 客户端1首先将牛奶加入购物车。这是第一次写入该主键的值，服务器保存成功然后分配版本1，服务器将值与版本号一起返回给该客户端1。
2. 客户端2将鸡蛋加入购物车，此时它并不知道客户端1已添加了牛奶，而是认为鸡蛋是购物车中的唯一物品。服务器为此写入并分配版本2，然后将鸡蛋和牛奶存储为两个单独的值，最好将这两个值与版本号2返回给客户端2。
3. 同理，客户端1也并不意识上述步骤2，想要将面粉加入购物车，且以为购物车的内容应该是[牛奶，面粉]，将此值与版本号1一起发送到服务器。服务器可以从版本号中知道[牛奶，面粉]的新值要取代先前值[牛奶]，但值[鸡蛋]则是新的并发操作。因此，服务器将版本3分配给[牛奶，面粉]并覆盖版本1的[牛奶]，同时保留版本2的值[鸡蛋]，将二者同时返回给客户端1。

![无主复制 - 图4](https://static.sitestack.cn/projects/ddia/img/fig5-13.png)

4. 同时，客户端2想要加入火腿，也不知道客户端1刚刚加了面粉。其在最后一个响应中从服务器收到的两个值是[牛奶]和[蛋]，现在合并这些值，并添加火腿形成一个新的值[鸡蛋，牛奶，火腿]。它将该值与前一个版本号2一起发送到服务器。服务器检测到版本2会覆盖[鸡蛋]，但与[牛奶，面粉]是同时发生，所以设置为版本4并将所有这些值发送给客户端2。
5. 最后，客户端1想要加培根。它以前在版本3中从服务器接收[牛奶，面粉]和[鸡蛋]，所以合并这些值，添加培根，并将最终值[牛奶，面粉，鸡蛋，培根]连同版本号3来覆盖[牛奶，面粉]，但与[鸡蛋，牛奶，火腿]并发，所以服务器会保留这些并发值。

​	图5-13操作之间的数据流可以通过图5-14形象展示。箭头表示某个操作发生在另一个操作之前，即后面的操作“知道”或是“依赖”于前面的操作。<u>在这个例子中，因为总有另一个操作同时进行，所以每个客户端都没有时时刻刻和服务器上的数据保持同步。但是，新版本值最终会覆盖旧值，且不会发生已写入值的丢失</u>。

![无主复制 - 图5](https://static.sitestack.cn/projects/ddia/img/fig5-14.png)

​	需要注意的是，服务器判断操作是否并发的依据主要依靠**对比版本号**，而并不需要解释新旧值本身(值可以是任何数据结构)。算法的工作流程如下：

+ 服务器为每个主键维护一个版本号，每当主键新值写入时递增版本号，并将新版本号与写入的值一起保存。
+ <u>当客户端读取主键时，服务器将返回所有(未被覆盖的)当前值以及最新的版本号。**且要求写之前，客户必须先发送读请求**</u>。
+ **客户端写主键，写请求必须包含之前读到的版本号、读到的值和新值合并后的集合**。写请求的响应可以像读操作一样，会返回所有当前值，这样就可以像购物车例子那样一步步链接起多个写入的值。
+ **当服务器收到带有特定版本号的写入时，覆盖该版本号或更低版本的所有值(因为知道这些值已经被合并到新传入的值集合中)，但<u>必须保存更高版本号的所有值(因为这些值与当前的写操作属于并发)</u>**。

​	**<u>当写请求包含了前一次读取的版本号时，意味着修改的是基于以前的状态。如果一个写请求没有包含版本号，它将与所有其他写入同时进行，不会覆盖任何已有值，其传入的值将包含在后续读请求的返回值列表当中</u>**。

#### * 合并同时写入的值

​	上述算法可以保证不会发生数据丢弃，但不幸的是，客户端需要做一些额外的工作：即如果多个操作并发发生，则<u>客户端必须通过**合并并发写入的值**来继承旧值</u>。Riak称这些并发值为**兄弟关系**。

​	合并本质上与先前讨论的多节点复制冲突解决类似(参阅本章前面的“处理写冲突”)。一个简单的方法是基于版本号或时间戳(即最后写入获胜)来选择其中的一个值，但这意味着会丢失部分数据。所以，需要在应用程序代码中额外些工作。

​	以购物车为例，合并并发值的合理方式是包含新值和旧值(union操作)。图5-14中，两个客户端最后的值分别是[牛奶，面粉，鸡蛋，熏肉]和[鸡蛋，牛奶，火腿]。注意，虽然牛奶和鸡蛋只是写入了一次，但它在两个客户端中均有出现。合并的最终值应该是[牛奶，面粉，鸡蛋，培根，火腿]，其中去掉了重复值。

​	<u>然而，设想一下人们也可以在购物车中删除商品，此时把并发值都合并起来可能会导致错误的结果：**如果合并了两个客户端的值，且其中有一个商品被某客户端删除掉，则被删除的项目会再次出现在合并的终值中**</u>。<u>为了防止该问题，项目在删除时不能简单地从数据库中删除，**系统必须保留一个对应的版本号以恰当的标记该项目需要在合并时被剔除。这种删除标记被称为墓碑**(之前我们在第3章“哈希索引”日志压缩时提到过)</u>。

​	<u>考虑到在应用代码中合并非常复杂且容易出错，因此可以设计一些专门的数据结构来自动执行合并，例如，Riak支持称为CRDT一系列数据结构(具体参见本章前面的“自动冲突解决”)，以合理的方式高效自动合并，包括支持删除标记</u>。

#### * 版本矢量

​	图5-13中的示例只有一个副本。如果存在多个副本但没有主节点，算法又该如何呢?

​	图5-13使用单个版本号来捕获操作之间的依赖关系，<u>当多个副本同时接受写入时，这是不够的。因此我们需要为每个副本和每个主键均定义一个版本号。每个副本在处理写入时增加自己的版本号，并且跟踪从其他副本看到的版本号。通过这些信息来指示要覆盖哪些值、该保留哪些并发值</u>。

​	<u>所有副本的版本号集合称为**版本矢量**</u>。这种思路还有一些变体，但最有趣的可能是在Riak 2.0中使用的虚线版本矢量。我们无法在此深入其细节，但是它的工作方式与购物车例子所展示的非常相似。

​	与图5-13中版本号类似，当读取数据时，数据库副本会返回版本矢量给客户端，而在随后写入时需要将将版本信息包含在请求当中一起发送到数据库。Riak将版本矢量编码为一个称之为因果上下文的字符串。**版本矢量技术使数据库可以区分究竟应该覆盖写还是保留并发值**。

​	另外，就像单副本的例子一样，应用程序仍然需要执行合并操作。版本矢量可以保证从某一个副本读取值然后写入到另一个副本，而这些值可能会导致在其他副本上衍生出来新的“兄弟”值，但至少不会发生数据丢失且可以正确合并所有并发值。

> **版本矢量和矢量时钟**
>
> 版本矢量有时也被称为矢量时钟，然而两者并不完全相同，细微差别可参阅文献。**<u>简而言之，当需要比较副本状态时，应当采用版本矢量</u>**。

## 5.5 小结

​	本章，我们详细探讨了复制相关的话题。复制或者多副本技术主要服务于以下目的：

+ 高可用性：即使某台机器(或多台机器，或整个数据中心)出现故障，系统也能保持正常运行。
+ 连接断开与容错：允许应用程序在出现网络中断时继续工作。
+ 低延迟：将数据放置在距离用户较近的地方，从而实现更快地交互。
+ 可扩展性：釆用多副本读取，大幅提高系统读操作的吞吐量。

​	在多台机器上保存多份相同的数据副本，看似只是个很简单的目标，但事实上复制技术是一个非常烧脑的问题。需要仔细考虑并发以及所有可能出错的环节，并小心处理故障之后的各种情形。<u>最最基本的，要处理好节点不可用与网络中断问题，这里甚至还没考虑一些更隐蔽的失效场景，例如由于软件bug而导致的无提示的数据损坏</u>。

我们主要讨论了三种多副本方案：

+ 主从复制

  所有的客户端写入操作都发送到某一个节点(主节点)，由该节点负责将数据更改事件发送到其他副本(从节点)。每个副本都可以接收读请求，但内容可能是过期值。

+ 多主节点复制

  系统存在多个主节点，每个都可以接收写请求，客户端将写请求发送到其中的一个主节点上，由该主节点负责将数据更改事件同步到其他主节点和自己的从节点。

+ **无主节点复制**

  <u>客户端将写请求发送到多个节点上，读取时从多个节点上并行读取，以此检测和纠正某些过期数据</u>。

​	每种方法都有其优点和缺点。<u>**主从复制非常流行，主要是因为它很容易理解，也不需要担心冲突问题**。而万一出现节点失效、网络中断或者延迟抖动等情况，多主节点和无主节点复制方案会更加可靠，不过背后的代价则是系统的复杂性和弱一致性保证</u>。

​	复制可以是同步的，也可以是异步的，而一旦发生故障，二者的表现差异会对系统行为产生深远的影响。在系统稳定状态下异步复制性能优秀，但仍须认真考虑一旦岀现复制滞后和节点失效两种场景会导致何种影响。**万一某个主节点发生故障，而一个异步更新的从节点被提升为新的主节点，要意识到最新确认的数据可能有丢失的风险**。

​	我们还分析了由于复制滞后所引起的一些奇怪效应，并讨论了以下一致性模型，来帮助应用程序处理复制滞后：

+ 写后读一致性

  保证用户总能看到自己所提交的最新数据。

+ **单调读**

  **用户在某个时间点读到数据之后，保证此后不会出现比该时间点更早的数据**。

+ **前缀一致读**

  **保证数据之间的因果关系，例如，总是以正确的顺序先读取问题，然后看到回答**。

​	最后，我们讨论了多主节点和无主节点复制方案所引入的并发问题。即由于多个写可能同时发生，继而可能产生冲突。为此，我们研究了一个算法使得数据库系统可以判定某操作是否发生在另一个操作之前，或者是同时发生。接下来，探讨采用合并并发更新值的方法来解决冲突。

​	下一章我们继续研究多节点上数据的分布问题，与本章不同的是，它是针对一个大型数据集而采用**分区技术**。

# 第6章 数据分区

​	第5章讨论了复制技术，即在不同节点上保存相同数据的多个副本。然而，<u>面对一些海量数据集或非常高的査询压力，复制技术还不够，我们还需要将数据拆分成为分区，也称为分片</u>。

> 术语澄清
>
> 这里我们所讨论的分区，在不同系统有着不同的称呼，例如它对应于MongoDB，Elasticsearch和Solrcloud中的shard，Hbase的region，Bigtable中的tablet，Cassandra和Riak中的vnode，以及Couchbase中的 vBucket。总体而言，分区是最普遍的术语。

​	**分区通常是这样定义的，即每一条数据(或者每条记录，每行或每个文档)只属于某个特定分区**。实现的方法有多种，稍后将逐一介绍。<u>实际上，每个分区都可以视为一个完整的小型数据库，虽然数据库可能存在一些跨分区的操作</u>。

​	<u>采用数据分区的主要目的是**提高可扩展性**</u>。不同的分区可以放在一个无共享集群(参阅第二部分关于无共享架构的定义)的不同节点上。<u>这样一个大数据集可以分散在更多的磁盘上，査询负载也随之分布到更多的处理器上</u>。

​	<u>对单个分区进行查询时，每个节点对自己所在分区可以独立执行查询操作，因此**添加更多的节点可以提高査询吞吐量**</u>。超大而复杂的査询尽管比较困难，但也可能做到跨节点的并行处理。

​	分区数据库最初在20世纪80年代由Teradata和Tandem NonStop SQL等率先推出，最近又被一些NoSQL数据库和基于Hadoop的数据仓库重视起来。这些系统有些是为事务型负载设计的，有些是分析型(参阅第3章的“事务处理与分析处理”)。二者的差异会显著影响系统的优化策略，然而分区技术的基本原理则可以普遍适用。

​	本章我们将首先介绍切分大型数据集的若干方法，讨论数据索引如何影响分区。接下来讨论**分区的再平衡**，这对动态添加或删除节点非常重要。最后，我们将介绍数据库如何将请求路由到正确的分区并执行查询。

## 6.1 数据分区与数据复制

​	<u>**分区通常与复制结合使用，即每个分区在多个节点都存有副本**</u>。<u>这意味着某条记录属于特定的分区，而同样的内容会保存在不同的节点上以提高系统的容错性</u>。

​	一个节点上可能存储了多个分区。图6-1展示了主从复制模型与分区组合使用时数据的分布情况。由图可知，每个分区都有自己的主副本，例如被分配给某节点，而从副本则分配在其他一些节点。<u>一个节点可能即是某些分区的主副本，同时又是其他分区的从副本</u>。

​	**第5章所讨论的所有复制相关的原理同样适用于对分区数据的复制**。考虑到分区方案的选择通常独立于复制，因此本章将力求简洁，而省略与复制相关的内容。

## 6.2 键-值数据的分区

​	假设面临海量数据，现在需要切分它们，那么该如何决定哪些记录放在哪些节点上呢？

​	**分区的主要目标是将数据和査询负载均匀分布在所有节点上**。如果节点平均分担负载，那么理论上10个节点应该能够处理10倍的数据量和10倍于单个节点的读写吞吐量(忽略复制)。

![分区与复制 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-1.png)

​	而如果<u>分区不均匀，则会出现某些分区节点比其他分区承担更多的数据量或查询负载，称之为**倾斜**</u>。倾斜会导致分区效率严重下降，在极端情况下，所有的负载可能会集中在一个分区节点上，这就意味着10个节点9个空闲，系统的瓶颈在最繁忙的那个节点上。这种负载严重不成比例的分区即成为**系统热点**。	

​	避免热点最简单的方法是将记录随机分配给所有节点上。这种方法可以比较均匀地分布数据，但是有一个很大的缺点：当试图读取特定的数据时，没有办法知道数据保存在哪个节点上，所以不得不并行查询所有节点。

​	可以改进上述方法。现在我们假设数据是简单的键-值数据模型，这意味着总是可以通过关键字来访问记录。例如，像一个纸质百科全书，可以通过标题来查找某一个条目；而所有的条目按字母序排序，因此可以做到快速查找条目。

### 6.2.1 基于关键字区间分区

​	一种分区方式是为每个分区分配一段连续的关键宇或者关键字区间范围(以最小值和最大值来指示)，如图6-2所示的纸质百科全书的卷目录。如果知道关键字区间的上下限，就可以轻松确定哪个分区包含这些关键字。如果还知道哪个分区分配在哪个节点，就可以直接向该节点发出请求(对于百科全书的例子，就是从书架上直接取到所要的书籍)。

![键值数据的分区 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-2.png)

​	关键字的区间段不一定非要均匀分布，这主要是因为数据本身可能就不均匀。例如，在图6-2中，卷1只包含以A和B开头的单词，但是卷12则包含了T、U、V、Ⅹ、Y和Z开始的单词。如果只是简单地规定每个分区包含两个字母，则可能会导致一些卷比其他卷要大很多。<u>为了更均匀地分布数据，分区边界理应适配数据本身的分布特征</u>。

​	分区边界可以由管理员手动确定，或者由数据库自动选择(我们将在本章后面的“分区再平衡”中更详细地讨论)。采用这种分区策略的系统包括Bigtable， Bigtable的开源版本Hbase，RethinkDB和2.4版本之前MongoDB。

​	<u>毎个分区内可以按照关键字排序保存(参阅第3章的“ SSTables和LSM-Trees”)。这样可以轻松支持区间查询，即将关键字作为一个拼接起来的索引项从而一次查询得到多个相关记录(参阅第3章的“多列索引”)</u>。例如，对于一个保存网络传感器数据的应用系统，选择测量的时间戳(年-月-日-时-分-秒)作为关键字，此时区间查询会非常有用，它可以快速获得某个月份内的所有数据。

​	然而，**基于关键字的区间分区的缺点是某些访问模式会导致热点**。如果关键字是时间戳，则分区对应于一个时间范围，例如每天一个分区。然而，当测量数据从传感器写入数据库时，所有的写入操作都集中在同一个分区(即当天的分区)，这会导致该分区在写入时负载过高，而其他分区始终处于空闲状态的。

​	为了避免上述问题，需要使用时间戳以外的其他内容作为关键字的第一项。例如，可以在时间戳前面加上传感器名称作为前缀，这样首先由传感器名称，然后按时间进行分区。假设同时有许多传感器处于活动状态，则写入负载最终会比较均匀地分布在多个节点上。接下来，当需要获取一个时间范围内、多个传感器的数据时，可以根据传感器名称，各自执行区间查询。

### * 6.2.2 基于关键字哈希值分区

​	对于上述数据倾斜与热点问题，许多分布式系统采用了基于关键字哈希函数的方式来分区。

​	**一个好的哈希函数可以处理数据倾斜并使其均匀分布**。例如一个处理字符串的32位哈希函数，当输入某个宇符串，它会返回一个0和2<sup>32</sup>~1之间近似随机分布的数值。即使输入的字符串非常相似，返回的哈希值也会在上述数字范围内均匀分布。

​	<u>用于数据分区目的的哈希函数不需要在加密方面很强</u>：例如，Cassandra和 MongoDB使用MD5， Voldemort使用 Fowler-Noll-Vo函数。许多编程语言也有内置的简单哈希函数(主要用于哈希表)，但是要注意这些内置的哈希函数可能并不适合分区，例如，Java的`Object.hashcode`和Ruby的`object#hash`，同一个键在不同的进程中可能返回不同的哈希值。

​	一旦找到合适的关键字哈希函数，就可以**为每个分区分配一个哈希范围(而不是直接作用于关键字范围)**，关键字根据其哈希值的范围划分到不同的分区中。如图6-3所示。

![键值数据的分区 - 图2](https://static.sitestack.cn/projects/ddia/img/fig6-3.png)

​	这种方法可以很好地将关键字均匀地分配到多个分区中。<u>分区边界可以是均匀间隔，也可以是伪随机选择(在这种情况下，该技术有时被称为**一致性哈希**)</u>。

​	然而，**通过关键字哈希进行分区，我们丧失了良好的区间查询特性**。<u>即使关键字相邻，但经过哈希之后会分散在不同的分区中，区间查询就失去了原有的有序相邻的特性</u>。<u>在 MongoDB中，如果启用了基于哈希的分片模式，则区间查询会发送到所有的分区上，而Riak、 Couchbase和Voldemort干脆就不支持关键字上的区间查询</u>。

> 一致性哈希
>
> 一致性哈希，由Karger等人首先提出，是一种平均分配负载的方法，最初用于内容分发网络(CDN)等互联网缓存系统。它采用随机选择的分区边界来规避中央控制或分布式共识。请注意，此处的一致性与副本一致性(第5章)或ACID致性(第7章)没有任何关联，它只描述了数据动态平衡的一种方法。
>
> 正如后面“分区再平衡”一节将要介绍的，**这种特殊的分区方法对于数据库实际效果并不是很好，所以目前很少使用(虽然某些数据库的文档仍采用一致性哈希的术语，但其实并不准确)**。为避免混淆，我们此处只用术语哈希分区。

​	<u>Cassandra则在两种分区策略之间做了一个折中。 Cassandra中的表可以声明为由多个列组成的复合主键。复合主键只有第一部分可用于哈希分区，而其他列则用作组合索引来对 Cassandra SSTable中的数据进行排序。因此，它不支持在第一列上进行区间査询，但如果为第一列指定好了固定值，可以对其他列执行高效的区间查询</u>。

​	**组合索引为一对多的关系提供了一个优雅的数据模型**。例如，在社交网站上，一个用户可能会发布很多消息更新。如果更新的关键字设置为`(user_id，update_timestamp)`的组合，那么可以有效地检索由某用户在一段时间内所做的所有更新，且按时间戳排序。<u>不同的用户可以存储在不同的分区上，但是对于某一用户，消息按时间戳顺序存储在一个分区上</u>。

### * 6.2.3 负载倾斜与热点

​	如前所述，基于哈希的分区方法可以减轻热点，但无法做到完全避免。<u>一个极端情况是，所有的读/写操作都是针对同一个关键字，则最终所有请求都将被路由到同一个分区</u>。

​	这种负载或许并不普遍，但也并非不可能：例如，社交媒体网站上，一些名人用户有数百万的粉丝，当其发布一些热点事件时可能会引发一场访问风暴，出现大量的对相同关键字的写操作(其中关键字可能是名人的用户ID，或者人们正在评论的事件ID)。此时，哈希起不到任何帮助作用，因为两个相同ID的哈希值仍然相同。

​	**大多数的系统今天仍然无法自动消除这种高度倾斜的负载，而只能通过应用层来减轻倾斜程**度。例如，如果某个关键字被确认为热点，一个简单的技术就是在关键字的开头或结尾处添加一个随机数。只需一个两位数的十进制随机数就可以将关键字的写操作分布到100个不同的关键字上，从而分配到不同的分区上。

​	<u>但是，随之而来的问题是，之后的任何读取都需要些额外的工作，必须从所有100个关键字中读取数据然后进行合并。因此通常只对少量的热点关键字附加随机数才有意义；而对于写入吞吐量低的绝大多数关键字，这些都意味着不必要的开销。此外，还需要额外的元数据来标记哪些关键字进行了特殊处理</u>。

​	也许将来某一天，数据系统能够自动检测负载倾斜情况，然后自动处理这些倾斜的负载。但截至目前，仍然需要开发者自己结合应用来综合权衡。

## * 6.3 分区与二级索引

​	我们之前所讨论的分区方案都依赖于键-值数据模型。键-值模型相对简单，即都是通过关键字来访冋记录，自然可以根据关键字来确定分区，并将读写请求路由到负责该关键字的分区上。

​	但是，如果涉及二级索引，情况会变得复杂(参阅第3章的“其他索引结构”)。**二级索引通常不能唯一标识一条记录，而是用来加速特定值的查询**，例如查找用户123的所有操作，找到所有含有hogwash的文章，查找所有颜色为红色的汽车等。

​	<u>二级索引是关系数据库的必备特性，在文档数据库中应用也非常普遍</u>。但考虑到其复杂性，许多键-值存储(如 Hbase和Voldemort)并不支持二级索引；但其他一些如Riak则开始增加对二级索引的支持。此外，二级索引技术也是Solr和Elasticsearch等全文索引服务器存在之根本。

​	**二级索引带来的主要挑战是它们不能规整的地映射到分区中**。有两种主要的方法来支持对二级索引进行分区：**基于文档的分区**和**基于词条的分区**。

### * 6.3.1 基于文档分区的二级索引

​	假设一个销售二手车的网站(见图6-4)。每个列表都有一个唯一的文档ID，用此ID对数据库进行分区，例如，ID0到499归分区0，ID500到999划为分区1。

​	现在用户需要搜索汽车，可以持按汽车颜色和厂商进行过滤，所以需要在颜色和制造商上设定二级索引(在文档数据库中这些都是字段；在关系数据库中则是列)。声明这些索引之后，数据库会自动创建索引。例如，每当一辆红色汽车添加到数据库中，数据库分区会自动将其添加到索引条目为“ color:red”的文档列表中。

![分片与次级索引 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-4.png)

​	**<u>在这种索引方法中，每个分区完全独立，各自维护自己的二级索引，且只负责自己分区内的文档而不关心其他分区中数据</u>**。<u>每当需要写数据库时，包括添加，删除或更新文档等，只需要处理包含目标文档ID的那一个分区</u>。因此**文档分区索引也被称为<u>本地索引</u>，而不是全局索引**，后者将在本章后面介绍。

​	<u>但读取时需要注意：除非对文档ID做了特别的处理，否则不太可能所有特定颜色或特定品牌的汽车都放在一个分区中，例如图6-4中，红色汽车就出现在分区0和分区1中。因此，如果想要搜索红色汽车，就**需要将查询发送到所有的分区，然后合并所有返回的结果**</u>。

​	这种査询分区数据库的方法有时也称为**分散/聚集**，显然这种二级索引的查询代价高昂。即使采用了并行査询，也容易导致读延迟显著放大(参阅第1章的“实践中的百分位数”)。尽管如此，它还是广泛用于实践：MongoDB、Riak、 Cassandra、ElasticSearch、 SolrCloud 和 VoltDB都支持基于文档分区二级索引。大多数数据库供应商都建议用户自己来构建合适的分区方案，尽量由单个分区满足二级索引查询，但现实往往难以如愿，尤其是当查询中可能引用多个二级索引时(例如同时指定颜色和制造商两个条件)。

### * 6.3.2 基于词条的二级索引分区

​	另一种方法，我们可以**对所有的数据构建全局索引，而不是每个分区维护自己的本地索引**。而且，为避免成为瓶颈，不能将全局索引存储在一个节点上，否则就破坏了设计分区均衡的目标。所以，**全局索引也必须进行分区，且可以与数据关键字采用不同的分区策略**。

![分片与次级索引 - 图2](https://static.sitestack.cn/projects/ddia/img/fig6-5.png)

​	以图65为例：所有数据分区中的颜色为红色的汽车被收录到在索引`color:red`中，而索引本身也是分区的，例如从a到r开始的颜色放在分区0中，从s到的颜色放在分区1中。类似的，汽车制造商的索引也被分区(两个分区的边界分别是字母f和字母h)。

​	我们将<u>这种索引方案称为**词条分区**，它**以待查找的关键字本身作为索引**</u>。例如颜色`color:red`。<u>名字词条源于全文索引(一种特定类型的二级索引)，term指的是文档中出现的所有单词的集合</u>。

​	<u>和前面讨论的方法一样，可以直接通过关键词来全局划分索引，或者对其取哈希值。直接分区的好处是可以支持高效的区间查询(例如，查询汽车报价在某个值以上)而采用哈希的方式则可以更均匀的划分分区</u>。

​	这种全局的词条分区相比于文档分区索引的主要优点是，**它的读取更为高效，即它不需要釆用 scatter/gather对所有的分区都执行一遍查询，相反，客户端只需要向包含词条的那一个分区发出读请求**。<u>然而全局索引的不利之处在于，**写入速度较慢且非常复杂**，主要因为单个文档的更新时，里面可能会涉及多个二级索引，而二级索引的分区又可能完全不同甚至在不同的节点上，由此势必引入显著的**写放大**</u>。

​	<u>理想情况下，索引应该时刻保持最新，即写入的数据要立即反映在最新的索引上。但是，对词条分区来讲，这需要个跨多个相关分区的分布式事务支持，写入速度会受到极大的影响，所以**现有的数据库都不支持同步更新二级索引**(参阅第7章和第9章)</u>。

​	**实践中，<u>对全局二级索引的更新往往都是异步的(也就意味着，如果在写入之后马上去读索引，那么刚刚发生的更新可能还没有反映在索引中)</u>**。例如， AmazonDynamoDB的二级索引通常可以在1秒之内完成更新，但当底层设施出现故障时，也有可能需要等待很长的时间。其他使用全局索引的系统还包括Riak的搜索功能和Oracle数据仓库，后者允许用户来选择是使用本地还是全局索引。在第12章我们会重新讨论如何实现全局二级索引。

## 6.4 分区再平衡

​	随着时间的推移，数据库可能总会出现某些变化：

+ 查询压力增加，因此需要更多的CPU来处理负载。
+ 数据规模增加，因此需要更多的磁盘和内存来存储数据。
+ 节点可能出现故障，因此需要其他机器来接管失效的节点。

​	<u>所有这些变化都要求数据和请求可以从一个节点转移到另一个节点</u>。这样一个迁移负载的过程称为**再平衡(或者动态平衡)**。无论对于哪种分区方案，分区再平衡通常至少要满足：

+ 平衡之后，负载、数据存储、读写请求等应该在集群范围更均匀地分布。
+ **再平衡执行过程中，数据库应该可以继续正常提供读写服务**。
+ 避免不必要的负载迁移，以加快动态再平衡，并尽量减少网络和磁盘IO影响。

### 6.4.1 动态再平衡的策略

​	将分区对应到节点上存在多种不同的分配策略，这里逐一介绍：

#### * 为什么不用取模？

​	我们在前面提到(见图6-3)，最好将哈希值划分为不同的区间范围，然后将每个区间分配给一个分区。例如，区间[0，b0)对应于分区0，[b0，b1)对应分区1等。

​	也许你会问为什么不直接使用mod(许多编程语言里的取模运算符%)。例如，`hash(key) mod 10`会返回一个介于0和9之间的数字，如果有10个节点，则依次对应节点0到9，这似乎是将每个关键字分配到节点的最简单方法。

​	**对节点数取模方法的问题是，如果节点数N发生了变化，会导致很多关键字需要从现有的节点迁移到另一个节点**。例如，假设hash(key)=123456，假定最初是10个节点，那么这个关键字应该放在节点6(123456 mod 10=6)；当节点数增加到11时，它需要移动到节点3(123456 mod 11=3)；当继续增长到12个节点时，又需要移动到节点0(123456 mod 12=0)。<u>这种频繁的迁移操作大大增加了再平衡的成本</u>。

​	因此我们需要一种减少迁移数据的方法。

#### * 固定数量的分区

​	幸运的是，有一个相当简单的解决方案：**首先，创建远超实际节点数的分区数，然后为每个节点分配多个分区**。例如，对于一个10节点的集群，数据库可以从一开始就逻辑划分为1000个分区，这样大约每个节点承担100个分区。

​	<u>接下来，如果集群中添加了一个新节点，该新节点可以从每个现有的节点上匀走几个分区，直到分区再次达到全局平衡。该过程如图6-6所示。如果从集群中删除节点，则采取相反的均衡措施</u>。

​	<u>选中的整个分区会在节点之间迁移，但分区的总数量仍维持不变，也不会改变关键字到分区的映射关系。**这里唯一要调整的是分区与节点的对应关系**</u>。考虑到节点间通过网络传输数据总是需要些时间，这样调整可以逐步完成，在此期间，旧的分区仍然可以接收读写请求。

![分区再平衡 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-6.png)

​	<u>原则上，也可以将集群中的不同的硬件配置因素考虑进来，即性能更强大的节点将分配更多的分区，从而分担更多的负载</u>。

​	目前，Riak、 Elasticsearch、 couchbase和Voldemort都支持这种动态平衡方法。

​	**使用该策略时，分区的数量往往在数据库创建时就确定好，之后不会改变**。<u>原则上也可以拆分和合并分区(稍后介绍)，但固定数量的分区使得相关操作非常简单，因此许多采用固定分区策略的数据库决定不支持分区拆分功能</u>。所以，在初始化时，已经充分考虑将来扩容增长的需求(未来可能拥有的最大节点数)，设置一个足够大的分区数。而每个分区也有些额外的管理开销，选择过高的数字可能会有副作用。

​	如果数据集的总规模高度不确定或可变(例如，开始非常小，但随着时间的推移可能会变得异常庞大)，此时如何选择合适的分区数就有些困难。每个分区包含的数据量的上限是固定的，实际大小应该与集群中的数据总量成正比。如果分区里的数据量非常大，则每次再平衡和节点故障恢复的代价就很大；但是如果一个分区太小，就会产生太多的开销。分区大小应该“恰到好处”，不要太大，也不能过小，**如果分区数量固定了但总数据量却高度不确定，就难以达到一个最佳取舍点**。

#### * 动态分区

​	对于采用关键字区间分区的数据库，如果边界设置有问题，最终可能会出现所有数据都挤在一个分区而其他分区基本为空，那么设定固定边界、固定数量的分区将非常不便：而手动去重新配置分区边界又非常繁琐。

​	因此，一些数据库如 HBase和 RethinkDB等采用了**动态创建分区**。<u>当分区的数据增长超过一个可配的参数阈值(Hbase上默认值是10GB)，它就拆分为两个分区，每个承担一半的数据量。相反，如果大量数据被删除，并且分区缩小到某个阈值以下，则将其与相邻分区进行合并</u>。该过程类似于B树的分裂操作(参阅第3章的“B-tree”)。

​	**<u>每个分区总是分配给一个节点</u>，而每个节点可以承载多个分区，这点与固定数量的分区一样。当一个大的分区发生分裂之后，可以将其中的一半转移到其他某节点以平衡负载**。<u>对于 HBase，分区文件的传输需要借助HDFS(底层分布式文件系统)</u>。

​	**动态分区的一个优点是分区数量可以自动适配数据总量**。<u>如果只有少量的数据，少量的分区就足够了，这样系统开销很小；如果有大量的数据，每个分区的大小则被限制在一个可配的最大值</u>。

​	但是，需要注意的是，<u>对于一个空的数据库，因为没有任何先验知识可以帮助确定分区的边界，所以会从一个分区开始。可能数据集很小，但直到达到第一个分裂点之前，所有的写入操作都必须由单个节点来处理，而其他节点则处于空闲状态</u>。为了缓解这个问题， HBase和 MongoDB允许在一个空的数据库上配置一组初始分区(这被称为**预分裂**)。对于关键字区间分区，预分裂要求已经知道一些关键字的分布情况。

​	动态分区不仅适用于关键字区间分区，也适用于基于哈希的分区策略。 MongoDB从版本2.4开始，同时支持二者，并且都可以动态分裂分区。

#### 按节点比例分区

​	采用动态分区策略，拆分和合并操作使每个分区的大小维持在设定的最小值和最大值之间，因此分区的数量与数据集的大小成正比关系。另一方面，对于固定数量的分区方式，其每个分区的大小也与数据集的大小成正比。两种情况，分区的数量都与节点数无关。

​	Cassandra和Ketama则采用了第三种方式，<u>使分区数与集群节点数成正比关系</u>。换句话说，**每个节点具有固定数量的分区**。此时，<u>当节点数不变时，每个分区的大小与数据集大小保持正比的增长关系</u>；当节点数增加时，分区则会调整变得更小。较大的数据量通常需要大量的节点来存储，因此这种方法也使每个分区大小保持稳定。

​	**当一个新节点加入集群时，它随机选择固定数量的现有分区进行分裂，然后拿走这些分区的一半数据量，将另一半数据留在原节点**。随机选择可能会带来不太公平的分区分裂，但是当平均分区数量较大时(Cassandra默认情况下，每个节点有256个分区)，新节点最终会从现有节点中拿走相当数量的负载。 Cassandra在3.0时推出了改进算法，可以避免上述不公平的分裂。

​	**随机选择分区边界的前提要求采用基于哈希分区(可以从哈希函数产生的数字范围里设置边界)**。这种方法也最符合本章开头所定义一致性哈希。一些新设计的哈希函数也可以以较低的元数据开销达到类似的效果。

### 6.4.2 自动与手动再平衡操作

​	动态平衡另一个重要问题我们还没有考虑：它是自动执行还是手动方式执行？

​	全自动式的再平衡(即由系统自动决定何时将分区从一个节点迁移到另一个节点，不需要任何管理员的介入)与纯手动方式(即分区到节点的映射由管理员来显式配置)之间，可能还有一个过渡阶段。例如， Couchbase，Riak和 Voldemort会自动生成一个分区分配的建议方案，但需要管理员的确认才能生效。

​	<u>全自动式再平衡会更加方便，它在正常维护之外所增加的操作很少。但是，也有可能出现结果难以预测的情况。再平衡总体讲是个比较昂贵的操作，它需要重新路由请求并将大量数据从一个节点迁移到另一个节点。万一执行过程中间出现异常，会使网络或节点的负载过重，并影响其他请求的性能</u>。

​	**将自动平衡与自动故障检测相结合也可能存在一些风险**。<u>例如，假设某个节点负载过重，对请求的响应暂时受到影响，而其他节点可能会得到结论：该节点已经失效；接下来激活自动平衡来转移其负载。客观上这会加重该节点、其他节点以及网络的负荷，可能会使总体情况变得更糟，**甚至导致级联式的失效扩散**</u>。

​	出于这样的考虑，**让管理员介入到再平衡可能是个更好的选择**。它的确比全自动过程响应慢一些，但它可以有效防止意外发生。

## 6.5 请求路由

​	现在我们已经将数据集分布到多个节点上，但是仍然有一个悬而未决的问题：当客户端需要发送请求时，如何知道应该连接哪个节点？如果发生了分区再平衡，分区与节点的对应关系随之还会变化。为了回答该问题，我们需要一段处理逻辑来感知这些变化，并负责处理客户端的连接，例如想要读/写关键字“foo”，需要连接哪个IP地址和哪个端口号。

​	这其实属于一类典型的**服务发现问题**，服务发现并不限于数据库，任何通过网络访问的系统都有这样的需求，尤其是当服务目标支持高可用时(在多台机器上有冗余配置)。许多公司已经开发了自己的内部服务发现工具，其中很多已经开源。

​	概括来讲，这个问题有以下几种不同的处理策略(分别如图6-7所示的三种情况)：

1. 允许客户端链接任意的节点(例如，采用**循环式的负载均衡器**)。如果某节点恰好拥有所请求的分区，则直接处理该请求；否则，将请求转发到下一个合适的节点，接收答复，并将答复返回给客户端。
2. 将所有客户端的**请求都发送到一个路由层，由后者负责将请求转发到对应的分区节点上**。路由层本身不处理任何请求，它仅充一个分区感知的负载均衡器。
3. 客户端**感知分区和节点分配关系**。此时，客户端可以直接连接到目标节点，而不需要任何中介。

​	<u>**不管哪种方法，核心问题是：作出路由决策的组件(可能是某个节点，路由层或客户端)如何知道分区与节点的对应关系以及其变化情况**</u>?

![请求路由 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-7.png)

​	这其实是一个很有挑战性的问题，所有参与者都要达成**共识**这一点很重要。否则请求可能被发送到错误的节点，而没有得到正确处理。分布式系统中有专门的共识协议算法，但通常难以正确实现(详见第9章)。

​	许多分布式数据系统依靠独立的协调服务(如 ZooKeeper)跟踪集群范围内的元数据，如图6-8所示。每个节点都向 ZooKeeper中注册自己， ZooKeeper维护了分区到节点的最终映射关系。其他参与者(如路由层或分区感知的客户端)可以向 ZooKeeper订阅此信息。一旦分区发生了改变，或者添加、删除节点，ZooKeeper就会主动通知路由层，这样使路由信息保持最新状态。

​	例如， LinkedIn的 Espresso使 Helix进行集群管理(底层是 ZooKeeper)，实现了图6-8所示的请求路由层。 HBase， SolrCloud和Kafka也使用 ZooKeeper来跟踪分区分配情况。 MongoDB有类似的设计，但它依赖于自己的配置服务器和mongos守护进程来充当路由层。

​	<u>Cassandra和Riak则采用了不同的方法，它们在节点之间使用gossip协议来同步群集状态的变化。请求可以发送到任何节点，由该节点负责将其转发到目标分区节点(图6-7中的方法1)。这种方式增加了数据库节点的复杂性，但是避免了对ZooKeeper之类的外部协调服务的依赖</u>。

![请求路由 - 图2](https://static.sitestack.cn/projects/ddia/img/fig6-8.png)

​	Couchbase并不支持自动再平衡功能，这简化了设计。它通过配置一个名为moxi的路由选择层，向集群节点学习最新的路由变化。

​	<u>当使用路由层或随机选择节点发送请求时，客户端仍然需要知道目标节点的IP地址。IP地址的变化往往没有分区节点变化那么频繁，采用DNS通常就足够了</u>。

### 6.5.1 并行查询执行

​	到目前为止，我们只关注了读取或写入单个关键字这样简单的查询(对于文档分区的二级索引，里面要求分散/聚集查询)。这基本上也是大多数NoSQL分布式数据存储所支持的访问类型。

​	然而对于**大规模并行处理(massively parallel processin，MPP)**这一类主要用于数据分析的关系数据库，在查询类型方面要复杂得多。**典型的数据仓库查询包含多个联合、过滤、分组和聚合操作。<u>MPP査询优化器会将复杂的査询分解成许多执行阶段和分区，以便在集群的不同节点上并行执行</u>。尤其是涉及全表扫描这样的查询操作，可以通过并行执行获益颇多**。

​	数据仓库中快速并行执行查询可以作为单独的话题。考虑到分析业务的重要性，目前它已得到了广泛的商业关注。我们将在第10章中讨论并行查询执行所需的一些技术。有关并行数据库更多相关技术细节请参考文献。

## 6.6 小结

​	本章，我们探讨了将大规模数据集划分成更小子集的多种方法。数据量如果太大，单台机器进行存储和处理就会成为瓶颈，因此需要引入数据分区机制。**分区的目地是通过多台机器均匀分布数据和查询负载，避免出现热点**。这需要选择合适的数据分区方案，**在节点添加或删除时重新动态平衡分区**。

​	我们讨论了两种主要的分区方法：

+ **基于关键字区间的分区**。先对关键字进行排序，每个分区只负责一段包含最小到最大关键字范围的一段关键字。<u>对关键字排序的优点是可以支持高效的区间查询，但是如果应用程序经常访问与排序一致的某段关键字，就会存在热点的风险</u>。釆用这种方法，当分区太大时，通常将其分裂为两个子区间，从而动态地再平衡分区。
+ **哈希分区**。将哈希函数作用于每个关键字，每个分区负责一定范围的哈希值。这种方法打破了原关键字的顺序关系，<u>它的区间查询效率比较低，但可以更均匀地分配负载</u>。采用哈希分区时，通常事先创建好足够多(但固定数量)的分区，让每个节点承担多个分区，当添加或删除节点时将某些分区从一个节点迁移到另个节点，也可以支持动态分区。

​	混合上述两种基本方法也是可行的，例如使用复合键：键的一部分来标识分区，而另部分来记录排序后的顺序。

​	我们还讨论了分区与二级索引，二级索引也需要进行分区，有两种方法：

+ **基于文档来分区二级索引(本地索引)**。二级索引存储在与关键字相同的分区中，这意味着写入时我们只需要更新一个分区，但<u>缺点是读取二级索引时需要在所有分区上执行 scatter/gather</u>。
+ **基于词条来分区二级索引(全局索引)**。它是基于索引的值而进行的独立分区。二级索引中的条目可能包含来自关键字的多个分区里的记录。<u>在写入时，不得不更新二级索引的多个分区；但读取时，则可以从单个分区直接快速提取数据</u>。

​	最后，我们讨论了如何将查询请求路由到正确的分区，包括简单的分区感知负载均衡器，以及复杂的并行査询执行引擎。

​	理论上，毎个分区基本保持独立运行，这也是为什么我们试图将分区数据库分布、扩展到多台机器上。但是，如果写入需要跨多个分区，情况就会格外复杂，例如，如果其中一个分区写入成功，但另一个发生失败，接下来会发生什么？我们将在下面的章节中讨论类似这样的技术挑战。

# * 第7章 事务

​	在一个苛刻的数据存储环境中，会有许多可能出错的情况，例如：

+ 数据库软件或硬件可能会随时失效(包括正在执行写操作的过程中)。
+ 应用程序可能随时崩溃(包括一系列操作执行到中间某一步)。
+ <u>应用与数据库节点之间的链接可能随时会中断，数据库节点之间也存在同样问题</u>。
+ 多个客户端可能同时写入数据库，导致数据覆盖。
+ 客户端可能读到一些无意义的、部分更新的数据。
+ 客户端之间由于边界条件竞争所引入的各种奇怪问题。

​	为了系统高可靠的目标，我们必须处理好上述问题，万一发生类似情况确保不会导致系统级的失效。然而，完善的容错机制需要大量的工作，要仔细考虑各种可能出错的可能，并进行充分的测试才能确保方案切实可靠。

​	<u>近十年来，事务技术一直是简化这些问题的首选机制。事务将应用程序的多个读、写操作捆绑在一起成为一个逻辑操作单元。即事务中的所有读写是一个执行的整体，整个事务要么成功(提交)、要么失败(中止或回滚)</u>。如果失败，应用程序可以安全地重试。这样，由于不需要担心部分失败的情况(无论出于何种原因)，应用层的错误处理就变得简单很多。

​	也许对于那些浸淫此领域多年的读者来说，事务的概念如此简单，但细究起来或许并非如此。事务不是一个天然存在的东西，它是被人为创造出来，目的是简化应用层的编程模型。有了事务，应用程序可以不用考虑某些内部潜在的错误以及复杂的并发性问题，这些都可以交给数据库来负责处理(我们称之为安全性保证)。

​	然而并非每个应用程序都需要事务机制，有时可以弱化事务处理或完全放弃事务(例如，为了实现更高的性能或更高的可用性)。一些安全相关的属性也可能会避免引入事务。

​	那该如何判断是否需要事务呢？为了回答这个问题，我们首先需要确切地理解事务能够提供哪些安全性保证，背后的代价又是什么。事务的概念看似简单，实际上却有许多微妙而关键的细节值得硏究。

​	本章，我们将首先分析可能出错的各种场景，探讨数据库防范这些问题的基本方法和算法设计。特别是在并发控制方面，深入讨论可能各种竞争条件，数据库所提供的多种隔离级别，例如读提交、快照隔离和可串行化等。

​	本章的内容可适用于单节点和分布式场景。在第8章中，我们将重点讨论分布式系统的特殊挑战。

## 7.1 深入理解事务

​	目前几乎所有的关系数据库和一些非关系数据库都支持事务处理。它们大多数都沿用了和IBM于1975年推出的第一个SQL数据库 System R相似的总体设计。尽管在一些具体实现方面有些不同，但事务的概念在这四十年中几乎没有发生变化，换句话说，MySQL、 Postgresql、 Oracle、 SQL Server等系统实现的事务与当年 System R非常相。

​	然而21世纪末，非关系(NoSQL)数据库开始兴起。它们的目标是通过提供新的数据模型(参见第2章)，以及内置的复制(参见第5章)和分区(参见第6章)等手段来改进传统的关系模型。然而事务却成了这场变革的受害者：<u>很多新一代的数据库完全放弃了事务支持，或者将其重新定义，即替换为比以前弱得多的保证</u>。

​	随着这种新型分布式数据库的炒作，很多人开始认为事务与可扩展性是相对立的两面，而大规模系统为了性能与高可用性将不得不牺牲事务的支持。但另一方面，还有一些数据库供应商则坚称事务是“关键应用”和“高价值数据”所必备的重要功能。而笔者看来，这两个观点未免都有些夸大其词。

​	与其他技术一样，事务有其优势，也有其自身的局限性。为了更好地理解事务设计的权衡之道，让我们考虑正常运行和各种极端(但确实存在)情况，详细分析事务可以为我们提供哪些保证。

### 7.1.1 ACID的含义

​	事务所提供的安全保证即大家所熟知的**ACID，分别代表原子性(Atomicity)，一致性(Consistency)，隔离性(Isolation)与持久性(Durability)**，取这四个特性的首字母。最早由 Theoharder和 Andreas Reuter于1983年为精确描述数据库的容错机制而定义。

​	但<u>实际上，各家数据库所实现的ACⅠD并不尽相同。例如，我们稍后就会看到，围绕着“隔离性”就存在很多含糊不清的争议。想法非常美好，细节方见真章。当听到一个系统声称自己“兼容ACID”时，其实你无法确信它究竟能提供了什么样的保证，现在的ACID更像是一个市场营销用语</u>。

​	而不符合ACID标准的系统有时被冠以**BASE**，取另外几个特性的首字母，**即基本可用性(Basically Available)，软状态(Soft State)和最终一致性(Eventualconsistency)**。听下来它似乎比ACID更加模棱两可。BASE唯一可以确定的是“它不是ACID”，此外它几乎没有承诺任何东西。

​	我们还是先搞凊楚原子性，一致性，隔离性和持久性的准确含义，目标是建立对事务思想的清晰而牢固的认识。

#### 原子性

​	通常，原子是指不可分解为更小粒度的东西。这个术语在计算机的不同领域里有着相似但却微妙的差异。例如，<u>多线程编程中，如果某线程执行一个原子操作，这意味着**其他线程是无法看到该操作的中间结果**。它只能处于操作之前或操作之后的状态，而不是两者之间的状态</u>。

​	而ACID中的原子性并不关乎多个操作的并发性，它并没有描述多个线程试图访向相同的数据会发生什么情况，后者其实是由ACID的隔离性所定义(参见本章后面的“隔离性”)。

​	ACID原子性其实描述了客户端发起一个包含多个写操作的请求时可能发生的情况，例如在完成了一部分写入之后，系统发生了故障，包括进程崩溃，网络中断，磁盘变满或者违反了某种完整性约束等；**把多个写操作纳入到一个原子事务，万一出现了上述故障而导致没法完成最终提交时，则事务会中止，并且数据库须丢弃或撤销那些局部完成的更改**。

​	<u>假如没有原子性保证，当多个更新操作中间发生了错误，就需要知道哪些更改已经生效，哪些没有生效，这个寻找过程会非常麻烦。或许应用程序可以重试，但情况类似，并且可能导致重复更新或者不正确的结果。原子性则大大简化了这个问题：如果事务已经中止，应用程序可以确定没有实质发生任何更改，所以可以安全地重试</u>。

​	**因此ACID中原子性所定义的特征是：<u>在出错时中止事务，并将部分完成的写入全部丢弃</u>。也许可中止性比原子性更为准确，不过我们还是沿用原子性这个惯用术语**。

#### 一致性

​	一致性非常重要，但它在不同场景有着不同的具体含义，例如：

+ 第5章我们讨论了副本一致性以及异步复制模型时，引出了最终一致性问题(参见第5章“复制滞后问题”)。
+ 一致性哈希则是某些系统用于动态分区再平衡的方法(参见第6章“一致性哈希”)。
+ CAP理论中，一致性一词用来表示线性化(参见第9章“可线性化”)。
+ 而在ACID中，一致性主要指数据库处于应用程序所期待的“预期状态”。

​	可以看出，同一个词至少有四种不同的含义。

​	**ACID中的一致性的主要是指对数据有特定的预期状态，任何数据更改必须满足这些状态约束(或者恒等条件)**。例如，对于一个账单系统，账户的贷款余额应和借款余额保持平衡。如果某事务从一个有效的状态开始，并且事务中任何更新操作都没有违背约束，那么最后的结果依然符合有效状态。

​	**这种一致性本质上要求应用层来维护状态一致(或者恒等)，应用程序有责任正确地定义事务来保持一致性**。这不是数据库可以保证的事情：即如果提供的数据修改违背了恒等条件，数据库很难检测进而阻止该操作(数据库可以完成针对某些特定类型的恒等约束检查，例如使用外键约束或唯一性约束。<u>但通常主要靠应用程序来定义数据的有效/无效状态，数据库主要负责存储</u>。

​	**原子性，隔离性和持久性是数据库自身的属性，而ACID中的一致性更多是应用层的属性**。<u>应用程序可能借助数据库提供的原子性和隔离性，以达到一致性，但一致性本身并不源于数据库。因此，字母C其实并不应该属于ACID。</u>

#### 隔离性

​	大多数数据库都支持多个客户端同时访问。如果读取和写入的是不同数据，这肯定没有什么问题；但如果访问相同的记录，则可能会遇到并发问题(即带来竞争条件)。

​	一个简单例子如图7-1所示。假设有两个客户端同时增加数据库中的一个计数器。每个客户首先读取当前值，再客户端增加1，然后写回新值(这里假设数据库尚不支持自增操作)。图7-1中，由于有两次相加，计数器应该由42增加到44，但实际上由于竞争条件最终结果却是43。

​	**ACⅠD语义中的隔离性意味着并发执行的多个事务相互隔离，它们不能互相交叉**。<u>经典的数据库教材把隔离定义为**可串行化**，这意味着可以假装它是数据库上运行的唯一事务</u>。**虽然实际上它们可能同时运行，但数据库系统要<u>确保当事务提交时，其结果与串行执行(一个接一个执行)完全相同</u>**。

![事务的棘手概念 - 图1](https://static.sitestack.cn/projects/ddia/img/fig7-1.png)

​	**然而实践中，由于性能问题很少使用串行化隔离**。一些流行的数据库，如Oracle 11g，甚至根本就没有实现它。 Oracle虽然也有声称“串行化”的功能，但它本质上实现的是**快照隔离**，后者提供了比串行化更弱的保证。我们将在本章后面“弱隔离级别”中讨论快照隔离以及其他形式的隔离。

#### 持久性

​	<u>数据库系统本质上是提供一个安全可靠的地方来存储数据而不用担心数据丢失等</u>。持久性就是这样的承诺，它**保证一旦事务提交成功，即使存在硬件故障或数据库崩溃，事务所写入的任何数据也不会消失**。

​	对于单节点数据库，持久性通常意味着数据已被写入非易失性存储设备，如硬盘或SSD。**在写入执行过程中，通常还涉及预写日志等**(参阅第3章“可靠的B-tree”)，这样万一磁盘数据损坏可以进行恢复。而**对于支持远程复制的数据库，持久性则意味着数据已成功复制到多个节点**。<u>**为了实现持久性的保证，数据库必须等到这些写入或复制完成之后才能报告事务成功提交**</u>。

​	正如第1章的“可靠性”所提到的，其实不存在完美的持久性。例如，所有的硬盘和所有的备份如果都同时被(人为)销毁了，那么数据库也无能为力。

> 复制与持久性
>
> 从历史上看，持久性最早意味着是写入磁带来存档，之后演变为写入磁盘或SSD。最近，它又代表着多节点间复制。对比起来，哪种方式更好呢?
>
> 答案是，没有任何一个可以称为完美：
>
> + 如果写入了磁盘但机器发生死机，即使数据没有丢失，在重启机器或者将磁盘转移到另一台机器完成之前，都无法访问任何数据。而基于复制的系统则可以继续可用。
> + 某些故障，例如停电或一个软件bug，可能在某个特定输入时触发了异常进而导致所有节点全部崩溃，甚至删除所有的副本(参阅第1章“可靠性“) 内存中的数据会丟失，因此对于内存数据库，写入磁盘仍然是需要的。
> + **在异步复制系统中，当主节点不可用时，最近的写入操作因为可能没有及时完成同步而导致更新丢失**(参阅第5章“处理节点失效。
> + **当电源突然断电时，特别对于固态硬盘可能发生意外：连fsync之后的数据也不能保证可以正确恢复**。和所有其他类型的软件类似，磁盘的固件也可能存在bug。
> + <u>考虑到数据库存储引擎与文件系统之间复杂而微妙的关系，其中也可能包含难以追踪的bug，并最终导致在系统发生崩溃后，磁盘上的文件也出现损坏</u>。
> + **磁盘上的数据可能会悄无声息地出现损坏但又没有被及时检测到。这种情况如果持续发生，则多个副本甚至最近的备份都可能已经损坏。此时需要从历史备份中恢复数据**。
> + **一项关于固态硬盘的研究发现，在部署的前四年，30%到80%的固态盘至少发生一个坏块。磁盘的坏道率比较低，但整盘发生完全失效的概率却比固态盘高**。
> + 某些固态盘如果遭遇突然断电，可能会在接下来的几周内发生丢失数据情况，温度在里面起了关键性作用。
>
> **现实情况是，没有哪一项技术可以提供绝对的持久性保证**。<u>这些都是帮助降低风险的手段，应该组合使用它们，包括写入磁盘、复制到远程机器以及备份等。因此对任何理论上的“保证”一定要谨慎对待</u>。

> [fsync(2) - Linux man page (die.net)](https://linux.die.net/man/2/fsync)

### 7.1.2 单对象与多对象事务操作

​	回顾一下，<u>ACID中的**原子性**和**隔离性**主要针对客户端在同一事务中包含多个写操作时，数据库所提供的保证</u>：

+ **原子性**

  如果一系列写操作中间发生了错误，则事务必须中止，并且事务中已完成的写入应该被丢弃。换言之，不用担心数据库的部分失败，它总是保证要么全部成功要么全部失败。

+ **隔离性**

  <u>同时运行的事务不应相互干扰</u>。例如，如果某个事务进行多次写入，则另一个事务应该观察到的是其全部完成(或者一个都没完成)的结果，而<u>不应该看到中间的部分结果</u>。

​	这些定义假定在一个事务中会修改多个对象(如行，文档，记录等)。这种多对象事务目的通常是为了在多个数据对象之间保持同步。图7-2展示了一个电子邮件应用的例子。要显示用户的未读邮件数量，可以执行查询如下：

`SELECT COUNT(*) FROM emails WHERE recipient_id = 2 AND unread_flag = true`

​	如果电子邮件太多，你会发现査询太慢，然后决定将未读的数量直接保存在一个单独的字段中(虽然这违反了范式要求)。这样毎当收到一个新邮件，需要增加未读计数器；当邮件标记为已读时，还需减少该计数器。

​	在图7-2中，用户2遇到些异常情况：邮箱列表已显示了未读消息，但计数器却还未更新，所显式的数目是0。<u>隔离性将保证用户2看到要么是更新后的电子邮件和更新后的计数器，要么是二者都未更新，而不会是两者不一致</u>。

![事务的棘手概念 - 图2](https://static.sitestack.cn/projects/ddia/img/fig7-2.png)

​	图7-3则说明了对原子性的需求：如果事务执行过程中发生错误，导致邮箱和未读计数器二者不同步。则事务将被中止，且此之前插入的电子邮件将被回滚。

![事务的棘手概念 - 图3](https://static.sitestack.cn/projects/ddia/img/fig7-3.png)

​	<u>多对象事务要求确定知道事务包含了哪些读写操作。对于关系数据库，客户端通常与数据库服务器建立TCP网络连接，因而对于特定的某个连接，SQL语句BEGIN TRANSACTION和COMMIT之间的所有操作都属于同一个事务</u>。

​	**而许多<u>非关系数据库</u>则不会将这些操作组合在一起**。即使他们可能支持多对象API(例如，键-值存储的multi-put API可以在一个操作中更新多个键)，但并不意味着具有事务语义，例如**可能出现某些键更新成功了而其他则发生了失败，最后结果是数据库处于部分更新的状态**。

#### * 单对象写入

​	原子性和隔离性也同样适用于单个对象的更新。例如，假设向数据库写入20KB的JSON文档：

+ 如果发送了第一个10KB之后网络连接中断，数据库是否只存储了无法完整解析的10 KB JSON片段呢?
+ **如果数据库在覆盖磁盘现有数据时发生电源故障，最终是否是新旧值混杂在一起**?
+ **如果另一个客户端在写入的过程中读取该文档，是否会看到部分更新的文档内容**?

​	这些问题着实让人头疼，因此存储引擎几乎必备的设计就是在单节点、单个对象层面上提供原子性和隔离性(比如key-value对)。例如，<u>出现宕机时，基于日志恢复来实现原子性</u>(参阅第3章“可靠的B-tree”)，**对每个对象采用加锁的方式(每次只允许一个线程访问对象)来实现隔离**。

​	<u>某些数据库还提供了高级的原子操作，例如**原子自增**操作，这样就不需要像图7-1那样执行读取-修改-写回，类似地还有**原子比较-设置**操作，即只有当前值没有被他人修改时才执行写入(参阅本章后面的“compare-and-set"</u>)。

​	这些单对象操作可以有效防止多个客户端并发修改同一对象时的更新丢失问题(参阅本章后面的“防止更新丟失”)。但需要注意，它们并不是通常意义上的事务。<u>虽然compare-and-set和其他单对象操作有时也被称为“轻量级事务”，甚至“ACID”，但这里其实存在一些误导性。**通常意义上的事务针对的是多个对象，将多个操作聚合为一个逻辑执行单元**</u>。

#### 多对象事务的必要性

​	许多分布式数据存储系统不支持多对象事务，主要是因为当出现<u>跨分区</u>时，多对象事务非常难以正确实现，同时在高可用或者极致性能的场景下也会带来很多负面影响。

​	但是，分布式数据库实现事务并非不可能，并不存在什么原理上的限制，我们将在第9章讨论分布式事务的实现。

​	但是否所有应用都需要多对象事务呢？是否可能只用键-值数据模型和单对象操作就可以满足应用需求?

​	的确有一些情况，只进行单个对象的插入、更新和删除就足够了。但是，还有许多其他情况要求写入多个不同的对象并进行协调：

+ 对于关系数据模型，表中的某行可能是另一个表中的外键。类似地，在图数据模型中，顶点具有多个边链接到其他的顶点。多对象事务用以确保这些外键引用的有效性，即当插入多个相互引用的记录时，保证外键总是最新、正确的，否则数据更新就变得亳无意义。
+ 对于文档数据模型，如果待更新的字段都在同一个文档中，则可视为单个对象，此时不需要多对象事务。但是，**缺少join支持的文档数据库往往会滋生反规范化**(参阅第2章“关系数据库与文档数据库现状”)，如图7-2所示，当更新这种非规范化数据时，就需要一次更新多个文档。此时多对象事务就可以有效防止非规范化数据之间出现不同步。
+ **对于带有二级索引的数据库(除了纯粹键-值存储以外几乎所有其他系统都支持二级索引)，每次更改值时都需要同步更新索引**。<u>从事务角度来看，这些索引是不同的数据库对象：如果没有事务隔离，就会出现部分索引更新。</u>

​	即使没有事务支持，或许上层应用依然可以工作，然而在没有原子性保证时，错误处理就会异常复杂，而缺乏隔离性则容易出现并发性方面的各种奇怪问题。我们将在本章后面“弱隔离级别”中讨论这些问题，并在第12章中探讨其他的一些方案。

#### 处理错误与中止

​	事务的一个关键特性是，如果发生了意外，所有操作被中止，之后可以安全地重试。**ACID数据库基于这样的一个理念：如果存在违反原子性、隔离性或持久性的风险，则完全放弃整个事务，而不是部分放弃**。

​	<u>然而，并不是所有的系统都遵循上述理念。例如，无主节点复制的数据存储(参阅第5章“无主节点复制”)会在“尽力而为”的基础上尝试多做些工作，可以概括理解为：数据库已经尽其所能，但万一遇到错误，系统并不会撤销已完成的操作，此时需要应用程序来负责从错误中进行恢复</u>。

​	确实我们无法彻底避免错误，然而许多开发人员喜欢只考虑正常的处理路径，而忽视错误处理。比如像流行的 Rails ActiveRecord和Django这样的对象关系映射(ORM)框架，当事务出现异常时不会进行重试而只是简单地抛出堆栈信息，用户虽然得到了错误提示，但所有之前的输入都会被抛弃。这绝对不应该，支持安全的重试机制才是中止流程的重点。

​	重试中止的事务虽然是一个简单有效的错误处理机制，但它并不完美：

+ **如果事务实际已经执行成功，但返回给客户端的消息在网络传输时发生意外(所以在客户端看来事务是失败)，那么重试就会导致重复执行，此时需要额外的应用级重复数据删除机制**。
+ <u>如果错误是由于系统超负荷所导致，则重试事务将使情况变得更糟</u>。为此，可以设定一个重试次数上限，例如指数回退，同时要尝试解决系统过载本身的问题。
+ 由临时性故障(例如死锁，隔离违例，网络闪断和节点切换等)所导致的错误需要重试。但如果出现了永久性故障(例如违反约束)，则重试亳无意义。
+ **如果在数据库之外，事务还产生其他副作用，即使事务被中止，这些副作用可能已事实生效**。例如，假设更新操作还附带发送一封电子邮件，肯定不希望每次重试时都发送邮件。如果想要确保多个不同的系统同时提交或者放弃，可以考虑采用两阶段提交(参阅第9章“原子提交与两阶段提交”)。
+ 如果客户端进程在重试过程中也发生失败，没有其他人继续负责重试，则那些待写入的数据可能会因此而丢失。

## 7.2 弱隔离级别

​	如果两个事务操作的是不同的数据，即不存在数据依赖关系，则它们可以安全地并行执行。**只有出现某个事务修改数据而另一个事务同时要读取该数据，或者两个事务同时修改相同数据时，才会引发并发问题(引入了竞争条件)**。

​	并发性相关的错误很难通过测试发现，这类错误通常只在某些特定时刻才会触发，这种时机相关的问题发生概率低，稳定重现比较困难。并发性也很难对其进行推理分析，特别是对于一个大型应用程序，几乎不可能知道哪些代码正在访问数据库：只有一个用户访问数据时，程序开发就足够困难了，当出现多用户并发时情况会变得更加复杂，每一块数据随时都可能被多个用户所修改。

​	正因如此，数据库一直试图通过事务隔离来对应用开发者隐藏内部的各种并发问题。<u>从理论上讲，隔离是假装没有发生并发，让程序员的生活更轻松，而可串行化隔离意味着数据库保证事务的最终执行结果与串行(即一次一个，没有任何并发)执行结果相同。</u>

​	实现隔离绝不是想象的那么简单。可串行化的隔离会严重影响性能，而许多数据库却不愿意牺牲性能，因而更多倾向于采用较弱的隔离级别，它可以防止某些但并非全部的并发问题。<u>这些弱隔离级别理解起来更为困难，甚至可能会带来一些难以捉摸的隐患，但在实践中还是被广泛使用</u>。

​	弱隔离所引发的并发性错误绝非仅是理论存在，它们已经造成了大量的资金损失，审计部门的调查，以及客户数据破坏等。**对此，有一个流行的说法是“如果你正在处理财务数据，请上ACID系统!”，但这样的建议其实没有太大实际意义，因为很多流行的关系数据库系统(通常被认为是“ACID兼容”)其实也采用的是弱级别隔离，所以它们未必可以阻止类似错误的发生**。

​	与其盲目地相信这些宣传，不如对存在的并发问题以及如何防范有一个全面、深刻的理解。然后，我们就可以使用所掌握的工具和方法来构建正确、可靠的应用。

​	本节将分析几个实际中经常用到的弱级别(非串行化)隔离，并详细讨论可能(或者不可能)发生的竞争条件，有了这些认识之后，可以帮助判断自己的应用更适合什么样的隔离级别。在下一节，我们将详细讨论可串行化。我们主要以示例的方式讨论隔离级别，如果你需要的是严格、正式的定义和分析，可以参考文献。

### 7.2.1 读提交

​	读提交是最基本的事务隔离级别，它只提供以下两个保证：

1. **读数据库时，只能看到已成功提交的数据(防止“脏读”)**。

2. **写数据库时，只会覆盖已成功提交的数据(防止“脏写”)**。

​	这两个保证更深入的介绍如下：

#### 防止脏读

​	假定某个事务已经完成部分数据写入，但事务尚未提交(或中止)，此时另一个事务是否可以看到尚未提交的数据呢？如果是的话，那就是脏读。

​	**读-提交级别的事务隔离必须做到防止发生脏读。这意味着事务的任何写入只有在成功提交之后，才能被其他人观察到(并且所有的写全部可见)**。如图7-4所示，用户1设置了x=3，在用户1的事务未提交之前，用户2的get x操作依旧返回的是旧值2。

![弱隔离级别 - 图1](https://static.sitestack.cn/projects/ddia/img/fig7-4.png)

​	当有以下需求时，需要防止脏读：

+ 如果事务需要更新多个对象，脏读意味着另一个事务可能会看到部分更新，而非全部。例如，图7-2就是一个电子邮件应用的脏读例子，用户看到了新的未读电子邮件，但看不到更新的计数器。观察到部分更新的数据可能会造成用户的困惑，并由此引发一些不必要的后续操作。
+ <u>如果事务发生中止，则所有写入操作都需要回滚(见图7-3)。如果发生了脏读，这意味着它可能会看到一些稍后被回滚的数据，而这些数据并未实际提交到数据库中。之后所引发的后果可能都会变得难以预测</u>。

#### * 防止脏写

​	如果两个事务同时尝试更新相同的对象，会发生什么情况呢？我们不清楚写入的顺序，但可以想象后写的操作会覆盖较早的写入。

​	但是，<u>如果先前的写入是尚未提交事务的一部分，是否还是被覆盖？如果是，那就是脏写</u>。<u>读-提交隔离级别下所提交的事务可以防止脏写，**通常的方式是推迟第二个写请求，直到前面的事务完成提交(或者中止)**</u>。

​	防止脏写可以避免以下并发问题：

+ 如果事务需要更新多个对象，脏写会带来非预期的错误结果。例如，考虑图7-5的二手车销售网站， Alice和Bob两个人试图购买同一辆车。而购买汽车需要两次数据库写入：网站上商品买主需要更新为新买家，销售发票也需要随之更新。对于图7-5的例子，车主被改为Bob(因为他成功地抢先更新了车辆表单)，但发票却发给了 Alice(因为她成功的先执行了发票表单)。读提交隔离要防止这种事故。
+ 但是，读-提交隔离并不能解决图7-1中计数器増量的竟争情况。对于后者，第一次写入确实在第一个事务提交后才执行，虽然不属于脏写，但结果仍然是错误的。在接下来的“防止更新丟失”中，我们将讨论如何安全递增计数器。

![弱隔离级别 - 图2](https://static.sitestack.cn/projects/ddia/img/fig7-5.png)

#### 实现读-提交

​	读-提交隔离非常流行，它是 Oracle 11g、 PostgreSQL、 SQL Server 2012、 MemSQL以及许多其他数据库的默认配置。

​	<u>数据库通常采用**行级锁**来防止脏写</u>：<u>当事务想修改某个对象(例如行或文档)时，它必须首先获得该对象的锁；然后一直**持有锁直到事务提交(或中止)**</u>。<u>给定时刻，只有一个事务可以拿到特定对象的锁，**如果有另一个事务尝试更新同一个对象，则必须等待，直到前面的事务完成了提交(或中止)后，才能获得锁并继续**</u>。**这种锁定是由处于读-提交模式(或更强的隔离级别)数据库自动完成的**。

​	那如何防止脏读呢？一种选择是使用相同的锁，所有试图读取该对象的事务必须先申请锁，事务完成后释放锁。从而确保不会发生读取一个脏的、未提交的值(因为锁在那段期间一直由一个事务所持有)。

​	然而，**读锁的方式在实际中并不可行，因为运行时间较长的写事务会导致许多只读的事务等待太长时间，这会严重影响只读事务的响应延迟，且可操作性差：由于读锁，应用程序任何局部的性能问题会扩散进而影响整个应用，产生连锁反应**。

​	因此，<u>大多数数据库注采用了图7-4所示的方法来防止脏读：**对于每个待更新的对象，数据库都会维护其旧值和当前持锁事务将要设置的新值两个版本**</u>。**<u>在事务提交之前，所有其他读操作都读取旧值；仅当写事务提交之后，才会切换到读取新值</u>**。

### * 7.2.2 快照级别隔离与可重复读

​	表面上看读-提交级别隔离，可能会认为它已经满足了事务所需要一切特征：它支持中止(原子性所必须的)，可以防止读取不完整的结果，并且防止并发写的混合。事实上，这些确实非常有用，相比没有事务的系统，它的确提供了更多的保证。

​	但是，在使用此隔离级别时，仍然有很多场景可能导致并发错误。如图7-6所示。

![弱隔离级别 - 图3](https://static.sitestack.cn/projects/ddia/img/fig7-6.png)

​	假设Alce银行有1000美元的存款，分为两个账户，每个500美元。现在有这样一笔转账交易从账户1转100美元到账户2。如果在她提交转账请求之后而银行数据库系统执行转账的过程中间，来查看两个账户的余额，她有可能会看到账号2在收到转账之前的余额(500美元)，和账户1在完成转账之后的余额(400美元)。对于Alice来说，貌似她的账户总共只有900美元，有100美元消失了。

​	这种异常现象被称为**不可重复读取(nonrepeatable read)**或**读倾斜(read skew)**。如果Alice在交易结束时再次读取账户1的余额，她将看到不同的值(600美元)。读倾斜在读提交隔离语义下是可以接受的， Alice所看到的账户余额的确都是账户当时的最新值。

> 倾斜(skew)这个词有些滥用了，我们以前使用它是由于热点而导致负载不平衡(参阅第6章“倾斜的负载与减轻热点”)，而这里则主要是指时间异常。

​	对于 Alice这个例子，这并非一个永久性问题，例如几秒钟之后当她重新加载银行页面，可能就能看到一致的账户余额。但是，还有些场景则不能容忍这种暂时的不一致：

+ **备份场景**

  备份任务要复制整个数据库，这可能需要数小时才能完成。<u>在备份过程中，可以继续写入数据库。因此，得到镜像里可能包含部分旧版本数据和部分新版本数据。如果从这样的备份进行恢复，最终就导致了永久性的不一致(例如那些消失的存款)</u>。

+ **分析査询与完整性检查场景**

  有时査询可能会扫描几乎大半个数据库。这类查询在分析业务中很常见(参阅第3章“事务处理或分析”)，亦或定期的数据完整性检査(即监视数据损坏情况)。如果这些查询在不同时间点观察数据库，可能会返回无意义的结果。

​	<u>**快照级别隔离**这是阶级上述问题最常见的手段</u>。**其总体想法是，<u>每个事务都从数据库的一致性快照中读取，事务一开始所看到是最近提交的数据，即使数据随后可能被另一个事务更改，但保证每个事务都只看到该特定时间点的旧数据</u>**。

​	<u>快照级别隔离对于长时间运行的只读查询(如备份和分析)非常有用。如果数据在执行査询的同时还在发生变化，那么査询结果对应的物理含义就难以理清</u>。而如果查询的是数据库在某时刻点所冻结的一致性快照，则査询结果的含义非常明确。

​	**快照级别隔离非常流行，目前 PostgreSQL， MySQL的 InnoDB存储引擎，Oracle，SQL server等都已经支持**。

#### * 实现快照级别隔离

​	与读-提交隔离类似，快照级别隔离的实现通常**采用写锁来防止脏写**(参阅本章前面的“实现读-提交”)，这意味着<u>正在进行**写操作**的事务会阻止同一对象上的其他事务</u>。但是，**读取则不需要加锁**。从性能角度看，**快照级别隔离的一个关键点是读操作不会阻止写操作，反之亦然**。这使得数据库可以在处理正常写入的同时，在一致性快照上执行长时间的只读查询，且两者之间没有任何锁的竞争。

​	为了实现快照级别隔离，数据库采用了一种类似于图7-4中防止脏读但却更为通用的机制。<u>考虑到多个正在进行的事务可能会在不同的时间点查看数据库状态，所以**数据库保留了对象多个不同的提交版本**，这种技术因此也被称为**多版本并发控制( Multi-Version Concurrency Control， MVCC )**</u>。

​	**如果只是为了提供读-提交级别隔离，而不是完整的快照级别隔离，则只保留对象的两个版本就足够了：一个已提交的旧版本和尚未提交的新版本。**所以，<u>**支持快照级别隔离的存储引擎往往直接采用MVCC来实现读-提交隔离**</u>。

+ 典型的做法是，在读-提交级别下，**对每一个不同的查询单独创建一个快照**；
+ 而快照级别隔离则是**使用一个快照来运行整个事务**。

​	图7-7说明了 PostgreSQL 如何实现基于MVCC的快照级别隔离(其他实现基本类似)。

+ **<u>当事务开始时，首先赋予一个唯一的、单调递增的事务ID(txid)</u>**。
+ <u>每当事务向数据库写入新内容时，**所写的数据都会被标记写入者的事务ID**</u>。

![弱隔离级别 - 图4](https://static.sitestack.cn/projects/ddia/img/fig7-7.png)

​	<u>表中的每一行都有一个 **created by字段**，其中包含了**创建该行的事务ID**。每一行还有个 **deleted by字段**，初始为空</u>。**如果事务要删除某行，该行实际上并未从数据库中删除，而只是将 deleted_by字段设置为请求删除的事务ID(仅仅标记为删除)**。<u>事后，当确定没有其他事务引用该标记删除的行时，数据库的垃圾回收进程才去真正删除并释放存储空间</u>。

​	<u>这样一笔**更新操作**在内部会被转换为**一个删除操作加一个创建操作**</u>。例如，图7-7中，事务13从账户2中扣除100美元，余额从500美元减为400美元。 accounts表里会出现两行账户2：一个余额为$500但标记为删除的行(由事务13删除)，另一个余额为$400，由事务13创建。

#### * 一致性快照的可见性规则

​	当事务读数据库时，通过事务ID可以决定哪些对象可见，哪些不可见。要想对上层应用维护好快照的一致性，需要精心定义数据的可见性规则。例如：

1. **<u>每笔事务开始时，数据库列出所有当时尚在进行中的其他事务(即尚未提交或中止)，然后忽略这些事务完成的部分写入(尽管之后可能会被提交)，即不可见</u>**。
2. **<u>所有中止事务所做的修改全部不可见</u>**。
3. **较晚事务ID(即晚于当前事务)所倣的任何修改不可见，不管这些事务是否完成了提交**。
4. 除此之外，其他所有的写入都对应用杳询可见。

​	以上规则可以适用于**创建操作**和**删除操作**。<u>在图7-7中，当事务12从账户2读取时，它看到的是$500的余额，这是因为删除操作是由稍后事务13所产生的(依据规则3，事务12对事务13所做的删除不可见)，同理，400美元余额的创建操作也不可见</u>。

​	换句话说，仅当以下两个条件都成立，则该数据对象对事务可见：

+ **事务开始的时刻，创建该对象的事务已经完成了提交**。
+ **对象没有被标记为删除；或者即使标记了，<u>但删除事务在当前事务开始时还没有完成提交</u>**。

​	<u>长时间运行的事务可能会使用快照很长时间，从其他事务的角度来看，它可能在持续访问正在被覆盖或删除的内容。由于**没有就地更新，而是每次修改总创建一个新版本**，因此数据库可以以较小的运行代价来维护一致性快照</u>。

#### * 索引与快照级别隔离

​	接下来一个问题是，这种多版本数据库该如何支持索引呢？<u>一种方案是索引直接指向对象的所有版本，然后想办法过滤对当前事务不可见的那些版本。当后台的垃圾回收进程决定删除某个旧对象版本时，对应的索引条目也需要随之删除</u>。

​	在实践中，有许多细节决定了多版本并发控制的实际性能表现。例如，**可以把同一对象的不同版本放在一个内存页面上， PostgreSQL采取这样的优化措施来避免更新索引**。

​	**CouchDB、 Datomic和LMDB则使用另一种方法。它们主体结构是B-tree(参阅第3章“B-tree”)，但采用了一种追加/写时复制的技术，当需要更新时，不会修改现有的页面，而总是创建一个新的修改副本**，拷贝必要的内容，然后让父结点，或者递归向上直到树的root结点都指向新创建的结点。那些不受更新影响的页面都不需要复制，保持不变并被父结点所指向。

​	<u>这种**采用追加式的B-tree，每个写入事务(或一批事务)都会创建一个新的B-tree root**，代表该时刻数据库的一致性快照</u>。这时就没有必要根据事务ID再去过滤掉某些对象，每笔写入都会修改现有的B-tree，因为之后的查询可以直接作用于特定快照B-tree(有利于査询性能)。采用这种方法依然需要后台进程来执行压缩和垃圾回收。

#### * 可重复读与命名混淆

​	**快照级别隔离对于只读事务特别有效**。但是，具体到实现，许多数据库却对它有着不同的命名。 <u>**Oracle称之为可串行化， PostgreSQL和 MySQL则称为可重复读**</u>。

​	这种命名混淆的原因是SQL标准并没有定义**快照级别隔离**，而仍然是基于老的 System R 1975年所定义的隔离级别)，而当时还没有出现快照级别隔离。标准定义的是“可重复读”，这看起来比较接近于快照级别隔离，所以 PostgreSQL和 MySQL称它们的快照级别隔为“可重复读”，这符合标准要求(即合规性)。

​	**然而必须指出，SQL标准对隔离级别的定义还是存在一些缺陷，<u>某些定义模棱两可不够精确，且不能做到与实现无关</u>**。尽管有几个数据库实现了可重复读，表面上看符合标准，但它们实际所提供的保证却大相径庭。可重复读有一个更为严谨的定义，然而大多数实现并没有遵循它。最后还要指出， **IBM DB2的“可重复读”实则是可串行化级别隔离**。

​	现在的结果是，我们已经搞不清楚“可重复读”究竟代表什么了。

### * 7.2.3 防止更新丢失

​	总结一下，**我们所讨论的读-提交和快照级别隔离主要都是为了解决只读事务遇到并发写时可以看到什么**(虽然中间也涉及脏写问题)，总体而言我们还没有触及另一种情况，即两个写事务并发，而<u>脏写只是写并发的一个特例</u>。

​	写事务并发还会带来其他一些值得关注的冲突问题，最著名的就是更新丢失问题，前面图7-1正是这样的一个例子。

​	更新丟失可能发生在这样一个操作场景中：应用程序从数据库读取某些值，根据应用逻辑做出修改，然后写回新值(**read-modify-write过程**)。当有两个事务在同样的数据对象上执行类似操作时，<u>由于隔离性，第二个写操作并不包括第一个事务修改后的值，最终会导致第一个事务的修改值可能会丢失</u>。这种冲突还可能在其他不同的场景下发生，例如:

+ 递增计数器，或更新账户余额(需要读取当前值，计算新值并写回更新后的值)。
+ 对某复杂对象的一部分内容执行修改，例如对JSON文档中一个列表添加新元素(需要读取并解析文档，执行更改并写回修改后的文档)。
+ 两个用户同时编辑wiki页面，且毎个用户都尝试将整个页面发送到服务器，覆盖数据库中现有内容以使更改生效。

​	并发写事务冲突是一个普遍问题，目前有多种可行的解决方案。

#### * 原子写操作

​	许多数据库提供了**原子更新操作**，以避免在应用层代码完成“读-修改-写回”操作，如果支持的话，通常这就是最好的解决方案。例如，以下指令在多数关系数据库中都是并发安全的。

`UPDATE counters SET value = value + 1 WHERE key = 'foo'`

​	类似地，像 MongoDB这样的文档数据库支持对JSON文档的某部分进行本地修改的原子操作， Redis也提供了对特定数据结构(如优先级队列)修改的原子操作。然而，<u>并非所有的应用更新操作都可以以原子操作的方式来表达，例如维基页面的更新涉及各种文本编辑</u>。**无论如何，如果原子操作可行，那么它就是推荐的最佳方式**。

+ <u>原子操作通常釆用对读取对象加**独占锁**的方式来实现，这样在更新被提交之前不会其他事务可以读它。这种技术有时被称为**游标稳定性**</u>。
+ <u>另一种实现方式是强制所有的原子操作都在**单线程**上执行</u>。

​	不过，<u>基于对象关系映射(ORM)框架可以很容易地就产生出来非“读-修改-写回”的应用层代码，导致无法使用数据库所提供的原子操作。假如你清楚知道自己在做什么，或许这并不会引发什么问题，但往往这种情况会埋下很多难以发现的潜在错误</u>。

#### * 显式加锁

​	如果数据库不支持内置原子操作，另一种和防止更新丢失的方法是由应用程序显式**锁**定待更新的对象。然后，应用程序可以执行“读-修改-写回”这样的操作序列，此时如果有其他事务尝试同时读取对象，则必须等待当前正在执行的序列全部完成。

​	例如，考虑一个多人游戏，其中几个玩家可以同时移动同一个数字。只靠原子操作可能还不够，因为应用程序还需要确保玩家的移动还需遵守其他游戏规则，这涉及一些应用层逻辑，不可能将其剥离转移给数据库层在査询时执行。此时，可以使用锁来防止两名玩家同时操作相同的棋子，参见示例7-1。

​	示例7-1：显式锁定行以防丢失更新

```sql
BEGIN TRANSACTION;
SELECT * FROM figures
    WHERE name = 'robot' AND game_id = 222
FOR UPDATE;
-- Check whether move is valid, then update the position
-- of the piece that was returned by the previous SELECT.
UPDATE figures SET position = 'c4' WHERE id = 1234;
COMMIT;
```

​	**<u>FOR UPDATE指令指示数据库对返回的所有结果行要加锁</u>**。

​	首先该方法是可行的，但要做到这一点，需要仔细考虑清楚应用层的逻辑。很多代码会忘记在必要的地方加锁，结果很容易引入**竞争冲突**。

#### * 自动检测更新丢失

​	<u>**原子操作**和**锁**都是通过强制“读-修改-写回”操作序列串行执行来防止丢失更新</u>。另一种思路则是**先让他们并发执行，但如果事务管理器检测到了更新丢失风险，则会中止当前事务，并强制回退到安全的“读-修改-写回”方式**。

​	<u>该方法的一个优点是数据库完全可以借助**快照级别隔离**来高效地执行检査</u>。的确，PostgreSQL的可重复读， Oracle的可串行化以及 SQL Server的快照级别隔离等，都可以自动检测何时发生了更新丢失，然后会中止违规的那个事务。但是， <u>MySQL InnoDB的可重复读却并不支持检测更新丢失</u>。**有一些观点认为，数据库必须防止更新丟失，要不然就不能宣称符合快照级别隔离，如果基于这样的定义，那么MySQL就属于没有完全支持快照级别隔离**。

​	更新丟失检测是一个非常赞的功能，应用层代码因此不用依赖于某些特殊的数据库功能。开发者可能会不小心忘记使用锁或原子操作，但更新丢失检测会自动生效，有效地避免这类错误。

#### * 原子比较和设置

​	在不提供事务支持的数据库中，有时你会发现它们支持原子“比较和设置”操作(之前“单对象写入”有提到)。使用该操作可以避免更新丢失，即**只有在上次读取的数据没有发生变化时才允许更新**；<u>**如果已经发生了变化，则回退到“读-修改-写回”方式**</u>。

​	例如，为了防止两个用户同时更新同一个wiki页面，可以尝试下面的操作，这样只有当页面从上次读取之后没发生变化时，才会执行当前的更新:

```sql
-- This may or may not be safe, depending on the database implementation
UPDATE wiki_pages SET content = '新内容'
  WHERE id = 1234 AND content = '旧内容';
```

​	**如果内容已经有了变化且值与“旧内容”不匹配，则更新将失败，需要应用层再次检查并在必要时进行重试**。需要注意，<u>**如果 WHERE语句是运行在数据库的某个旧的快照上，即使另一个并发写入正在运行，条件可能仍然为真，最终可能无法防止更新丢失问题。所以在使用之前，应该首先仔细检査“比较-设置”操作的安全运行条件**</u>。

#### * 冲突解决与复制

​	对于支持多副本的数据库(参见第5章)，防止丢失更新还需要考虑另一个维度：由于多节点上的数据副本，不同的节点可能会并发修改数据，因此必须采取一些额外的措施来防止丢失更新。

​	**<u>加锁</u>和<u>原子修改</u>都有个前提即只有一个最新的数据副本**。<u>然而，对于**多主节点**或者**无主节点**的**多副本数据库**，由于支持多个并发写，且通常以异步方式来同步更新，所以会出现多个最新的数据副本。**此时加锁和原子比较将不再适用**(我们将在第9章“线性化”详细讨论这个问题)</u>。

​	正如第5章“检测并发写”所描述的，<u>多副本数据库通常支持多个**并发写**，然后**保留多个冲突版本(互称为兄弟)**，之后由应用层逻辑或依靠特定的数据结构来解决、合并多版本</u>。

​	**如果操作可交换(顺序无关，在不同的副本上以不同的顺序执行时仍然得到相同的结果)，则原子操作在多副本情况下也可以工作**。例如，计数器递增或向集合中添加元素等都是典型的可交换操作。这也是Riak 2.0新数据类型的设计思路，<u>当一个值被不同的客户端同时更新时，Riak自动**将更新合并**在一起，**避免发生更新丢失**。</u>

​	而<u>“**最后写入获胜(LWW)**”(详见第5章)冲突解决方法则容易丢失更新</u>。**不幸的是，目前LWW是许多多副本数据库的默认配置**。

### * 7.2.4 写倾斜与幻读

​	当多个事务同时写入同一对象时引发了两种竞争条件，即前面章节所讨论的**脏写**和**更新丢失**。为了避免数据不一致，需要借助数据库的一些内置机制，或者采取**手动加锁**、执行**原子操作**等。

​	然而，这还不是并发写所引发的全部问题。本节马上将看到更为微妙的写冲突的例子。

​	首先，设想这样一个例子：你正在开发一个应用程序来帮助医生管理医院的轮班。通常，医院会安排多个医生值班，医生也可以申请调整班次(例如他们自己生病了)，但前提是确保至少一位医生还在该班次中值班。

​	现在情况是， Alice和Bob是两位值班医生。两人碰巧都感到身体不适，因而都决定请假。不幸的是，他们几乎同一时刻点击了调班按钮。接下来发生的事情如图7-8所示。

​	<u>每笔事务总是首先检查是否至少有两名医生目前在值班。如果是的话，则有一名医生可以安全离开。由于数据库正在使用**快照级别隔离**，两个检查都返回有两名医生，所以两个事务都安全地进入到下一个阶段。接下来 Alice更新自己的值班记录为离开，同样，Bob也更新自己的记录。两个事务都成功提交，最后的结果却是没有任何医生在值班，显然这违背了至少一名医生值班的业务要求</u>。

#### * 定义写倾斜

​	这种异常情况称为**写倾斜**。<u>它既不是一种脏写，也不是更新丢失</u>，两笔事务更新的是两个不同的对象(分别是 Alice和Bob的值班记录)。这里的写冲突并不那么直接，但很显然这的确是某种竞争状态：试想，如果两笔事务是串行执行，则第二个医生的申请肯定被拒绝；只有同时执行两个事务时才会引发该异常。

![弱隔离级别 - 图5](https://static.sitestack.cn/projects/ddia/img/fig7-8.png)

​	<u>**可以将写倾斜视为一种更广义的更新丢失问题**</u>。

+ 即**<u>如果两个事务读取相同的一组对象，然后更新其中一部分：不同的事务可能更新不同的对象，则可能发生写倾斜</u>**；
+ 而**不同的事务如果更新的是<u>同一个对象</u>，则可能发生<u>脏写</u>或<u>更新丢失</u>(具体取决于时间窗口)**。

​	我们已经给出了多种防范更新丢失的手段。然而对于写倾斜，可选的方案则有很多限制：

+ **由于涉及多个对象，<u>单对象的原子操作</u>不起作用**。

+ <u>基于快照级别隔离来实现更新丢失自动检测</u>也有问题：<u>目前所有的数据库实现包括 PostgreSQL 的可重复读， MySQL/InnoDB可重复读， Oracle可串行化以及SQL Server的**快照级别隔离级别都不支持检测写倾斜问题**</u>。**自动防止写倾斜要求真正的<u>可串行化隔离</u>**(参阅本章后面“可串行化”)。

+ 某些数据库支持自定义约束条件，然后由数据库代为检查、执行约束(例如，唯一性，外键约束或限制一些特定值)。<u>但是，至少一名医生值班这样的要求涉及对多个对象进行约束，**目前大多数数据库不支持这种类型约束**，所以取决于具体的数据库，开发者可能可以采用触发器或物化视图来自己实现类似约束</u>。

+ **如果不能使用可串行化级别隔离，一个次优的选择是对事务依赖的行来显式的加锁**。对于上述医生值班的例子，可以这样：

  ```sql
  BEGIN TRANSACTION;
  SELECT * FROM doctors
    WHERE on_call = TRUE 
    AND shift_id = 1234 FOR UPDATE;
  UPDATE doctors
    SET on_call = FALSE
    WHERE name = 'Alice' 
    AND shift_id = 1234;
  COMMIT;
  ```

  **“FOR UPDATE”语句会通知数据库对返回的所有结果行自动加锁**。

> [mysql --- select ...for update - D_戴同学 - 博客园 (cnblogs.com)](https://www.cnblogs.com/daijiabao/p/11284934.html)
>
> [mysql innodb中 select for update 和直接update 的差别是什么？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/340712418)
>
> [MySQL 共享锁 (lock in share mode)，排他锁 (for update) | Laravel China 社区 (learnku.com)](https://learnku.com/articles/12800/lock-in-share-mode-mysql-shared-lock-exclusive-lock-for-update)

#### * 更多写倾斜的例子

​	写倾斜可能看起来很晦涩拗口，可一旦深刻意识到问题的本质，就会注意到还有更多可能发生的场景。下面就是一些例子：

+ 会议室预订系统

  假设要求同一时间、同个会议室不能被预订两次。当有人想要预订时，首先检查是否有冲突的预订(即对同一房间的预订存在时间范围重叠)，如果没有，则提交申请，参见示例7-2。

  示例7-2：会议室预订系统，<u>试图避免重复预订(但在快照级别隔离下不安全)</u>

  ```sql
  BEGIN TRANSACTION;
  -- Check for any existing bookings that overlap with the period of noon-lpm
  SELECT COUNT(*) FROM bookings
  WHERE room_id = 123 AND 
      end_time > '2015-01-01 12:00' AND start_time < '2015-01-01 13:00';
  -- If the previous query returned zero :
  INSERT INTO bookings(room_id, start_time, end_time, user_id)
    VALUES (123, '2015-01-01 12:00', '2015-01-01 13:00', 666);
  COMMIT;
  ```

  需要指出，**快照级别隔离**无法阻止并发用户预订同一个会议室。<u>为了保证预订不会产生冲突，需要**可串行化的隔离**</u>。

+ 多人游戏

  在示例7-1中，我们使用加锁来防止更新丢失(即两个玩家不能同时移动同一个数字)。但是，锁并不能阻止玩家将两个不同的数字移动到棋盘上的同一个位置上，或者其他可能违反游戏规则的移动。这取决于具体的游戏规则，可能需要更多的条件约束，否则很容易发生写倾斜。

+ 声明一个用户名

  网站通常要求每个用户有唯一的用户名，两个用户可能同时尝试创建相同的用户名。可以采用事务的方式首先来检查名称是否被使用，如果没有，则使用该名称创建账户。但是，和之前的例子类似，在**快照级别隔离**下这是不安全的。不过，对于该例子，一个简单的方案是采用**唯一性约束**(第二个事务由于违反约束，会中止创建相同用户名)。

+ 防止双重开支

  <u>支付或积分相关的服务通常需要检查用户的花费不能超过其限额</u>。**一种方式是在用户的账户中插入一个临时的支出项目，然后对于所有项目，检查总开销并对比限额。由于写倾斜问题，可能会同时插入两个支出项目，<u>两个交易各自都不超额，也不会注意到对方，但加在一起则会超额</u>**。

#### * 为何产生写倾斜

​	上述所有写倾斜的例子都遵循以下类似的模式：

1. 首先输入一些匹配条件，即**采用 SELECT 查询所有满足条件的行**(例如，至少有两名医生正在值班，同一时刻房间没有预订，棋盘的某位置没有出现数字，用户名还没有被占用，账户里还有余额等。
2. **根据查询的结果，应用层代码来决定下一步的操作**(有可能继续，或者报告错误并中止)。

3. **如果应用程序决定继续执行，它将发起数据库写入(INSERT， UPDATE或DELETE)并提交事务**。而<u>**这个写操作会改变步骤2做出决定的前提条件**</u>。换句话说，**<u>如果提交写入之后再重复执行步骤1的 SELECT 查询，就会返回完全不同的结果，原因是刚刚的写操作改变了决定的前提条件</u>**(现在只有一名医生在值班，现在会议室已被预订，现在棋盘位置已经出现了数字，现在用户名已被占用，现在余额巳经不足等)。

​	上述步骤可能有不同的执行顺序，例如，可以先写入，然后是 SELECT 查询，最后根据查询来决定是否提交或者放弃。

​	对于医生值班的例子，步骤3中所修改的行恰好是步骤1查询结果的一部分，因此如果先修改值班记录并加锁(SELECT FOR UPDATE)，再查询可以保证事务安全，避免写倾斜。<u>然而，对于其他例子则并不适用，它们检査的是不满足给定搜索条件的行(预期结果为空)，接下来添加符合条件的行。**如果步骤1的查询根本没有返回任何行，则 SELECT FOR UPDATE也就无从加锁**</u>。

+ <u>这种**在一个事务中的写入改变了另一个事务查询结果**的现象，称为<big>**幻读**</big></u>。

+ **快照级别隔离**可以避免**只读查询**时的**幻读**，但是对于我们上面所讨论那些**读-写事务**，它却无法解决棘手的**写倾斜**问题。

#### * 实体化冲突

​	**如果问题的关键是查询结果中没有对象(空)可以加锁，或许可以人为引入一些可加锁的对象**?

​	例如，对于会议室预订的例子，<u>构造一个时间-房间表，表的每一行对应于特定时间段(例如最小15分钟间隔)的特定房间</u>。我们提前，例如对接下来的6个月，创建好所有可能的房间与时间的组合。

​	现在，预订事务可以查询并锁定(SELECT FOR UPDATE)表中与查询房间和时间段所对应的行。加锁之后，即可检查是否有重叠，然后像之前一样插入新的预订。<u>注意，这种附加表格并不存储预订相关的信息，它**仅仅用于方便加锁**，防止同一房间和时间范围内的重复预订</u>。

​	<u>这种方法称为**实体化冲突(或物化冲突)**，它把**幻读**问题转变为针对数据库中一组具体行的锁冲突问题</u>。

​	然而，弄清楚如何实现实体化往往也具有挑战性，实现过程也容易出错，这种把一个并发控制机制降级为数据模型的思路总是不够优雅。出于这些原因，**除非万不得已，没有其他可选方案，我们不推荐采用<u>实体化冲突</u>**。而在**大多数情况下，<u>可串行化隔离</u>方案更为可行**。

## * 7.3 串行化

​	我们已经分析了很多容易出现竟争条件的例子。采用**读-提交**和**快照隔离**可以防止其中一部分，但并非对所有情况都有效，例如**写倾斜**和**幻读**所导致的棘手问题。最后你会发现面临以下挑战：

+ 隔离级别通常难以理解，而且<u>不同的数据库的实现不尽一致</u>(例如“可重复读取”的含义在各家数据库的差别很大)。
+ 如果去检杳应用层的代码，往往很难判断它在特定的隔离级别下是否安全，特别是对于大型应用系统，几乎<u>无法预测所有可能并发情况</u>。
+ 同时，还缺乏好的工具来帮助检测竟争状况。<u>理论上，静态分析可能有所帮助，但更多的还只是学术研究缺乏实用性</u>。**测试并发性问题往往效率很低，一切取决于时机，它只有在特定的情景下才会出现，存在很大的不确定性**。

​	而这些都不是新问题，自20世纪70年代引入**弱隔离级别**以后，这种情况就一直存在。长久以来，研究人员给出的答案都很简单：**采用可串行化隔离**!

​	<u>可串行化隔离通常被认为是最强的隔离级别</u>。**它保证即使事务可能会并行执行，但<u>最终的结果与毎次一个即串行执行结果相同</u>**。这意味着，如果事务在单独运行时表现正确，那么它们在并发运行时结果仍然正确，换句话说，数据库可以防止所有可能的竞争条件。

​	如果串行化隔离比其他各种弱隔离级别好得多，那么为什么没有广泛使用呢？要回答这个问题，我们需要看看可串行化究竟是什么，以及如何执行。目前大多数提供可串行化的数据库都使用了以下三种技术之一，我们将依次探讨：

+ 严格按照串行顺序执行(参阅本章后面的“实际的串行执行”)。
+ **<u>两阶段锁定(参阅本章后面的“两阶段加锁”)，几十年来这几乎是唯一可行的选择</u>**。
+ **乐观并发控制技术**，例如**可串行化的快照隔离**(参阅本章后面的“可串行化的快照隔离“)。

​	我们首先限定在单节点背景下讨论这些技术。在接下来的第9章，我们将其推广到分布式系统多节点场景。

### 7.3.1 实际串行执行

​	解决并发问题最直接的方法是**避免并发**：即在一个线程上按顺序方式每次只执行一个事务。这样我们完全回避了诸如检测、防止事务冲突等问题，其对应的隔离级别一定是严格串行化的。

​	看上去这是个很直白的想法，但数据库设计人员直到2007年前后才完全确信，采用单线程循环来执行事务是可行的。如果多线程并发在过去的30年中一只被认为是提升性能的关键，那么现在转向单线程执行，这意味着什么呢?

​	有以下两方面的进展促使我们重新做出思考：

+ <u>内存越来越便宜</u>，现在许多应用可以将整个活动数据集都加载到内存中(参阅第3章“将所有内容加载到内存”)。当事务所需的所有数据都在内存中时，事务的执行速度要比等待磁盘I/O快得多。
+ 数据库设计人员意识到<u>OLTP事务通常执行很快</u>，只产生少量的读写操作(参阅第3章“事务处理或分析”)。相比之下，运行时间较长的分析查询则通常是只读的，可以在一致性快照(使用快照隔离)上运行，而不需要运行在串行主循环里。

​	**VoltDB/H-Store、 Redis和Datomic等采用串行方式执行事务**。单线程执行有时可能会比支持并发的系统效率更高，尤其是可以**避免锁开销**。但是，**其吞吐量上限是单个CPU核的吞吐量**。<u>为了充分利用单线程，相比于传统形式，事务也需要做出相应调整</u>。

#### * 采用存储过程封装事务

​	**在数据库的早期应用阶段，采用事务机制是希望能囊括用户的所有操作序列**。例如，预订机票涉及多个步骤(搜索路线，票价和可用座位，决定行程，在行程的某个航班上预订座位，输入乘客信息，最后是付款)。数据库设计者认为，如果整个过程是一个事务，那么就可以方便地原子化执行。

​	然而，人类做出决定并回应的速度通常比较慢。如果数据库事务总是需要等待来自用户的输入，同时还要支持潜在大量并发需求，那么系统大部分时间将处于空闲状态。这样数据库无法高效运行，所以<u>几乎所有的OLTP应用程序都避免在事务执行中等待用户交互从而使事务非常简洁</u>。<u>对于Web，这意味着事务会在一个HTTP请求中提交，而不会跨越多个请求。新的事务往往需要开启新的HTTP请求</u>。

​	即使把人为交互从关键路径移除掉，**事务总体沿用的依然是交互式客户端/服务器风格，一次一个请求语句**。应用程序来提交査询，读取结果，可能会根据前一个査询的结果来进行其他査询，依此类推。请求与结果在应用层代码(某台机器)和数据库服务器(另一台机器)之间来回交互。

​	<u>对于这种交互式的事务处理，大量时间花费在应用程序与数据库之间的网络通信。如果不允许事务并发，而是一次仅处理一个，那么吞吐量非常低，数据库总是在等待应用提交下一个请求。在这种类型的数据库中，为了获得足够的吞吐性能，需要能够**同时处理多个事务**</u>。

​	<u>出于这个原因，采用**单线程串行执行**的系统往往不支持**交互式**的**多语句事务**。应用程序必须提交整个事务代码作为**存储过程**打包发送到数据库</u>。这之间的差异如图7-9所示。把事务所需的所有数据全部加载在内存中，使存储过程高效执行，而无需等待网络或磁盘I/O。

![可序列化 - 图1](https://static.sitestack.cn/projects/ddia/img/fig7-9.png)

#### * 存储过程的优缺点

​	关系型数据库支持存储过程已经有很长一段时间了，自1999年以来巳是SQL标准(SQL/PSM)。然而由于各种原因，存储过程的声誉有所下降：

+ **每家数据库厂商都有自己的存储过程语言**(Oracle的PL/SQL， SQL Server的T-SQL， PostgreSQL的PL/pgSQL等)。<u>这些语言并没有跟上通用编程语言的发展，如果从今天的角度来看，这些语义都相当丑陋、过时，而且缺乏如今大多数编程语言所常用的函数库</u>。
+ **<u>在数据库中运行代码难以管理：与应用服务器相比，调试更加困难，版本控制与部署复杂，测试不便，并且不容易和指标监控系统集成</u>**。
+ 因为数据库实例往往被多个应用服务器所共享，所以数倨库通常比应用服务器要求更高的性能。**<u>数据库中一个设计不好的存储过程(例如，消耗大量内存或CPU时间)要比同样低效的应用服务器代码带来更大的麻烦</u>**。

​	不过这些问题也是可以克服的。最新的存储过程已经放弃了PL/SQL，而是使用现有的通用编程语言，例如 VoltDB使用Java或Groovy，Datomic使用Java或 Clojure，而Redis使用Lua。

​	**<u>存储过程与内存式数据存储使得单线程上执行所有事务变得可行。它们不需要等待I/O，避免加锁开销等复杂的并发控制机制，可以得到相当不错的吞吐量</u>**。

​	<u>ⅤoltDB还借助存储过程来执行**复制**：它并非将事务的执行结果从一个节点复制到另一个节点，而是**在每个副本上都执行相同的存储过程**</u>。因此， VoltDB要求存储过程必须是确定性的(即不同的节点上运行时，结果必须完全相同)。如果事务需要获得当前的日期和时间等，必须通过专门的确定性API来实现。

#### * 分区

​	串行执行所有事务使得并发控制更加简单，但是数据库的吞吐量被限制在单机单个CPU核。虽然只读事务可以在单独的快照上执行，但是<u>对于那些**高写入**需求的应用程序，**单线程事务处理**很容易成为严重的瓶颈</u>。

​	为了扩展到多个CPU核和多节点，可以对数据进行分区(参见第6章)， VoltDB支持这种配置模式。<u>如果你能找到一个方法来对数据集进行分区，使得**每个事务只在单个分区内读写数据**，这样**每个分区都可以有自己的事务处理线程且独立运行**。此时为**每个CPU核分配一个分区**，则数据库的**总体事务吞吐量**可以到达与CPU核的数量成**线性**比例关系</u>。

​	但是，对于跨分区的事务，数据库必须在涉及的所有分区之间协调事务。<u>存储过程需要跨越所有分区**加锁**执行，以确保整个系统的**可串行化**</u>。

​	<u>由于跨分区事务具有额外的协调开销，其性能比单分区内要慢得多</u>。 VoltDB报告的跨区事务吞吐量大约只有1000次/秒，这比单分区吞吐量低了好几个数量级，而且没法通过增加更多机器的方式来扩展性能。

​	<u>事务是否能只在单分区上执行很大程度上取决于应用层的数据结构</u>。简单的键-值数据比较容易切分，而**带有多个二级索引的数据则需要大量的跨区协调(参阅第6章“分区与二级索引”)，因此不太合适**。

#### * 串行执行小结

​	当满足以下约束条件时，串行执行事务可以实现串行化隔离：

+ **事务必须简短而高效**，否则一个缓慢的事务会影响到所有其他事务的执行性能。
+ **仅限于活动数据集完全可以加载到内存的场景**。有些很少访问的数据可能会被移到磁盘，但万一单线程事务需要访问它，就会严重拖累性能。
+ **写入吞吐量必须足够低**，才能在单个CPU核上处理；否则就需要采用分区，**最好没有跨分区事务**。
+ <u>**跨分区事务虽然也可以支持，但是占比必须很小**</u>。

### * 7.3.2 两阶段加锁

​	近三十年来，可以说数据库只有一种被广泛使用的串行化算法，那就是**两阶段加锁(two-phase locking， 2PL)**。

> 2PL不是2PC
>
> 虽然两阶段加锁(2PL)听起来和两阶段提交( two-phase commit，2PC)很相近，但它们是完全不同的东西。我们将在第9章讨论2PC。

​	之前我们看到，可以使用**加锁**的方法来**防止脏写**(参阅本章前面的“防止脏写”)：即如果两个事务同时尝试写入同一个对象时，以加锁的方式来确保第二个写入等待前面事务完成(包括中止或提交)。

​	<u>两阶段加锁方法类似，但锁的强制性更高。多个事务可以同时读取同一对象，但只要出现**任何写操作**(包括修改或删除)，则必须加锁以**独占**访问</u>：

+ 如果事务A已经读取了某个对象，此时事务B想要写入该对象，那么B必须等到A提交或中止之才能继续。以确保B不会在事务A执行的过程中间去修改对象。
+ <u>如果事务A已经修改了对象，此时事务B想要**读取**该对象，则B必须等到A提交或中止之后才能继续。**对于2PL，不会出现读到旧值的情况**</u>(参见图7-1的示例)。

​	因此<u>**2PL不仅在并发写操作之间互斥，读取也会和修改产生互斥**</u>。快照级别隔离的口号“读写互不干扰”(参阅本章前面的“实现快照级别隔离”)非常准确地点明了它和两阶段加锁的关键区别。另一方面，<u>因为**2PL提供了串行化**，所以它可以防止前面讨论的所有竞争条件，包括**更新丢失**和**写倾斜**</u>。

#### * 实现两阶段加锁

​	目前，2PL已经用于 MySQL(InnoDB)和 SQL Server中的“可串行化隔离”，以及DB2中的“可重复读隔离“。

​	此时数据库的每个对象都有一个读写锁来隔离读写操作。即锁可以处于**共享模式**或**独占模式**。基本用法如下：

+ **如果事务要读取对象，必须先以共享模式获得锁**。可以有多个事务同时获得一个对象的共享锁，但是如果某个事务已经获得了对象的独占锁，则所有其他事务必须等待。
+ **如果事务要修改对象，必须以独占模式获取锁**。不允许多个事务同时持有该锁(包括共享或独占模式)，换言之，如果对象上已被加锁，则修改事务必须等待。
+ **<u>如果事务首先读取对象，然后尝试写入对象，则需要将共享锁升级为独占锁</u>**。<u>升级锁的流程等价于直接获得独占锁</u>。
+ **<u>事务获得锁之后，一直持有锁直到事务结束(包括提交或中止)</u>**。<u>这也是名字“两阶段”的来由，在第一阶段即事务执行之前要获取锁，第二阶段(即事务结束时)则释放锁</u>。

​	由于使用了这么多的锁机制，所以很容易出现死锁现象，例如事务A可能在等待事务B释放它的锁，而事务B在等待事务A释放所持有的锁。<u>数据库系统会**自动检测**事务之间的**死锁情况**，并强行中止其中的一个以打破僵局，这样另一个可以继续向前执行。而**被中止的事务需要由应用层来重试**</u>。

#### * 两阶段加锁的性能

​	两阶段加锁的主要缺点，或者说自1970年以来并不被所有人接纳的主要原因在于性能：**<u>其事务吞吐量和查询响应时间相比于其他弱隔离级别下降非常多</u>**。

​	部分原因在于锁的获取和释放本身的开销，但更重要的是**其降低了事务的并发性**。<u>按2PL的设计，两个并发事务如果试图做任何可能导致竞争条件的事情，其中一个必须等待对方完成</u>。

​	传统的关系数据库并不限制事务的执行时间，且当初是为和人类输入等交互式应用而设计的。结合2PL，最终结果是，当一个事务还需要等待另一个事务时，那么最终的等待时间几乎是没有上限的。即使可以保证自己的事务足够简短、高效，但<u>一旦出现多个事务同时访问同一对象，会形成一个等待队列，事务就必须等待队列前面所有其他事务完成之后才能继续</u>。

​	因此，在2PL模式下数据库的访问延迟具有非常大的不确定性，如果工作负载存在严重竞争，以百分比方式观察延迟指标会发现非常缓慢(参阅第1章“描述性能”)。试想这种情况，**某个事务本身很慢，或者是由于需要访问大量数据而获得了许多锁，则它还会导致系统的其他部分都停顿下来**。如果应用需要稳定如一的性能，这种不确定性就是致命的。

​	**同样是基于加锁方式的读-提交隔离也可能发生<u>死锁</u>**，然而在2PL下，取决于事务的访问模式，死锁可能变得更为频繁。因而导致另一个性能问题，即如果事务由于死锁而被强行中止，应用层就必须从头重试，假如死锁过于频繁，则最后的性能和效率必然大打折扣。

#### * 谓词锁

​	对于加锁，我们还忽略了一个微妙但重要的细节。如本章前面“写倾斜与幻读”中的幻读问题，即一个事务改变另一个事务的查询结果。**可串行化隔离也必须防止幻读问题**。

​	以会议室预订为例，如果事务在查询某个时间段内一个房间的预订情况(参见示例7-2)，则<u>另一个事务不能同时去插入或更新同一时间段内该房间的预订情况，但它可以修改其他房间的预订情况，或者在不影响当前查询的情况下，修改该房间的其他时间段预订</u>。

​	如何实现呢？技术上讲，我们需要引入一种谓词锁(或者属性谓词锁)。它的作用类似于之前描述的共享/独占锁，而区别在于，**它并不属于某个特定的对象(如表的某一行)，而是<u>作用于满足某些搜索条件的所有查询对象</u>**，例如:

```sql
SELECT * FROM bookings
WHERE room_id = 123 AND
      end_time > '2018-01-01 12:00' AND 
      start_time < '2018-01-01 13:00';
```

​	谓词锁会限制如下访问:

+ <u>如果事务A想要读取某些满足匹配条件的对象，例如采用 SELECT 查询，它必须以**共享模式**获得査询条件的**谓词锁**</u>。**如果另一个事务B正持有任何一个匹配对象的<u>互斥锁</u>，那么A必须等到B释放锁之后才能继续执行查询**。
+ <u>如果事务A想要插入、更新或删除任何对象，则必须首先检查所有旧值和新值是否与现有的任何**谓词锁**匹配(即冲突)</u>。**如果事务B持有这样的<u>谓词锁</u>，那么A必须等到B完成提交(或中止)后才能继续**。

​	这里的关键点在于，<u>**谓词锁**甚至可以保护数据库中那些尚不存在但可能马上会被插入的对象**(幻读**)。**将两阶段加锁与谓词锁结合使用，数据库可以防止所有形式的写倾斜以及其他竞争条件，隔离变得真正可串行化**</u>。

#### * 索引区间锁

​	不幸的是，**谓词锁性能不佳**：如果活动事务中存在许多锁，那么检査匹配这些锁就变得非常耗时。因此，**大多数使用2PL的数据库实际上实现的是<u>索引区间锁(或者next-key locking)</u>**，<u>本质上它是对谓词锁的简化或者近似</u>。

​	<u>简化谓词锁的方式是**将其保护的对象扩大化**，首先这肯定是**安全**的</u>。例如，如果一个谓词锁保护的是查询条件是：房间123，时间段是中午至下午1点，则一种方式是通过扩大时间段来简化，即保护123房间的所有时间段；或者另一种方式是扩大房间，即保护中午至下午1点之间的所有房间(而不仅是123号房间)。<u>这样，**任何与原始谓词锁冲突的操作肯定也和近似之后的区间锁相冲突**</u>。

​	对于房间预订数据库，通常会在room_id列上创建索引，和/或在 start_time和end_time上有索引(否则前面的查询在大型数据库上会很慢)：

+ <u>假设索引位于 room_id上，数据库使用此索引查找123号房间的当前预订情况。现在，数据库可以简单地**将共享锁附加到此索引条目**，表明事务已搜索了123号房间的所有时间段预订</u>。
+ <u>或者，如果数据库使用基于时间的索引来査找预订，则可以**将共享锁附加到该索引中的一系列值**，表示事务已经搜索了该时间段内的所有值(例如直到2020年1月1日)</u>。

​	**无论哪种方式，查询条件的近似值都附加到某个索引上**。

​	<u>接下来，如果另一个事务想要插入、更新或删除同一个房间和/或重叠时间段的预订，则肯定需要更新这些索引，一定就会与共享锁冲突，因此会自动处于等待状态直到共享锁释放</u>。

​	这样就有效防止了**写倾斜**和**幻读**问题。的确，<u>**索引区间锁不像谓词锁那么精确**(会锁定更大范围的对象，而超出了串行化所要求的部分)，但由于**开销低得多**，可以认为是一种很好的折衷方案</u>。

​	<u>如果没有合适的**索引**可以施加**区间锁**，则数据库可以回退到对**整个表施加共享锁**</u>。这种方式的性能肯定不好，它甚至会阻止所有其他事务的写操作，但的确可以保证安全性。

### * 7.3.3 可串行化的快照隔离

​	本章已经给大家展示了数据库并发方面很多让人纠结、黯淡的一面。

+ <u>两阶段加锁虽然可以保证串行化，但性能差强人意且无法扩展(由于串行执行)</u>；
+ <u>弱级别隔离虽然性能不错，但容易引发各种边界条件(如**更新丢失**，**写倾斜**，**幻读**等)</u>。

​	那么，串行化隔离与性能是不是从根本上就是互相冲突而无法兼得吗？

​	或许并非如此。最近一种称为**可串行化的快照隔离(Serializable Snapshot Isolation，SSⅠ)**算法看起来让人眼前一亮。**它提供了完整的可串行性保证，而性能相比于快照隔离损失很小**。SSI算法面世至今不过数年，它于2008年被首次提出，后来成为Michael Cahil博士论文研究主题。

​	<u>目前，SSI可用于单节点数据库(PostgreSQL 9.1之后的可串行化隔离)或者分布式数据库(如 FoundationDB釆用了类似的算法)。相比于其他并发控制机制，SSI尚需在实践中证明其性能。即使如此，它很有可能成为未来数据库的标配。</u>

> [Postgresql可串行化快照隔离浅析 - 尚码园 (shangmayuan.com)](https://www.shangmayuan.com/a/aeed06ae6dbb42feb138a047.html)
>
> [Jepsen发现PostgreSQL重大Bug：在单个PostgreSQL实例上以可串行化隔离执行的事务实际上是不可串行的 (jdon.com)](https://www.jdon.com/54407)
>
> [解读年度数据库PostgreSQL：如何处理并发控制（一）_数据和云的博客-CSDN博客](https://blog.csdn.net/enmotech/article/details/94683664)

#### * 悲观与乐观的并发控制

​	<u>两阶段加锁是一种典型的**悲观并发控制**机制</u>。<u>它基于这样的设计原则：如果某些操作可能出错(例如与其他并发事务发生了锁冲突)，那么直接放弃，采用等待方式直到绝对安全。这**和多线程编程中互斥锁是一致的**</u>。

​	某种意义上讲，**串行执行**是种极端悲观的选择：<u>事务执行期间，等价于事务对**整个数据库(或数据库的一个分区)**持有**互斥锁**</u>。而我们只能假定事务执行得足够快、持锁时间足够短，来稍稍弥补这种悲观色彩。

​	相比之下，<u>**可串行化的快照隔离**则是一种**乐观并发控制**</u>。<u>在这种情况下，如果可能发生潜在冲突，事务会继续执行而不是中止，寄希望一切相安无事；而当事务提交时(只有可串行化的事务被允许提交)，数据库会检查是否确实发生了冲突(即违反了隔离性原则)，如果是的话，**中止事务并接下来重试**</u>。

​	**乐观并发控制**其实是一个古老的想法，关于其优点和缺点已经争论了很长时间：

+ **如果冲突很多，则性能不佳(许多事务试图访问相同的对象)，大量的事务必须中止**。
+ **如果系统已接近其最大吞吐量，反复重试事务会使系统性能变得更差**。

​	但是，如果系统还有足够的性能提升空间，且如果事务之间的竞争不大，乐观并发控制会比悲观方式高效很多。通过可交换的原子操作还可以减少一些竞争情况。例如，如果多个事务同时试图增加某个计数器，那么不管以什么样的顺序去增加(只要同一事务不去读计数器)，最后的结果总是等价的，并发提交多个增量操作是可行的。

​	顾名思义，**SSI基于快照隔离**，也就是说，<u>事务中的**所有读取操作**都是基于数据库的一致性快照</u>(请参阅本章前面的“快照隔离”和“可重复读”)。这是与早期的乐观并发控制主要区别。<u>在快照隔离的基础上，SSI新增加了相关算法来**检测写入之间的串行化冲突从而决定中止哪些事务**。</u>

#### * 基于过期的条件做决定

​	我们在讨论**写倾斜**(参阅本章前面的“写倾斜与幻读”)时，介绍了这样一种使用场景：事务首先査询某些数据，根据査询的结果来决定采取后续操作，例如修改数据。<u>而在快照隔离情况下，数椐可能在査询期间就已经被其他事务修改，导致原事务在提交时决策的依据信息已出现变化</u>。

​	换句话说，事务是基于某些前提条件而决定采取行动，在事务开始时条件成立，例如“目前有两名医生值班”，而当事务要提交时，数据可能已经发生改变，条件已不再成立。

​	当应用程序执行查询时(例如“当前有多少医生在值班?”)，数据库本身无法预知应用层逻辑如何使用这些查询结果。安全起见，数据库假定对查询结果(决策的前提条件)的任何变化都应使写事务失效。换言之，**<u>查询与写事务</u>之间可能存在<u>因果依赖关系</u>**。<u>**为了提供可串行化的隔离，数据库必须检测事务是否会修改其他事务的查询结果，并在此情况下中止写事务**</u>。

​	数据库如何知道查询结果是否发生了改变呢？可以分以下两种情况：

+ <u>**读取是否作用于一个(即将)过期的MVCC对象(读取之前已经有未提交的写入)**</u>。
+ <u>**检查写入是否影响即将完成的读取(读取之后，又有新的写入)**</u>。

#### * 检测是否读取了过期的MVCC对象

​	回想一下，**快照隔离通常采用多版本并发控制技术(MVCC，见图7-10)来实现**。<u>当事务从MⅤCC数据库一致性快照读取时，它会忽略那些在创建快照时**尚未提交**的事务**写入**</u>。例如图7-10中，事务42(修改 Alice的值班状态)未被提交，因此事务43中Alice查询到的oncall是true；当事务43提交时，事务42已经完成了提交。**换言之从快照读取时被忽略的写入已经生效，并且直接导致事务43做决定的前提已不再成立**。

![可序列化 - 图2](https://static.sitestack.cn/projects/ddia/img/fig7-10.png)

​	**为防止这种异常，<u>数据库需要跟踪那些由于MVCC可见性规则而被忽略的写操作</u>**。**<u>当事务提交时，数据库会检查是否存在一些当初被忽略的写操作现在已经完成了提交，如果是则必须中止当前事务</u>**。

​	为什么要等到提交：当检测到读旧值，为何不立即中止事务43呢？可以考虑这些情况：

+ 首先，如果事务43是个只读事务，没有任何<u>写倾斜</u>风险，就不需要中止；<u>而事务43读取数据库时，数据库还不知道事务是否稍后有任何写操作</u>。
+ 此外，事务43提交时，<u>有可能事务42发生了中止或者还处于未提交状态</u>，因此读取的并非是过期值。

​	**<u>通过减少不必要的中止，SSI可以高效支持那些需要在一致性快照中运行很长时间的读事务</u>**。

#### * 检测写是否影晌了之前的读

​	第二种要考虑的情况是，<u>在读取数据之后，另一个事务修改了数据</u>。如图7-11所示。

![可序列化 - 图3](https://static.sitestack.cn/projects/ddia/img/fig7-11.png)

​	在“两阶段加锁”中，我们讨论了<u>索引区间锁</u>(参阅本章前面的“索引区间锁”)。**它可以锁定与某个查询条件匹配的所有行**，例如 `WHERE shift_id = 1234`。这里使用了类似的技术，只有一点差异：**<u>SSI锁不会阻塞其他事务</u>**。

​	在图7-11中，事务42和事务43都在查询轮班1234期间的值班医生。<u>如果在 shift_id上建有**索引**，数据库可以通过索引条目1234来记录事务42和事务43都查询了相同的结果。如果没有索引，可以在**表级别**跟踪此信息。**该额外记录只需保留很小一段时间，当并发的所有事务都处理完成(提交或中止)之后，就可以丢弃**</u>。

​	**当另一个事务尝试修改时，它首先检查索引，从而确定是否最近存在一些读目标数据的其他事务**。<u>这个过程类似于在受影响的字段范围上获取写锁，但它并**不会阻塞读取**，而是**直到读事务提交时**才进一步通知他们：所读到的数据现在已经发生了变化</u>。

​	图7-11中，**事务43和事务42会互相通知对方先前的读已经过期**。<u>虽然事务43的修改的确影响了事务42，但事务43当时并未提交(修改未生效)，而事务42首先尝试提交，所以可以成功；**随后当事务43试图提交时，来自42的冲突写已经提交生效，事务43不得不中止**</u>。

#### * 可串行化快照隔离的性能

​	有许多工程方面的细节会直接影响算法在实践中的效果。例如， 一个需要**权衡考虑的是关于跟踪事务读、写的粒度**。

+ 如果非常详细地跟踪毎个事务的操作，确实可以准确推测有哪些事务受到影响、需要中止，但是记录元数据的开销可能很大；
+ 而粗粒度的记录则速度占优，但可能会扩大受影响的事务范围。

​	<u>有时，读取过期的数据并不会造成太大影响，这完全取决于所处的具体场景</u>。有时可以确信执行的最终结果是可串行化的， PostgreSQL采用这样的信条来减少不必要的中止。

​	**<u>与两阶段加锁相比，可串行化快照隔离的一大优点是事务不需要等待其他事务所持有的锁</u>**。**这一点和快照隔离一样，<u>读写通常不会互相阻塞</u>**。这样的设计使得査询延迟更加稳定、可预测。<u>特别是，在一致性快照上执行**只读查询不需要任何锁**，这对于读密集的负载非常有吸引力</u>。

​	<u>与串行执行相比，**可串行化快照隔离可以突破单个CPU核的限制**</u>。 FoundationDB将冲突检测分布在多台机器上，从而提髙总体吞吐量。即使数据可能跨多台机器进行分区，事务也可以在多个分区上读、写数据并保证可串行化隔离。

​	<u>**需要指出，事务中止的比例会显著影响SSI的性能表现**</u>。例如，一个运行很长时间的事务，读取和写入了大量数据，因而产生冲突并中止的概率就会增大，所以**<u>SSI要求读-写型事务要简短(而长时间执行的只读事务则没有此限制)</u>**。但**总体讲，相比于<u>两阶段加锁</u>与<u>串行执行</u>，<u>SSI</u>更能容忍那些执行缓慢的事务**。

## * 7.4 小结

​	事务作为一个抽象层，使得应用程序可以忽略数据库内部一些复杂的并发问题，以及某些硬件、软件故障，从而简化应用层的处理逻辑，大量的错误可以转化为简单的事务中止和应用层重试。

​	本章，我们例举了很多事务能够预防的问题。尽管并非所有的应用程序都会面临这些可题，例如那些简单的访问模式，只读或者只写，可能根本无需事务的帮助。但对于复杂的访问模式，事务可以大大简化需要考虑的潜在错误情况。

​	如果没有事务，各种错误情况(如进程崩溃、网络中断、停电、磁盘已满、并发竞争等)会导致数据可能出现各种不一致。例如，反规格化数据模式和相关操作会导致与源数据不同步。假如没有事务，处理那些复杂交互访问最后所导致的数据库混乱就会异常痛苦。

​	本章，我们深入探讨了并发控制这一主题。介绍了多个广泛使用的隔离级别，特别是**读-提交**，**快照隔离(或可重复读取)**与**可串行化**。通过分析如何处理边界条件来阐述这些隔离级别的要点：

+ 脏读

  客户端读到了其他客户端尚未提交的写入。读-提交以及更强的隔离级别可以防止脏读。

+ **脏写**

  **客户端覆盖了另一个客户端尚未提交的写入**。<u>几乎所有的数据库实现都可以防止脏写</u>。

+ **读倾斜(不可重复读)**

  客户在不同的时间点看到了不同值。快照隔离是最用的防范手段，即事务总是在某个时间点的一致性快照中读取数据。通常采用多版本并发控制(MVCC)来实现快照隔离。

+ 更新丢失

  两个客户端同时执行读-修改-写入操作序列，出现了**其中一个覆盖了另一个的写入**，但又没有包含对方最新值的情况，最终导致了部分修改数据发生了丢失。<u>快照隔离的一些实现可以自动防止这种异常，而另一些则需要手动锁定查询结果(SELECT FOR UPDATE)</u>。

+ **写倾斜**

  **事务首先査询数据，根据返回的结果而作出某些决定，然后修改数据库。<u>当事务提交时，支持决定的前提条件已不再成立</u>。<u>只有可串行化的隔离才能防止这种异常</u>**。

+ **幻读**

  **事务读取了某些符合査询条件的对象，同时另一个客户端执行写入，改变了先前的査询结果**。<u>**快照隔离可以防止简单的幻读，但写倾斜情况则需要特殊处理，例如采用区间范围锁**</u>。

​	<u>**弱隔离级别**可以防止上面的某些异常，但还需要应用开发人员手动处理其他复杂情况(例如，显式加锁)。只有**可串行化**的隔离可以防止所有这些问题</u>。我们主要讨论了实现可串行化隔离的三种不同方法：

+ 严格串行执行事务

  如果毎个事务的执行速度非常快，且单个CPU核可以满足事务的吞吐量要求，严格串行执行是一个非常简单有效的方案。

+ **两阶段加锁**

  **几十年来，这一直是实现可串行化的标准方式，但还是有很多系统出于性能原因而放弃使用它**。

+ **可串行化的快照隔离(SSⅠ)**

  **一种最新的算法，可以避免前面方法的大部分缺点。它秉持乐观预期的原则，允许多个事务并发执行而不互相阻塞；仅当事务尝试提交时，才检查可能的冲突，如果发现违背了串行化，则某些事务会被中止**。

​	本章中的示例都采用关系数据模型。但是，正如本章前面的“多对象事务的需求”所描述的，**无论对哪种数据模型，事务都是非常有用的数据库功能**。

​	本章所介绍的算法、方案主要针对单节点。对于分布式数据库，还会面临更多、更复杂的挑战，我们将在接下来的两章中继续展开讨论。

# 第8章 分布式系统的挑战

​	本章对分布式系统可能出现的故障做了一个全面、近乎悲观的总结。故障可能来自**网络问题**(详见本章后面的“不可靠的网络”)，以及**时钟与时序问题(**详见本章后面的“不可靠的时钟”)等，并讨论这些问题的可控程度。坦白讲，这些问题一直在困扰着人们，因此我们将探讨如何认清分布式系统的状态本质，并据此来评估所发生的各种故障(详见本章后面的“知识，真相与谎言”)。

## 8.1 故障与部分失效

​	在单节点上开发程序时，通常它应该以一种确定性的方式运行：要么工作，要么出错。有bug的软件可能最终会在某天暴露出来(而问题通常可以由重启机器来解决)，但问题的主要原因可能还是软件本身的bug。

​	单台节点上的软件通常不应该出现模棱两可的现象：当硬件正常工作时，相同的操作通常总会产生相同的结果(即**确定性**)；而如果硬件存在问题(例如，内存损坏或接口松动)，结果往往是系统性故障，如内核崩溃，监屏死机，启动失败等。因此<u>在单节点上一个质量合格的软件状态要么是功能正常，要么是完全失效，而不会介于两者之间</u>。

​	这背后涉及计算机设计一个非常审慎的选择：**如果发生了某种内部错误，我们宁愿使计算机全部崩溃，而不是返回一个错误的结果，错误的结果往往更难处理**。因此，计算机隐藏了一切模糊的物理世界，呈现以—个理想化的系统模型，像以数学完美的方式运行。CPU指令通常以确定性方式操作，如果写入一些数据到内存或磁盘，那么这些数据将保持不变且不会被随机破坏。这和确定-正确性的设计目标可以一直追溯到第一台数字计算机。

​	然而当涉及多台节点时，情况发生了根本性变化。对于这种分布式系统，理想化的标淮正确模型不再适用，我们必须面对一个可能非常混乱的现实。在这样一个现实世界中，各种各样的事情都可能会出错。

​	<u>在分布式系统中，可能会出现系统的一部分工作正常，但其他某些部分出现难以预测的故障，我们称之为“**部分失效**”</u>。问题的难点就在于这种部分失效是不确定的：如果涉及多个节点和网络，几乎肯定会碰到有时网络正常，有时则莫名的失败。正如接下来马上要看到的，通过网络发送消息的延迟非常不确定，有时甚至根本不知道执行是否成功。

​	正是由于这种不确定性和部分失效大大提高了分布式系统的复杂性。

### 8.1.1 云计算和超算

​	关于如何构建大规模计算系统有以下几种不同的思路：

+ 规模的一个极端是<u>高性能计算(high-performance computing，HPC)</u>。包含成千上万个CPU的超级计算机构成一个庞大的集群，通常用于计算密集型的科学计算任务，如天气预报或分子动力学(模拟原子和分子的运动)。
+ 另一个极端是<u>云计算</u>。虽然云计算的定义并非那么明确，但通常它具有以下特征：多租户数据中心，通用计算机，用IP以太网链接，弹性/按需资源分配，并按需计费。
+ 传统企业数据中心则位于以上两个极端之间。

​	不同的集群枃建方式所对应的错误处理方法也不尽相同。<u>对于高性能计算，通常会定期对任务状态进行快照，然后保存在持久存储上，当某节点出现故障，解决方案是简单地停止整个集群的任务；等故障节点修复之后，从最近的快照检查点继续执行</u>。从这一点上看，高性能计算其实更像是一个单节点系统而不是分布式系统，它**将局部失效升级为整体失效**，例如系统的任何部分发生了故障，就干脆让系统停下来，这和单机上內核崩溃类似。

​	本书的重点是基于互联网的服务系统，这些系统与上述高性能计算有很多不同之处：

+ 许多互联网服务都是在线的，需要随时(如7×24h)为用户提供低延迟服务。任何服务不可用情况，如停下集群来修复故障，都是不可取的。相比之下，像天气模拟这样的离线(批处理)作业则可以暂停然后重启，影响相对较小。
+ 高性能计算通常采用专用硬件，每个节点的可靠性很髙，节点间主要通过共享内存或者远程内存直接访问(RDMA)等技术进行通信。而云计算中的节点多是由通用机器构建，岀于大规模部署时经济因素的考虑，单节点的成本低廉，依靠较高的集群聚合性能，但另一方面也具有较高的故障率。

+ 大型数据中心网络通常基于IP和以太网，采用Clos拓扑结构提供等分带宽。高性能计算则通常特定的网络拓扑结构，例如多维网格和toruses，它们可以为HPC特定工作负载提供更好的性能。
+ 系统越大，其中局部组件失效的概率就越大。在长时间运行期间，失效，修复，再失效可以看成一个反复的过程。在一个包含成千上万个节点的系统中，我们几乎总是可以假定某些东西发生了失效。此时，如果采用的是简单的停止-修复错误处理策略，对于这样一个庞大的集群系统，最终将花费大量时间在错误恢复上而不是正常的任务执行。
+ 如果系统可以容忍某些失败的节点，而使整体继续工作，则对系统运维帮助极大。例如，支持<u>滚动升级</u>(参阅第4章)：每次重启一个节点，而集群总体对外不中断服务。在云环境中，如果发现某台虚拟机有问题，可以将其关闭然后重新启动另一个。
+ 对于全球分散部署的多数据中心(使用户访问地理靠近的数据中心，从而降低延迟)，通信很可能经由广域网，与本地网络相比，速度更慢且更加不可靠。而高性能计算通常假设所有节点位置靠近、紧密连接。

​	要使分布式系统可靠工作，就必然面临部分失效，这就<u>需要依靠软件系统来提供容错机制</u>。换句话说，我们需要在不可靠的组件之上构建可靠的系统。另外，正如在第1章“可靠性”所讨论的，世界上不存在完美的可靠性，我们需要的现实可行的保证。

​	<u>即使对于只有几个节点的小型系统，也很有必要审视部分失效问题</u>。在一个小系统中，很可能大部分组件在大部分时间都正常工作，但迟早某一天有一部分系统会出现故障，此时软件必须可以有效处理。这里要强调的是，**故障处理是软件设计的重要组成部分**。<u>作为系统运维者，需要知道在发生敞障时，系统的预期行为是什么</u>。

​	不能假定故障不可能发生而总是期待理想情况。最好仔细考虑各种可能的出错情况包括那些小概率故障，然后尝试人为构造这种测试场景来充分检测系统行为。<u>可以说，在分布式系统中，怀疑，悲观和偏执狂才能生存</u>。

## 8.2 不可靠的网络

​	正如第二部分所介绍的，本书关注的主要是<u>分布式无共享系统</u>，即通过网络连接的多个节点。网络是跨节点通信的唯一途径，我们还假定毎台机器都有自己的内存和磁盘，一台机器不能直接访问另一台机器的内存或磁盘除非通过网络向对方发出请求。

> 基于不可靠的组件构建可靠的系统
>
> 直观来看，系统的可靠性应该取决于最不可靠的组件(即最薄弱的环节)。然而，对于计算机系统来讲，事实并非如此，在低可靠性部件上构建高可靠的系统一直是计算机领域的惯用手段山。例如：
>
> + <u>纠错码</u>可以在各种通信链路上准确传輸数据，包括那些可能偶尔传输岀错的情况，例如无线网络出现信号干扰。
> + IP(Internet协议)层本身并不可靠，可能会出现丢包、延迟、重复发送以及乱序等情况。但TCP(传输控制协议)在IP之上提供了更可靠的传输层，可以保证丟失的数据包被重传，消除重复包，包的顺序以发送的顺序重新组合等。
>
> 需要指出，系统整体虽然变得比底层组件更为可靠，但可靠性总有其现实上限。例如，纠错码只能处理某几个位错误，但如果信号被彻底干扰，就很难保证最终可以正确接受多少数据。而TCP虽然能够提供重传、重组等，但是它肯定不能神奇地消除网终传输延迟。
>
> 尽管系统并不完美，但这样的高可靠系统仍然非常有用，它可以帮我们处理底层些棘手的故障，使其他故障更容易理解和进一步处理。我们将在第12章进步探讨这个问题。

​	首先要说明，<u>无共享并不是构建集群系统的唯一方式，但它却是构建互联网服务的主流方式</u>。主要是由于以下几个原因：由于不需要专门的硬件因此成本相对低廉，可以采用通用的商品化硬件，可以采用跨区域的多数据中心来实现高可靠性。

​	<u>互联网以及大多数数据中心的内部网络(通常是以太网)都是异步网络</u>。在这种网络中，一个节点可以发送消息(数据包)到另一个节点，但是网络并不保证它什么时候到达，甚至它是否一定到达。发送之后等待响应过程中，有很多事情可能会出错(见图8-1所示的例子)：

1. 请求可能已经丢失(比如有人拔掉了网线)。
2. <u>请求可能正在某个队列中等待，无法马上发送(也许网络或接收方已经超负荷)。</u>
3. 远程接收节点可能已经失效(例如崩溃或关机)。

4. 远程接收节点可能暂时无法响应(例如正在运行长时间的垃圾回收，请参阅本章后面的“进程暂停”)。
5. <u>远程接收节点已经完成了请求处理，但回复却在网络中丢失(例如网络交换机配置错误)</u>。
6. 远程接收节点已经完成了请求处理，但回复却被延迟处理(例如网络或者发送者的机器超出负荷)。

![不可靠的网络 - 图1](https://static.sitestack.cn/projects/ddia/img/fig8-1.png)

​	发送者甚至不清楚数据包是否完成了发送，只能选择让接收者来回复响应消息，但回复也有可能丢失或延迟。这些问题在一个异步网络中无法明确区分，发送者拥有的唯一信息是，尚未收到响应，但却无法判定具体原因。

​	处理这个问题通常采用**超时机制**：等待一段时间之后，如果仍然没有收到回复则选择放弃，并且认为响应不会到达。但是，即使判定超时，仍然并不清楚远程节点是否收到了请求(一种情况，请求仍然在某个地方排队，即使发送者放弃了，但最终请求会发送到接收者)。

### 8.2.1 现实中的网络故障

​	我们已经有几十年的计算机网络构建经验，人们也许想当然地会认为现在已经掌握了足够的技能使网络变得非常可靠。但是，情况似乎并非如此。

​	一些系统研究和大量的侧面证据表明，网络问题出人意料地普遍，包括那些由公司运营的数据中心。一家中型数据中心完成的调查发现，每月大约有12次网络故障，其中有一半涉及单台机器，有一半甚至是整个机架断网。另一项研究分析了机架式交换机，汇聚层交换机和负载均衡器等组件的故障率，结果发现增加冗余网络设备并不会像期望的那样可以显著减少故障，主要是因为无法有效防范人为错误(例如配置错误)，而后者是造成网络中断的主要原因。

​	公共云服务如AWS因多次出现临时性网络故障而影响颇大，管理良好的私有数据中心网络或许更为稳定一些。尽管如此，所有人都曾遭受网络问题的困扰：例如，<u>交换机软件升级可能会触发网络拓扑重新配置，在此期间数据包的延迟显著增加，甚至超过了一分钟</u>；有报道发现鲨鱼咬破了海底电缆。其他令人惊讶的故障还包括网络接口会丢弃所有入向数据包，但可以成功发送出向数据包，原因仍然在于网络接口配置问题。

> 网络分区
>
> 当网络的一部分由于网络故障而与其他部分断开，称之为网络分区或网络分割。为避免与第6章存储系统的分区(分片)产生混淆，本书采用更通用的术语“网络故障”。

​	即使网络故障在你的环境中比较少见，但故障可能发生也要求软件需要能够处理它们。事实上，只要有网络通信，就可能会出现故障，这一点始终无法彻底避免。

​	如果没有处理或者测试网络故障，可能会发生意想不到的后果。例如，集群可能会死锁，即使网络恢复了也无法提供服务，甚至可能误删除数据。如果触发了一些软件未定义的情形，则发生任何意外都不奇怪。

​	处理网络故障并不意味着总是需要复杂的容错措施：假定你的网络通常非常可靠，而万一出现问题，一种简单的方法是对用户提示错误信息。但前提是，必须非常清楚接下来软件会如何应对，以确保系统最终可以恢复。我们推荐有计划地人为触发网络问题，目的是测试系统的反应情况(这也是 Chaos Monkey系统背后的想法，请参阅第1章“可靠性”)。

### 8.2.2 检测故障

​	许多系统都需要自动检测节点失效这样的功能。例如：

+ <u>负载均衡器需要避免向已失效的节点继续分发请求</u>(即将其下线处理)。
+ 对于主从复制的分布式数据库，如果主节点失败，需要将某个从节点提升为主节点(参阅第5章“处理节点失效”)。不过，由于网络的不确定性很难准确判断节点是否确实失效。

+ 然而不幸的是，由于网络的不确定性使得判断节点是否失效非常困难；而只有在某些特定场景下，或许你可以明确知道哪里出错了。
+ 假设可以登录节点，但发现服务进程没有侦听目标端口(例如，由于进程已经崩溃)，那么操作系统会返回RST或FIN标志的数据包来辅助关闭或拒绝TCP连接。但是，如果节点在处理请求的过程中发生了崩溃，则很难知道该节点实际处理了多少数据。
+ <u>如果服务进程崩溃(或被管理员杀死)，但操作系统仍正常运行，可以通过脚本通知其他节点，以便新节点来快速接管而跳过等待超时。 Hbase釆用了这样的方法</u>。
+ 如果有权访问数据中心网络交换机，则可以通过管理接口查询是否存在硬件级别的链路故障(例如远程节点掉电)。不过，该方法也有局限性，例如通过互联网连接，或者是处于共享数据中心而没有访问交换机的权限，以及由于网络问题而根本无法登录管理界面等。
+ <u>如果路由器已经确认目标节点不可访问，则会返回ICMP“目标不可达”数据包来回复请求。但是，路由器本身并不具有什么神奇的检测能力，从这一点来讲，它和网络上其他节点并无本质区别</u>。

​	能快速告知远程节点的关闭状态自然有用，但也不是万能的。例如，即使TCP确认一个数据包已经发送到目标节点，但应用程序也可能在处理完成之前发生崩溃。如果你想知道一个请求是否执行成功，就需要应用级别的回复。

​	<u>总之，如果出现了问题，你可能会在应用堆栈的某个级别拿到了一个关于错误的回复，但最好假定最终收不到任何错误报告。接下来尝试重试(TCP重试是透明的，但也可以在应用级别重试)，等待超时之后，如果还是没有收到响应，则最终声眀节点已经失效</u>。

> [FIN与TCP连接中的RST - 问答 - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/ask/128721)
>
> [向收到RST和FIN的套接字写入数据会怎样？ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/56824467)

### 8.2.3 超时与无限期的延迟

​	如果超时是故障检测唯一可行的方法，那么超时应该设多长呢?不幸的是没有标准的答案。

​	设置较长的超时值意味着更长时间的等待，才能宣告节点失效(在此期间，用户只能等待或者拿到错误信息)。较短的超时设置可以帮助快速检测故障，但可能会出现误判，例如实际上节点只是出现暂时的性能波动(由于节点或网络上的高负载峰值)结果却被错误地宣布为失效。

​	如果节点实际上是活着(例如，正在发送一封电子邮件)，过早地将其声明为失效会带来一些问题，新节点会尝试接管，然后出现一些操作在两个节点上执行两次。我们将在后面详细讨论这个问题。

​	<u>当一个节点被宣告为失效，其承担的职责要交给到其他节点，这个过程会给其他节点以及网络带来额外负担，特别是如果此时系统已经处于高负荷状态</u>。**例如节点只是负载过高而出现了响应缓慢，转移负载到其他节点可能会导致失效扩散，产生级联扩大效应，在极端情况下，所有节点都宣告对方死亡，造成服务处于事实停止状态**。

​	<u>设想一个虚拟的系统，其网络可以保证数据包的最大延迟在一定范围内：要么在时间d内完成交付，要么丢失。此外，假定一个非故障节点总能够在一段时间r内完成请求处理。此时，可以确定成功的请求总能够在2d+r时间内收到响应，如果在此时间内没有收到响应，则可以推断网络或者远程节点发生了失效，那么2d+r是一个理想的超时设置</u>。

​	然而事实上绝大多数系统都没有类似的保证：**异步网络理论上的延迟无限大**(即使尽力发送数据包，但数据包到达时间并没有上确界)，<u>多数服务端也无法保证在给定的某个时间内一定完成请求处理(参阅本章后面的“响应时间保证”)。如果超时设置太小，只需要一个短暂的网络延迟尖峰就会导致包超时进而将系统标记为失效</u>。

#### 网络拥塞与排队

​	驾车时往往由于交通堵塞，导致行车时间变化很大。同样，**计算机网络上数据包延的变化根源往往在于排队**：

+ <u>当多个不同节点同时发送数据包到相同的目标节点时，网络交换机会出现排队，然后依次将数据包转发到目标网络(见图8-2)。如果网络负载过重，数据包可能必须等待一段时间才能获得发送机会(即网络拥塞)。如果数据量太大，交换机队列塞满，之后的数据包则会被丢弃，网络还在运转，但会引发大量数据包重传</u>。
+ <u>当数据包到达目标机器后，如果所有CPU核都处于繁忙状态，则网络数据包请求会被操作系统排队，直到应用程序能够处理</u>。根据机器的配置和负载情况，这里也会引入一段不确定的等待时间。
+ 在虚拟化环境下，CPU核会切换虚拟机，从而导致正在运行的操作系统会突然暂停几十毫秒。在这段时间，客户虚机无法从网络中接收任何数据，入向的包会被虚拟机管理器排队缓冲，进一步增加了网络延迟的不确定性。
+ <u>TCP执行流量控制(也称为拥塞消除，或背压)时，节点会主动限制自己的发送速率以避免加重网络链路或接收节点负载。这意味着数据甚至在进入网络之前，已经在发送方开始了排队</u>。

![不可靠的网络 - 图2](https://static.sitestack.cn/projects/ddia/img/fig8-2.png)

​	而且，<u>对于TCP如果在某个超时范围内(根据往返时间来推算)没有收到确认，则认为数据包已经丢失进而触发自动重传。虽然这一过程对应用程序透明，但肯定会引入额外的延迟(等待超时到期，等待重传的数据包得到确认)</u>。

> TCP与UDP
>
> **一些对延迟敏感的应用程序(如视频会议和IP语音VoIP)使用了UDP而不是TCP**。<u>这是一个可靠性与可变性之间的权衡考虑。UDP不支持流量控制也不会重传丟失的数据包，因此可以避免一些网络延迟不确定的因素(但它仍受队列和调度延迟等方面的影响)</u>。
>
> **如果延迟或丟弃的数据价值不大，UDP是个不错的选择**。例如，IP电话情况可能没有足够的时间重传丢失的数据包，或者重传并没有太大的意义，应用程序会采用静音填充丢失的位置(出现短暂声音中断)，然后数据流必须尽快向前继续，由人为即通话双方来沟通重试。类似这样，“请你再说一遍好吗？刚刚声音没有听到。“

​	所有以上因素都会造成网络延迟的变化或者不确定性。当系统还有足够的处理能力，排队之后可以快速处理；但<u>当系统接近其最大设计上限时，系统负载过高，队列深度就会显著加大，排队对延迟的影响变得特别明显</u>。

​	在公有云和多租户数据中心中，许多客户共享网络资源包括交换机、机器的网卡以及CPU(如虚拟机)等。批处理工作负载例如 MapReduce(参阅第10章)很容易使网络带宽达到饱和。通常用户无法控制或者没有权限观测其他用户对共享资源的使用情况，如果不巧旁边有个疯狂的邻居(如虚拟机)正在大量使用资源，网络延迟就会波动很大。

​	在这种环境下，只能通过实验方式一步步设置超时。先在多台机器上，多次测量网络往返时间，以确定延迟的大概范围；然后结合应用特点，在故障检测与过早超时凤险之间选择一个合适的中间值。

​	<u>更好的做法是，超时设置并不是一个不变的常量，而是持续测量响应时间及其变化(抖动)，然后根据最新的响应时间分布来自动调整。可以用 Phi Accrual故障检测器完成，该检测器目前已在Akka和 Cassandra中使用。TCP的重传超时也采用了类似的机制</u>。

### * 8.2.4 同步与异步网络

​	如果网络层可以在规定的延迟内保证完成数据包的发送，且不会丢弃数据包，那么分布式系统就会简单很多。为什么我们不能考虑在硬件层解决这个问题呢？使网络足够可靠，然后软件就无需如此担心。

​	为了回答这个问题，可以将数据中心网络与传统的固定电话网络(非移动蜂窝，非VoIP)进行对比分析，首先后者非常可靠：语音延迟和掉话现象极为罕见。这样的固定电话网络需要有持续端到端的低延迟和足够的带览来传输音频数据。计算机网络能否实现类似的高可靠性和确定性呢?

​	<u>当通过电话网络拨打电话时，系统会动态建立一条电路：在整个线路上为呼叫分配个固定的、带宽有保证通信链路，该电路一直维持到通话结束</u>。例如，ISDN网络固定以每秒4000帧的速率运行。当新的呼叫建立后，在每个帧(每个方向)内分配16bit的空间，在通话期间，每一方都可以保证在250us内完成发送一条16bit的音频数据。

​	<u>这种网络本质是同步的：即使数据中间经过了多个路由器，16bit空间在电路建立时已经在网络中得到预留，不会受到排队的影响。由于没有排队，网络最大的端到端延迟是固定的。我们称之为**有界延迟**</u>。

#### * 网络延迟是否可预测？

​	请注意，固定电话网络中的电路与TCP连接存在很大不同：

+ <u>电路方式总是预留固定带宽，在电路建立之后其他人无法使用</u>；
+ <u>而TCP连接的数据包则会尝试使用所有可用的网络带宽</u>。TCP可以传送任意大小可变的数据块(例如电子邮件或网页)，它会尽力在最短的时间内完成数据发送。而当TCP连接空闲时，通常不占用任何带宽。

​	<u>假设数据中心网络和互联网基于**电路交换网络**，一旦电路建立完成则可以保证最大往返时间。然而，事实是以太网和IP都是基于**分组交换协议**，这种协议注定受到排队的影响，从而导致网络延迟不确定，在这些协议里完全没有电路的概念</u>。

​	那为什么数据中心网络和互联网采用分组交换呢？答案是，它们针对突发流量进行了很多优化。<u>电路非常适合音频或视频通话，通话期间只需毎秒传送固定数量的数据。但对于访问网页，发送电子邮件或传输文件等无法事先确定带宽需求，我们只是希望它尽快完成</u>。

​	如果你想通过电路链接来传输文件，将不得不预估一个待分配的带宽。如果预估值太低，传输速度就特别缓慢，甚至无法实际可用；如果预估带宽太高，电路其至无法完成建立(因为如果无法预留所需的带宽，电路就无法建立)。所以，<u>对于突发数据的传输，电路网络无法充分利用网络容量，导致发送缓慢。相比之下，TCP动态调整传输速率则可以充分利用所有可用的网络容量</u>。

​	曾经也有一些尝试建立支持电路交换和分组交换的混合网络，比如ATM。InfiniBand网络有一些相似之处：它在链路层实现端到端的流量控制，从而减少了网络中的排队，但仍可能会由于链路拥塞而影响延迟；然后通过服务质量(QoS，数据包的优先级和调度)和准入控制(限制发送速率)，最终可以在分组网络上模拟电路交换，或者说提供统计意义上的有限延迟。

​	但是，目前此类QoS在多租户数据中心、公有云和广域网中并未启用。<u>总之，当前广泛部署的技术无法为我们提供延迟或可靠性方面的硬件级保证，我们必须假设会出现网络拥塞，排队和无上限的延迟。基于此，超时设置并不存在某个绝对正确的值，而是需要通过实验的方式来确定</u>。

> **延迟与资源利用率**
>
> 从更广泛的意义讲，可以将延迟的波动归为**动态资源分区**的结果。
>
> <u>假设两台电话交换机之间有一根线路可冋时支撑10000个呼叫。线路上的每条电路都占用其中一个槽位，因此，可以将线路视为最多允许10000个并发用户所共享的资源。只是资源是以静态方式分配的：即使现在你是线路上唯一的通话者，且剩余的其他9999个槽位都是空闲状态，你的电路也只能使用固定数量的带宽。</u>
>
> <u>相反，互朕网则是动态分享网络带宽。多个发送方互相竞争，以尽快地通过网络发送他们的数据，网络交換机也是动态决定该发送哪个数据包。这种方法存在排队的缺点，但优点是最大限度地利用了带宽。线路本身对应一个固定的成本，如果可以更充分的使用，那么发送每个字节的平均摊薄成本自然就会下降</u>。
>
> CPU也是类似的情况：如果多个线程动态共享CPU核，当有线程正在CPU上运行时，其他线程必须在操作系统的运行队列上等待，这样会出现长短不一的执行暂停。但是，相比于为每个线程静态分配固定的CPU执行周期(参见本章后面的“响应时间保证”)，它可以更好地利用硬件，这是虛拟化技术非常重要的动机。
>
> 如果资源总是静态分配(例如，专用硬件和预留带宽分配)，则某些环境下可以保证延迟的确定性。但是，这是以降低资源使用率为代价的，换句话说，其成本过于昂贵。而多租户、动态資源分配方式则可以提供更高的资源使用率，因而成本更低，当然也引入了可变延迟的缺点。
>
> **简言之，网络中的可变延迟并不是一种自然规律，只是成本与收益相互博弈的结果**。

> [计网基础-数据交换之电路交换 - hoanfir - 博客园 (cnblogs.com)](https://www.cnblogs.com/hoanfir/p/9196711.html)
>
> [电路交换_百度百科 (baidu.com)](https://baike.baidu.com/item/电路交换/2472932?fr=aladdin)

## 8.3 不可靠的时钟

时钟和计时非常重要。有许多应用程序以各种方式依赖于时钟，例如：

1. 某个请求是否超时了？

2. 某项服务的99%的响应时间是多少？
3. 在过去的五分钟内，服务平均每秒处理多少个查询？
4. 用户在我们的网站上浏览花了多长时间？
5. 这篇文章什么时候发表？

6. 在什么时间发送提醒邮件？
7. 这个缓存条目何时过期？
8. 日志文件中错误消息的时间戳是多少？

​	上述1~4测量的持续时间(例如请求发送与响应接收之间的时间间隔)，而5~8所描述的某个时间点(在特定日期，特定时间发生的事件)。

​	在分布式系统中，时间总是件棘手的问题，由于跨节点通信不可能即时完成，消息经由网络从一台机器到另一台机器总是需要花费时间。收到消息的时间应该晚于发送的时间，但是由于网络的不确定延迟，精确测量面临着很多挑战。这些情况使得多节点通信时很难确定事情发生的先后顺序。

​	而且，网络上的毎台机器都有自己的时钟硬件设备，通常是石英晶体振荡器。这些设备并非绝对准确，即毎台机器都维护自己本地的时间版本，可能比其他机器稍快或更慢。可以在一定程度上同步机器之间的时钟，最常用的方法是**网络时间协议(Network Time Protocol，NTP)**，它可以根据一组专门的时间服务器来调整本地时间，时间服务器则从精确更高的时间源(如GPS接收机)获取高精度时间。

### * 8.3.1 单调时钟与墙上时钟

​	现代计算机内部至少有两种不同的时钟：一个是墙上时钟(或称钟表时间)，一个是单调时钟。虽然它们都可以衡量时间，但要仔细区分二者，本质上他们是服务于不同的目的。

#### 墙上时钟

​	<u>墙上时钟根据某个日历(也称为墙上时间)返回当前的日期与时间</u>。例如， Linux的`clock_gettime(CLOCK_REALTIME)`和Java中的`System.currentTimeMill()` 会返回自纪元1970年1月1日(UTC)以来的秒数和亳秒数，不含闰秒。而有些系统则使用其他日期作为参考点。

​	**墙上时钟可以与NTP同步**。但是，如下一节所述，这里还存在一些奇怪问题。<u>特别是，如果本地时钟远远快于NTP服务器，强行重置之后会跳回到先前的某个时间点。这种跳跃以及经常忽略闰秒，导致其不太适合测量时间间隔</u>。

​	曾经墙上时钟粗糙的精度也被人诟病，例如在较早的Windows系统上只能提供10毫秒的精度。不过在较新的系统上，精度已经不成问题。

#### 单调时钟

​	<u>单调时钟更适合测量持续时间段(时间间隔)，例如超时或服务的响应时间</u>：Linux上的`clock_ gettime(CLOCK_MONOT0NC)`和ava中的`System.nanoTime()`返回的即是单调时钟。**单调时钟的名字来源于它们保证总是向前(而不会出现墙上时钟的回拨现象)**。

​	可以在一个时间点读取单调时钟的值，完成某项工作，然后再次检查时钟。时钟值之间的差值即两次检查之间的时间间隔。注意，单调时钟的绝对值并没有任何意义，它可能电脑启动以后经历的纳秒数或者其他含义。因此比较不同节点上的单调时钟值毫无意义，它们没有任何相同的基准。

​	<u>如果服务器有多路CPU，则每个CPU可能有单独的计时器，且不与其他CPU进行同步</u>。<u>由于应用程序的线程可能会调度到不同的CPU上，此时，操作系统会补偿多个计时器之间的偏差，从而为应用层提供统一的单调递增计时。不过最好还是对这种偏差补偿持谨慎态度</u>。

​	<u>如果NTP检测到本地石英比时间服务器上更快或者更慢，NTP会调整本地石英的震动频率(这被称为摆动)。默认情况下，NTP允许速率加快或减慢的最大幅度为0.05%，但NTP并不会直接调整单调时钟向前或者回拨。单调时钟的精度通常很高，在如今大多数系统中，可以测量几微秒或更短的时间间隔</u>。

​	<u>在分布式系统中，可以采用单调时钟测量一段任务的持续时间(例如超时)，它不假定节点间有任何的时钟同步，且可以容忍轻微测量误差</u>。

### 8.3.2 时钟同步与准确性

​	单调时钟不需要同步，但是墙上时钟需要根据NTP服务器或其他外部时间源做必要的调整。然而，我们获取时钟的方法并非预想那样可靠或准确，硬件时钟和NTP可能会出现一些莫名其妙的现象。举几个例子：

+ 计算机中的石英钟不够精确，存在漂移现象(运行速度会加快或减慢)。时钟漂移主要取决于机器的温度。谷歌假设其服务器的时钟偏移为200ppm(百万分之一)，相当于如果毎30秒与服务器重新同步一次，则可能出现的最大偏差为6亳秒，或者每天一次同步，则最大偏差为17秒。即使其他一切工作正常，漂移问题也限制了可以达到的最佳精度。
+ <u>如果时钟与NTP服务器的时钟差别太大，可能会出现拒绝同步，或者本地时钟将被强制重置。在重置前后应用程序观察可能会看到时间突然倒退或突然跳跃的现象</u>。
+ 由于某些原因，如果与NTP服务器链接失败(如防火墙)，可能会很长一段时间没有留意到错误配置最终导致同步失败。有证据表明，实践中确实发生这类问题。
+ NTP同步会受限于当时的网络环境，特别是延迟。如果网络拥塞、数据包延迟变化不定，则NTP同步的谁确性会受影响。实验表明，当通过互联网进行同步时，可能会产生至少35毫秒的偏差，最坏时(如网络抖动出现包发送失败)则可能超过1秒。取决于具体的参数配置情况，如果网络延迟特别严重，则NTP客户端可能会被迫放弃同步。
+ 一些NTP服务器本身出现故障、或者配置错误，其报告的时间可能存在数小时的偏差。NTP客户端往往比较稳定，可以同时査询多个服务器并忽略掉异常值。尽管如此，相信互联网上一个陌生的服务器告诉你的时间，还是值得警惕。
+ 闰秒会产生一分钟为59秒或61秒的现象，这会在使一些对闰秒毫无防范的系统出现混乱。闰秒曾经使许多大型系统崩潢，过去的教训表明，不经意之间许多系统已经埋下了对时钟不正确处理的隐患。而<u>处理闰秒的推荐方式是，不管NTP服务器具体如何实现，在NTP服务器汇报时间时故意做些调整，目的是在一天的周期内逐步调整闰秒</u>(称为<u>拖尾</u>)。
+ <u>在虚拟机中，由于硬件时钟也是被虚拟化的，这对于需要精确计时的应用程序提出了额外的挑战。当虚拟机共享一个CPU核时，每个虚拟机会出现数十毫秒内的暂停以便切换客户虛机。但从应用的角度来看，这种停顿会表现为时钟突然向前发生了跳跃</u>。

+ <u>如果运行在未完全可控的设备上(例如，移动设备或嵌入式设备)，需要留意不能完全相信设备上的硬件时钟</u>。某些用户会故意将其硬件时钟设置为错误的日期和时间，例如为了规避游戏的时间限制。因此，获取的时钟可能是某个过去的或者将来的时间。

​	如果确实需要投入大量资源，是可以达到非常高的时钟精度。例如，针对金融机构的欧洲法规草案 MiFID II就明确要求所有高频交易基金必须在UTC时间100微秒内同步时钟，以便调试“崩盘”等市场异常并检测市场操纵等违规行为。

​	高精度的时钟可以釆用GPS接收机，精确时间协议(PTP)并辅以细致的部署和监测。但通常也意味着大量的资源投入和技术门槛，并持续监控时钟同步可能出现的错误情况。例如NTP守护程序配置错误，防火墙阻止NTP通信等，避免导致时钟误差迅速变大。

### 8.3.3 依赖同步的时钟

​	时钟虽然看起来简单，但却有不少使用上的陷阱：<u>一天可能不总是86400秒，时钟会向后回拨，一个节点上的时间可能与另一个节点上的时间完全不同</u>。

​	本章前面讨论了网络丢包和数据包延迟。对此，我们的建议是，即使网络在大多数情况下表现良好，软件也必须假设网络偶尔会出现故障，因此要有对应的措施来妥善处理这些故障。时钟也是如此：尽管大多数时间工作很好，但仍需以备不测。

​	假如一台机器的CPU出现了故障或者网络有问题，系统可能根本无法工作，所以很快就会被注意到进而得到修复，但时钟问题却不那么容易被及时发现。如果石英时钟有缺陷，或者NTP客户端配置错误，最后出现了时间偏差，对大多数功能可能并没太大影响。但对于一些高度依赖于精确时钟的软件，出现的后果可能是隐式的，或许会丟失一小部分数据而不是突然的崩溃。

​	因此，<u>如果应用需要精确同步的时钟，最好仔细监控所有节点上的时钟偏差</u>。<u>如果某个节点的时钟漂移超出上限，应将其宣告为失效，并从集群中移除。这样的监控的目的是确保在造成重大影响之前尽早发现并处理问题</u>。

#### * 时间戳与事件顺序

​	对于一个常见的功能：跨节点的事件排序，如果它高度依赖时钟计时，就存在一定的技术风险。例如，两个客户端同时写入分布式数据库，谁先到达？哪一个操作是最新的呢？

​	图8-3给出了这样的危险例子，即多主节点复制的分布式数据库高度依赖于墙上时钟(图5-9是另一个类似的例子)。客户端A在节点1上写入x=1，写入被复制到节点3；客户端B在节点3上增加x(现在x=2)。最后，这两个写入都被复制到节点2上。

​	在图8-3中，写入被复制到其他节点时，会根据源写入节点上的墙上时钟来标记时间戳。在该例子中，时钟同步机制稳定工作，节点1和节点3之间的时钟偏差小于3ms，这比实践中的多数情况可能都要更好。

![不可靠的时钟 - 图1](https://static.sitestack.cn/projects/ddia/img/fig8-3.png)

​	但是，这样的时间戳却不能正确排序事件：写入x=1的时间戳为42.004秒，写入x=2虽然后续发生，时间戳却是42.003s。当节点2牧到这两个事件时，会根据时间戳错误地判断x=1是最新值，然后决定丢弃x=2，就给就导致客户端B的增量操作丟失。

​	这种冲突解决策略被称为**最后写入获胜(LWW)**，在多主节点以及无主节点复制数据库(如 Cassandra和Riak )中广泛使用(参阅第5章“最后写入获胜”)。有些实现会在客户端生成时间戳而非服务器端，但无论如何没有改变LWW的根本问题：

+ <u>数据库写入可能会奇怪地丢失：明明后续发生的写操作却没法覆盖另一个较早的值，原因是后者节点的时钟太快了。这会导致一些数量未知的数据被悄悄地丢弃，并且不会向应用报告任何错误</u>。
+ **LWW无法区分连续快速发生的连续写操作(图8-3中客户端A写入之后才发生了客户端B的增量操作)和并发写入(毎个写操作都不依赖于其他写)**。<u>需要额外的**因果关系跟踪**机制(例如版本向量)来防止因果冲突(参阅第5章“检测并发写”)</u>。
+ <u>由于时钟精度的限制(例如毫秒级)，两个节点可能各自独立产生了完全相同的时间戳。为了解决这样的冲突，需要一个额外的仲裁值(可以简单地引入一个大的随机数)，但该方法还是无法区分因果关系</u>。

​	因此，通过保持“最新”值并丢弃其他值来解决冲突看似不锴，但要注意，<u>“最新”的定义如果取决于墙上时钟就会引入偏差。即使采用了NTP同步时钟，依然可能会出现在时间戳100毫秒时(根据发送者的时钟)发送了某个数据包，却在时间戳99毫秒(根据接收者的时钟)到达，这看起来好像是数据包还没发送就先到达了</u>。

​	那么NTP时钟同步能否做到极高的精度从而避免这种错误的顺序问题呢？很难做到。因为除了石英漂移等误差来源之外，NTP同步精度本身要受限于所在的网络延迟。要达到正确的排序，需要时钟源精度要远远高于被测量的对象(即网络延迟)。

​	<u>对于排序来讲，基于递增计数器而不是振荡石英晶体的**逻辑时钟**是更可靠的方式(参见第5章“检测并发写”)。逻辑时钟并不测量一天的某个时间点或时间间隔，而是事件的相对顺序(事件发生的相对前后关系)。与之对应的，墙上时钟和单调时钟都属于物理时钟。我们会在第9章“顺序保证”中继续讨论顺序问题</u>。

#### 时钟的置信区间

​	或许墙上时钟会返回微秒甚至纳秒级别的信息，但是这种精度的测量值其实并不可信。如前所述，石英漂移问题可导致偏差高达几亳秒，即使每分钟都与本地网络的NTP服务器进行同步，也无法保证上述精度。如果使用公共互联网上的NTP服务器，最好的精度也只能到几十毫秒，而一旦出现网络拥塞，偏差很容易就超过100毫秒。

​	因此，我们不应该将时钟读数视为一个精确的时间点，而更应该视为带有置信区间的时间范围。例如，系统可能有95%的置信度认为目前时间介于10.3-10.5秒之间。如果我们可完全相信的精度为+/-100毫秒，那么时间戳中那些微秒级的读数并无实际总义。

​	可以根据具体的时间源来推算出时钟误差的上限。如果节点上直接装有GPS接收器或原子(铯)时钟，那它的误差范围通常可查询制造商的手册。如果节点是从服务器获取时间，则不确定性取决于上次服务器同步以来的石英漂移范围，加上NTP服务器的不确定性，再加上与服务器之间的网络往返时间(对于第一次同步，我们假定服务器完全可信)。

​	可惜大多数系统并不提供这种误差査询接口。例如，当调用`cock_ gettime()`时，返回值没有任何误差信息，所以无法确切知道置信区间应该是五毫秒还是五年。

​	这里有趣的是Google Spanner中的TrueTime API，它会明确地报告本地时钟的置信区间。当查询当前时间时，你会得到两个值：[不早于，不晚于]分别代表误差的最大偏差范围。基于上述两个时间戳，可以知道实际时间应在其范围之内。该间隔的范围主要取决于本地石英钟最后与高精时钟源同步后所经历的时间长短。

#### * 全局快照的同步时钟

​	在第7章“快照隔离与可重复读”中，我们介绍了快照隔离，它广泛用于小数据量、快速读写的事务以及大数据量，长时间运行的只读事务(例如备份或分析)，可以在数据库的某个一致状态上不需加锁、不违背读写祸离性的前提下高效支持只读事务。

​	<u>常见的快照隔离实现中需要**单调递增事务ID**。如果写入发生在快照之后(以写入具有比快照史大的事务ID)，那么该写入对于快照不可见。在单节点数据库上，一个简单的计数器足以生成事务ID</u>。

​	但是，当数据库分布在多台机器上(可能跨越多个数据中心)时，由于需要复杂的协调以产生全局的、单调递增的事务ID(跨所有分区)。事务ID要求必须反映因果关系：事务B如果要读取事务A写入的值，则B的事务ID必须大于A的事务ID，否则快照将不一致。<u>考虑到大量、频繁的小数据包，在分布式系统中创建事务ID通常会引入瓶颈</u>。

​	<u>能否使用同步后的墙上时钟作为事务ⅠD呢?如果我们能够获得足够可靠的同步时钟，自然它可以符合事务ID属性要求：后发生的事务具有更大的时间戳。然而问题还是时钟凊度的不确定性</u>。

​	<u>Google Spanner采用以下思路来实现跨数据中心的快照隔离。它根据 TrueTime API返回的时钟置信区间，并基于以下观察结果：如果有两个置信区间，每个置信区间都包含最早和最新可能的时间戳(A=[A<sub>earliest</sub>，A<sub>lastest</sub>]和B=[B<sub>earliest</sub>，B<sub>lastest</sub>])，且这两个区间没有重叠(即A<sub>earliest</sub> < A<sub>lastest</sub> < B<sub>earliest</sub> < B<sub>lastest</sub>)，那么可以断定B一定发生在A之后。只有发生了重叠，A和B发生顺序才无法明确</u>。

​	<u>为了确保事务时间戳反映因果关系， Spanner在提交读写事务之前故意等待置信区间的长度</u>。<u>这样做的目的是，确保所有读事务要足够晚才发生，避免与先前的事务的置信区间产生重叠</u>。而为了尽量缩短潜在的等待时间， Spanner需要使时钟的误差范围尽可能的小，为此， Google在每个数据中心都部署了一个GPS接收器或原钟，保证所有时钟同步在约7ms之内完成。

​	**借助时钟同步来处理分布式事务语义是一个非常有趣和活跃的研究领域。但除了 Google以外，目前主流数据库中还没有更多的实现**。

### 8.3.4 进程暂停

​	另一个分布式系统中危险使用时钟的例子：假设数据库每个分区只有一个主节点，只有主节点可以接受写入。那么其他节点该如何确信该主节点没有被宣告失效，可以安全地写入呢？

​	一种思路是主节点从其他节点获得一个租约，类似一个带有超时的锁。某一个时间只有一个节点可以拿到租约，某节点获得租约之后，在租约到期之前，它就是这段时间内的主节点。为了维持主节点的身份，节点必须在到期之前定期去更新租约。如果节点发生了故障，则续约失败，这样另一个节点到期之后就可以接管。

​	典型处理流程如下所示:

```java
while(true){
    request=getIncomingRequest();
    // Ensure that the lease always has at least 10 seconds remaining
    if (lease.expiryTimeMillis-System.currentTimeMillis() < 10000){
        lease = lease.renew();
    }
    if(lease.isValid()){
        process(request);
    }}
}
```

​	这代码有什么问题么？首先，它依赖于同步的时钟：租约到期时间由另一台机器所设置(例如，另一台机器的当前时间加上30秒得到到期时间)，并和本地时钟进行比较。如果时钟之间有超过几秒的差异，这段代码会出现些奇怪的事情。

​	其次，即使我们改为仅使用本地单调时钟，还有另一个问题：代码假定时间检查点`System.currentTimeMills()`与请求处理`process(request)`间隔很短，通常代码运行足够快，所以设置10秒的缓冲区来确保在请求处理过程中租约不会过期。

​	但是，如果程序执行中出现了某些意外的暂停呢？例如，假设线程在`lease.isValid()`消耗了整15秒。那么当开始处理请求时，租约已经过期，另一个节点已经接管了主节点。可惜我们无法有效通知线程暂停了这么长时间了，后续代码也不会注意到租约已经到期，除非运行到下一个循环迭代。不过，到那个时候它已经做了一些不安全的请求处理。

​	那么，一个线程可能会暂停这么长时间么？这是可能的，发生这种情况的原因有很多种：

+ <u>许多编程语言(如Java虚拟机)都有垃圾收集器(GC)，有时运行期间会暂停所有正在运行的线程</u>。这些GC暂停甚至有时会持续数分钟！即使像 HotSpot JVM CMS所谓的“并发”垃圾收集器也不能完全与应用代码并行运行，需要时不时地停止活动的线程。通过改变分配模式或调整GC某些参数可以减少一些暂停，但是我们还是要防范最差情况以提供可靠的保证。
+ 在虚拟化环境中，可能会暂停虚拟机(暂停所有执行进程并将内存状态保存到磁盘)然后继续(从内存中加载数据然后继续执行)。暂停可能发生进程运行的任一时刻，并且可能持续很长的时间。该功能通常用于实时迁移，即把虚拟机从个主机迁移到另一个主机而不需要重启，这种情况下，暂停的长度主要取决于进程写入内存的速率。
+ 运行在终端用户设备(如笔记本电脑)时，执行也可能发生暂停，例如用户关闭了笔记本电脑或休眠。
+ 当操作系统执行线程上下文切换时，或者虚拟机管理程序切换到另一个虚机时，正在运行的线程可能会在代码的任意位置被暂停。在虚拟机环境中，这种被其他虛拟机中断的CPU时间称为窃取时间。如果机器负载很高(即等待运行的线程很长)，被暂停的线程可能需要一段时间之后才能再次运行。
+ <u>如果应用程序执行同步磁盘操作，则线程可能暂停并等待磁盘I/O完成</u>。<u>在许多语言中，即使代码并没有明确执行文件操作，也可能意外引入磁盘I/O</u>。例如，Java类加载器在第一次使用类文件时会推迟加载，最终可能发生在执行时的任何时刻。I/O暂停和GC暂停甚至可能会同时发生，从而进一步恶化情况。如果磁盘其实是个网络文件系绕或网络块设备(如亚马逊的EBS)，I/O还要受到网络延迟变化的影响。

+ <u>如果操作系统配置了基于磁盘的内存交换分区，内存访问可能触发缺页中断，进而需要从磁盘中加载內存页</u>。I/O进行时(通常比较慢)线程为暂停。如果内存使用压力很大，还可能迫使更多的页面换出到磁盘。极端的情况下，操作系统可能会花费大量时间在页面换入换出上，而实际工作完成很少(所谓的颠簸)。**为了避免此类问题，通常在服务器上禁用分页(宁愿杀死一些进程来释放内存而不是反复抖动)**。
+ 通过发送 SIGSTOP信号来暂停UNIX进程，例如在shell中按下Ctrl-Z。这个信号会立即停止进程避免其拿到更多的CPU周期，直到接下来收到信号SIGCONT之后才从停止的地方继续运行。另外也不排除SIGSTOP信号是由运维人员不小心意外发送。

​	所有上述情况都可能随时抢占一个正在运行的线程，然后在之后的某个时间点再恢复线程的执行，而线程自身却对此一无所知。这个问题类似于在一台机器上运行多线程代码且保证线程安全。<u>总之，你不能假定任何有关时间的事情，记住上下文切换和并行性可能随时可以发生</u>。

​	在单台机器上编写多线程代码时，有不少工具可以帮助实现线程安全：互斥量，信号量，原子计数器，无锁数据结构，阻塞队列等。不幸的是，这些工具无法直接转为分布式系统，因为分布式系统通常不采用共享内存，而是在不可靠的网络上发送消息。

​	分布式系统中的一个节点必须假定，执行过程中的任何时刻都可能被暂停相当长一段时间，包括运行在某个函数中间。暂停期间，整个集群的其他部分都在照常运行，甚至会一致将暂停的节点宣告为故障节点。最终，暂停的节点可能会回来继续运行，除非再次检查时钟，否则它对刚刚过去的暂停毫无意识。

#### 响应时间保证

​	如上所述，在许多编程语言和操作系统中，线程和进程可能会暂停相当长的时间。如果仔细调教系统，可以做到避免很多这种暂停。

​	某些软件如果在指定时间內无法响应则会导致严重后果，这些运行环境包括：飞机火箭，机器人，汽车和其他需要对输入传感器快速做岀响应的组件等。对于这些系统，软件有一个必须做出响应的上限：如果无法满足，会导致系统级故障，这就是所谓的**硬实时系统**。

> 实时真的是实时吗？
>
> 在嵌入式系统中，实时通常意味着系统经过了精心设计和测试，以满足各种情况下执行时间约束。这与Web上模糊的“实时”术语的有着鲜明的对比，后者主要描述了一种持续的流式处理方式，但并没有强的时间约束。

​	例如，如果车载传感器检测到当前正在经历碰撞，肯定不希望系统由于不合适宜的GC暂停导致无法及时释放安全气囊。

​	提供实时保证需要来自软件栈的多个层面的支持：首先是一个**实时操作系统(real-time operating system，RTOS)**，保证进程在给定的间隔内完成CPU时间片的调度分配；其次，库函数也必须考虑最坏的执行时间；然后，动态内存分配很可能要受限或者完全被禁止(如果存在实时垃圾收集器，确保GC不能处理太多任务)；最终还是需要大量、充分的测试和验证，以确保满足要求。

​	显然，这需要大量额外的工作，也严重限制了可使用的编程语言、库和工具的范围(因为大多数语言和工具库不提供实时保证)。由于这些原因，开发实时系统代价昂贵，通常只用于对安全至关重要的关键性嵌入式设备中。另外，“实时”与“高性能”不一样。实际上，<u>实时系统往往吞吐量较低，它须优先考虑并响应高优先级的请求</u>(参阅本章前面的“延迟与资源利用率”)。

​	对于大多数服务器端数据处理系统来说，实时性保证并不经济或者不合适。因此，现在这些运行在非实时环境下的系统就得承受如进程暂停、时钟不稳定等困扰。

#### * 调整垃圾回收的影响

​	无需昂贵的实时调度，还有一些其他措施可以减轻进程暂停所导致的负面影响。语言绑定的垃圾回收机制可以跟踪对象的分配情况以及剩余的空闲内存，因而可以在运行时灵活控制垃圾回收。

​	**现在一个较新的想法是把GC暂停视为节点的一个计划内的临时离线，当节点启动垃圾回收时，通知其他节点来接管客户端的请求**。<u>此外，系统可以提前为前端应用发出预警，应用会等待当前请求完成，但停止向该节点发送新的请求，这样垃圾回收可以在无干扰的情况下更加高效运行。这个技巧以某种方式对客户端隐藏垃圾回收，降低负面影响。目前一些对延迟敏感的系统(如金融交易系统)已经采用了该方法</u>。

​	<u>该方法的一个变种是，只对短期对象(可以快速回收)执行垃圾回收，然后在其变成长期存活对象之前，采取定期重启的策略从而避免对长期存活对象执行全面回收。每次选择一个节点重新启动，在重启之前，重新平衡节点之间的流量，思路与滚动升级类似(参阅第4章)</u>。

​	这些措施虽然并不能完全避免垃圾回收导致的进程暂停，但可以有效地减少对应用层的影响。

## 8.4 知识，真相与谎言

​	本章到目前为止，已经探索了分布式系统与单节点程序的许多不同之处。例如，很少使用共享内存，通过不可靠网络传递消息且延迟不确定，可能遭受部分失效，不可靠的时钟以及进程暂停等。

​	如果还没有深谙分布式系统之道，那么下面些问题看起来有些难以理解。网络中的一个节点无法确信信息，只能通过网络收到(或没有收到)的消息来猜测。节点只能通过消息交换来获得其他节点当前的状态(存储了哪些数据，是否正常工作等)。如果远程节点没有响应，由于没法区分网络本身的问题还是节点的问题，就无从知道节点究竟处于什么状态。

​	进一步深究上述问题，可能会涉及一些哲学相关话题：我们从系统获得的信息哪些是真实的、哪些是假的？如果感知和测量的手段都不可靠，那么获得的信息究竟有多大的可信度？软件系统是否应该遵循物理世界的那些通用法则(例如因果关系)呢？

​	还好，我们并不是非要搞清楚生命的意义才能回答上面的问题。在分布式系统中，我们可以明确列出对系统行为(系统模型)所做的若干假设，然后以满足这些假设条件为目标来构建实际运行的系统。在给定系统模型下，可以验证算法的正确性。这也意味着即使底层模型仅提供了少数几个保证，也可以在系统软件层面实现可靠的行为保证。

​	显然这不是一件容易的事情。在本章接下来的部分，我们将进一步探讨分布式系统中知识和真相，目的是帮忙我们审视可以做岀哪些合理假设，以及通常可以提供哪些保证。第9章将介绍具体的例子，例如典型分布式系统和算法在特定条件下提供的哪些特定保证。

### 8.4.1 真相由多数决定

​	假定在一个发生了非对称故障的网络环境中，即某节点能够收到发送给它的消息，但是该节点发出的所有消息要么被丢弃，要么被推迟发送。该节点即使本身运行良好，可以接收来自其他节点的请求，但其他节点却无法顺利收到响应。当消息超时之后，由于都收不到回复，其他节点就会一致声明上述节点发生失效。打个比方，就像这种情况：处于半连接的节点被强行摁倒在车里，即使它满心不情愿，即使在里面不停地哭喊“我没有问题”，可没人听到其呼救声，其他人只能忍痛将其塞进车里。

​	接下来是一个情况稍好的场景，半断开的节点可能会注意到其发送的消息没有被其他节点所确认，因此意识到网络一定发生了某种故障。尽管如此，节点还是会被其他节点错误地宣告为失效，改变不了该节点最终的命运。

​	第三种情况，该节点上垃圾回收运行了很长吋间，所有线程包括那些事务处理任务都被GC抢占并暂停了足足一分钟，在此期间，没有处理任何请求，也没有发送任何响应。那么其他节点只能苦苦等待，不停重试，最后无奈宣布该节点已经失效，然后将其塞进车里。可节点最终还是完成了垃圾回收，原有的工作线程得以继续，好像什么都没有发生。此时，轮到其他节点感到惊讶：刚刚宣告有问题的节点突然从车里爬出来，活蹦乱跳，甚至和周围兴奋地聊天。显然，运行垃圾回收的节点根本没有意识它中间休克了一分钟，从它的角度看，自上次正常的通信之后，一切正常，没有奇怪的停顿。

​	我们讲这些故事的寓意是，**节点不能根据自己的信息来判断自身的状态。由于节点可能随时会失效，可能会暂停假死，甚至最终无法恢复，因此，分布式系统不能完全依赖于单个节点。目前，许多分布式算法都依靠法定票数，即在节点之间进行投票(参阅第5章“读写quorum”)。任何决策都需要来自多个节点的最小投票数，从而减少对特定节点的依赖**。

​	这其中包括关于宣告节点失效的决定。如果有法定数量的节点声明另一个节点失效，即使该节点仍感觉活得很自在，那它也必须接受失效的裁定，所有个体节点必须遵循法定投票的决议然后离线。

​	最常见的法定票数是取系统节点半数以上(也有其他类型的法定人数)。如果某些节点发生故障， quorum机制可以使系统继续工作(对于三个节点的系统，可以容忍一个节点失效；五个节点则可以容忍两个节点故障)。<u>由于系统只可能存在一个多数，绝不会有两个多数在同时做出相互冲突的决定，因此系统的决议是可靠的</u>。第9章介绍一致性算法时，我们将更详细地讨论如何使用 quorum。

#### * 主节点与锁

​	有很多情况，我们需要在系统范围内只能有一个实例。例如：

+ <u>只允许一个节点作为数据库分区的主节点，以防止出现**脑裂**</u>(参阅第5章“处理节点失效”)。
+ 只允许一个事务或客户端持有特定资源的锁，以防止同时写入从而导致数据破坏。
+ 只允许一个用户来使用特定的用户名，从而确保用户名可以唯一标识用户。

​	在分布式系统实现时需要额外注意：即使某个节点自认为它是“唯一的那个”(例如分区的主节点，锁的持有者，成功拿走用户名的请求)，但不一定获得了系统法定票数的同意！一个节点可能以前确实是主节点，但其他节点有可能在此期间已宣布其失效(例如，出现了网络中断或GC暂停)，节点已被降级而系统选出了另一个主节点。

​	<u>当多数节点声明节点已失效，而该节点还继续充当“唯一的那个”，如果系统设计不周就会导致负面后果</u>。该节点会按照自认为正确的信息向其他节点发送消息，其他节点如果还选择相信它，那么系统就会出现错误的行为。

​	**例如图8-4展示了由于不正确的加锁而导致数据破坏的例子。该bug并非只是理论存在， HBase曾遭遇过该问题为。其设计目标是确保存储系统的文件一次只能由一个客户端访问，如果多个客户端试图同时写入该文件，文件就会被破坏。因此，在访问文件之前客户端需要从锁服务获取访问租约**。

![知识、真相与谎言 - 图1](https://static.sitestack.cn/projects/ddia/img/fig8-4.png)

​	**这个问题属于前面“进程暂停”中的一种情况：持有租约的客户端被暂停太久直到租约到期。然后另一个客户端已经获得了文件的锁租约，并开始写文件。接下来，当暂停的客户端重新回来时，它仍然(错误地)认为合法持有锁并尝试写文件。结果导致客户2的文件写入被破坏**。

#### * Fencing令牌

​	<u>当使用**锁**和**租约**机制来保护资源的并发访问时(见图8-4)，必须确保过期的“唯一的那个”节点不能影响其他正常部分。要实现这一目标，可以采用一种相当简单的技术fencing(栅栏，隔离之意)，如图8-5所示</u>。

​	**我们假设每次锁服务在授予锁或租约时，还会同时返回一个fencing令牌，该令牌(数字)毎授授予一次就会递增(例如，由锁服务增加)。然后，要求客户端每次向存储系统发送写请求时，都必须包含所持有的fencing令牌**。

​	图8-5中，客户端1获得锁租约的同时得到了令牌号33，但随后陷入了一个长时间的暂停直到租约到期。这时客户端2已经获得了锁租约和令牌号34，然后发送写请求(以及令牌号34)到存储服务。接下来客户端1恢复过来，并以令牌号33来尝试写入，存储服务器由于记录了最近已经完成了更高令牌号(34)，因此拒绝令牌号33的写请求。

![知识、真相与谎言 - 图2](https://static.sitestack.cn/projects/ddia/img/fig8-5.png)

​	<u>当使用 ZooKeeper作为锁服务时，可以用事务标识zxid或节点版本cversion来充当fending令牌，这两个都可以满足**单调递增**的要求</u>。

​	<u>请注意，只靠客户端自己检查锁状态是不够的，这种机制要求资源本身必须主动检査所持令牌信息，如果发现已经处理过更高令牌的请求，要拒绝持有低令牌的所有写请求。如果资源不支持额外的令牌检査，可以采取一些临时技巧来绕过去(例如，对于访问文件存储服务的情况，可以将令牌信息内嵌在文件名中)。总之，为了避免在锁保护之外发生请求处理，需要进行额外的检査机制</u>。

​	**在服务器端检査令牌可能看起来有些复杂，但其实是推荐的正确做法**：<u>系统服务不能假定所有的客户端都表现符合预期，事实上客户端通常由权限级别相对较低的人来操作运行，因此存在一定的误用、滥用风险，从安全角度讲，服务端必须防范这种来自客户端的滥用</u>。

### 8.4.2 拜占庭故障

​	<u>fencing令牌可以检测并阻止那些**无意的误操作**(例如节点并没有发现其租约已经过期)</u>。但是，如果节点故意试图破坏系统，在发送消息时可以简单地伪造令牌即可。

​	本书总是假设节点虽然不可靠但一定是诚实的：它们尽管运行很慢或者由于故障而无法响应，或者状态可能已经过期(例如由于GC暂停或网络延迟)，但一旦做出了响应，则一定是完全基于其所知的全部信息和事先协议约定好的行为准则，响应代表了其所知的“真相“。

​	<u>如果节点存在“撒谎”的情况(即故意发送错误的或破坏性的响应)，那么分布式系统处理的难度就上了一个台阶。例如，节点明明没有收到某条消息，但却对外声称收到了。这种行为称为**拜占庭故障**</u>，<u>在这样不信任的环境中需要达成共识的问题也被称为**拜占庭将军问题**</u>。

> 拜占庭将军问题
>
> 拜占庭将军问题是所谓“两将军问题”的更抽象表示，后者假定有两名将军需要就战斗计划达成一致。由于他们在两个不同的地点建立了营地，中间只能通过信使进行汋通，而信使在传遆消息时可能会岀现延迟或丢失(就像网络中的信息包一样)。我们将在第9章讨论共识问题。
>
> 而在拜占庭版本中，有n位将军需要达成共识，并且其中存在一些叛徒试图阻挠达成共识。大多数的将军都是忠诚的，发出了真实的信息，但是叛徒则试图通过发送虛假或不真实的信息来欺骗和混淆他人(同时努力隐藏自己)。而且大家事先并不知道叛徒是谁。
>
> 拜占庭是一个古希腊城市，即随后的君士坦丁堡，目前位于土耳其的伊斯坦布尔。没有任何历史证据表明，拜占庭将军比其他地方的将军更诡计多端或者善于密谋。这个名字其实隐喻着拜占庭式的过分复杂，官僚，含混不淸的意思，早在计算机之前已在政治中广泛引用。拜占庭问题的提出者兰波特当初只是想选个不会冒犯任何人/国籍的名字，据传他曾被明确通牒，不准使用阿尔巴尼亚将军问题来命名。

​	<u>如果某个系统中即使发生部分节点故障，甚至不遵从协议，或者恶意攻击、干扰网络，但仍可继续正常运行，那么我们称之为**拜占庭式容错系统**</u>。这些担忧在某些特定场景是合理的。例如：

+ 在航空航天领域，计算机内存或CPU寄存器中的数据可能会被辐射而发生故障，导致以不可预知的方式响应其他节点。这种情况下如果将系统下线，代价将异常昂贵(例如，可能岀现飞机撞毁，杀死船员，或致使火箭与国际空间站相撞等)，飞行控制系统必须做到容忍拜占庭故障。
+ 在有多个参与者的系统中，某些参与者可能会作弊或者欺骗他人。这时节点不能完全相信另一个节点所发送的消息，它可能就是恶意的。例如，<u>像比特币和其他区块链一样的点对点网络就是让互不信任的当事方就某项交易达成一致，且不依赖于集中的机制</u>。

​	然而，在本书所讨论的这些系统中，我们可以安全地假定没有拜占庭式的故障。在数据中心里，所有的节点都是由一个组织来集中控制(可信任)，辐射水平也足够低因而内存损坏可以忽略。**解决拜占庭容错的系统协议异常复杂，而容错的嵌入式系统还依赖于硬件层面的支持**。<u>因而在绝大多数服务器端数据系统中，部署拜占庭容错解决方案基本不太可行</u>。

​	Web应用程序确实可能会接收来自任意客户端(如Web浏览器)的请求，其中可能带有恶意行为。因此需要输入验证、安全监测和输出转义等步骤，例如防止SQL注入和跨站恶意脚本。但我们通常并不使用拜占庭容错协议，而只是全权让服务器决定什么是可接受的客户端行为，什么是不允许的。**只有在没有这种中央决策机制的点对点网络中，拜占庭容错才更为必要**。

​	<u>另外，软件中的bug可以被认为是拜占庭式故障，但如果将相同的软件部署到所有节点上，那么即使拜占庭式的容错算法也无法解决问题</u>。大多数拜占庭容错算法要求系统超过三分之二的节点即绝大多数要功能正常(如果有四个节点，则最多允许一台发生故障)。要采用这类算法对付bug，必须有四个不同的软件实现，然后寄希望该bug只出现在四个实现中的一个。

​	试想如果有这样一个协议或者算法能同时保护我们免受漏洞，安全防范降级以及恶意攻击等，那将是多么美好。不幸的是，这是不现实的。<u>通常如果攻击者可以入侵一个节点，则很可能会攻陷几乎所有节点(由于运行相同的软件)。因此，传统的安全措施如认证、访问控制、加密、防火墙等，仍是防范攻击的主要保护机制</u>。

#### 弱的谎言形式

​	尽管我们假设节点通常是诚实的，但依然推荐增加必要的机制来防范一些不那么恶意的“谎言”。例如由于硬件问题造成的无效消息、软件bug和配置错误。这种保护机制显然并不是完整的拜占庭式容错，无法防范敌手的攻击，但它们更为简单实用，可以帮助提高软件系统的可靠性和健壮性。例如：

+ 由于硬件问题或操作系统、驱动程序、路由器等方面的错误，导致网络数据包有时出现损坏。通常，可以借助TCP/UDP中内置的数据包**校验和**来发现这类问题，但有时他们会逃避检测。此时，一个简单的防范措施是在应用层添加校验和。
+ <u>对公众开放的应用必须仔细检查用户的所有输入，例如输入值是否在合理的范围内，并限制字符串的大小，防止分配超大内存导致拒绝服务攻击</u>。位于防火墙后面的内部服务可能不用太严格的检查，但依然推荐执行基本的安全检杳(例如，接口协议解析)。

+ <u>NTP客户端最好配置多个时间服务器。同步时间时，连接到多个时间服务器，收到回应之后要评估时间偏差，使得多数服务器就一定的时间范围达成一致</u>。只要大多数服务器都正常，某个错误的服务器就可以被检测出来，从而排除在同步结果之外。总之，使用多台NTP服务器可以比仅使用一台服务器更为鲁棒。

### 8.4.3 理论系统模型与现实

​	目前分布式系统方面已有许多不错的具体算法，如第9章中要介绍的共识算法。这些算法需要容忍本章所讨论的各种故障。

​	<u>算法的实现不能过分依赖特定的硬件和钦件配置。这就要求我们需要对预期的系统错误进行形式化描述</u>。我们通过定义一些系统模型来形式化描述算法的前提条件。

​	关于计时方面，有三种常见的系统模型：

+ 同步模型

  <u>同步模型假定有上界的网络延迟，有上界的进程暂停和有上界的时钟误差</u>。注意，这并不意味着完全同步的时钟或者网络延迟为零。它只意味着你清楚地了解网络延迟、暂停和时钟漂移不会超过某个固定的上限。<u>大多数实际系统的实际模型并非同步模型，因为(如本章所讨论的)无限延迟和暂停确实可能发生</u>。

+ 部分同步模型

  部分同步意味着系统在大多数情况下像一个同步系统一样运行，但有时候会超出网络延迟，进程暂停和时钟漂移的预期上界。这是一个比较现实的模型：大多数情况下，网络和进程比较稳定(否则几乎不可能提供持续的服务)，但是我们必须考虑到任何关于时机的假设都有偶尔违背的情况，而一旦发生，网络延迟，暂停和时钟偏差可能会变得非常大。

+ 异步模型

  在这个模型中，一个算法不会对时机做任何的假设，甚至里面根本没有时钟(也就没有超时机制)。某些算法可以支持纯异步模型，但并不常见。

​	除了时机之外，我们还需要考虑节点失效。有以下三种最常见的节点失效系统模型：

+ 崩溃-中止模型

  在崩溃中止模型中，算法假设一个节点只能以一种方式发生故障，即遭遇系统崩溃。这意味着节点可能在任何时候突然停止晌应，且该节点以后永远消失，无法恢复。

+ 崩溃-恢复模型

  节点可能会在任何时候发生崩溃，且可能会在一段(未知的)时间之后得到恢复并再次响应。<u>在崩溃-恢复模型中，节点上持久性存储(即非易失性存储)的数据会在崩溃之后得以保存，而内存中状态可能会丢失</u>。

+ 拜占庭(任意)失效模型

  如上节所述，节点可能发生任何事情，包括试图作弊和欺骗其他节点。

​	对于真实系统的建模，最普遍的组合是崩溃-恢复模型结合部分同步模型。那么接下来上层的分布式算法该如何应对这样的模型呢？

#### 算法的正确性

​	为了定义算法的正确性，我们可以描述它的属性信息。例如，排序算法的输出具有以下特性：对于输出列表中的任何两个不同的元素，左边的元素小于右边的元素。这就是一个对列表进行排序的正确性描述。

​	类似的思路，我们可以通过描述目标分布式算法的相关属性来定义其正确性。例如，对于锁服务的 fencing令牌生成算法(参阅本章前面的“fencing令牌”)，要求算法具有以下属性：

+ 唯一性

  两个令牌请求不能获得相同的值。

+ 单调递增

  如果请求x返回了令牌t，请求y返回了令牌t，且x在y开始之前先完成，那么t<sub>x</sub> < t<sub>y</sub>。

+ 可用性

  请求令牌的节点如果不发生崩溃则最终一定会收到响应。

​	<u>如果针对某个系统模型的算法在各种情况下都能满足定义好的属性要求，那么我们称这个算法是正确的</u>。这有何意义呢？如果换一个角度来看一种极端情况，所有节点全部崩溃，或者所有的网络延迟突然变得无限长，那么所有的算法都不可能完成其预期功能。

#### * 安全与活性

​	为进一步加深理解，有必要区分两种不同的属性：安全性和活性。<u>在上面的例子中唯一性和单调递增属于安全属性，而可用性则属于活性</u>。

​	这两种性质有何区别呢？一种理解思路是，活性的定义中通常会包括暗示“最终”一词(是的，你猜对了，最终一致性也是一种活性)。

​	**安全性通常可以理解为“没有发生意外”，而活性则类似“预期的事情最终一定会发生”**。这个非正式定义中，有很多主观因素，所以不用过度解读。安全性和活性其实有准确和数学化的定义描述，请参阅文献：

+ <u>如果违反了安全属性，我们可以明确指向发生的特定的时间点(例如，唯一性如果被违反，我们可以定位到具体哪个操作产生了重复令牌)。且一旦违反安全属性，违规行为无法撤销，破坏已实际发生</u>。
+ <u>活性则反过来：可能无法明确某个具体的时间点(例如一个节点发送了一个请求，但还没有收到响应)，但总是希望在未来某个时间点可以满足要求(即收到回复)</u>。

​	区分安全性和活性的一个好处是可以帮助简化处理一些具有挑战性的系统模型。**通常对于分布式算法，要求在所有可能的系统模型下，都必须符合安全属性**。**也就是说，即使所有节点发生崩溃，或者整个网络中断，算法确保不会返回错误的结果**。

​	<u>而对于活性，则存在一些必要条件。例如，我们可以说，只有在多数节点没有崩溃，以及网络最终可以恢复的前提下，我们才能保证最终可以收到响应。部分同步模型的定义即要求任何网络中断只会持续一段有限的时间，然后得到了修复，系统最终返回到同步的一致状态</u>。

#### 将系统模型映射到现实世界

​	安全性、活性以及所建立的系统模型对于评测分布式算法的正确性意义重大。然而，很明显系统模型只是对现实情况的简化抽象，实践中具体实施算法时，各种因素混杂在一起会提出更严峻的挑战。

​	例如，在崩溃-恢复模型中，算法通常假设保存在持久性介质的数据可以安然无恙。但是，如果硬盘上的数据发生损坏，或者由于硬件错误，配置错误等导致数据被清除，会发生什么后果呢？即使硬盘本身正确连接到服务器，但服务器存在固件错误，导致重启时无法正确识别硬盘，又会发生什么情况？

​	Quorum算法(参见第5章“读写Quorum”)要求节点必须记录之前对外所宣告的数据。如果节点发生意外而丟弃存储的数据，会打破法定条件并破坏算法的正确性。或许此时我们需要一个新的系统模型，它假定通常情况下数据存储非常可靠，但还是有丢失的可能。但那个模型就变得更难以推理了。

​	<u>算法的理论摧述可以简单地宣称某些事情绝不会发生，例如在非拜占庭系统中，我们就会明确假定一些可能和不可能发生的错误。不过，真正去实现时最好还是有一些必要的代码来简单处理一些几乎不可能发生的事情，即使只是去输出一些提醒信息(如printf)和程序退出错误代码(如exit-666)，以方便操作人员来清理最后的烂摊。这些错误处理也很好地体现了计算机科学和软件工程之间的差异</u>。

​	这绝对不是说抽象的系统模型没有价值，恰恰相反，它把实际系统中的复杂性提炼成一个更容易理解、更具可控性的抽象错误集合，可以有效帮助我们理解冋题之本质，然后设计系统性方法来最终解决问题。如果我们可以确定在给定的系统模型中，算法总能满足属性要求，那么我们可以证明算法就是正确的。

​	证明算法正确却并不意味着真实系统上的某个具体实现一定是正确的。毫无疑问，这依然是极其重要的第一步。理论分析并不能覆盖现实系统隐藏的毎一个问题细节，例如某些边界条件一旦触发，会有负面影响，但这些边界条件归根结底一定是违背了模型的某个前提假设(例如，关于时间点)。我们可以说，理论性分析与实证性检验对最终的成功同等重要。

## 8.5 小结

​	本章讨论了分布式系统中可能发生的各种典垩问题，包括：

+ 当通过网络发送数据包时，数据包可能会丢失或者延迟;同样，回复也可能会丢失或延迟。所以如果没有收到回复，并不能确定消息是否发送成功。
+ 节点的时钟可能会与其他节点存在明显的不同步(尽管尽最大努力设置了NTP服务器)，时钟还可能会突然向前跳跃或者倒退，依靠精确的时钟存在一些风险，没有特别简单的办法来精确测量时钟的偏差范围。
+ 进程可能在执行过程中的任意时候遭遇长度未知的暂停(一个重要的原因是垃圾回收)，结果它被其他节点宣告为失效，尽管后来又恢复执行，却对中间的暂停亳无所知。

​	部分失效可能是分布式系统的关键特征。只要软件试图跨节点做任何事情，就有可能出现失败，或者随机变慢，或者根本无应答(最终超时)。<u>对于分布式环境，我们的目标是建立容忍部分失效的软件系统，这样即使某些部件发生失效，系统整体还可以继续运行</u>。

​	为了容忍错误，第一步是检测错误，但即使这样也很有挑战。<u>多数系统没有检测节点是否发生故障的准确机制，因此分布式算法更多依靠超时来确定远程节点是否仍然可用</u>。但是，超时无法区分网络和节点故障，且可变的网络延迟有时会导致节点被误认为发生崩溃。此外，节点可能处于一种降级状态：例如，由于驱动程序错误，千兆网络接口可能突然降到1kb/s的吞吐量。这样一个处于“残废”的节点比彻底挂掉的故障节点更难处理。

​	检测到错误之后，让系统容忍失效也不容易。在典型的分布式环境下，没有全局变量，没有共享内存，没有约定的尝试或其他跨节点的共享状态。节点甚至不太清楚现在的准确时间，更不用说其他更高级的了。信息从一个节点流动到另一个节点只能是通过不可靠的网络来发送。单个节点无法安全的做出任何决策，而是需要多个节点之间的共识协议，并争取达到法定票数。

​	如果习惯于编写单节点理想化环境运行的软件(即同一个操作总是确定性地返回相同的结果)，当转向分布式系统时，种种看似凌乱的现实可能着实让人震惊。相反，如果在单节点上即可解决问题，那么对于一个分布式系统工程师通常会被认为该问题微不足道(现在单节点确实可以完成很多任务)。如果可以避免打开潘多拉之盒，那么把工作都放在一台机器也值得一试。

​	<u>但正如在第二部分开头所讨论的那，可护展性并不是使用分布式系统的唯一原因。容错与低延迟(将数据放置在距离用户较近的地方)也是同样重要的目标，而后两者无法靠单节点来实现</u>。

​	本章，我们也探讨了网络、时钟和进程的不可靠性是否是不可避免的自然规律。我们对此给出的结论是否定的：**的确有可能在网络中提供硬实时的延迟保证或者具有上确界的延迟，但代价昂贵，且硬件资源利用率很低。除了安全关键场景，目前绝大多数都选择了低成本(和不可靠)**。

​	我们还谈到了高性能计算，它们多采用更加可靠的组件，发生故障时完全停止系统，之后重新启动。相比之下，分布式系统会长时间不间断运行，以避免影响服务级别；故障处理和系统维护多以节点为单位进行处理，或者理论上如此(实际上，如果错误的配置不小心被应用到集群的所有节点，仍然会导致整个集群系统瘫痪)。

​	这样看起来本章揭露的全是问题，前景黯淡。那么在下一章，我们将讨论解决方案，重点是针对这些问题而设计相关的分布式算法。

# 第9章 一致性与共识

​	正如第8章所述，分布式系统存在太多可能出错的场景。而处理故障最简单的办法就是直接让整个服务停下来，然后向用户提示出错信息。但如果不能接受服务中止，就需要更加容错的解决方案，这样即使某些内部组件发生了故障，整个系统依然可以对外提供服务。

​	本章我们将讨论构建容错式分布式系统的相关算法和协议。这里假设第8章中所有的故障都可能发生，这包括网络数据包可能会丢失、顺序紊乱、重复发送或延迟，时钟也有一定偏差，节点可能发生暂停(例如由于垃圾回收)甚至随时崩溃。

​	<u>为了构建容错系统，最好先建立一套通用的抽象机制和与之对应的技术保证，这样只需实现一次，其上的各种应用程序都可以安全地信赖底层的保证。这与第7章我们引入事务的道理相同：通过事务，应用程序可以假装没有崩溃(原子性)，没有与其他人并发访问数据库(隔离性)，且存储设备是完全可靠的(持久性)；总之，抽象的事务机制可以屏蔽系统内部很多复杂的问题，例如发生崩溃、边界条件、磁盘故障等，使得应用层轻松无忧</u>。

​	现在继续沿着这个思路，尝试建立可以让分布式应用忽略内部各种问题的抽象机制。例如，分布式系统最重要的抽象之一就是**共识**：<u>所有的节点就某一项提议达成一致</u>。通过本章的介绍，最后你会发现面对各种网络故障和进程失效，可靠地达成共识是件多么了不起的事情。

​	一旦解决了共识问题，就可以服务于应用层很多的目标需求。例如，对于一个主从复制的数据库，如果主节点发生失效，就需要切换到另一个节点，此时数据库节点可以采用共识算法来选举新的主节点。正如第5章“处理节点失效”所强调的，某一时刻必须只有一个主节点，所有的节点必须就此达成一致。<u>如果有两个节点都自认为是主节点，就会发生**脑裂**，导致数据丢失</u>。正确实现共识算法则可以避免此类问题。

​	本章我们将主要硏究解决共识问题的相关算法。在此之前，首先我们会简要讨论分布式系统可提供的若干保证和抽象机制。

​	我们需要了解系统能力的边界，即哪些可行，哪些不可行。在什么情况下，系统可以容忍故障并继续工作；而其他情况，却无法保证。在理论证明和具体实现两方面，业界对此都有着非常深入的研究，在此本章即将为大家总体介绍各种系统边界情况。

​	事实上，几十年来分布式系统领域的研究人员一直在持续探索，积累了大量的硏究资料，由于篇幅所限，我们无法一一触及。所以本章会省去相关的形式化模型与证明细节，更多的是给出较为直观的示例和解释。如有兴趣，本章后面的“参考文献”提供了更详细的信息。

## 9.1 一致性保证

​	在第5章“复制滞后问题”中，我们探讨了**复制数据库时的计时问题**。如果在同一时刻查询数据库的两个节点，则可能会看到不同的数据，这主要是因为<u>写请求会在不同的时间点到达不同的节点。无论数据库采用何种复制方法(包括主从复制，多主节点复制或者无主节点复制)，都无法完全避免这种不一致情况</u>。

​	<u>大多数多副本的数据库都至少提供了**最终的一致性**</u>，这意味着如果停止更新数据库，并等待一段时间(长度未知)之后，最终所有读请求会返回相同的内容。换句话说，不一致现象是暂时的，最终会达到一致(假设网络故障最终会被修复)。换言之，最终一致性意味着“收敛”，即预期所有的副本最终会收敛到相同的值。

​	<u>但是，这是一个非常弱的保证，它无法告诉我们系统何时会收敛。而在收敛之前，读请求可能会返回任何值甚至读失败。例如，如果完成一笔更新操作之后立即读取，由于读取可能会路由到不同的副本，系统不保证一定读到刚刚写入的值(参阅第5章“读自己的写”)</u>。

​	对于应用开发人员而言，最终一致性会带来很大的处理挑战，这与普通的单线程程序中变量读写行为大相径庭。对于后者，如果将对变量赋予某个值，接下来去读变量，不可能发生读失败或者读不到刚赋的值。数据库表面上看起来像一个可以进行读写的变量，但事实上它内部有无比复杂的更多语义要求。

​	当面对只提供了弱保证的数据库时，需要清醒地认清系统的局限性，切不可过于乐观。应用可能在大多数情况下都运行良好，但数据库内部可能已经发生了非常微妙的错误，<u>只有当系统出现故障(例如网络中断)或高并发压力时，最终一致性的临界条件或者错误才会对外暴露出来，因而测试与发现错误变得非常困难</u>。

​	因此本章将探索更强的一致性模型。不过，这也意味着更多的代价，例如性能降低或容错性差。尽管如此，更强的保证的好处是使上层应用逻辑更简单，更不容易出错。当了解、对比了多种不同的一致性模型之后，可以结合自身需求，从中选择最合适的。

​	分布式一致性模型与我们之前讨论过的多种事务隔离级别有相似之处(参阅第7章弱隔离级别”)。虽然存在某些重叠，但总体讲他们有着显著的区别：**事务隔离主要是为了处理并发执行事务时的各种临界条件，而分布式一致性则主要是针对延迟和故障等问题来协调副本之间的状态**。

​	本章内容涵盖非常广泛，但细究起来，这些内容之间存在着密切联系：

+ 我们首先介绍**线性化**，这是最强的一致性模型，并考察其优缺点。
+ 然后，我们将探讨分布式系统中事件顺序问题(参阅本章后面的“顺序保证”)，特别是**因果关系**和**全局顺序**。
+ 最后“分布式事务与共识”小节，我们将探索如何自动提交**分布式事务**，并最终解决共识问题。

## 9.2 可线性化

​	在最终一致性数据库中，同时查询两个不同的副本可能会得到两个不同的答案。这会使应用层感到困惑。<u>如果数据库能够对上提供只有单个副本的假象，情况会不会大为简化呢？这样让毎个客户端都拥有相同的数据视图，而不必担心复制滞后</u>。

​	这就是**可线性化**(也称为原子一致性，强一致性等)的思想。线性化的确切定义比较微妙，我们将稍后再详细探讨。其**基本的想法是让一个系统看起来好像只有一个数据副本，且所有的操作都是原子的**。<u>有了这个保证，应用程序就不需要关心系统内部的多个副本</u>。

​	**在一个可线性化的系统中，一旦某个客户端成功提交写请求，所有客户端的读请求一定都能看到刚刚写入的值**。<u>这种看似单一副本的假象意味着它可以**保证读取最近最新值**，而不是过期的缓存</u>。换句话说，<u>可线性化是一种就近的保证</u>。为了解释该想法，我们先来看一个非线性化系统的例子。

![线性一致性 - 图1](https://static.sitestack.cn/projects/ddia/img/fig9-1.png)

​	图9-1是一个非线性化的体育网站。 Alice和Bob坐在同一个房间里各自观看自己的手机，焦急地等待2014年FIFA世界杯决赛的结果。刚刚宣布了最终比分之后， Alice刷新页面马上看到获胜者，然后兴奋地告诉Bob。于是Bob马上在自己的手机上刷新页面，但他的请求发向了某个落后的数据库副本，结果却显示比赛还在进行之中。

​	如果 Alice和Bob几乎同时刷新页面得到两个不同的结果，他们也不清楚服务器端究竟何时接受、处理这些请求，因此可能并不会特别惊讶。然而，现在的情况却是Bob听到了 Alice兴奋的比分之后再单击刷新，他希望至少是看到刚刚 Alice播报的最近比分，但却读到过期的结果，这就违背了线性化规则。

### 9.2.1 如何达到线性化？

​	可线性化背后的基本思想很简单：使系统看起来好像只有一个数据副本。然而，细究起来还有更多的含义。为了更好地理解可线性化，来看看更多的例子。

​	图9-2展示了三个客户端在线性化数据库中同时读写相同的主键x。在分布式语义下，ⅹ被称为寄存器，例如，它可以是键-值存储中的一个键，关系数据库中的一行或文档数据库中的一个文档。

![线性一致性 - 图2](https://static.sitestack.cn/projects/ddia/img/fig9-2.png)

​	为简单起见，图9-2仅展示了客户端所观察到请求内容，而不是数据库内部。每条线代表一个客户端请求，虚线的开始表示发送请求的时间，结尾则是收到响应的时间。由于网络延迟不确定，客户端并不清楚数据库具体何时处理请求，而只知道它是在发送之后、响应之前的某个中间时间点。

​	在这个例子中，寄存器有两类操作：

+ read(x)→ν表示客户端读取x的值，数据库返回了值v。
+ write(x，ν)→r表示客户端将x设置为值ν，数据库返回处理结果r(可能表示处理成功或者发生了失败)。

​	在图9-2中，x的初始值为0，客户端C提交写请求将其设置为1。同时，客户端A和B在反复轮询数据库以读取最新值。A和B可能会分别读到什么样的返回值呢？

+ 客户端A的第一个读取操作在写入开始之前已完成，因此返回的是旧值0。
+ 客户端A的最后一次读操作是在写操作完成之后才开始的，如果数据库是可线性化的，它肯定会返回新值1。道理很简单，执行写操作肯定是在写请求发送之后并且响应之前；执行读操作同理。如果写入结東后开始读取，那么读一定发生在执行写之后，所以看到的一定是写入的新值。
+ <u>与写操作有时间重叠的任何读取操作则可能返回0或者1，这是因为读写之间存在并发，无法确切知道在执行读取时，写入是否已经生效</u>。

​	然而，这还没有精确描述线性化：如果与写并发的读操作可能返回旧值或新值，那么在这个过程中，不同的读客户端会看到旧值和新值之间来回跳变的情况。这肯定不符合我们所期望的模拟“单一数据副本”。

​	为使系统可线性化，我们需要添加一个重要的约束，如图9-3所示。

![线性一致性 - 图3](https://static.sitestack.cn/projects/ddia/img/fig9-3.png)

​	在一个可线性化的系统中，在写操作的开始与结束之间必定存在某个时间点，x的值发生了从0到1的跳变。<u>如果某个客户端的读取返回了新值1，即使写操作尚未提交那么所有后续的读取也必须全部返回新值</u>。

​	图9-3中的箭头表示时序依赖关系。客户端A首先读到新值1，在A读取返回之后，B开始读取，由于B的读取严格在A的读取之后发生，因此即使C的写入仍在进行之中，也必须返回1。这和图9-1中 Alice和Bob的情况类似，在 Alice读取新值之后，Bob也预期读取新值。

​	可以进一步细化时序图来可视化每步操作具体在哪个时间点生效，如图9-4所示。

​	在图9-4中，除了读写之外，我们引入了第三种类型的操作：

+ cas(x，v<sub>old</sub>，v<sub>new</sub>)→r表示一个原子比较-设置操作( compare-and-set，CAS)(参阅第7章“原子比较和设置”)。如果寄存器x当前值等于v<sub>old</sub>，则将其原子设置为v<sub>new</sub>；否则保留现有x值不变，然后返回错误，r是返回值，表示成功或者失败。

​	图9-4中的每个操作都有一条竖线，表示可能的执行时间点。这些标记以前后关系依次连接起来，最终的结果必须是一个有效的寄存器读写顺序，即<u>每个读操作须返回最近写操作所设置的值</u>。

​	可线性化要求，如果连接这些标记的竖线，它们必须总是按时间箭头(从左到右)向前移动，而不能向后移动。这个要求确保了之前所讨论的<u>就近性保证：一旦新值被写入或读取，所有后续的读都看到的是最新的值，直到被再次覆盖</u>。

![线性一致性 - 图4](https://static.sitestack.cn/projects/ddia/img/fig9-4.png)

​	图9-4中有一些有趣的细节值得仔细分析：

+ 客户端B首先发送读x的请求，接下来客户端D发送请求将x置为0，紧接着客户端A又发送请求将x置为1，而最终返回给B的值为1(A所写入的值)。这是可能的，它意味着数据库执行的顺序是：首先处理D的写入0，然后是A的写入1，最后是B的读取。虽然这并不是请求发送的顺序，但考虑到请求并发以及网络延迟等情况，例如或许B的读请求网络延迟更大，导致在两次写执行之后才到达数据库，因此这是一个合法的可接受的处理顺序。
+ <u>客户端A在收到数据库写响应之前，客户端B即读到了值1，这表明写入已成功。这是可能的，但它并不代表执行读发生在执行写之前，只是意味着很可能由于网络延迟而耽搁了客户端A接受响应</u>。

+ <u>模型没有假定事务间的隔离，即另一个并发客户端可能随时会修改值</u>。例如，C首先读取到1，然后读到2，原因是两次读取之间值被客户端B修改了。我们可以使用原子比较和设置(cas)操作来检査值是否被其他并发客户端修改，例如客户端B和C的cas请求成功，但是D的cas操作失败(因为数据库处理时，x的值已不再为0)。
+ 客户B的最后一次读取(阴影的方框)不满足线性化。该操作与C的cas写操作同时发生，后者将x从2更新为4。在没有其他请求时，B读取可以返回2。<u>但是在B读取开始之前，客户端A已经读取了新值4，所以不允许B读到比A更老的值</u>。这点与图9-1中的 Alice和Bob的情况是一样的。

​	以上就是线性化背后的直觉含义。正式的定义请参考文献。**通过记录所有请求和响应的时序，然后检查它们是否可以顺序排列，可以用来测试系统是否可线性化(这里存在额外的计算开销)**。

> 可线性化与可串行化
>
> **可线性化(Linearizability)**非常容易与**可串行化(Serializability)**发生混淆(请参阅第7章“可串行化”)，两个词似乎都在表达类似“可以按顺序排列”的意思。但是它们完全不同，需要仔细区分：
>
> + **可串行化**
>
>   可串行化是事务的隔离属性，其中每个事务可以读写多个对象(行，文档，记录等，请参阅第7章“单对象与多对象事务操作”)。<u>它用来确保事务执行的结果与**串行执行**(即每次执行一个事务)的结果完全相同，即使串行执行的顺序可能与事务实际执行顺序不同</u>。
>
> + **可线性化**
>
>   **可线性化是读写寄存器(单个对象)的<u>最新值保证</u>**。它并不要求将操作组合到事务中，因此无法避免写倾斜等问题(请参阅第7章“写倾斜与幻读”)，除非采取其他额外措施(如实现实体化冲突，参阅第7章“实体化冲突”)。
>
> <u>数据库可以同时支持**可串行化**与**线性化**，这种组合又被称为严格的可串行化或者强的单副本可串行化(strong one-copy serializability， strong-1SR)</u>。**基于两阶段加锁(参阅第7章“两阶段加锁”)或者实际以串行执行(参阅第7章“实际串行执行”)都是典型的可线性化**。
>
> 但是，**可串行化的快照隔离(参阅第7章“可串行化的快照隔离”)则不是线性化的：按照设计，它可以从一致性快**照中读取，以避免读、写之间的竞争。<u>一致性快照的要点在于它里面不包括快照点创建时刻之后的写入数据，因此从快照读取肯定不满足线性化</u>。

### * 9.2.2 线性化的依赖条件

​	那什么情况下应该使用线性化呢？上面足球比赛比分的例子只是个最简单的情况，结果存在几秒的延迟通常不会造成实质的伤害。然而，在有些场景下，线性化对于保证系统正确工作至关重要。

#### 加锁与主节点选举

​	<u>**主从复制**的系统需要确保有且只有一个主节点，否则会产生**脑裂**</u>。<u>选举新的主节点常见的方法是使用**锁**：即每个启动的节点都试图获得锁，其中只有一个可以成功即成为主节点</u>。<u>不管锁具体如何实现，它必须满足**可线性化**：所有节点都必须同意哪个节点持有锁，否则就会出现问题</u>。

​	提供协调者服务的系统如 Apache ZooKeeper和etcd等通常用来实现**分布式锁**和**主节点选举**。它们都使用了支持**容错的共识算法**确保<u>可线性化</u>(本章后面的“支持容错的共识”会详细讨论这些算法)。虽然目前也有像 Apache Curator这样的上层库，在 ZooKeeper之上提供了更高级别的接口以方便使用，但正确实现加锁和选举其实还有很多重要的设计细节(参阅第8章“主节点与锁”)。归根结底，线性化存储服务是所有这些协调服务的基础。

​	在一些分布式数据库如 Oracle Real Application Clusters(RAC)，<u>分布式锁有更细粒度的实现：RAC为每个磁盘页面均设置一把锁，多个节点因此可以并发地共享访问存储系统</u>。这些可线性化的锁处于事务执行的关键路径上，出于性能考虑，RAC部署时通常都要求专用的集群互连网络来连接数据库节点。

#### 约束与唯一性保证

​	唯一性约束在数据库中很常见。例如，用户名或电子邮件地址必须唯一标识一个用户，文件存储服务中两个文件不能具有相同的路径和文件名。如果要在写入数据时强制执行这些约束(例如，如果两个人试图同时创建具有相同名称的用户或文件，其中个必须返回错误)，则也需要线性化。

​	<u>这种情况本质上与加锁非常类似</u>：用户注册等同于试图对用户名进行加锁操作。该操作也类似于原子比较和设置：如果当前用户名尚未被使用，就设置用户名与客户ID进行关联。

​	其他类似约束包括银行账户余额不应出现负值，或者避免出售库存里已经没有的商品，或者不能同时预定航班或者剧院的相同的座位。这样的约束条件都要求所有节点就某个最新值达成一致(例如账户余额，库存水平，座位占用率)。

​	<u>当然在某些实际场合中，有时可以放宽这些限制(例如，如果航班发生超额预订，可以将客户转移到其他的航班并提供必要的补偿)。在这种情况下，或许不需要线性化，我们将在第12章“时效性与完整性”讨论这些松散的约東条件</u>。

​	然而，**硬性的唯一性约束，常见如关系型数据库中<u>主键的约束</u>，则需要线性化保证。其他如外键或属性约束，则并不要求一定线性化**。

#### 跨通道的时间依赖

​	请注意图9-1中的一个细节：如果 Alice没有高呼比分，Bob可能就不会知道他的查询结果是过期的。或许他会在几秒之后再次刷新页面，然后看到最终的比方。<u>线性化违例之所以被注意到，是因为系统中存在其他的通信渠道</u>(例如，Alice对Bob发出的声音来传递信息)。

​	计算机系统也会出现类似的情况。例如，用户可以上传照片到某网站，有一个后台进程将照片调整为更低的分辨率(即缩略图)以方便更快下载。该网站架构和数据流如图9-5所示。

​	这里需要明确通知图像调整模块来调整哪些图片，系统采用了消息队列将此命令从Web服务器发送到调整器。因为大多数消息队列系统并不适合大数据流，而考虑到照片的大小可能到数兆字节，因此Web服务器并不会把照片直接放在队列中。相反，照片会先写入文件存储服务，当写入完成后，把调整的命令放入队列。

​	如果文件存储服务是可线性化的，那么系统应该可以正常工作。否则，这里就会引入竞争条件：消息队列(图9-5中的步骤3和步骤4)可能比存储服务内部的复制执行更快。在这种情况下，当调整模块在读取图像(步骤5)时，可能会看到图像的某个旧版本，或者根本读不到任何内容。如果它碰巧读到了旧版本的图像并进行处理，会导致文件存储中的全尺寸图片与调整之后图片出现永久的不一致。

![线性一致性 - 图5](https://static.sitestack.cn/projects/ddia/img/fig9-5.png)

​	之所以出现这个问题是因为Web服务器和调整模块之间存在两个不同的通信通道文件存储器和消息队列。如果没有线性化的就近性保证，这两个通道之间存在竞争条件。这种情况类似于图9-1：网页数据库有数据复制通道，而在计算机系统之外Alice和Bob之间还有口耳相传的通道。

​	线性化并非避免这种竞争的唯一方法，但却是最容易理解的。如果可以控制某一个通信通道(例如消息队列，但注意不适合 Alice和Bob的例子，因为后者并非计算机系统)，可以尝试第5章的“读自己的写”方法，但会引入额外的复杂性。

### * 9.2.3 实现线性化系统

​	现在我们已经看了几个线性化的例子，接下来考虑如何实现这样可线性化的系统。

​	由于线性化本质上意味着“表现得好像只有一个数据副本，且其上的所有操作都是原子的”，所以<u>最简单的方案自然是只用一个数据副本。但显然，该方法无法容错：如果仅有的副本所在的节点发生故障，就会导致数据丢失，或者至少在重启之前都无法访问服务</u>。

​	**系统容错最常见的方法就是采用复制机制**。我们再来回顾一下第5章所介绍的多种复制方案，看看哪些满足可线性化：

+ 主从复制(**部分支持可线性化**)

  在主从复制的系统中(参阅第5章的“主节点与从节点”)，只有主节点承担数据写入，从节点则在各自节点上维护数据的备份副本。**如果从主节点或者同步更新的从节点上读取，则可以满足线性化**。<u>但并非每个主从复制的具体数据库实例都是可线性化的，主要是因为它们可能采用了快照隔离的设计，或者实现时存在并发方面的bug。</u>

  **而从主节点上读取的前提是你确定知道哪个节点是主节点**。<u>正如在第8章“真相由多数决定”中所讨论的，某节点可能自认为是主节点，但事实并非如此，这个“自以为是”的主节点如果对外提供服务，就会违反线性化</u>。如果使用了<u>异步复制</u>，故障切换过程中甚至可能会丢失一些已提交的写入(参阅第5章“处理节点失效”)，结果是同时违反持久性和线性化。

+ **共识算法(可线性化)**

  我们本章稍后即将讨论的一些共识算法，与主从复制机制相似。不过共识协议通过内置一些措施来防止**脑裂**和**过期的副本**。正是由于这些专门的设计，共识算法可以安全地实现线性化存储，<u>这些系统包括ZooKeeper和etcd等</u>。

+ 多主复制(**不可线性化**)

  <u>具有多主节点复制的系统通常无法线性化的，主要由于它们同时在多个节点上执行并发写入，并将数据异步复制到其他节点</u>。因此它们可能会产生冲突的写入需要额外的解决方案(参阅第5章的“处理写冲突”)。这类冲突其实正是多副本所引入的结果。

+ 无主复制(可能不可线性化)

  对于无主节点复制的系统(即 Dynamo风格，参阅第5章的“无主节点复制”)，有些人认为只要配置法定读取和写入满足(w+r>n)就可以获得“强一致性”。但这完全取决于具体的 quorum的配置，以及如何定义强一致性，它可能井不保证线性化。

  <u>例如基于**墙上时钟**(包括Cassandra，参阅第8章“依赖于同步的时钟”)的“最后写入获胜”冲突解决方法几乎肯定是非线性化，因为这种时间戳无法保证与实际事件顺序一致(例如由于时钟偏移)</u>。不规范的 quorum(参阅第5章“宽松的quorum与数据回传”)也会破坏线性化。**甚至即使是严格的quorum，正如之后即将介绍的，也会发生违背线性化的情况。**

#### 线性化与quorum

​	直觉上，对于 Dynamo风格的复制模型，如果读写遵从了严格 quorum，应该是可线性化的。然而如果遭遇不确定的网络延迟，就会出现竞争条件，如图9-6所示。

![线性一致性 - 图6](https://static.sitestack.cn/projects/ddia/img/fig9-6.png)

​	图9-6中，x的初始值为0，写客户端向所有三个副本(n=3，w=3)发送写请求将x更新为1。与此同时，客户端A从两个节点(r=2)读取数据，然后在其中一个节点上看到新值1。与此同时，客户端B从两个节点的读取，两者在都返回了旧值0。

​	我们发现它虽然满足了仲裁条件(w+r>n)，但很明显这不是线性化的：B的请求在A的请求完成之后才开始，A返回了新值，但B却得到了旧值。这又类似图9-1中Alice和Bob的情况。

​	<u>有趣的是，可以使 Dynamo风格的复制系统以牺牲性能为代价来满足线性化：读操作在返回结果给应用之前，必须同步执行**读修复**(参阅第5章“读修复与反熵”)；而写操作在发送结果之前，必须读取quorum节点以获取最新值</u>。然而，由于会显著降低性能，Riak并不支持同步读修复；Cassandra确实会等待读修复完成，但是，它使用了“最后写入获胜”冲突解决方案，当出现同一个主键的并发写入时，就会丧失线性化。

​	**此外，这种方式只能实现<u>线性化读、写操作</u>，但<u>无法支持线性化的“比较和设置”</u>操作，后者需要共识算法的支持**。

​	总而言之，最安全的假定是类似Dynamo风格的无主复制系统无法保证线性化。

#### 线性化的代价

​	由于有一部分复制方案能够保证线性化，而其他则无法保证，因此有必要更加深入地探讨线性化的优缺点。

​	在第五章我们已经讨论了不同复制方案各自适合的场景。例如，多主复制非常适合多数据中心(参阅第5章“多数据中心操作”)。图9-7给出了这样的部署例子。

![线性一致性 - 图7](https://static.sitestack.cn/projects/ddia/img/fig9-7.png)

​	如果两个数据中心之间发生网络中断，会发生什么情况？我们假设每个数据中心内的网络工作正常，客户端可以到达就近的数据中心，但数据中心之间却无法互连。

​	基于多主复制的数据库，每个数据中心内都可以继续正常运行：由于从一个数据中心到另一个数据中心的复制是异步，期间发生的写操作都暂存在本地队列，等网络恢复之后再继续同步。

​	与之对比，如果是主从复制，则主节点肯定位于其中的某一个数据中心。所有写请求和线性化读取都必须发送给主节点，因此，对于那些连接到非主节点所在数据中心的客户端，读写请求都必须通过数据中心之间的网络，同步发送到主节点所在的数据中。

​	因此，<u>对于这样的主从复制系统，数据中心之间的网络一旦中断，连接到从数据中心的客户端无法再联系上主节点，也就无法完成任何数据库写入和线性化读取</u>。从节点可以提供读服务，但內容可能是过期的(非线性化保证)。所以，如果应用程序要求线性化读写，则网络中断一定会违背这样的要求。

​	另一种情况，<u>如果客户端可以直接连接到主节点所在的数据中心，则可以避免此问题。否则，只能等到数据中心之间的网络恢复之后才能继续正常工作</u>。

#### * CAP理论

​	**不仅仅是主从复制和多主复制才有上面的问题，无论如何实现，任何可线性化的数据库都有这样问题**；事实上，这个问题也不局限于多数据中心部署的情况，<u>即使在一个数据中心内部，只要有不可靠的网络，都会发生违背线性化的风险</u>。我们可以做以下的权衡考虑：

+ **如果应用要求线性化，但由于网络方面的问题，某些副本与其他副本断开连接之后无法继续处理请求，就必须等待网络修复，或者直接返回错误。无论哪种方式，结果是服务不可用**。
+ **如果应用不要求线性化，那么断开连接之后，每个副本可独立处理请求，例如写操作(多主复制)。此时，服务可用，但结果行为不符合线性化**。

​	因此，**不要求线性化的应用更能容忍网络故障**。<u>这种思路通常被称为CAP定理</u>，它由 Eric Brewer于2000年正式命名，但基本想法可追溯到20世纪70年代，很多分布式数据库设计者当时都已经注意到这种现象。

​	CAP最初是作为一个经验法则而提出的，并没有准确的定义，目的也只是帮助大家深入探讨数据库设计的权衡之道。当时，许多分布式数据库仍热衷于在集中共享的存储集群上提供可线性化的语义，而CAP则事实上在鼓励大家去探索无共享系统，后者更适合于大规模的Web服务，拥有更广阔的发展前景。CAP确实值得称赞，正是由于这种思路的转变，我们才见证了2000年以来新型数据库技术爆炸式的增长(可以概括为NoSQL系统)。

> CAP理论是否有用？
>
> **CAP有时也代表一致性，可用性，分区容错性**，系统只能支持其中两个特性。不过，这种理解存在误导性，<u>网络分区是一种故障，不管喜欢还是不喜欢，它都可能发生，所以无法选择或逃避分区的问题</u>。
>
> **在网络正常的时候，系统可以同时保证一致性(线性化)和可用性。而一旦发生了网络故障，必须要么选择线性(一致性)，要么可用性**。<u>因此，更准确的称呼应该是“网络分区情况下，选择一致还是可用”</u>。高可靠的网络会帮助减少发生的概率，但无法做到彻底避免。
>
> 有必要指出，在CAP的诸多讨论中，术语可用性存在争议，其形式化定理中的可用性与通常意义上的理解有些差别。许多所谓的“高可用性”(容错)系统实际上并不符合CAP对可用性的特殊定义。总之，围绕着CAP有太多的误解与困扰，最后反而无法帮助我们更好地理解系统，所以本人建议最好避免使用CAP。

​	**正式定义的CAP定理范围很窄，它只考虑了一种一致性模型(即线性化)和一种故障(网络分区，节点仍处于活动状态但相互断开)，而没有考虑网络延迟、节点失败或其他需要折中的情况**。<u>因此，尽管CAP在历史上具有重大的影响力，但对于一个具体的系统设计来说，它可能没有太大的实际价值</u>。

​	分布式系统中还有很多有趣的研究结果，目前CAP已被更精确的研究成果所取代，所以它现在更多的是代表历史上曾经的一个关注热点而已。

#### * 可线性化与网络延迟

​	**虽然线性化是个很有用的保证，但实际上很少有系统真正满足线性化**。例如，**现代多核CPU上的内存甚至就是非线性化**：如果某个CPU核上运行的线程修改一个内存地址，紧接着另一个CPU核上的线程尝试读取，则系统无法保证可以读到刚刚写入的值，<u>除非使用了内存屏障或fence指令</u>。

​	<u>出现这种现象的原因是每个CPU核都有自己独立的 cache和寄存器。内存访问首先进入 cache系统，所有修改默认会**异步**地刷新到主存</u>。由于访问 cache比访问主存要快得多，所以这样的异步刷新特性对于现代CPU的性能至关重要。但是，这就导致出现了多个数据副本(一个在主存，另外几个在不同级别的cache中)，而副本更新是异步方式，无法保证线性化。

​	为什么这样呢？首先，**CAP理论不适用于当今的多核-内存一致性模型**：在计算机内部，我们通常假设通信是可靠的，例如我们不会假定一个CPU核在与其他核断开之后还能安然工作。**之所以放弃线性化的原因就是性能，而不是为了容错**。

​	**许多分布式数据库也是类似，它们选择不支持线性化是为了提高性能，而不是为了保住容错特性**。<u>无论是否发生了网络故障，线性化对性能的影响都是巨大的</u>。

​	那我们是否能找到一个更有效的线性化实现方案呢？目前看来答案是否定的。 **Attiya和Welch证明<u>如果想要满足线性化，那么读、写请求的响应时间至少要与网络中延迟成正比</u>**。考虑到多数计算机网络高度不确定的网络延迟(参阅第7章“超时与无限延迟”)，线性化读写的性能势必非常差。虽然没有足够快的线性化算法，但弱一致性模型的性能则快得多，这种取舍对于延迟敏感的系统非常重要。在第12章，我们将讨论一些避免线性化但又可以确保正确性的方法。

> [一文解决内存屏障 - 简书 (jianshu.com)](https://www.jianshu.com/p/64240319ed60)

## 9.3 顺序保证

​	我们之前曾说过，线性化寄存器对外呈现的好像只有一份数据拷贝，而且每一个操作似乎都是原子性生效。这意味着操作是按照某种顺序执行，如图9-4所展示的执行顺序。

​	顺序是本书反复出现的主题，某种程度也表明它确实是一个非常重要的基本概念。让我们简要回顾一下本书讨论顺序时涉及的前后上下文：

+ 在第5章，我们看到<u>主从复制系统中主节点的主要作用是确定复制日志中的写入顺序，这样使从节点遵从相同的顺序执行写入</u>。如果没有这样的唯一主节点，则可能由于并发操作而引发冲突(参阅第5章“处理写冲突”)。
+ 第7章中讨论的<u>可串行化则是确保事务的执行结果与按照某种顺序方式执行一样</u>。实现方式可以是严格顺序执行，或者允许并发但需要相应的冲突解决方案(例如加锁或冲突-中止)。
+ 第8章讨论了分布式系统的时间戳与时钟(参阅第8章“依赖于同步的时钟”)，试图将顺序引入到无序的操作世界，例如确定两个写操作哪一个先发生。

​	事实证明，**排序、可线性化与共识之间存在着某种深刻的联系**。尽管这些概念听起来比本书的其他部分更加理论和抽象，但它对于理解系统能什么和不能儆什么非常有帮助。

### 9.3.1 顺序与因果关系

​	之所以反复出现“顺序”问题，其中的一个原因是它有助于保持因果关系。我们已经有好几个这样的例子来反复说明因果关系的重要性：

+ 在第5章“一致前缀读”(见图5-5)中，我们看到这样的例子，一个对话的观察者首先看到了问题的答案接着才是问题本身。这违背了我们对因果的直觉认识，因而感觉困惑：问题之所以被回答，一定是有问题在先，意味着回答的人一定是先看到了问题(当然，假定他们没有精神问题，也无法预知未来)。此时，问题与回答之间存在因果关系。
+ 图5-9类似，三个主节点之间进行数据复制，由于网络延迟，一些写操作会覆盖其他的写入。从某个副本的角度来看，好像是发生了一个对不存在数据行的更新。这里的因果意味着首先必须先创建数据行，然后才能去更新。
+ 在第5章“检测并发写”，如果有两个操作A和B，则它们之间一共有三种可能性：A发生在B之前，B发生在A之前，或者A和B并发。这种“A发生在B之前”其实是因果关系的另一种表示。如果A发生在B之前，意味着B可能已经知道了A，或者基于A的操作，或者依赖于A。如果A和B是井发关系，则它们之间不存在因果关系；换言之，互相不知道对方。
+ 在事务的快照隔离上下文中(参阅第7章“快照隔离与可重复读”)，事务是从一致性快照读取。这里的“一致性”又是什么含义呢？这意味着与因果关系一致：如果快照中包含了答案，则它也必须包含所提的问题。这样才能确保在某个时间点观察数据库时符合因果关系：快照创建时刻点之前的所有数据都要可见，但此后发生的事件则不可见。<u>读倾斜(例如图7-6的不可重复读取)则违反了因果关系因而读到了本不可见的数据</u>。
+ 事务之间写倾斜的例子(参阅第7章“写倾斜与幻读”)也说明因果关系。在图7-8中， Alice申请调班成功是因为事务以为Bob仍在值班，反之亦然。在这种情况下，调班动作的因果关系取决于当前是谁在值班。<u>可序列化的快照隔离(参阅第7章“可串行化的快照隔离”)主要通过跟踪事务之间的因果依赖关系从而达到检测写倾斜的目的</u>。
+ 在Alice和Bob一起看足球的例子中(见图9-1)，当听到 Alice惊呼比分之后，Bob从服务器却得到过期的结果违背了因果关系。 Alice之所以高呼是因为她先看到了最新的比分，因而道理上讲，Bob也应该看到和 Alice一样的比分。另一个类似例子则是通过两个通道调整图片大小。

​	因果关系对所发生的事件施加了某种排序：发送消息先于收到消息；问题出现在答案之前等，或者就像在现实生活中一样，一件事情会导致另一件事情：一个节点根据读取的数据做出决定，然后写入结果，另一个节点读取写入的结果之后再写入新的内容，诸如此类。这些因果关系的依赖链条定义了系统中的因果顺序，即某件事应该发生另一件事情之前，

​	**如果系统服从因果关系所规定的顺序，我们称之为因果一致性**。例如，快照隔离提供了因果一致性：当从数据库中读数据时，如果查询到了某些数据，也一定能看到触发该数据的前序事件(假设期间没有发生删除操作)。

#### 因果顺序并非全序

​	**全序关系支持任何两个元素之间进行比较，即对于任意两个元素，总是可以指出哪个更大，哪个更小**。例如，自然数符合全序关系，随便给出两个数字比如5和13，都可以进行比较。

​	但是，有些集合并不符合全序，例如集合{a，b}大于集合{b，c}么？因为它们都不是对方的子集，所以无法直接比较它们。**我们称之为不可比较，数学集合只能是偏序**。某些情况下，一个集合可以包含另一个（如果该集合包含另一个集合的所有元素），否则则无法比较。

​	全序和偏序的差异也会体现在不同的数据库一致性模型中：

+ 可线性化

  **在一个可线性化的系统中，存在全序操作关系**。系统的行为就好像只有一个数据副本，且毎个操作都是原子的，这意味着对于任何两个操作，我们总是可以指出哪个操作在先。这种全序排列序如图9-4中的时间线所示。

+ 因果关系

  如果两个操作都没有发生在对方之前，那么这两个操作是并发关系(参阅第5章“Happens-before关系与并发”)。换言之，如果两个事件是因果关系(一个发生在另一个之前)，那么这两个事件可以被排序；而并发的事件则无法排序比较。这表明<u>因果关系至少可以定义为偏序，而非全序</u>。

​	因此，**<u>根据这个定义，在可线性化数据存储中不存在并发操作，一定有一个时间线将所有操作都全序执行</u>。可能存在多个请求处于等待处理的状态，但是数据存储保证了在特定的时间点执行特定的操作，所以是<u>单个时间轴，单个数据副本，没有并发</u>**。

​	并发意味着时间线会出现分支和合并，而不同分支上的操作无法直接比较。第5章中我们给出了这种例子，如图5-14并非一条直线式的全序，而是多个不同的操作同时进行。图中的箭头表明这只是因果关系，即部分操作之间的偏序。

​	如果熟悉像Git这样的分布式版本控制系统，那么它们的版本历史非常类似于因果关系图。通常情况下，提交会以直线形式呈现，但有时会产生分支(特别是多个人同时在一个项目上工作时)，当同时有多个提交时就需要进行合并。

#### * 可线性化强于因果一致性

​	那么因果序和可线性化之间是什么关系呢？答案是**可线性化一定意味着因果关系**：**任何可线性化的系统都将正确地保证因果关系**。特别是，如果系统存在多个通信通道(图9-5中的消息队列和文件存储服务)，<u>可线性化确保了因果关系会自动全部保留，而不需要额外的工作(比如在不同组件之间的传递时间戳)</u>。

​	可线性化可以确保因果性这一结论，使线性化系统更加简单易懂而富有吸引力。但是，正如在“线性化的代价”将要阐述的，<u>线性化会显著降低性能和可用性，尤其是在严重网络延迟的情况下(倒如多数据中心)。面因如此，一些分布式数据系统已经放弃了线性化，以换来更好的性能，但也存在可能无法正确工作的风险</u>。

​	好消息是线性化并非是保证因果关系的唯一途径，还有其他方法使得系统可以满足因果一致性而免于线性化所带来的性能问题。**事实上，因果一致性可以认为是，不会由于网络延迟而显著影响性能，又能对网络故障提供容错的最强的一致性模型**。

​	<u>在许多情况下，许多看似需要线性化的系统实际上真正需要的是因果一致性，后者的实现可以高效很多</u>。基于这样的观察，研究人员正在探索新的数据库来保证因果关系，其性能与可用性特征与最终一致性类似。

​	由于这些研究还处于早期阶段，大多数还没有投入到生产系统，也存在其他一些挑战需要克服。但是，毫无疑问，这是未来非常有希望的方向。

#### * 捕获因果依赖关系

​	我们此处不会深入探讨非线性化系统是如何保证因果一致性的细节，而只是简单地介绍一些关键思想。

​	为保持因果关系，需要知道哪个操作发生在前。这里只需偏序关系，或许并发操作会以任意顺序执行，但如果一个操作发生在另一个操作之前，那么每个副本都应该按照相同的顺序处理。因此，<u>当某个副本在处理一个请求时，必须确保所有因果在前的请求都已完成处理；否则，后面的请求必须等待直到前序操作处理完毕</u>。

​	为了确定请求的因果依赖关系，我们需要一些手段来描述系统中节点所知道的“知识”。如果节点在写入Y时已经看到X值，则X和Y可能是属于因果关系。这种分析使用了类似针对欺诈指控刑事调查中的推理方法：某个CEO在做出决定事项Y时是否已经知道了消息X？

​	<u>确定请求的先后顺序与第五章“检测并发写”中所讨论的技巧类似。后者针对的是无主复制中的因果关系，该场景需要去检测对同一个主键的并发写请求，从而避免更新丢失。因果一致性则要更进一步，它需要跟踪整个数据库请求的因果关系，而不仅仅是针对某个主键。版本向量技术可以推广为一种通用的解决方案</u>。

​	<u>为了确定因果关系，数据库需要知道应用程序读取的是哪个版本的数据</u>。这就是为什么在图5-13中先前读操作的版本号在提交时要传回到数据库。SSI的冲突检测也是类似想法，如第7章“可串行化的快照隔离”所介绍的：当事务提交时，数据库要检査事务曾经读取的数据版本现在是否仍是最新的。为此，数据库需要跟踪事务读取了哪些版本的数据。

### 9.3.2 序列号排序

​	<u>虽然因果关系很重要，但实际上跟踪所有的因果关系不切实际</u>。在许多应用程序中，客户端在写入之前会先读取大量数据，系统无法了解之后的写入究竟是依赖于全部读取内容，还是仅仅是其中一小部分。但很明显，<u>显式跟踪所有已读数据意味着巨大的运行开销</u>。

​	<u>这里还有一个更好的方法：我们可以使用序列号或时间戳来排序事件</u>。时间戳不一定来自墙上时钟(或者物理时钟，但正如第8章所讨论的，物理时钟存在很多问题)。它可以只是一个逻辑时钟，例如采用算法来产生一个数字序列用以识别操作，通常是递增的计数器。

​	这样的序列号或时间戳非常紧凑(只有几字节的大小)，但它们保证了全序关系。也就是说，毎一个操作都有唯一的顺序号，并且总是可以通过比较来确定哪个更大(即操作发生在后)。

​	特别是，我们可以按照与因果关系一致的顺序来创建序列号：保证如果操作A发生在B之前，那么A一定在全序中出现在B之前(即A的序列号更小)。并行操作的序列可能是任意的。这样的全局排序可以捕获所有的因果信息，但也强加了比因果关系更为严格的顺序性。

​	<u>在主从复制数据库中(参阅第5章“主节点与从节点”)，复制日志定义了与因果关系一致的写操作全序关系</u>。主节点可以简单地为每个操作递增某个计数器，从而为复制日志中的毎个操作赋值一个单调递增的序列号。从节点按照复制日志出现的顺序来应用写操作，那结果一定满足因果一致性(虽然从节点的数据可能会滞后于主节点)。

#### 非因果序列发生器

​	如果系统不存在这样唯一的主节点(例如可能是多主或者无主类型的数据库，或者数据库本身是分区的)，如何产生序列号就不是那么简单了。实践中可以采用以下方法：

+ <u>每个节点都独立产生自己的一组序列号</u>。例如，如果有两个节点，则一个节点只生成奇数，而另点只生成偶数。还可以在序列号中保留一些位用于嵌入所属节点的唯一标识符，确保不同的节点永远不会生成相同的序列号。
+ <u>可以把墙上时间戳信息(物理时钟)附加到每个操作上</u>。时间戳可能是不连续的，但是只要它们有足够高的分辨率，就可以用来区分操作。“最后写获胜”的冲突解决方案也使用类似的方法(参阅第8章“时间戳与事件顺序”)。
+ <u>可以预先分配序列号的区间范围</u>。例如，节点A负责区间1~1000的序列号，节点B负责1001~2000。然后每个节点独立地从区间中分配序列号，当序列号出现紧张时就分配更多的区间。

​	上述三种思路都可行，**相比于把所有请求全部压给唯一的主节点**，具有更好的扩展性。它们为毎个操作生成一个唯一的、近似增加的序列号。不过，它们也都存在一个问题：<u>所产生的序列号与因果关系并不严格一致</u>。

​	所有这些序列号发生器都无法保证正确捕获跨节点操作的顺序，因而有在因果关系方面的问题：

+ 每个节点可能有不同的处理速度，如每秒请求数。因此，某个节点产生偶数而另一个产生奇数，偶数的计数器产生速度可能落后于奇数的计数器，反之亦然。这样就无法准确地知道哪个操作在先。
+ 物理时钟的时间戳会受到时钟偏移的影响，也可能导致与实际因果关系不一致。例如图8-3，后来发生的操作实际上被分配了一个较低的时间戳。
+ <u>对于区间分配器，一个操作可能被赋予从1001~2000之间的某个序列号，而后发生的操作则路由到另一个节点，拿到了某个1~1000之间的序列号，导致与因果序不一致</u>。

#### * Lamport时间戳

​	刚才所描述的三个序列号发生器可能与因果关系存在不一致，但还有一个简单的方法可以产生与因果关系一致的序列号。它被称为**兰伯特时间戳(Lamport timestamp)**，由 Leslie Lamport于1978年提出，该文献也是现代分布式系统领域被引用最多的经典论文之一。

​	图9-8给出了 Lamport时间戳的示例。首先每个节点都有一个唯一的标识符，且每个节点都有一个计数器来记录各自已处理的请求总数。 **Lamport时间戳是一个值对(计数器，节点ID)。两个节点可能会有相同的计数器值，但时间戳中还包含节点ID信息，因此可以确保毎个时间戳都是唯一的**。

![顺序保证 - 图1](https://static.sitestack.cn/projects/ddia/img/fig9-8.png)

​	**Lamport时间戳与物理墙上时钟并不存在直接对应关系，但它可以保证全序：给定两个 Lamport时间戳，计数器较大那个时间戳大；如计数器值正好相同，则节点ID越大，时间戳越大。**

​	到目前为止，该思路与上一节所描述的奇数/偶数计数器并无本质不同。但是 Lamport时间戳的核心亮点在于使它们与因果性保持一致，具体如下所示：**每个节点以及每个客户端都跟踪迄今为止所见到的最大计数器值<u>，并在每个请求中附带该最大计数器值</u>。当节点收到某个请求(或者回复)时，<u>如果发现请求内嵌的最大计数器值大于节点自身的计数器值，则它立即把自己的计数器修改为该最大值</u>**。

​	如图9-8所示，客户端A从节点2收到计数器值5，然后将最大值5发送到节点1。此时，节点1的计数器仅为1，但是它立即向前跳到5，所以下一个操作将获得计数器值6。

​	只要把最大计数器值嵌入到每一个请求中，该方案可以确保 Lamport时间戳与因果关系一致，而请求的因果依赖性一定会保证后发生的请求得到更大的时间戳。

​	Lamport时间戳有时会与版本向量发生混淆(第5章“检测并发写”中介绍了版本向量)。虽然存在一些相似之处，但它们的目的不同：**版本向量用以区分两个操作是并发还是因果依赖，而 Lamport时间戳则主要用于确保全序关系**。**<u>即使 Lamport时间戳与因果序一致，但根据其全序关系却无法区分两个操作属于并发关系，还是因果依赖关系</u>。 Lamport时间戳优于版本向量之处在于它更加紧凑和高效**。

#### * 时间戳排序依然不够

​	虽然 Lamport时间戳定义了与因果序一致的全序关系，但还不足以解决实际分布式系统中许多常见的问题。

​	例如，一个账户系统需要确保用户名唯一标识用户。即两个用户如果同时尝试使用相同的用户名创建账户时，确保其中一个成功，另一个必须失败。

​	乍看之下，似乎全序关系(例如使用 Lamport时间戳)应该可以解决问题：如果有这样并发的操作，则选择时间戳较低的那个作为获胜者(先申请用户名的那个请求)，而让时间戳大的请求失败。由于时间戳有序，所以这样的比较方法也应该是可行的。

​	但是，**这种方法确定胜利者有这样一个前提条件：需要收集系统中所有的用户创建请求，然后才可以比较它们的时间戳**。然而，当节点刚刚收到用户的创建请求时，它无法当时就做出决定该请求应该成功还是失败。此时，节点根本不知道是否有另一个节点在同时创建相同用户名(以及那个请求所附带的时间戳)。

​	而为了获得上述两点信息，系统就必须检查每个节点，询问它们在做什么。如果万一某个节点出现故障或者由于网络问题而无法连接，那么方法就无法正常运转。显然这不是我们所期望的容错系统。

​	**这里问题的关键是，只有在收集了所有的请求信息之后，才能清楚这些请求之间的全序关系**。如果另一个节点执行了某些操作，但你无法知道那是什么，就无法构造出最终的请求序列。也许，来自该未知操作确实需要插入到全序集合中才能正确评估出下一步。

​	<u>总而言之，为了实现像用户名唯一性约束这样的目标，仅仅对操作进行全序排列还是不够的，还需要知道这些操作是否发生、何时确定等</u>。假如能够在创建用户名时，已经确定知道了没有其他节点正在执行相同用户名的创建，你大可以直接安全返回创建成功。

​	**要想知道什么时候全序关系已经确定就需要之后的“全序关系广播”**。

### * 9.3.3 全序关系广播

​	如果程序只运行在一个CPU核上，可以非常简单地定义出操作的全序关系，即在单核上执行的顺序。但是，在分布式系统中，让所有的节点就全序关系达成一致就面临巨大挑战。在前面一节中，我们讨论了按时间戳或序列号排序，发现它不如主从复制那么直接有效(如果使用时间戳排序来实现唯一性约束，会丧失容错性)。

​	如前所述，<u>主从复制首先确定某一个节点作为主节点，然后在主节点上顺序执行操作。接下来的主要挑战在于，如何扩展系统的吞吐量使之突破单一主节点的限制，以及如何处理主节点失效时的故障切换(参阅第5章“处理节点失效”)。在分布式系统研究文献中，这些问题被称为**全序关系广播**或者**原子广播**。</u>

> 顺序保证的范围
>
> <u>每个分区只有一个主节点的数据库通常只会维护分区内的顺序，即它们不提供跨分区的一致性保证(例如，一致性快照，外键引用等)</u>。跨分区的全序关系并非不可能，但需要非常多的额外工作。

​	<u>全序关系广播通常指节点之间交换消息的某种协议。下面是一个非正式的定义，它要求满足两个基本安全属性</u>：

+ **可靠发送**

  **没有消息丢失，如果消息发送到了某一个节点，则它一定要发送到所有节点**。

+ **严格有序**

  **消息总是以相同的顺序发送给每个节点**。

​	**即使节点或网络出现了故障，全序关系广播算法的正确实现也必须保证上述两条。当然，网络中断时是不可能发送成功的，但算法要继续重试，直到最终网络修复，消息发送成功(且必须以正确的顺序发送)。**

#### * 使用全序关系广播

​	<u>像 ZooKeeper和etcd这样的**共识**服务实际上就实现了**全关系序广播**</u>。这也暗示了全序关系广播与共识之间有着密切联系，本章稍后会揭示这一点。

​	全序关系广播正是数据库复制所需要的：如果每条消息代表数据库写请求，并且每个副本都按相同的顺序处理这些写请求，那么所有副本可以保持一致(或许有些滞后)。该原则也被称为**状态机复制**，我们将在第11章中详细介绍。

​	**可以使用全序关系广播来实现可串行化事务**。<u>如第7章“实际串行执行”所述，如果毎条消息表示一个确定性事务并且作为存储过程来执行，且每个节点都遵从相同的执行顺序，那么可以保证数据库各分区以及各副本之间的一致性。</u>

​	<u>全序关系广播另一个要点是**顺序在发送消息时已经确定**，如果消息发送成功，节点不允许追溯地将某条消息插入到先前的某个位置上</u>。这一点使得全序关系广播比基于时间戳排序要求更强。

​	<u>理解全序关系广播的另一种方式是将其视为日志(如复制日志，事务日志或预写日志)。传递消息就像追加方式更新日志。由于所有节点必须以相同的顺序发送消息，因此所有节点都可以读取日志并看到相同的消息序列</u>。

​	全序关系广播对于提供**fencing令牌**的**锁**服务也很有用(参阅第8章的“Fencing令牌”)。<u>每个获取锁的请求都作为消息附加到日志中，所有消息按照日志中的顺序依次编号。序列号还可以作为令牌，它符合单调递增要求。在 ZooKeeper中，该序列号被称为zxid</u>。

#### * 采用全序关系广播实现线性化存储

​	如图9-4所示，在一个可线性化的系统中有全序操作集合。这是否意味着可线性化与全序关系广播是完全相同呢？不完全是，但两者之间有着密切的联系。

+ **全序关系广播是基于异步模型：保证消息以固定的顺序可靠地发送，但是不保证消息何时发送成功(因此某个接收者可能明显落后于其他接收者)。**
+ **而可线性化则强调就近性：读取时保证能够看到最新的写入值**。

​	<u>如果有了全序关系广播，就可以在其上构建线性化的存储系统</u>。例如，确保用户名唯标识一个用户。

​	设想一下，对于毎一个可能的用户名，都可以有一个带有原子比较-设置操作的线性化寄存器。每个寄存器初始值为空(表示尚未使用)。当用户创建一个用户名时，对该用户名的寄存器执行比较设置操作：仅当寄存器值为空时，将其设置为新的用户账号。如果多个用户试图同时获取相同的用户名，则只有一个原子比较设置操作成功。

​	<u>可以通过使用全序关系广播以**追加日志**的方式来实现**线性化**的**原子比较-设置**操作</u>，步骤如下所示：

1. 在日志中追加一条消息，并指明想要的用户名。
2. 读取日志，将其广播给所有节点，并等待回复。
3. <u>检查是否有任何消息声称该用户名已被占用。如果第一条这样的回复来自于当前节点，那么就成功获得该用户名，可以提交该获取声明(也许附加另一条消息到日志)并返回给客户端。反之，如果声称占用的第一条回复消息来自其他节点，则中止操作</u>。

​	<u>由于日志条目以相同的顺序发送到所有节点，而如果存在多个并发写入，则所有节点将首先决定哪个请求在先。**选择第一个写请求作为获胜者，并中止其他请求，以确保所有节点同意一个写请求最终要么提交成功要么中止**。类似的方法还可以用来在日志之上实现可串行化的多对象事务</u>。

​	**虽然此过程可确保线性化写入，但它却无法保证线性化读取，即从异步日志更新的存储中读取数据时，可能是旧值**。<u>具体来说，这里只提供了**顺序一致性**，有时也称为**时间线一致性**，它弱于线性化保证</u>。为了同时满足线性化读取，有以下几个方案：

+ <u>可以采用追加的方式把读请求排序、广播，然后各个节点获取该日志，当本节点收到消息时才执行真正的读操作。消息在日志中的位置已经决定了读取发生的时间点。etcd的quorum读取和这个思路有相似之处</u>。
+ <u>如果可以以线性化的方式获取当前最新日志中消息的位置，则查询位置，等待直到该位置之前的所有条目都已经发送给你，接下来再执行读取。这与 ZooKeeper的`sync()`操作思路相同</u>。
+ <u>可以从同步更新的副本上进行读取，这样确保总是读取最新值</u>。这种技术可以用于链式复制，具体参阅第5章的“复制研究”。

#### * 采用线性化存储实现全序关系广播

​	前面一节介绍了如何基于全序关系广播来构建线性化的原子比较-设置操作。我们也可以反过来，假定已有了线性化的存储，在其上构建全序关系广播。

​	最简单的方法是假设有一个线性化的寄存器来存储一个计数，然后使其支持原子自增读取操作或者原子比较设置操作。

​	**算法思路很简单：对于每个要通过全序关系广播的消息，原子递增并读取该线性化的计数，然后将其作为序列号附加到消息中。接下来，将消息广播到所有节点(如果发生丢失，则重新发送)，而接受者也严格按照序列化来发送回复消息**。

​	请注意，与 Lamport 时间戳不同，**通过递增线性化寄存器获得的数字不会存在任何间隙**。因此，如果节点完成了消息4的发送，且接收到了序列化6的消息，那么在它对消息6回复之前必须等待消息5。 <u>Lamport时间戳则不是这样，而这也是区别全序关系广播与基于时间戳排序的关键。</u>

​	<u>使用原子自增操作来创建线性化整数有多难呢？答案还是那样，如果不存在失效，就非常容易，甚至可以把它保存在某个节点的内存变量中。难点在于处理节点的网络中断，以及节点失效时如何恢复该值</u>。**事实上，如果对线性化的序列号发生器深思熟虑之后所得到的最终结果，往往亳无意外地指向了共识算法**。

​	**这并非巧合，可以证明，线性化的原子比较-设置(或自增)寄存器与全序关系广播二者都等价于共识问题**。也就是说，如果你能解决其中的一个问题，那么就可以把方案用于解决其他问题。这样的结论是多么的深刻和震撼!

​	好了，现在终于是时候正面处理共识问题了，这是本章剩余部分的重点。

## 9.4 分布式事务与共识

​	**共识问题是分布式计算中最重要也是最基本的问题之一**。表面上看，目标只是让几个节点就某件事情达成一致。这似乎很简单，或者至少不应该太难。不幸的是，许多失败的系统正是由于低估了这个问题所导致的。

​	共识问题既然这么重要，本书已过大半，我们才正式揭开其面纱似乎有点姗姗来迟，这主要是因为共识主题非常之艰深，欣赏其精妙之处需要很多准备知识。即使在学术界，对共识的深刻认识也只是最近几十年才逐渐形成的，中间还纠缠着很多的误解。在探讨了复制(第5章)，事务(第7章)，系统模型(第8章)，线性化和全序关系广播(本章)等问题之后，现在终于完成必要的准备，可以开始直面共识问题了。

​	有很多重要的场景都需要集群节点达成某种一致，例如：

+ **主节点选举**

  对于主从复制的数据库，所有节点需要就谁来充当主节点达成一致。如果由于网络故障原因出现，节点之间无法通信，就很容易出现争议。此时，共识对于避免错误的故障切换非常重要，后者会导致两个节点都自认为是主节点即**脑裂**(参阅第5章“处理节点失效”)。<u>如果集群中存在两个这样的主节点，每个都在接受写请求，最终会导致数据产生分歧、不一致甚至数据丢失</u>。

+ **原子事务提交**

  对于支持跨节点或跨分区事务的数据库，会面临这样的问题：某个事务可能在一些节点上执行成功，但在其他节点却不幸发生了失败。<u>为了维护事务的原子性(即ACID，参阅第7章“原子性”)，所有节点必须对事务的结果达成一致：要么全部成功提交(假定没有出错)，要么中止/回滚(如果出现了错误)。这个共识的例子被称为原子提交问题</u>。

> 共识的不可能性
>
> 或许你可能听说过FLP结论，其字母源于三位作者 Fischer， Lynch和Paterson。**FLP表眀如果节点存在可能崩溃的风险，则不存在总是能够达成共识的稳定算法**。在分布式系统中，我们必须假设节点可能会崩溃，我们在讨论如何达成共识，而这里又说可靠的共识无法实现。到底怎么回事呢？
>
> 答案是FLP结论是基于**异步**系统模型而做的证明(请参阅第8章“理论系统糢型与现实”)，这是一个非常受限的模型，它假定确定性算法都不能使用任何时钟或超时机制。**如果算法可以使用超时或其他方法来检测崩溃节点(即使怀疑可能是误报)，那么可以实现稳定的共识方案。另外，即使算法使用了随机数来检测节点故障也可以绕过FLP结论**。
>
> 因此，FLP结论有其重要的理论意义，但对于实际的分布式系统通常达成共识是可行的。

​	本节我们将首先详细研究原子提交问题。具体来讲，我们将集中于<u>**两阶段提交(2PC)算法**，这是解决原子提交最常见的方法，在各种数据库、消息系统和应用服务器中都有实现</u>。事实证明，2PC是一种共识算法，虽然谈不上多么优秀。

​	2PC学习之后，我们将继续探索更好的共识算法实现，比如Zookeeper(Zab)和etcd(Raft)所使用的算法。

### * 9.4.1 原子提交与两阶段提交

​	第7章我们了解到事务原子性的目的是，当一个包含多笔写操作的事务在执行过程出现任何意外，原子性可以为上层应用提供非常简单的语义：事务的结果要么是成功提交(所有事务的写入都是持久的)，要么是中止(此时所有事务的写入都被回滚，即撤销或者丢弃)。

​	原子性可以防止失败的事务破坏系统，避免形成部分成功夹杂着部分失败。这对于多对象事务(参阅第7章“单对象与多对象事务操作”)和维护二级索引格外重要。每个二级索引都有与主数据不同的数据结构，因此，如果修改了某些数据，则相应的二级索引也需要随之更新<u>。原子性可以确保二级索引与主数据总是保持一致(如果发生了不一致，那么索引的作用将会大打折扣)</u>。

#### * 从单节点到分布式的原子提交

​	<u>对于在单个数据库节点上执行的事务，原子性通常由存储引擎来负责。当客户端请求数据库节点提交事务时，数据库首先使事务的写入持久化(通常保存在预写日志中，请参阅第3章“可靠的B-tree”)，然后把提交记录追加写入到磁盘的日志文件中。如果数据库在该过程中间发生了崩溃，那么当节点重启后，事务可以从日志中恢复：如果在崩溃之前提交记录已成功写入磁盘，则认为事务已安全提交；否则，回滚该事务的所有写入</u>。

​	因此，**在单节点上，事务提交非常依赖于数据持久写入磁盘的顺序关系：先写入数据，然后再提交记录**。<u>事务提交(或中止)的关键点在于磁盘完成日志记录的时刻：在完成日志记录写之前如果发生了崩溃，则事务需要中止；如果在日志写入完成之后，即使发生崩溃，事务也被安全提交。这就是在单一设备上(某个特定的磁盘连接到一个特定的节点)上实现原子提交的核心思路</u>。

​	但是，如果一个事务涉及多个节点呢？例如，一个分区数据库中多对象事务，或者是基于词条分区的二级索引(其中索引条目可能位于与主数据不同的节点上，请参阅第6章“分区与二级索引”)。**虽然大多数NoSQL分布式数据都不支持这样的分布式事务，但有很多集群关系数据库则支持(参阅本章后面“实践中的分布式事务”)**。

​	<u>向所有节点简单地发送一个提交请求，然后各个节点独立执行事务提交是绝对不够的。这样做很容易发生部分节点提交成功，而其他一些节点发生失败，从而违反了原子性保证</u>：

+ 某些节点可能会检测到违反约束或有冲突，因而决定中止，而其他节点则可能成功提交。
+ 某些提交请求可能在网络中丟失，最终由于超时而中止，而其他提交请求则顺利通过。
+ 某些节点可能在日志记录写入之前发生崩溃，然后在恢复时回滚，而其他节点则成功提交。

​	如果一部分节点提交了事务，而其他节点却放弃了事务，节点之间就会变得不一致(见图7-3)。**而且某个节点一旦提交了事务，即使事后发现其他节点发生中止，它也无法再撤销已提交的事务**。正因如此，如果有部分节点提交了事务，则所有节点也必须跟着提交事务。

​	<u>事务提交不可撤销，不能事后再改变主意(在提交之后再追溯去中止)。这些规则背后的深层原因是，一旦数据提交，就被其他事务可见，继而其他客户端会基于此做出相应的快策。这个原则构成了读-提交隔离级別的基础(参阅第7章“读-提交”)。**如果允许事务在提交后还能中止，会违背之后所有读-提交的事务，进而被迫产生级联式的追溯和撤销**</u>。

​	<u>当然已提交事务的效果可以被之后一笔新的事务来抵消掉，即**补偿性事务**。不过，从数据库的角度来看，前后两个事务完全互相独立。类似这种跨事务的正确性需要由应用层来负责</u>。

#### * 两阶段提交

​	**两阶段提交(two-phase commit，2PC)是一种在多节点之间实现事务原子提交的算法，用来<u>确保所有节点要么全部提交，要么全部中止</u>**。它是分布式数据库中的经典算法之一。2PC在某些数据库内部使用，或者以XA事务形式(例如Java Transaction API)或 SOAP Web服务WS-AtomicTransaction的形式提供给应用程序。

​	2PC的基本流程如图9-9所示。不同于单节点上的请求提交，2PC中的**提交/中止**过程分为两个阶段(因此得名2PC)。

![分布式事务与共识 - 图1](https://static.sitestack.cn/projects/ddia/img/fig9-9.png)

> 不要混淆2PC和2PL
>
> 两阶段提交(2PC)和两阶段加锁(参阅第7章“两阶段加锁”)是两个完全不同的事情。
>
> + **2PC在分布式数据库中负责原子提交，**
> + **而2PL则提供可串行化的隔离**。
>
> 为避免混淆，最好将它们视为完全独立的概念，并忽略名称中那一点相似性。

​	2PC引入了单节点事务所没有的一个新组件：**协调者(也称为事务管理器)**。<u>协调者通常实现为共享库，运行在请求事务相同进程中(例如嵌入在 Java EE容器中)，但也可以是单独的进程或服务</u>。常见协调者的例子包括 Narayana，JOTM，BTM或MSDTC。

​	通常，2PC事务从应用程序在多个数据库节点上执行数据读/写开始。我们将这些数据库节点称为事务中的参与者。当应用程序准备提交事务时，<u>协调者开始阶段1：发送一个准备请求到所有节点，询问他们是否可以提交。协调者然后跟踪参与者的回应</u>：

+ 如果所有参与者回答“是”，表示他们已准备好提交，那么协调者接下来在阶段2会发出提交请求，提交开始实际执行。

+ 如果有任何参与者回复“否”，则协调者在阶段2中向所有节点发送放弃请求。

​	这个过程有点像西方传统的婚姻仪式：主持人会询问新郎和新娘是否愿意与对方结为夫妇，通常双方都会回答“我愿意”，在确认之后和所有与会者共同见证下，方可宜布完成了婚姻承诺。而万一新郎或者新娘没有肯定回复，理论上仪式应该中止。

#### * 系统的承诺

​	只从上面简单的描述可能还是不清楚为什么两阶段提交可以确保跨节点的原子性，而单一提交却做不到。试想，即使对于2PC，准备和提交请求也一样可能发生丢失，那么2PC究竟为何不同？

​	为了理解其工作原理，我们来更详细地分解这个过程：

1. <u>当应用程序启动一个分布式事务时，它首先向协调者请求事务ID。**该ID全局唯一**</u>。
2. 应用程序在每个参与节点上执行**单节点事务**，并将**全局唯一事务ID**附加到事务上。此时，读写都是在单节点内完成。如果在这个阶段出现问题(例如节点崩溃或请求超时)，则协调者和其他参与者都可以安全中止。
3. **当应用程序淮备提交时，协调者向所有参与者发送准备请求，并附带全局事务ID。如果准备请求有任何一个发生失败或者超时，则协调者会通知所有参与者放弃事务**。
4. 当参与者在收到准备请求之后，确保在任何情况下都可以提交事务，包括安全地将事务数据写入磁盘(不能以任何借口稍后拒绝提交，包括系统崩溃，电源故障或磁盘空间不足等)，并检査是否存在冲突或约束违规。**一旦向协调者回答“是”，节点就承诺会提交事务。换句话说，尽管还没有真正提交，但参与者已表态此后不会行使放弃事务的权利**。
5. 当协调者收到所有准备请求的答复时，就是否提交(或放弃)事务要做出明确的决定(即只有所有参与者都投赞成票时才会提交)。<u>**协调者把最后的决定写入到磁盘的事务日志中**，防止稍后系统崩溃，并可以恢复之前的决定。这个时刻称为**提交点**</u>。
6. <u>协调者的决定写入磁盘之后，接下来向所有参与者发送提交(或放弃)请求。**如果此请求出现失败或超时，则协调者必须一直重试，直到成功为止**</u>。**此时，所有节点不允许有任何反悔：开弓没有回头箭，一旦做了决定，就必须贯彻执行，即使需要很多次重试。而如果有参与者在此期间出现故障，在其恢复之后，也必须继续执行。这是因为之前参与者都投票选择了“是”，对于做出的承诺同样没有反悔的余地**。

​	由此可见，该协议有两个关键的“不归路”：首先，当参与者投票“是”时，它做出了肯定提交的承诺(尽管还取决于其他的参与者的投票，协调者才能做出最后决策)。其次，协调者做出了提交(或者放弃)的决定，这个决定也是不可撤销。正是这两个承诺确保了2PC的原子性(而单节点原子提交其实是将两个事件合二为一，<u>写入事务日志即提交</u>)。

​	回到婚姻的例子里，在说“我愿意”之前，新娘/新郎都有“放弃”承诺的自由，比如说“我不愿意!”。而在做岀肯定的承诺之后，就不能随便撤销。假如在说了“我愿意”之后不巧晕倒在地，即使他/她没有听到“你们现在已结为夫妻”的证词，也不会因此就能改变事实。当稍后恢复了意识，可以询问证婚人(通过事务ID状态)是否已完成婚礼，或者等待证婚人安排下一次仪式(即重试将一直持续下去)。

#### * 协调者发生故障

​	如果参与者或者网络在2PC期间发生失败，例如在第一阶段，任何一个准备请求发生了失败或者超时，那么协调者就会决定中止交易；或者在第二阶段发生提交(或中止)请求失败，则协调者将无限期重试。但是，如果协调者本身发生了故障，接下来会发生什么现在还不太清楚。

​	<u>如果协调者在发送准备请求之前就已失败，则参与者可以安全地中止交易。但是一旦参与者收到了准备请求并做了投票“是”，则参与者不能单方面放弃，它必须等待协调者的决定。如果在决定到达之前，出现协调者崩溃或网络故障，则参与者只能无奈等待。此时参与者处在一种不确定的状态</u>。

​	情况如图9-10所示。在该例子中，协调者实际上做出了提交决定，数据库2已经收到了提交请求。但是，协调者在将提交请求发送到数据库1之前发生了崩溃，因此数据库1不知道该提交还是中止。超时机制也无法解决问题：如果超时之后数据库1决定单方面中止，最终将与完成提交的数据库2产生不一致。同理，参与者也不能单方面决定提交，因为可能有些参与者投了否决票导致协调者最终的决定是放弃。

​	<u>没有协调者的消息，参与者无法知道下一步的行动(是提交还是放弃)。理论上，参与者之间可以互相通信，通过了解毎个参与者的投票情况并最终达成一致，不过这已经不是2PC协议的范畴了</u>。

![分布式事务与共识 - 图2](https://static.sitestack.cn/projects/ddia/img/fig9-10.png)

​	**2PC能够顺利完成的唯一方法是等待协调者恢复。这就是为什么协调者必须在向参与者发送提交(或中止)请求之前要将决定写入磁盘的事务日志：等协调者恢复之后，通过读取事务日志来确定所有未决的事务状态。如果在协调者日志中没有完成提交记录就会中止。此时，2PC的提交点现在归结为协调者在常规单节点上的原子提交**。

#### * 三阶段提交

​	两阶段提交也被称为阻塞式原子提交协议，因为2PC可能在等待协调者恢复时卡住。理论上，可以使其改进为非阻塞式从而避免这种情况。但是，实践中要想做到这一点并不容易。

​	作为2PC的替代方案，目前也有三阶段提交算法。**然而，3PC假定一个有界的网络延迟和节点在规定时间内响应。考虑到目前大多数具有无限网络延迟和进程暂停的实际情况(见第8章)，它无法保证原子性**。

​	<u>通常，非阻塞原子提交依赖于一个完美的故障检测器，即有一个非常可靠的机制可以判断出节点是否已经崩溃。在无限延迟的网络环境中，超时机制并不是可靠的故障检测器，因为即使节点正常，请求也可能由于网络问题而最终超时。正是由于这样的原因，尽管大家已经意识到上述协调者潜在的问题，但还在普遍使用2PC</u>。

### * 9.4.2 实践中的分布式事务

​	分布式事务，尤其是那些通过两阶段提交所实现的事务，声誉混杂。一方面，它们被看作是提供了一个其他方案难以企及的重要的安全保证；但另一方面，他们由于操作上的缺陷、性能问题、承诺不可靠等问题而遭受诟病。<u>目前，许多云服务提供商由于运维方面的问题而决定不支持分布式事务</u>。

​	分布式事务的某些实现存在严重的性能问题。例如，有报告显示 MySQL的分布式事务比单节点事务慢10倍以上，所以不建议使用也就不足为奇了。**两阶段提交性能下降的主要原因是为了防崩溃恢复而做的磁盘IO( fsync)以及额外的网络往返开销**。

​	但是，我们不应该就这么直接地抛弃分布式事务，而应该更加审慎的对待，从中获取些重要的经验教训。首先，我们还是要明确“分布式事务”的确切含义。目前由两种截然不同的分布式事务概念：

+ 数据库内部的分布式事务

  某些分布式数据库(例如那些标配支持复制和分区的数据库)支持跨数据库节点的内部事务。例如，ⅤoltDB和 MySQL Cluster的NDB存储引擎就支持这样的内部分布式事务。此时，所有参与节点都运行着相同的数据库软件。

+ 异构分布式事务

  在异构分布式事务中，存在两种或两种以上不同的参与者实现技术。例如来自不同供应商的数据库，甚至是非数据库系统(如消息中间件)。即使是完全不同的系统，跨系统的分布式事务业必须确保原子提交。

​	<u>数据库内部事务由于不必考虑与其他系统的兼容，因此可以使用任何形式的内部协议并采取有针对性的优化。因此，数据库内部的分布式事务往往可行且工作不错，但异构环境的事务则充满了挑战</u>。

#### * Exactly-once 消息处理

​	异构的分布式事务旨在无缝集成多种不同的系统。例如，当且仅当数据库中处理消息的事务成功提交，消息队列才会标记该消息已处理完毕。这个过程是通过自动提交消息确认和数据库写入来实现的。即使消息系统和数据库两种不同的技术运行在不同的节点上，采用分布式事务也能达到上述目标。

​	<u>如果消息发送或数据库事务任何一个发生失败，则两者都须中止，消息队列可以在稍后**再次重传**消息</u>。因此，**通过自动提交消息和消息处理的结果，可以确保消息可以有效处理有且仅有一次(成功之前有可能需要重试)**。而如果事务最后发生中止，则会放弃所有部分完成的结果。

​	<u>需要指出，只有在所有受影响的系统都使用**相同的原子提交协议**的前提下，这种分布式事务才是可行</u>。例如，如果处理结果之一是发送一封邮件，而邮件服务器却不支持两阶段提交，此时如果某个环节出错需要重试，就会导致邮件系统重复发送两次或更多。但如果假定所有结果或者副作用都可以在事务中止时回滚，就可以安全地重新处理消息，好像之前什么都没发生过一样。

​	在之后第11章我们还会回到消息的恰好一次处理问题。这里我们继续探讨异构环境下分布式事务的原子提交。

#### * XA交易

​	<u>X/Open XA(extended Architecture，XA)是**异构环境**下实施**两阶段提交**的一个工业标准，于1991年推出并得到广泛推广</u>。目前，许多传统关系数据库(包括PostgreSQL、 MySQL、DB2、 SQL Server和 Oracle)和消息队列(包括 ActiveMQ、HornetQ、MSMQ和 IBM MQ)都支持XA。

​	<u>XA并不是一个网络协议，而是一个与事务协调者进行通信的C API</u>。当然，它也支持其他语言的API绑定，例如 Java EE中，XA事务是由Java事务API(Java Transaction APⅠ，JTA)来实现，JTA可以支持非常多JDBC(Java Database Connectivity)驱动和消息队列驱动(通过Java消息服务，JMS)。

​	ⅩA假定应用程序通过网络或客户端的库函数与参与者(包括数据库、消息服务)节点进行通信。如果驱动程序支持ⅩA，意味着应用可以调用XA API来确定操作是否是异构分布式事务的一部分。如果是，则发送必要的信息给数据库服务器。**它还支持回调，这样协调者可以通过回调函数来通知所有参与者执行准备或者提交(或者中止)**。

​	<u>事务协调者需要实现XA API。虽然标准并没有详细要求该如何实现，但实际上，协调者通常也是一个API库，它与产生事务的应用程序运行在相同的进程中。这些API会跟踪事务中所有的参与者，协调节点进行准备(通过回调)工作，然后负责收集参与者的投票，并在本地磁盘的日志文件里记录事务最终的决定</u>。

​	如果应用程序进程发生崩溃，或者所在的节点出现故障，协调者就需要做相应的处理。**此时，所有完成了准备阶段但尚未提交的参与者就会陷入停顿**。<u>由于事务日志保存在应用服务器的本地磁盘上，该节点必须先**重启**，然后协调者通过XA API读取日志、进而恢复事务的决定。完成这些之后，协调者才能继续使用数据库驱动ⅩA回调来要求所有参与者执行提交(或中止)</u>。数据库服务器无法直接与协调者进行通信，而须通过相应的API接口。

#### * 停顿时仍持有锁

​	为什么我们非常关注陷入停顿的参与者节点(即不确定该提交还是中止)呢？难道系统不能选择忽略(并最终清理)这些节点，这样系统不就可以继续工作么?

​	问题的关键在于**锁**。正如第7章“读-提交”所讨论的，数据库事务通常持有待修改行的行级独占锁，用以防止脏写。此外，如果要使用可串行化的隔离，则两阶段锁的数据库还会对事务曾经读取的行持有读-共享锁(参阅第7章“两阶段加锁”)。

​	**在事务提交(或中止)之前，数据库都不会释放这些锁(图9-9中的阴影区域所示)**。因此，在两阶段提交时，事务在整个停顿期间一直持有锁。换句话说，如果协调者崩溃并且需要20分钟才能重启恢复，那么这些对象将被锁定20分钟；**如果协调者的日志由于某种原因而彻底丟失，<u>这些数据对象将永久处于加锁状态</u>，至少管理员采用手动方式解决之前只能如此**。

​	数据处于加锁时，其他事务就无法执行修改。取决于数据库的具体实现，其他事务甚至无法读取这些行。因此，其他的事务事实上无法有效执行。这可能会导致很多上层应用基本处于不可用状态，所以必须解决处于停顿状态的那些事务。

#### * 从协调者故障中恢复

​	理论上，如果协调者崩溃之后重新启动，它应该可以从日志中恢复那些停顿的事务。然而，在实践中，孤立的不确定事务确实会发生。无论何种原因，例如由于软件bug导致交易日志丢失或者损坏，最终协调者还是出现了恢复失败。<u>那些悬而未决的事务无法自动解决，而是永远留在那里，而且还持有锁并阻止其他事务</u>。

​	**即使重启那些处于停顿状态的数据库节点也无法解决这个问题，这是由于<u>2PC的正确实现要求即使发生了重启，也要继续保持重启之前事务的加锁(否则就会违背原子性保证)</u>。所以，这的确非常棘手**。

​	<u>唯一的岀路只能是让管理员**手动**决定究竟是执行提交还是回滚</u>。管理员必须仔细检査每个有问题的参与者，确定是否有节点已经事实完成了提交(或中止)，然后要将相同的结果一一应用于所有的参与者上。这种方案可能需要大量的手工操作，而且很可能处在关键生产环境的中断间隙，背负着巨大的压力和时间限制(要不然，为什么协调者容易出现这种问题)。

​	许多XA的实现都支持某种紧急避险措施称之为**启发式决策**：<u>这样参与者节点可以在紧急情况下单方面做出决定，放弃或者继续那些停顿的事务，而不需要等到协调者发出指令</u>。<u>需要说明的是，这里的**启发式**其实是可能破坏原子性的委婉说法，它的确违背了两阶段提交所做出的承诺。因此，这种**启发式决策只是为了应急，不能作为常规手段来使用**</u>。

#### * 分布式事务的限制

​	XA事务解决了多个参与者之间如何达成一致这样一个非常现实而重要的问题，但正如上面所看到的，它也引入了不少操作方面的限制。特别是，<u>核心的事务协调者本身就是一种数据库(存储事务的投票结果)，因此需要和其他重要的数据库一样格外小心</u>：

+ 如果协调者不支持数据复制，而是在单节点上运行，那么它就是整个系统的单点故障(因为它的故障导致了很多应用阻塞在停顿事务所持有的锁上)。**而现实情况是，有许多协调者的实现默认情况下并非高可用，或者只支持最基本的复制**。
+ 许多服务器端应用程序都倾向于无状态模式(因为更受HTTP的青睐)，而所有的持久状态都保存在数据库中，这样应用服务器可以轻松地添加或删除实例。但是，当协调者就是应用服务器的一部分时，部署方式就发生了根本的变化。突然间，<u>协调者的日志成为可靠系统的重要组成部分，它要求与数据库本身一样重要(需要协调者日志恢复那些有疑问的事务)。**这样的应用服务器已经不再是无状态**</u>。
+ **由于XA需要与各种数据系绕保持兼容，它最终其实是多系统可兼容的最低标准**。例如，它无法深入检测不同系统之间的死锁条件(因为这就将需要另一个标谁化协议，使得多个系统交换事务所等待的锁信息)，而且不适用于SSI(参阅第7章“可串行化的快照隔离”)，后者要求一个复杂的协议来识别不同系统间的写冲突。
+ <u>对于数据库内部的分布式事务(而不是XA)，限制则少很多，例如SSI的分布式版本是可行的</u>。然而，2PC要成功提交事务还是存在潜在的限制，它要求必须所有参与者都投票赞成，如果有任何部分发生故障，整个事务只能失败。所以**分布式事务有扩大事务失败的风险，这与我们构建容错系统的目标有些背道而驰**。

​	这是否意味着我们应该放弃保持多个系统一致的希望呢？不完全是，还有其他的方法可以实现这一目标且无需担心异构分布式事务的这些负面影响，具体将在第11章和第12章呈现。现在，我们应该可以总结一下共识问题。

### * 9.4.3 支持容错的共识

​	**通俗理解，共识是让几个节点就某项提议达成一致**。例如，多个人同时尝试预订飞机的最后一个座位或剧院中的同一座位，或者尝试使用相同的用户名注册账户，此时可以用共识算法来决定这些不相容的操作之中谁是获胜者。

​	**共识问题通常形式化描述如下：一个或多个节点可以提议某些值，由共识算法来决定最终值**。对于预约座位的例子，当多个顾客同时试图购买最后一个座位时，处理顾客请求的每个节点可以提议它所服务的顾客ID，最后的决定则是关于由哪个顾客获得座位。

​	在这个描述中，共识算法必须满足以下性质：

+ **协商一致性(Uniform agreement)**

  所有的节点都接受相同的决议。

+ **诚实性(Integrity)**

  所有节点不能反悔，<u>即对一项提议不能有两次决定</u>。

+ **合法性(Validity)**

  如果决定了值v，则v一定是由某个节点所提议的。

+ **可终止性(Termination)**

  <u>节点如果不崩溃则最终一定可以达成决议</u>。

​	**协商一致性和诚实性属性定义了共识的核心思想：<u>决定一致的结果，一旦决定，就不能改变</u>**。有效性属性主要是为了排除些无意义的方案：例如，无论什么建议，都可以有一个总是为空(NULL)的决定，虽然可以满足一致性和诚实性，但没有任何实际效果。

​	如果不关心容错，那么满足前三个属性很容易：可以强行指定某个节点为“独裁者”，由它做出所有的决定。但是，如果该节点失败，系统就无法继续做出任何决定。其实这就是在两阶段提交时所看到的：如果协调者失败了，那些处于不确定状态的参与者就无从知道下一步该做什么。

​	**可终止性则引入了容错的思想。它重点强调一个共识算法不能原地空转，永远不做事情**，换句话说，它必须取得实质性进展。即使某些节点岀现了故障，其他节点也必须最终做出决定。**<u>可终止性属于一种活性，而另外三种则属于安全性方面的属性(参阅第8章“安全性与活性”</u>**)。

​	上述共识的系统模型假定当某个节点发生崩溃后，节点就彻底消失，永远不再回来。可以这样设想，这不是由于软件错误，而是遭遇了地震，整个数据中心包括所有节点被山体滑坡所摧毁，所以必须假设节点已经深埋于30英尺下的瓦砾之中，不可能重新上线。在这样的系统模型下，所有采取等待节点恢复的算法都无法满足终止性，特别是**2PC不符合可终止性要求**。

​	<u>当然，如果所有的节点都崩溃了，那么无论何种算法都不可能继续做出决定。算法所能够容忍的失败次数和规模都有一定的限制</u>。**事实上，可以证明任何共识算法都需要至少大部分节点正确运行才能确保终止性。而这个多数就可以安全地构成 quorum(参阅第5章“读写 quorum”)**。

​	**因此，可终止性的前提是，发生崩溃或者不可用的节点数必须小于半数节点**。<u>即便是多数节点出现了故障或者存在严重的网络问题，现在有很多实现的共识系统也可以满足**安全属性**：协商一致性，诚实性和合法性。**所以大规模的失效情况可能会导致系统无法处理请求，但不会破坏系统做出无效的决定**</u>。

​	大多数共识算法都假定系统不存在拜占庭式错误。对于“拜占庭式错误”(参阅第8章)，即节点没有遵循协议(例如故意发送相互矛盾的消息)，从而破坏协议的安全属性。**研究表明，只要发生拜占庭故障的节点数小于三分之一，也可以达成共识**。不过篇幅所限，我们无法就此展开细节讨论。

#### * 共识算法与全序广播

​	<u>最著名的容错式共识算法包括VSR，Paxos，Raft和Zab</u>。这些算法存在诸多相似之处，但又不完全相同。篇幅所限，本书无法详细介绍这些算法，除非你决定要自己实现一套这样的共识系统(这可能是非常不明智的做法，极具失败的可能)，否则只需了解它们共同的设计思想就足够了。

​	**这些算法大部分其实并不是直接使用上述的形式化模型(提议并决定某个值，同时满足上面4个属性)。相反，他们是决定了一系列值，然后采用全序关系广播算法(参阅本章前面的“全序关系广播”)**。

​	**<u>全序关系广播的要点是，消息按照相同的顺序发送到所有节点，有且只有一次。如果仔细想想，这其实相当于进行了多轮的共识过程：在每一轮，节点提出他们接下来想要发送的消息，然后决定下一个消息的全局顺序</u>**。

​	所以，**全序关系广播相当于持续的多轮共识**(每一轮共识的决定对应于一条消息)：

+ 由于协商一致性，所有节点决定以相同的顺序发送相同的消息。
+ 由于诚实性，消息不能重复。

+ 由于合法性，消息不会被破坏，也不是凭空捏造的。
+ 由于可终止性，消息不会丢失。

​	**VSR、Raft和Zab都直接采取了<u>全序关系广播</u>，这比重复性的一轮共识只解决一个提议更加高效。而 Paxos则有对应的优化版本称之为Multi-Paxos**。

#### * 主从复制与共识

​	第5章讨论了主从复制(参阅第5章“主节点与从节点”)，所有的写入操作都由主节点负责，并以相同的顺序发送到从节点来保持副本更新。这不就是基木的全序关系广播么？那在主从复制时我们怎么没有考虑共识问题呢?

​	<u>答案取决于如何选择主节点</u>。<u>如果主节点是由运营人员手动选择和配置的，那基本上就是一个独裁性质的“一致性算法”</u>：只允许一个节点接受写入(并决定复制日志中的写入顺序)，如果该节点发生故障，系统将无法写入，直到操作人员再手动配置新的节点成为主节点。这样的方案也能在实践中很好地发挥作用，但它需要人为干预才能取得进展，<u>不满足共识的可终止性</u>。

​	<u>一些数据库支持自动选举主节点和故障切换，通过选举把某个从节点者提升为新的主节点(参阅第5章“处理节点失效”)。这样更接近容错式全序关系广播，从而达成共识</u>。

​	但是，还有一个问题，我们之前曾讨论过脑裂：所有的节点都需要同意主节点，否则两个主节点会导致数据库出现不一致。因此，我们需要共识算法选出一位主节点。但是，如果这里描述的共识算法实际上是全序关系广播;且全序关系广播很像主从复制，但主从复制现在又需要选举主节点等。

​	看起来要选举一个新的主节点，我们首先需要有一个主节点。要解决共识，必须先处理共识。怎么摆脱这样一个奇怪的循环?

#### * Epoch和Quorum

​	<u>目前所讨论的所有共识协议在其内部都使用了某种形式的主节点</u>，虽然主节点并不是固定的。相反，他们都采用了一种**弱化的保证**：<u>协议定义了一个世代编号(epoch number，对应于 Paxos中的 ballot number，VSP中 view number，以及Raft中的term number)，并保证在每个世代里，主节点是唯一确定的</u>。

​	**如果发现当前的主节点失效，节点就开始一轮投票选举新的主节点。选举会赋予一个单调递增的epoch号。如果出现了两个不同的主节点对应于不同 epoch号码(例如，上个epoch号码的主节点其实并没有真正挂掉)，则具有更高epoch号码的主节点将获胜**。

​	<u>在主节点做出任何决定之前，它必须首先检查是否存在比它更高的 epoch号码，否则就会产生冲突的决定</u>。主节点如何知道它是否已被其他节点所取代了呢？还记得上章“真理由多数决定”么？节点不能依靠自己所掌握的信息来决策，例如自认为是主节点并不代表其他节点都接受了它的“自认为”。

​	相反，<u>它必须从quorum节点中收集投票(参阅第5章“读写 quorum”)</u>。主节点如果想要做岀某个决定，须将提议发送给其他所有节点，等待 quorum节点的响应。quorum通常(但不总是)由多数节点组成。并且，<u>只有当没有发现更高 epoch主节点存在时，节点才会对当前的提议(带有 epoch号码)进行投票</u>。

​	因此，**这里面实际存在两轮不同的投票：首先是投票决定谁是主节点，然后是对主节点的提议进行投票**。其中的关键一点是，<u>**参与两轮的 quorum必须有重叠**：如果某个提议获得通过，那么其中参与投票的节点中必须至少有一个也参加了最近一次的主节点选举</u>。换言之，如果在针对提议的投票中没有出现更高 epoch号码，那么可以得出这样的结论：<u>**因为没有发生更高 epoch的主节点选举，当前的主节点地位没有改变，所以可以安全地就提议进行投票**</u>。

​	<u>投票过程看起来很像两阶段提交(2PC)。最大的区别是，2PC的协调者并不是依靠选举产生；另外**容错共识算法只需要收到多数节点的投票结果即可通过决议，而2PC则要求毎个参与者都必须做出“是”才能最终通过**</u>。此外，共识算法还定义了恢复过程，出现故障之后，通过该过程节点可以选举出新的主节点然后进入一致的状态，确保总是能够满足安全属性。所有这些差异之处都是确保共识算法正确性和容错性的关键。

> [分布式共识算法 - coding-for-self - 博客园 (cnblogs.com)](https://www.cnblogs.com/longjiang-uestc/p/12438987.html)

#### * 共识的局限性

​	共识算法对于分布式系统来说绝对是一个巨大的突破，它为一切不确定的系统带来了明确的安全属性(一致性，完整性和有效性)，此外它还可以支持容错(只要大多数节点还在工作和服务可达)。<u>共识可以提供全序关系广播，以容错的方式实现线性化的原子操作(参阅本章前面“采用全序关系广播实现线性化存储”)</u>。

​	不过，也不是所有系统都采用了共识，因为好处的背后都是有代价的。这包括：

​	<u>在达成一致性决议之前，节点投票的过程是一个**同步复制过程**</u>。如第5章“同步与异步复制”所述，数据库通常配置为异步复制，存在某些已提交的数据在故障切换时丢失的风险，即使这样，很多系统还是采用异步复制(而非同步复制)，原因正是为了更好的性能。

​	**共识体系需要严格的多数节点才能运行**。这意味着需要至少三个节点才能容忍一个节点发生敞故障(剩下的三分之二形成多数)，或者需要最少五个节点来容忍两个节点故障(其余五分之三形成多数)。<u>**如果由于网络故障切断了节点之间的连接，则只有多数节点所在的分区可以继续工作，剩下的少数节点分区则处于事实上的停顿状态(参阅本章前面“线性化的代价”)**</u>。

​	**多数共识算法假定一组固定参与投票的节点集，这意味着不能动态添加或删除节点**。动态成员资格的扩展特性可以在集群中的按需调整节点数，但相比于静态的成员组成，其理解程度和接受程度要低很多。

​	**共识系统通常依靠超时机制来检测节点失效**。在网络延迟高度不确定的环境中，特别是那些跨区域分布的系统，经常由于网络延迟的原因，导致节点错误地认为主节点发生了故障。虽然这种误判并不会损害安全属性，但频繁的主节点选举显著降低了性能，系统最终会花费更多的时间和资源在选举主节点上而不是原本的服务任务。

​	此外，**共识算法往往对网络问题特别敏感**。例如，Raft已被发现存在不合理的边界条件处理：如果整个网络中存在某一条网络连接持续不可靠，Raft会进入一种奇怪的状态：它不断在两个节点之间反复切换主节点，当前主节点不断被赶下台，这最终导致系统根本无法安心提供服务。**其他共识算法也有类似的问题，所以面对不可靠网络，如何设计更具鲁棒性的共识算法仍然是一个开放性的研究问题**。

### 9.4.4 成员与协调服务

​	Zookeeper或etcd这样的项目通常称为“分布式键值存储”或“协调与配置服务”。从它们对外提供服务的API来看则与数据库非常相像：读取、写入对应主键的值，或者遍历主键。如果他们只是个普通数据库的话，为什么要花大力气实现一个共识算法呢？它们与其他数据库有何不同之处？

​	为了帮助理解，我们还是先简单探讨一下通常如何使用 Zookeeper这样的服务。应用程序开发者其实很少直接使用 Zookeeper，主要因为它并非通用的数据库。绝大多数情况是通过其他很多项目来间接地依赖于 Zookeeper，例如 HBase， Hadoop YARN，OpenStack Nova和 Kafka等都依赖于在后头运行的 Zookeeper服务。那么这些项目为什么需要它呢？

​	Zookeeper和etcd主要针对保存少量、可完全载入内存的数据(虽然它们最终仍要写入磁盘以支持持久性)而设计，所以不要用它们保存大量的数据。<u>它们通常采用**容错的全序广播算法**在所有节点上复制这些数据从而实现高可靠</u>。<u>正如之前所讨论的，全序广播主要用来实现数据库复制：每条消息代表的是数据库写请求，然后按照相同的顺序在多个节点上应用写操作，从而达到多副本之间的一致性</u>。

​	Zookeeper的实现其实模仿了 Google的 Chubby分布式锁服务，但它不仅实现了全序广播(因此实现了共识)，还提供了其他很多有趣的特性。所有这些特性在构建分布式系统时格外重要：

+ **线性化的原子操作**

  使用**原子比较-设置**操作，可以实现加锁服务。例如如果多个节点同时尝试执行相同的操作，则确保其中只有一个会成功。共识协议保证了操作满足原子性和线性化，即使某些节点发生故障或网络随时被中断。<u>分布式锁通常实现为一个带有到期时间的租约</u>，这样万一某些客户端发生故障，可以最终释放锁(参阅第8章“进程暂停”)。

+ 操作全序

  如第8章“主节点与锁”所述，当资源被锁或者租约保护时，需要 **fencing令牌**来防止某些客户端由于发生进程暂停而引起锁冲突。 <u>fencing令牌确保每次加锁时数字总是单调增加</u>。 Zookeeper在实现该功能时，采用对所有操作执行全局排序，然后为每个操作都赋予一个单调递增的事务ID(zxid)和版本号(cversion) 。

+ 故障检测

  <u>客户端与 Zookeeper节点维护一个长期会话，客户端会周期性地与 Zookeeper服务节点互相交换心跳信息，以检查对方是否存活</u>。即使连接出现闪断，或者某个 Zookeeper节点发生失效，会话仍处于活动状态。但是，如果长时间心跳停止且超过了会话超时设置， Zookeeper会声明会话失败。此时，<u>所有该会话持有的锁资源可以配置为自动全部释放</u>(Zookeeper称之为 **ephemeral nodes即临时节点**)。

+ **更改通知**

  客户端不仅可以读取其他客户端所创建的锁和键值，还可以监视它们的变化。因此，客户端可以知道其他客户端何时加入了集群(基于它写入 Zookeeper的值)以及客户端是否发生了故障(会话超时导致节点消失)。<u>通过订阅通知机制，客户端不需要频繁地轮询服务即可知道感兴趣对象的变化情况</u>。

​	在上述特征中，<u>其实只有线性化的原子操作才依赖于共识</u>。然而 Zookeeper集成了所有这些功能，在分布式协调服务中发挥了关键作用。

#### * 节点任务分配

​	Zookeeper和 Chubby系统非常适合的一个场景是，如果系统有多个流程或服务的实例，并且需求其中的一个实例充当主节点；而如果主节点失效，由其他某个节点来接管。显然，这非常吻合主从复制数据库，此外，它对于作业调度系统(或类似的有状态服务)也非常有用。

​	<u>还有另一个场景，对于一些分区资源(可以是数据库，消息流，文件存储，分布式actor systen等)，需要决定将哪个分区分配给哪个节点。当有新节点加入集群吋需要将某些现有分区从当前节点迁移到新节点，从而实现负载动态平衡(参阅第5章“分区再平衡”)。而当节点移除或失败时，其他节点还需要接管失败节点</u>。

​	上述场景中的任务，可以借助 Zookeeper中的原子操作， ephemeral nodes和通知机制来实现。<u>如果实现无误，它可以非常方便地使应用程序达到自动故障中恢复，减少人工干预</u>。虽然目前出现了如 Apache Curator等(基于 Zookeeper客户端API)提供了更高级别的封装接口，但总体上讲真正做起来并非易事。即使这样，它仍然比从头开始实现一套共识算法好很多，全新开发共识系统非常有挑战性，目前成功记录寥寥可数。

​	应用程序最初可能只运行在单节点，之后可能最终扩展到数千节点。试图在如此庞大的集群上进行多数者投票会非常低效。 <u>Zookeeper通常是在固定数量的节点(通常三到五个)上运行投票，可以非常高效地支持大量的客户端。因此， Zookeeper其实提供了一种将跨节点协调服务(包括共识，操作排序和故障检测)专业外包的方式</u>。

​	通常情况下， **Zookeeper所管理的数据变化非常缓慢**，类似“分区7的主节点在`10.1.1.23`”这样的信息，**其变化频率往往在分钟甚至是小时级别**。**它不适合保存那些应用实时运行的状态数据**，后者可能每秒产生数千甚至百万次更改。如果这样，应该考虑使用其他工具(如 Apache Bookkeeper)。

#### * 服务发现

​	此外， ZooKeeper、eted和 Consul还经常用于服务发现。例如需要某项服务时，应该连接到哪个IP地址等。在典型的云环境中，虚拟机可能会起起停停，这种动态变化的节点无法提前知道服务节点的IP地址，因此，可以这样配置服务，每当节点启动时将其网络端口信息向 ZooKeeper等服务注册，然后其他人只需向 ZooKeeper的注册表中询问即可。

​	但是，关于服务发现是否需要共识还缺乏统一认识。通过服务名称来获取IP地址传统的査询方式是基于DNS，它使用多层缓存来实现高性能与高可用性。**从DNS读取肯定不满足线性化，而现实情况是，如果DNS返回的结果是过期的旧值，通常也不会产生什么大问题**。<u>总体讲，DNS对于网络产生中断时服务可用性和鲁棒性更为重要一些</u>。

​	<u>即使服务发现不需要共识，但主节点选举则肯定需要</u>。因此，如果共识系统已经明确知道哪一个是主节点，那它可以利用这些信息来帮助次级服务来发现各自的主节点。现在一些共识系统支持只读的缓存副本。这些副本异步地接收其他共识算法所达成的决议日志，但自身并不怎么参与投票，而主要是提供不需要支持线性化的读取服务。

#### * 成员服务

​	ZooKeeper等还可以看作是成员服务范畴的一部分。关于成员服务的研究历史可追溯到20世纪80年代，它对于构建高可靠的系统(例如空中交管)非常重要。

​	**成员服务用来确定当前哪些节点处于活动状态并属于集群的有效成员**。正如在第8章中所介绍的，由于无限的网终延迟，无法可靠地检测一个节点究竟是否发生了故障。但是，**可以将故障检测与共识绑定在一起，让所有节点就节点的存活达成一致意见**。

​	**这里依然存在发生误判的可能性，即节点其实欠于活动状态却被错误地宣判为故障。即便这样，系统就成员资格冋题的决定是全体一致的，这是最重要的**。例如，选举主节点的方式可能是简单地投票选择编号最小的节点，一旦节点对于当前包含哪些成员出现了不同意见，那么共识过程就无法继续。

## 9.5 小结

​	本章从多个不同的角度审视了一致性与共识问题。深入研究了**线性化**(一种流行的一致性模型)：<u>其目标是使多副本对外看起来好像是单一副本，然后所有操作以原子方式运行，就像一个单线程程序操作变量一样</u>。线性化的概念简单，容易理解，看起来很有吸引力，但它的主要问题在于性能，特别是在网络延迟较大的环境中。

​	我们接下来探讨了**因果关系**，<u>因果关系对事件进行了某种排序(根据事件发生的原因结果依赖关系)</u>。**线性化**是将所有操作都放在唯一的、全局有序时间线上，而因果性则不同，它为我们提供了一个弱一致性模型：允许存在某些并发事件，所以**版本历史**是一个包含多个分支与合并的时间线。<u>因果一致性避免了线性化昂贵的协调开销，且对网络延迟的敏感性要低很多</u>。

​	然而，即使意识到因果顺序(例如采用 **Lamport 时间戳**)，我们发现有时无法在实践中采用这种方式，在“时间戳排序还不够”一节有这样的例子：确保用户名唯一并拒绝对同一用户名的并发注册请求。如果某个节点要同意请求，则必须以某种方式查询其他节点是否存在竞争请求。这个例子最终引导我们去探究系统的共识问题。

​	**共识意味着就某一项提议，所有节点做出一致的决定，而且决定不可撤销**。通过逐分析，事实证明，多个广泛的问题最终都可以归结为共识，并且彼此等价(这就意味着，如果找到其中一个解决方案，就可以比较容易地将其转换为其他问题的解决方案)。这些等价的问题包括：

+ **可线性化的比较-设置**寄存器

  寄存器需要根据当前值是否等于输入的参数，来自动决定接下来是否应该设置新值。

+ **原子事务提交**

  数据库需要决定是否提交或中止分布式事务。

+ **全序广播**

  消息系统要决定以何种顺序发送消息。

+ **锁与租约**

  当多个客户端争抢锁或租约时，要决定其中哪一个成功。

+ 成员/协调服务

  对于失败检测器(例如超时机制)，系统要决定节点的存活状态(例如基于会话超时)。

+ **唯一性约束**

  当多个事务在相同的主键上试图并发创建冲突资源时，约束条件要决定哪一个被允许，哪些违反约束因而必须失败。

​	如果系统只存在一个节点，或者愿意把所有决策功能都委托给某一个节点，那么事情就变得很简单。这和主从复制数据库的情形是一样的，即由主节点负责所有的决策事宜，正因如此，这样的数据库可以提供线性化操作、唯一性约束、完全有序的复制日志等。

​	然而，如果唯一的主节点发生故障，或者出现网络中断而导致主节点不可达，这样的系统就会陷入停顿状态。有以下三种基本思路来处理这种情况：

1. **系统服务停止，并等待主节点恢复。许多XA/JTA事务协调者采用了该方式**。本质上，这种方法并没有完仝解决共识问题，因为它不满足终止性条件，试想如果主节点没法恢复，则系统就会永远处于停顿状态。
2. **人为介入来选择新的主节点，并重新配置系统使之生效。许多关系数据库都采用这种方法**。本质上它引入了一种“上帝旨意”的共识，即在计算机系统之外由人类来决定最终命运。故障切换的速度完仝取决于人类的操作，通常比计算机慢。
3. **采用算法来自动选择新的主节点**。这需要一个共识算法，我们建议采用那些经过验证的共识系统来确保正确处理各种网络异常。

​	虽然主从数据库提供了线性化操作，且在写操作粒度级别上并不依赖于共识算法，但它仍然<u>需要共识来维护主节点角色和处理主节点变更情况</u>。因此，某种意义上说，唯一的主节点只是其中的一步，系统在其他环节依然需要共识(虽然不那么的频驚)。好在容错算法与共识的系统可以共存，我们在本章做了简要地介绍。

​	Zookeeper等工具以一种类似外包方式为应用提供了重要的**共识服务**、**故障检测**和**成员服务**等。虽然用起来依然有挑战，但远比自己开发共识算法要好得多(正确处理好第8章的所有问题绝非易事)。如果面临的问题最终可以归结为共识，并且还有容错需求，那么这里给的建议是采用如 Zookeeper等验证过的系统。

​	尽管如此，**并不是毎个系统都需要共识。例如无主复制和多主复制复制系统通常并不支持全局共识**。正因如此，这些系统可能会发生冲突(参阅第5章“处理写冲突”)，但或许也可以接受或者寻找其他方案，例如没有线性化保证时，就需要努力处理好数据多个冲突分支以及版本合并等。

​	本章引用了大量分布式系统理论方面的研究。虽然理论性文章和证明有时理解起来有些困难，有时甚至包含了一些不太合理的假设条件，但它们对于指导实际工作还是极具价值：例如<u>帮助推理系统的边界在哪里；帮助发现一些违反直觉的分布式系统潜在的缺陷</u>。如果有时间，建议仔细阅读这些参考资料。

​	至此我们终于完成了本书第二部分，其中包括复制(第5章)，分区(第6章)，事务第7章)，分布式系统失效模型(第8章)以及最后的一致性与共识(第9章)。相信我们已经奠定了坚实的理论基础，接下来第三部分我们将面向实际环境，讨论<u>如何基于**异构模块**来构建强大的应用系统</u>。

