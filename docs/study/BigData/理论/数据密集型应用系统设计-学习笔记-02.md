# 数据密集型应用系统设计-学习笔记-02

# 第二部分 分布式数据系统

​	本书第一部分主要讨论了单台机器存储系统设计的主要技术。在第二部分，我们将继续向前迈进，当需要多台机器提供数据存储和检索服务时，又会有哪些挑战和方案呢？

​	主要出于以下目的，我们需要在多台机器上分布数据：

+ 扩展性

  当数据量或者读写负载巨大，严重超出了单台机器的处理上限，需要将负载分散到多台机器上。

+ 容错与高可用性

  当单台机器(或者多台，以及网络甚至整个数据中心)出现故障，还希望应用系统可以继续工作，这时需要采用多台机器提供冗余。这样某些组件失效之后，冗余组件可以迅速接管。

+ **延迟考虑**

  如果客户遍布世界各地，通常需要考虑在全球范围内部署服务，以方便用户就近访问最近数据中心所提供的服务，从而避免数据请求跨越了半个地球才能到达目标。

## **系统扩展能力**

​	**当负载增加需要更强的处理能力时，最简单的办法就是购买更强大的机器(有时称为垂直扩展)**。<u>由一个操作系统管理更多的CPU，内存和磁盘，通过**高速内部总线**使每个CPU都可以访问所有的存储器或磁盘。在这样一个共享内存架构中，所有这些组件的集合可看作一台大机器</u>。

​	共享内存架构的问题在于，成本增长过快甚至超过了线性：即如果把一台机器内的CPU数量增加一倍，内存扩容一倍，磁盘容量加大一倍，则最终总成本增加不止一倍。<u>并且由于性能瓶颈因素，这样一台机器尽管拥有了两倍的硬件指标但却不一定能处理两倍的负载</u>。

​	<u>共享内存架构能够提供有限的容错能力，例如高端的服务器可以热插拔很多组件(在不关闭机器的情况下更换磁盘，内存模块，甚至是CPU)</u>。但很显然，它仍局限于某个特定的地理位置，无法提供异地容错能力。

​	另一种方法是共享磁盘架构，它拥有多台服务器，每个服务器各自拥有独立的CPU和内存，然后将数据存储在可共享访问的磁盘阵列上，服务器与磁盘阵列之间往往通过高速网络连接。<u>这种架构多适用于数据仓库等负载，然而通常由于资源竞争以及锁的开销等限制了其进一步的扩展能力</u>。

## 无共享结构

​	相比之下，无共享架构(也称为水平扩展)则获得了很大的关注度。当采用这种架构时，运行数据库软件的机器或者虚拟机称为节点。每个节点独立使用本地的CPU，内存和磁盘。<u>节点之间的所有协调通信等任务全部运行在传统网络(以太网)之上且核心逻辑主要依靠软件来实现</u>。

​	无共享系统不需要专门的硬件，具有较高的性价比。它可以跨多个地理区域分发数据，从而减少用户的访问延迟，甚至当整个数据中心发生灾难时仍能继续工作。通过云计算虛拟机的部署方式，即便是没有Google级别规模的小公司，也可以轻松拥有跨区域的分布式架构和服务能力。

​	本部分内容将重点放在无共享体系架构上，并不是因为它一定是所有应用的最佳选择，而是因为它需要应用开发者更多的关注和深入理解。例如把数据分布在多节点上，就需要了解在这样一个分布式系统下，背后的权衡设计和隐含限制，数据库并不能魔法般地把所有复杂性都屏蔽起来。

​	虽然分布式无共享体系架构具有很多优点，但也会给应用程序带来更多的复杂性，有时甚至会限制实际可用的数据模型。例如在某些极端情况下，一个简单的单线程程序可能比一个拥有100多个CPU核的集群性能更好。而另一方面，无共享系统也可以做到性能非常强大。接下来的几章里我们将详细讨论数据分布时所面临的主要问题。

## 复制与分区

​	将数据分布在多节点时有两种常见的方式：

+ 复制

  **在多个节点上保存相同数据的副本**，每个副本具体的存储位置可能不尽相同。复制方法可以提供冗余：如果某些节点发生不可用，则可以通过其他节点继续提供数据访问服务。复制也可以帮助提高系统性能。我们在第5章将主要讨论复制技术。

+ 分区

  **将一个大块头的数据库拆分成多个较小的子集即分区，不同的分区分配给不同的节点(也称为分片)**。我们在第6章主要介绍分区技术。

​	这些是不同的数据分布机制，然而它们经常被放在一起组合使用。参见图II-1的示例。

​	在了解以上概念之后，我们会讨论分布式环境中错综复杂的权衡之道，很可能你会在实际设计系统时不得不面对这些艰难选择。我们将在第7章介绍事务，帮助理解数据系统中各种可能岀错的情况以及处理方法，而第8章和第9章将深入分析分布式系统内在的局限性，之后结束本部分。

![第二部分：分布式数据 - 图1](https://static.sitestack.cn/projects/ddia/img/figii-1.png)

​	在之后本书的第三部分，我们将讨论如何把多个(可能每一个都是分布式)数据存储组件集成到一个更大的系统中，以满足更复杂的应用需求。但在那之前，我们首先来谈谈分布式数据系统。

# 第5章 数据复制

​	复制主要指通过互联网络在多台机器上保存相同数据的副本。正如第二部分开头所介绍的，通过数据复制方案，人们通常希望达到以下目的：

+ 使数据在地理位置上更接近用户，从而**降低访问延迟**。
+ 当部分组件出现故障，系统依然可以继续工作，从而**提高可用性**。
+ 扩展至多台机器以同时提供数据访问服务，从而**提高读吞吐量**。

​	本章我们将假设数据规模比较小，集群的每一台机器都可以保存数据集的完整副本。在接下来的第6章中，我们放宽这一假设，讨论单台机器无法容纳整个数据集的情况(即必须分区)。在后面的章节中，我们还将讨论复制过程中可能出现的各种故障，以及该如何处理这些故障。

​	如果复制的数据一成不变，那么复制就非常容易：只需将数据复制到每个节点，一次即可搞定。然而所有的技术挑战都在于处理那些持续更改的数据，而这正是本章讨论的核心。我们将讨论三种流行的复制数据变化的方法：**主从复制、多主节点复制和无主节点复制**。几乎所有的分布式数据库都使用上述方法中的某一种，而三种方法各有优缺点，我们稍后会详细解读。

​	复制技术存在许多需要折中考虑的地方，例如采用同步复制还是异步复制，以及如何处理失败的副本等。数据库通常采用可配置选项来调整这些处理策略，虽然在处理细节方面因数据库实现而异，但存在一些通用的一般性原则。本章我们还将在讨论不同选项可能出现的后果。

​	<u>数据库复制其实是个很古老的话题。因为网络的基本约束条件没有发生本质的改变，可以说自1970年所研究的基本复制原则，时至今日也没有发生太大的变化</u>。然而，除了学术研究，实践中很多开发人员仍然假定数据库只运行在单节点上，分布式数据库成为主流也只是最近发生的事情。由于许多应用开发人员在这方面经验还略显不足，对诸如“最终一致性”等问题存在一些误解。因此，在“复制滞后问题”中，我们会详细讨论最终一致性，包括读自己的写和单调读等。

## * 5.1 主节点与从节点

​	**每个保存数据库完整数据集的节点称之为副本**。当有了多副本，不可避免地会引入个问题：<u>如何确保所有副本之间的数据是一致的</u>？

​	**对于每一笔数据写入，所有副本都需要随之更新；否则，某些副本将出现不一致**。最常见的解决方案是基于主节点的复制(也称为主动被动，或主从复制)，如图5-1所示。主从复制的工作原理如下：

1. 指定某一个副本为主副本(或称为主节点)。当客户写数据库时，必须将写请求首先发送给主副本，主副本首先将新数据写入本地存储。
2. **其他副本则全部称为从副本(或称为从节点)。主副本把新数据写入本地存储后，然后将数据更改作为复制的日志或更改流发送给所有从副本。每个从副本获得更改日志之后将其应用到本地，且严格保持与主副本相同的写入顺序**。
3. 客户端从数据库中读数据时，可以在主副本或者从副本上执行査询。再次强调，**只有主副本才可以接受写请求；从客户端的角度来看，从副本都是只读的**。

​	许多关系型数据库都内置支持主从复制，例如 PostgreSQL(9.0版本以后)、MYSQL、 Oracle Data Guard和 SQL Server的AlwaysOn Availability Groups。而些非关系数据库如 MongoDB、 RethinkDB和 Espresso也支持主从复制。另外，主从复制技术也不仅限于数据库，还广泛用于分布式消息队列如 Kafka 和 RabbitMQ可以及一些网络文件系统和复制块设备(如DRBD)。

![领导者与追随者 - 图1](https://static.sitestack.cn/projects/ddia/img/fig5-1.png)

### * 5.1.1 同步复制与异步复制

​	复制非常重要的一个设计选项是同步复制还是异步复制。<u>对于关系数据库系统，同步或异步通常是一个可配置的选项；而其他系统则可能是硬性指定或者只能二选一</u>。

​	结合图5-1的例子，网站用户需要更新首页的头像图片。其基本流程是，客户将更新请求发送给主节点，主节点接收到请求，接下来将数据更新转发给从节点。最后，由主节点来通知客户更新完成。

​	图5-2则进一步描述了系统各个模块间的通信情况，包括客户端，主节点和两个从节点。时间从左到右。请求或响应标记为粗箭头。

![领导者与追随者 - 图2](https://static.sitestack.cn/projects/ddia/img/fig5-2.png)

​	图5-2中，从节点1的复制是同步的，即主节点需等待直到从节点1确认完成了写入，然后才会向用户报告完成，并且将最新的写入对其他客户端可见。而从节点2的复制是异步的：主节点发送完消息之后立即返回，不用等待从节点2的完成确认。

​	图5-2中，从节点2在接收复制日志之前有一段很长的延迟。通常情况下，复制速度会非常快，例如多数数据库系统可以在一秒之内完成所有从节点的更新。但是，系统其实并没有保证一定会在多长时间内完成复制。有些情况下，从节点可能落后主节点几分钟甚至更长时间，例如，由于从节点刚从故障中恢复，或者系统已经接近最大设计上限，或者节点之间的网络出现问题。

​	**同步复制的优点是，一旦向用户确认，从节点可以明确保证完成了与主节点的更新同步，数据已经处于最新版本。万一主节点发生故障，总是可以在从节点继续访问最新数据。缺点则是，如果同步的从节点无法完成确认(例如由于从节点发生崩溃，或者网络故障，或任何其他原因)，写入就不能视为成功。主节点会阻塞其后所有的写操作，直到同步副本确认完成**。

​	因此，把所有从节点都配置为同步复制有些不切实际。因为这样的话，任何一个同步节点的中断都会导致整个系统更新停滞不前。**<u>实践中，如果数据库启用了同步复制，通常意味着其中某一个从节点是同步的，而其他节点则是异步模式</u>**。**万一同步的从节点变得不可用或性能下降，则将另一个异步的从节点提升为同步模式。这样可以保证至少有两个节点(即主节点和一个同步从节点)拥有最新的数据副本。这种配置有时也称为<u>半同步</u>**。

​	<u>主从复制还经常会被配置为**全异步**模式</u>。此时如果主节点发生失败且不可恢复，则所有尚未复制到从节点的写请求都会丟失。这意味着即使向客户端确认了写操作，却无法保证数据的持久化。<u>但全异步配置的优点则是，不管从节点上数据多么滞后，主节点总是可以继续响应写请求，系统的吞吐性能更好</u>。

​	**异步模式这种弱化的持久性听起来是一个非常不靠谱的折中设计，但是异步复制还是被广泛使用，特别是那些从节点数量巨大或者分布于广域地理环境**。我们将在本章后面的“复制滞后问题”继续这个话题。

> 复制问题研究
>
> 主节点发生故障时异步复制系统可能会丢失数据，这是一个非常严重的问题，因此在保证数据不丢失的前提下，研究人员尝试了各种办法来提高复制性能与系统可用性。例如，链式复制是同步复制的一种变体，已经在一些系统(如Microsoft Azure存储)中得以实现。
>
> 多副本一致性与共识之间有着密切的联系(即让多个节点对数据状态达成一致)，我们将在第9章详细探讨这一点。本章主要集中于数据库实践中常用的、相对简单的复制技术。

#### 配置新的从节点

​	当如果岀现以下情况时，如需要增加副本数以提高容错能力，或者替换失败的副本，就需要考虑增加新的从节点。但如何确保新的从节点和主节点保持数据一致呢?

​	简单地将数据文件从一个节点复制到另一个节点通常是不够的。主要是因为客户端仍在不断向数据库写入新数据，数据始终处于不断变化之中，因此常规的文件拷贝方式将会导致不同节点上呈现出不同时间点的数据，这不是我们所期待的。

​	或许应该考虑锁定数据库(使其不可写)来使磁盘上的文件保持一致，但这会违反高可用的设计目标。好在我们可以做到在不停机、数据服务不中断的前提下完成从节点的设置。逻辑上的主要操作步骤如下:

1. 在某个时间点对主节点的数据副本产生一个**一致性快照**，这样避免长时间锁定整个数据库。目前大多数数据库都支持此功能，快照也是系统备份所必需的。而在某些情况下，可能需要第三方工具，如MySQL的 innobackupex。
2. **将此快照拷贝到新的从节点**。
3. **从节点连接到主节点并请求快照点之后所发生的数据更改日志**。因为在第一步创建快照时，快照与系统复制日志的某个确定位置相关联，这个位置信息在不同的系统有不同的称呼，如PostgreSQL将其称为“log sequence number”(日志序列号)，而 MySQL将其称为“ binlog coordinates”。
4. 获得日志之后，从节点来应用这些快照点之后所有数据变更，这个过程称之为追赶。接下来，它可以继续处理主节点上新的数据变化。并重复步骤1~步骤4。

​	建立新的从副本具体操作步骤可能因数据库系统而异。某些系统中，这个过程是全自动化的，而其他系统中所涉及的步骤、流程可能会比较复杂，甚至需要管理员手动介入。

#### * 处理节点失效

​	系统中的任何节点都可能因故障或者计划内的维护(例如重启节点以安装内核安全补丁)而导致中断甚至停机。如果能够在不停机的情况下重启某个节点，这会对运维带来巨大的便利。我们的目标是，<u>尽管个别节点会出现中断，但要保持系统总体的持续运行，并尽可能减小节点中断带来的影响</u>。

​	那么如何通过主从复制技术来实现系统高可用呢?

+ 从节点失效：追赶式恢复

  从节点的本地磁盘上都保存了副本收到的数据变更日志。如果从节点发生崩溃，然后顺利重启，或者主从节点之间的网络发生暂时中断(闪断)，则恢复比较容易，**根据副本的复制日志，从节点可以知道在发生故障之前所处理的最后一笔事务，然后连接到主节点，并请求自那笔事务之后中断期间内所有的数据变更**。在收到这些数据变更日志之后，将其应用到本地来追赶主节点。之后就和正常情况一样持续接收来自主节点数据流的变化。

+ 主节点失效：节点切换

  **处理主节点故障的情况则比较棘手：选择某个从节点将其提升为主节点；客户端也需要更新，这样之后的写请求会发送给新的主节点，然后其他从节点要接受来自新的主节点上的变更数据**，这一过程称之为**切换**。故障切换可以手动进行，例如通知管理员主节点发生失效，采取必要的步骤来创建新的主节点；或者以自动方式进行。自动切换的步骤通常如下：

  1. **确认主节点失效**。有很多种岀错可能性，例如由于系统崩溃，停电，网络问题等。没有万无一失的方法能够确切地检测到究竟问题出在哪里，所以**大多数系统都采用了基于超时的机制：节点间频蘩地互相发生发送心跳存活消息，如果发现某一个节点在一段比较长时间内(例如30s)没有响应，即认为该节点发生失效**(如果主节点在计划内出于维护目的而故意下线，则不在此讨论范围)。
  2. **选举新的主节点**。可以通过选举的方式(超过多数的节点达成共识)来选举新的主节点，或者由之前选定的某控制节点来指定新的主节点。候选节点最好与原主节点的数据差异最小，这样可以最小化数据丢失的凤险。让所有节点同意新的主节点是个典型的**共识问题**，会在第9章详细讨论。
  3. 重新配置系统使新主节点生效。客户端现在需要将写请求发送给新的主节点(细节将在第6章的“请求路由”中讨论)。<u>如果原主节点之后重新上线，可能仍然自认为是主节点，而没有意识到其他节点已经达成共识迫使其下台。**这时系统要确保原主节点降级为从节点，并认可新的主节点**</u>。

  ​	然而，上述切换过程依然充满了很多变数：

  + <u>如果使用了异步复制，且失效之前，新的主节点并未收到原主节点的所有数据；在选举之后，原主节点很快又重新上线并加入到集群，接下来的写操作会发生什么？新的主节点很可能会收到冲突的写请求，这是因为原主节点未意识的角色变化，还会尝试同步其他从节点，但其中的一个现在已经接管成为现任主节点</u>。**常见的解决方案是，原主节点上未完成复制的写请求就此丢弃，但这可能会违背数据更新持久化的承诺**。
  + **如果在数据库之外有其他系统依赖于数据库的内容并在一起协同使用，丟弃数据的方案就特别危险**。例如，<u>在 Github的一个事故中，某个数据并非完全同步的MySQL从节点被提升为主副本，数据库使用了自增计数器将主键分配给新创建的行，但是因为新的主节点计数器落后于原主节点(即二者并非完全同步)，它重新使用了已被原主节点分配出去的某些主键，而恰好这些主键已被外部Redis所引用，结果出现 MYSQL和 Redis之间的不一致，最后导致了某些私有数据被错误地泄露给了其他用户</u>。
  + **在某些故障情况下(参见第8章)，可能会发生两个节点同时都自认为是主节点。这种情况被称为脑裂**，它非常危险：<u>两个主节点都可能接受写请求，并且没有很好解决冲突的办法(参阅本章后面的“多主节点复制技术”)，最后数据可能会丟失或者破坏</u>。作为一种安全应急方案，有些系统会采取措施来强制关闭其中一个节点。然而，如果设计或者实现考虑不周，可能会出现两个节点都被关闭的情况。
  + 如何设置合适的超时来检测主节点失效呢？<u>主节点失效后，超时时间设置得越长也意味着总体恢复时间就越长</u>。<u>但如果超时设置太短，可能会导致很多不必要的切换</u>。例如，突发的负载峰值会导致节点的响应时间变长甚至超时，或者由于网络故障导致延迟增加。**如果系统此时已经处于高负载压力或网络已经出现严重拥塞，不必要的切换操作只会使总体情况变得更糟**。

​	**坦白讲，对于这些问题没有简单的解决方案。因此，即使系统可能支持自动故障切换，有些运维团队仍然更愿意以手动方式来控制整个切换过程**。

​	上述这些问题，包括**节点失效、网络不可靠、副本一致性、持久性、可用性与延迟**之间各种细微的权衡，实际上正是分布式系统核心的基本问题。在第8章和第9章中，我们还会进一步讨论。

### * 5.1.2 复制日志的实现

​	主从复制技术到底是如何工作呢？实践中有多种不同的实现方法，此处我们逐一做些介绍。

#### 基于语句的复制

​	<u>最简单的情况，主节点记录所执行的每个**写请求(操作语句)**并将该操作语句作为日志发送给从节点</u>。对于关系数据库，这意味着每个 INSERT、 UPDATE或 DELETE语句都会转发给从节点，并且每个从节点都会分析并执行这些SQL语句，如同它们是来自客户端那样。

​	听起来很合理也不复杂，但这种复制方式有一些不适用的场景：

+ <u>任何调用非确定性函数的话句，如NOW()获取当前时间，或RAND()获取一个随机数等，可能会在不同的副本上产生不同的值</u>。

+ 如果语句中使用了自增列，或者依赖于数据库的现有数据(例如， `UPDATE ... WHERE <某些条件>`)，则所有副本必须按照完全相同的顺序执行，否则可能会带来不同的结果。进而，<u>如果有多个同时并发执行的事务时，会有很大的限制</u>。
+ <u>有副作用的语句(例如，触发器、存储过程、用户定义的函数等)，可能会在每个副本上产生不同的副作用</u>。

​	<u>有可能采取一些特殊措施来解决这些问题，例如，主节点可以在记录操作语句时将非确定性函数替换为执行之后的确定的结果，这样所有节点直接使用相同的结果值。但是，这里面存在太多边界条件需要考虑，因此目前通常首选的是其他复制实现方案</u>。

​	<u>**MySQL5.1版本之前采用基于操作语句的复制**。现在由于逻辑紧凑，依然在用，**但是默认情况下，如果语句中存在一些不确定性操作，则 MySQL会切换到基于行的复制(稍后讨论)**。 VoltDB使用基于语句的复制，它通过事务级别的确定性来保证复制的安全</u>。

#### 基于预写日志(WAL)传输

​	在第3章中，我们讨论了存储引擎的磁盘数据结构，<u>通常毎个**写操作**都是以**追加写**的方式写入到日志中</u>：

+ 对于日志结构存储引擎(参阅第3章的“SSTables和LSM-trees”)，日志是主要的存储方式。日志段在后台压缩并支持垃圾回收。
+ 对于采用覆盖写磁盘的B-tree(参阅第3章的“B-tree”)结构，<u>**每次修改会预先写入日志**，如系统发生崩溃，通过索引更新的方式迅速恢复到此前一致状态</u>。

​	**不管哪种情况，所有对数据库写入的字节序列都被记入日志。因此可以使用完全相同的日志在另一个节点上构建副本：除了将日志写入磁盘之外，主节点还可以通过网络将其发送给从节点**。

​	从节点收到日志进行处理，建立和主节点内容完全相同的数据副本。

​	PostgreSQL、 Oracle以及其他系统等支持这种复制方式。其**主要缺点是日志描述的数据结果非常底层**：**一个WAL包含了哪些磁盘块的哪些字节发生改变，诸如此类的细节**。**这使得复制方案和存储引擎紧密耦合**。<u>如果数据库的存储格式从一个版本改为另一个版本，那么系统通常无法支持主从节点上运行不同版本的软件</u>。

​	看起来这似乎只是个有关实现方面的小细节，但可能对运营产生巨大的影响。<u>如果复制协议允许从节点的软件版本比主节点更新，则可以实现数据库软件的不停机升级：首先升级从节点，然后执行主节点切换，使升级后的从节点成为新的主节点</u>。相反，**<u>复制协议如果要求版本必须严格一致(例如WAL传输)，那么就势必以停机为代价</u>**。

#### 基于行的逻辑日志复制

​	另一种方法是**<u>复制和存储引擎采用不同的日志格式，这样复制与存储逻辑剥离</u>**。<u>这种复制日志称为**逻辑日志**，以区分物理存储引擎的数据表示</u>。

​	<u>关系数据库的**逻辑日志**通常是指一系列记录来描述数据表**行级别**的**写请求**</u>：

+ 对于行插入，日志包含所有相关列的新值。
+ <u>对于行删除，日志里有足够的信息来唯一标识已删除的行，通常是靠主键，但如果表上没有定义主键，就需要记录所有列的旧值</u>。
+ 对于行更新，日志包含足够的信息来唯一标识更新的行，以及所有列的新值(或至少包含所有已更新列的新值)。

​	**如果一条事务涉及多行的修改，则会产生多个这样的日志记录，并在后面跟着一条记录，指出该事务已经提交**。 MySQL的二进制日志binlog(当配置为**基于行的复制**时)使用该方式。

​	**由于逻辑日志与存储引擎逻辑解耦，因此可以更容易地保持向后兼容，从而使主从节点能够运行不同版本的软件甚至是不同的存储引擎**。

​	对于外部应用程序来说，逻辑日志格式也更容易解析。如果要将数据库的內容发送到外部系统(如用于离线分析的数据仓库)，或构建自定义索引和缓存等，基于逻辑日志的复制更有优势。该技术也被称为**变更数据捕获**，我们将在第11章中继续讨论。

#### 基于触发器的复制

​	到目前为止所描述的复制方法都是由数据库系统来实现的，不涉及任何应用程序代码。通常这是大家所渴望的，不过，在某些情况下，我们可能需要更高的灵活性。例如，只想复制数据的一部分，或者想从一种数据库复制到另一种数据库，或者需要订制、管理冲突解决逻辑(参阅本章后面的“处理写冲突”)，则需要将复制控制交给应用程序层。

​	有一些工具，例如Oracle Golden Gate，可以通过读取数据库日志让应用程序获取数据变更。另一种方法则是借助许多关系数据库都支持的功能：<u>触发器</u>和<u>存储过程</u>。

​	触发器支持注册自己的应用层代码，使得当数据库系统发生数据更改(写事务)时自动执行上述自定义代码。通过触发器技术，可以将数据更改记录到一个单独的表中，然后外部处理逻辑访问该表，实施必要的自定义应用层逻辑，例如将数据更改复制到另一个系统。 Oracle的Databus和Postgres的Bucardo就是这种技术的典型代表。

​	<u>基于触发器的复制通常比其他复制方式开销更高，也比数据库内置复制更容易出错，或者暴露一些限制。然而，其高度灵活性仍有用武之地</u>。

## * 5.2 复制滞后问题

​	<u>容忍节点故障</u>只是使用复制其中的一个原因。正如第二部分开头所介绍的，其他原因包括<u>可扩展性</u>(采用多节点来处理更多的请求)和<u>低延迟</u>(将副本部署在地理上距离用户更近的地方)。

​	**主从复制要求所有写请求都经由主节点，而任何副本只能接受只读査询**。对于读操作密集的负载(如Web)，这是一个不错的选择：创建多个从副本，将读请求分发给这些从副本，从而减轻主节点负载并允许读取请求就近满足。

​	在这种扩展体系下，只需添加更多的从副本，就可以提高读请求的服务吞吐量。<u>但是，这种方法实际上只能用于**异步复制**，如果试图同步复制所有的从副本，则单个节点故障或网络中断将使整个系统无法写入</u>。而且**节点越多，发生故障的概率越高，所以完全同步的配置现实中反而非常不可靠**。

​	不幸的是，<u>如果一个应用正好从一个**异步**的从节点读取数据，而该副本落后于主节点，则应用可能会读到过期的信息。这会导致数据库中出现明显的不一致：由于并非所有的写入都反映在从副本上，如果同时对主节点和从节点发起相同的査询，可能会得到不同的结果</u>。这种不一致只是一个暂时的状态，如果停止写数据库，经过一段时间之后，从节点最终会赶上并与主节点保持一致。这种效应也被称为**最终一致性**。

​	"最终"一词有些含糊不清，总的来说，副本落后的程度理论上并没有上限。正常情况下，主节点和从节点上完成写操作之间的时间延迟(复制滞后)可能不足1秒，这样的滞后，在实践中通常不会导致太大影响。但是，<u>如果系统已接近设计上限，或者网络存在问题，则滞后可能轻松增加到几秒甚至几分钟不等</u>。

​	当滞后时间太长时，导致的不一致性不仅仅是一个理论存在的问题，而是个实实在在的现实问题。在本节中，我们将重点介绍三个复制滞后可能出现的问题，并给出相应的解决思路。

### 5.2.1 读自己的写

​	许多应用让用户提交一些数据，接下来查看他们自己所提交的内容。例如客户数据库中的记录，亦或者是讨论主题的评论等。**提交新数据须发送到主节点，但是当用户读取数据时，数据可能来自从节点**。这对于**读密集**和偶尔写入的负载是个非常合适的方案。

​	然而对于异步复制存在这样一个问题，如图5-3所示，**用户在写入不久即查看数据，则新数据可能尚未到达从节点**。对用户来讲，看起来似乎是刚刚提交的数据丢失了，显然用户不会高兴。

![复制延迟问题 - 图1](https://static.sitestack.cn/projects/ddia/img/fig5-3.png)

​	对于这种情况，我们需要“**写后读一致性**”，也称为**读写一致性**。该机制保证如果用户重新加载页面，他们总能看到自己最近提交的更新。但对其他用户则没有任何保证，这些用户的更新可能会在稍后才能刷新看到。

​	基于主从复制的系统该如何实现写后读一致性呢？有多种可行的方案，以下例举一二：

+ **如果用户访问可能会被修改的内容，从主节点读取；否则，在从节点读取**。这背后就要求有一些方法在实际执行查询之前，就已经知道内容是否可能会被修改。例如，社交网络上的用户首页信息通常只能由所有者编辑，而其他人无法编辑。因此，这就形成一个简单的规则：总是从主节点读取用户自己的首页配置文件，而在从节点读取其他用户的配置文件。
+ **<u>如果应用的大部分内容都可能被所有用户修改，那么上述方法将不太有效，它会导致大部分内容都必须经由主节点，这就丧失了读操作的扩展性</u>**。此时需要其他方案来判断是否从主节点读取。例如，<u>跟踪最近更新的时间，如果更新后一分钟之內，则总是在主节点读取；并监控从节点的复制滞后程度，避免从那些滞后时间超过一分钟的从节点读取</u>。
+ <u>客户端还可以记住最近更新时的时间戳，并附带在读请求中，据此信息，系统可以确保对该用户提供读服务吋都应该至少包含了该时间戳的更新。如果不够新要么交由另一个副本来处理，要么等待直到副本接收到了最近的更新</u>。时间戳可以是**逻辑时间戳**(例如用来指示写入顺序的日志序列号)或实际系统时钟(在这种情况下，时钟同步又称为一个关键点，请参阅第8章“不可靠的时钟”)。
+ 如果副本分布在多数据中心(例如考虑与用户的地理接近，以及高可用性)，情况会更复杂些。<u>必须先把请求路由到主节点所在的数据中心</u>(该数据中心可能离用户很远)。

​	如果同一用户可能会从多个设备访问数据，例如一个桌面Web浏览器和一个移动端的应用，情况会变得更加复杂。此时，要提供跨设备的写后读一致性，即如果用户在某个设备上输入了一些信息然后在另一台设备上查看，也应该看到刚刚所输入的内容。

​	在这种情况下，还有一些需要考虑的问题：

+ 记住用户上次更新时间戳的方法实现起来会比较困难，因为在一台设备上运行的代码完全无法知道在其他设备上发生了什么。此时，<u>元数据必须做到全局共享</u>。
+ 如果副本分布在多数据中心，无法保证来自不同设备的连接经过路由之后都到达同一个数据中心。例如，用户的台式计算机使用了家庭宽带连接，而移动设备则使用蜂窝数据网络，不同设备的网络连接线路可能完全不同。<u>如果方案要求必须从主节点读取，则首先需要想办法确保将来自不同设备的请求路由到同一个数据中心</u>。

### 5.2.2 单调读

​	在前面异步复制读异常的第二个例子里，出现了用户数据向后回滚的奇怪情况。

​	假定用户从不同副本进行了多次读取，如图5-4所示，用户刷新一个网页，读请求可能被随机路由到某个从节点。用户2345先后在两个从节点上执行了两次完全相同的查询(先是少量滞后的节点，然后是滞后很大的从节点)，则很有可能出现以下情况。第一个査询返回了最近用户1234所添加的评论，但第二个查询因为滞后的原因，还没有收到更新因而返回结果是空。实际上，第二个查询结果代表了更早时间点的状态。如果第一个查询没有返回任何内容，用户2345并不知道用户1234最近的评论，情况还不算太糟糕，但当用户2345看到了用户1234的评论之后，紧接着评论又消失了，他就会感觉很困惑。

![复制延迟问题 - 图2](https://static.sitestack.cn/projects/ddia/img/fig5-4.png)

​	单调读一致性可以确保不会发生这种异常。这是一个比强一致性弱，但比最终一致性强的保证。当读取数据时，**<u>单调读保证，如果某个用户依次进行多次读取，则他绝不会看到回滚现象，即在读取较新值之后又发生读旧值的情况</u>**。

​	<u>实现单调读的一种方式是，确保每个用户总是从**固定的同一副本执行读取**(而不同的用户可以从不同的副本读取)</u>。例如，<u>基于用户ID的哈希的方法而不是随机选择副本。但如果该副本发生失效，则用户的查询必须重新路由到另一个副本</u>。

### 5.2.3 前缀一致读

​	第三个由于复制滞后导致因果反常的例子。例如Poons先生与Cake夫人之间的以下对话：

+ Poons先生

  Cake夫人，您能看到多远的未来？

+ Cake夫人

  通常约10s， Poons先生。

​	这两句话之间存在因果关系：Cake夫人首先是听到了Poons先生的问题，然后再去回答该问题。

​	现在，想象第三个人正在通过从节点收听上述对话。Cake夫人所说的话经历了短暂的滞后到达该从节点，但 Poons先生所说的经历了更长的滞后才到达(见图5-5)。观察者听到的对话变成这样:

+ Cake夫人

  通常约10s， Poons先生。

+ Poons先生

  Cake夫人，您能看到多远的未来?

​	对于观察者来说，似乎在Poon先生提出问题之前，Cake夫人就开始了回答问题。首先，这种超自然的力量确认令人印象深刻，但逻辑上却是混乱的。

​	<u>防止这种异常需要引入另一种保证：**前缀一致读**。该保证是说，对于一系列按照某个顺序发生的写请求，那么读取这些内容时也会按照当时写入的顺序</u>。

​	这是分区(分片)数据库中出现的一个特殊问题，细节将在第6章中讨论。**如果数据库总是以相同的顺序写入，则读取总是看到一致的序列，不会发生这种反常**。<u>**然而，在许多分布式数据库中，不同的分区独立运行，因此不存在全局写入顺序。这就导致当用户从数据库中读数据时，可能会看到数据库的某部分旧值和另一部分新值**</u>。

![复制延迟问题 - 图3](https://static.sitestack.cn/projects/ddia/img/fig5-5.png)

​	**一个解决方案是确保任何具有因果顺序关系的写入都交给一个分区来完成，但该方案真实实现效率会大打折扣**。现在有一些新的算法来显式地追踪事件因果关系，在本章稍后的"**Happened-before关系与并发**"继续该问题的探讨。

### 5.2.4 复制滞后的解决方案

​	使用最终一致性系统时，最好事先就思考这样的问题：如果复制延迟增加到几分钟甚至几小时，那么应用层的行为会是什么样子？如果答案是"没问题"，那没得说。但是，如果带来糟糕的用户体验，那么在设计系统时，就要考虑提供一个更强的一致性保证，比如**写后读**；<u>如果系统设计时假定是同步复制，但最终它事实上成为了异步复制，就可能会导致灾难性后果</u>。

​	正如前面所讨论的，<u>在应用层可以提供比底层数据库更强有力的保证。例如只在主节点上进行特定类型的读取，而代价则是，应用层代码中处理这些问题通常会非常复杂，且容易出错</u>。

​	如果应用程序开发人员不必担心这么多底层的复制问题，而是假定数据库"做正确的事情"，情况就变得很简单。而这也是事务存在的原因，事务是数据库提供更强保证的一种方式。

​	**单节点上支持事务已经非常成熟**。<u>然而，在转向分布式数据库(即支持复制和分区)的过程中，有许多系统却选择放弃支持事务，并声称事务在性能与可用性方面代价过高，然后断言在可扩展的分布式系统中最终的一致性是无法避免的终极选择</u>。关于这样的表述，首先它有一定道理，但情况远不是所说的那么简单，我们将在本书其余部分展开讨论，尝试形成一个更为深入的观点。例如在第7章和第9章将理解事务，然后在第三部分再介绍其他一些替代机制。

## 5.3 多主节点复制

​	到目前为止，我们只考虑了单个主节点的主从复制架构。主从复制方法较为常见，但也存在其他一些有趣的方案。

​	首先，**主从复制存在一个明显的缺点：系统只有一个主节点，而所有写入都必须经由主节点**。如果由于某种原因，例如与主节点之间的网络中断而导致主节点无法连接，主从复制方案就会影响所有的写入操作。

​	<u>对主从复制模型进行自然的扩展，则可以配置多个主节点，每个主节点都可以接受写操作，后面复制的流程类似：处理写的每个主节点都必须将该数据更改转发到所有其他节点。这就是多主节点(也称为主-主，或主动/主动)复制</u>。此时，**每个主节点还同时扮演其他主节点的从节点**。

### 5.3.1 适用场景

​	**在一个数据中心内部使用多主节点基本没有太大意义，其复杂性已经超过所能带来的好处**。但是，在以下场景这种配置则是合理的。

#### 多数据中心

​	为了容忍整个数据中心级别故障或者更接近用户，可以把数据库的副本横跨多个数据中心。而如果使用常规的基于主从的复制模型，主节点势必只能放在其中的某一个数据中心，而所有写请求都必须经过该数据中心。

​	有了多主节点复制模型，则可以在每个数据中心都配置主节点，如图5-6所示的基本架构。**在每个数据中心内，采用常规的主从复制方案；而在数据中心之间，由各个数据中心的主节点来负责同其他数据中心的主节点进行数据的交换、更新**。

![多主复制 - 图1](https://static.sitestack.cn/projects/ddia/img/fig5-6.png)

​	可以对比一下在多数据中心环境下，部署单主节点的主从复制方案与多主复制方案之间的差异：

+ 性能

  **对于主从复制，每个写请求都必须经由广域网传送至主节点所在的数据中心。这会大大增加写入延迟，并基本偏离了采用多数据中心的初衷(即就近访问)**。<u>**而在多主节点模型中，每个写操作都可以在本地数据中心快速响应，然后采用异步复制方式将变化同步到其他数据中心**</u>。因此，对上层应用有效屏蔽了数据中心之间的网络延迟，使得终端用户所体验到的性能更好。

+ 容忍数据中心失效

  对于主从复制，如果主节点所在的数据中心发生故障，必须切换至另一个数据中将其中的一个从节点被提升为主节点。在多主节点模型中，每个数据中心则可以独立于其他数据中心继续运行，发生故障的数据中心在恢复之后更新到最新状态。

+ 容忍网络问题

  数据中心之间的通信通常经由广域网，它往往不如数据中心内的本地网络可靠。对于主从复制模型，由于写请求是同步操作，对数据中心之间的网络性能和稳定性等更加依赖。多主节点模型则通常采用异步复制，可以更好地容忍此类问题，例如临时网络闪断不会妨碍写请求最终成功。

​	有些数据库已内嵌支持了多主复制，但有些则借助外部工具来实现，例如 MySQL的Tungsten Replicator， PostgreSQL的BDR以及Oracle的Golden Gate。

​	尽管多主复制具有上述优势，但也存在一个很大的缺点：**<u>不同的数据中心可能会同时修改相同的数据，因而必须解决潜在的写冲突(如图5-6中的“冲突解决”)</u>**。我们会在本章稍后的“处理写入冲突”详细介绍。

​	由于多主复制在许多数据库中还只是新增的高级功能，所以可能存在配置方面的细小缺陷；在与其他数据库功能(例如自增主键，触发器和完整性约束等)交互时有时会出现意想不到的副作用。出于这个原因，一些人认为多主复制比较危险，应该谨慎使用或者避免使用。

#### 离线客户端操作

​	另一种多主复制比较适合的场景是，应用在与网络断开后还需要继续工作。

​	比如手机，笔记本电脑和其他设备上的日历应用程序。无论设备当前是否联网，都需要能够随时查看当前的会议安排(对应于读请求)或者添加新的会议(对应于写请求)。在离线状态下进行的任何更改，会在下次设备上线时，与服务器以及其他设备同步。

​	这种情况下，<u>每个设备都有一个充当主节点的本地数据库(用来接受写请求)</u>，然后在所有设备之间采用异步方式同步这些多主节点上的副本，同步滞后可能是几小时或者数天，具体时间取决于设备何时可以再次联网。

​	从架构层面来看，上述设置基本上等同于数据中心之间的多主复制，只不过是个极端情况，即一个设备就是数据中心，而且它们之间的网络连接非常不可靠。多个设备同步日历的例子表明，多主节点可以得到想要的结果，但中间过程依然有很多的未知数。

​	有一些工具可以使多主配置更为容易，如CouchDB就是为这种操作模式而设计的。

#### 协作编辑

​	实时协作编辑应用程序允许多个用户同时编辑文档。例如， Etherpad 和 Google Docs允许多人同时编辑文本文档或电子表格(算法简要会在本章后面的“自动冲突解决”中讨论)。

​	我们通常不会将协作编辑完全等价于数据库复制问题，但二者确实有很多相似之处。当一个用户编辑文档时，所做的更改会立即应用到本地副本(Web浏览器或客户端应用程序)，然后异步复制到服务器以及编辑同一文档的其他用户。

​	如果要确保不会发生编辑冲突，则应用程序必须先将文档锁定，然后才能对其进行编辑。如<u>果另一个用户想要编辑同一个文档，首先必须等到第一个用户提交修改并释放锁。这种协作模式相当于主从复制模型下在主节点上执行事务操作</u>。

​	为了加快协作编辑的效率，可编辑的粒度需要非常小。例如，单个按键甚至是全程无锁。然而另一方面，也会面临所有多主复制都存在的挑战，即如何解决冲突。

### 5.3.2 处理写冲突

​	多主复制的最大冋题是可能发生写冲突，这意味着必须有方案来解决冲突。

​	例如，两个用户同时编辑Wiki页面，如图5-7所示。用户1将页面的标题从A更改为B，与此同时用户2却将标题从A改为C。每个用户的更改都顺利地提交到本地主节点。但是，当更改被异步复制到对方时，却发现存在冲突。注意，正常情况下的主从复制则不会出现这种情况。

![多主复制 - 图2](https://static.sitestack.cn/projects/ddia/img/fig5-7.png)

#### 同步与异步冲突检测

​	**如果是主从复制数据库，第二个写请求要么会被阻塞直到第一个写完成，要么被中止(用户必须重试)**。<u>然而在多主节点的复制模型下，这两个写请求都是成功的，并且只能在稍后的时间点上才能异步检测到冲突，那时再要求用户层来解决冲突为时已晚</u>。

​	理论上，也可以做到同步冲突检测，即等待写请求完成对所有副本的同步，然后再通知用户写入成功。但是，这样做将会失去多主节点的主要优势：允许每个主节点独立接受写请求。<u>如果确实想要同步方式冲突检测，或许应该考虑采用**单主节点**的主从复制模型</u>。

#### 避免冲突

​	处理冲突最理想的策略是避免发生冲突，即如果应用层可以保证对特定记录的写请求总是通过同一个主节点，这样就不会发生写冲突。**现实中，由于不少多主节点复制模型所实现的冲突解决方案存在瑕疵，因此，<u>避免冲突反而成为大家普遍推荐的首选方案</u>**。

​	<u>例如，一个应用系统中，用户需要更新自己的数据，那么我们确保**特定用户的更新请求总是路由到特定的数据中心**，并在该数据中心的主节点上进行读/写。不同的用户则可能对应不同的主数据中心(例如根据用户的地理位置来选择)。从用户的角度来看，这基本等价于主从复制模型</u>。

​	但是，有时可能需要改变事先指定的主节点，例如由于该数据中心发生故障，不得不将流量重新路由到其他数据中心，或者是因为用户已经漫游到另一个位置，因而更靠近新数据中心。此时，冲突避免方式不再有效，必须有措施来处理同时写入冲突的可能性。

#### * 收敛于一致状态

​	**对于主从复制模型，数据更新符合顺序性原则**，即<u>**如果同一个字段有多个更新，则最后一个写操作将决定该字段的最终值**</u>。

​	对于多主节点复制模型，由于不存在这样的写入顺序，所以最终值也会变得不确定。在图5-7中，主节点1接受到请求把标题更新为B，然后更新为C；而在主节点2，则是相反的更新顺序。两者都无法辩驳谁更正确。

​	**<u>如果每个副本都只是按照它所看到写入的顺序执行，那么数据库最终将处于不一致状态</u>**。例如主节点1看到最终值C，而主节点2看到的是B，这绝对是不可接受的，**<u>所有的复制模型至少应该确保数据在所有副本中最终状态一定是一致的</u>**。因此，数据库必须以一种收敛趋同的方式来解决冲突，这也意味着当所有更改最终被复制、同步之后，所有副本的最终值是相同的。

​	实现收敛的冲突解决有以下可能的方式：

+ 给每个写入分配唯一的ID，例如，一个时间戳，一个足够长的随机数，一个UUID或者一个基于键-值的哈希，挑选最高ID的写入作为胜利者，并将其他写入丢弃。**如果基于时间戳，这种技术被称为最后写入者获胜**。虽然这种方法很流行但是很容易造成数据丢失。我们将在本章最后部分来详细解释。
+ 为每个副本分配一个唯一的ID，并制定规则，例如<u>序号高的副本写入始终优先于序号低的副本</u>。这种方法也可能会导致数据丢失。
+ 以某种方式将这些值合并在一起。例如，按字母顺序排序，然后拼接在一起(图5-7中，合并的标题可能类似于“B/C”)。
+ 利用预定义好的格式来记录和保留冲突相关的所有信息，然后依靠应用层的逻辑，事后解决冲突(可能会提示用户)。

#### * 自定义冲突解决逻辑

​	<u>解决冲突最合适的方式可能还是依靠应用层</u>，所以大多数多主节点复制模型都有工具来让用户编写应用代码来解决冲突。可以在写入时或在读取时执行这些代码逻辑：

+ 在写入时执行

  **只要数据库系统在复制变更日志时检测到冲突，就会调用应用层的冲突处理程序**。例如，Bucardo支持编写一段Perl代码。这个处理程序通常不能在线提示用户，而只能在后台运行，这样速度更快。

+ 在读取时执行

  **<u>当检测到冲突时，所有冲突写入值都会暂时保存下来</u>。下一次读取数据时，会将数据的多个版本读返回给应用层。应用层可能会提示用户或自动解决冲突，并将最后的结果返回到数据库**。 CouchDB采用了这样的处理方式。

​	**<u>注意，冲突解决通常用于单个行或文档，而不是整个事务</u>**。因此，**<u>如果有一个原子事务包含多个不同写请求(如第7章)，每个写请求仍然是分开考虑来解决冲突</u>**。

#### 什么是冲突？

​	有些冲突是显而易见的。在图5-7的例子中，两个写操作同时修改同一个记录中的同一个字段，并将其设置为不同的值。毫无疑问，这就是一个冲突。

​	而其他类型的冲突可能会非常微妙，更难以发现。例如一个会议室预订系统，它主要记录哪个房间由哪个人在哪个时间段所预订。这个应用程序需要确保每个房间只能有一组人同时预定(即不得有相同房间的重复预订)。如果为同一个房间创建两个不同的预订，可能会发生冲突。<u>尽管应用在预订时会检查房间是否可用，但如果两个预订是在两个不同的主节点上进行，则还是存在冲突的可能</u>。

> **自动冲突解决**
>
> ​	冲突解决的规则可能会变得越来越复杂，且自定义代码很容易出错。亚马逊是个经常被引用的反面例子：有一段时间，购物的冲突解决逻辑依靠用户的购物车页面，后者保存了所有的物品，但顾客有时候会发现之前已经被拿掉的商品，再次出现在他们的购物车中。
>
> ​	有一些有意思的研究尝试自动解决并发修改所引起的冲突。下面这些方法值得一看：
>
> 1. **无冲突的复制数据类型(Conflict-free Replicated Datatypes，CRDT)**。CRDT是可以由多个用户同时编辑的数据结构，包括map、 ordered list、计数器等，并且以内置的合理方式自动地解决冲突。一些CRDT已经在Riak2.0中得以具体实现。
> 2. **可合并的持久数据结构(Mergeable persistent data)。它跟踪变更历史，类似于Git版本控制系统，并提出三向合并功能(three-way merge function，CRDT采用双向合并)**。
> 3. **操作转换(Operational transformation)** 。它是Etherpad和Google Docs等协作编辑应用背后的冲突解决算法。专为可同时编辑的有序列表而设计，如文本文档的字符列表。
>
> ​	这些算法总体来讲还处于早期阶段，但将来它们可能会被整合到更多的数据系统中。这些自动冲突解决方案可以使主复制模型更简单、更容易被应用程序来集成。

​	很遗憾，这里没有现成的答案。通过接下来的几章，我们将对这个问题进行深入的剖析和讲解。我们将在第7章看到更多的冲突示例，在第12章中讨论检测和解决冲突的可扩展方法。

### 5.3.3 拓扑结构

​	**复制的拓扑结构描述了写请求从一个节点的传播到其他节点的通信路径**。如果有两个主节点，如图5-7所示，则只存在一个合理的拓扑结构：主节点1必须把所有的写同步到主节点2，反之亦然。但如果存在两个以上的主节点，则会有多个可能的同步拓扑结构，如图5-8所示。

![多主复制 - 图3](https://static.sitestack.cn/projects/ddia/img/fig5-8.png)

​	**最常见的拓扑结构是全部-至-全部，见图5-8(c)，每个主节点将其写入同步到其他所有主节点**。而其他一些拓扑结构也有普遍使用，例如，<u>**默认情况下MySQL只支持环形拓扑结构**，其中的每个节点接收来自前序节点的写入，并将这些写入(加上自己的写入)转发给后序节点</u>。另一种流行的拓扑是星形结构：一个指定的根节点将写入转发给所有其他节点。星形拓扑还可以推广到树状结构。

​	在环形和星形拓扑中，写请求需要通过多个节点才能到达所有的副本，即中间节点需要转发从其他节点收到的数据变更。<u>为防止无限循环，每个节点需要赋予一个唯一的标识符，在复制日志中的每个写请求都标记了已通过的节点标识符</u>。如果某个节点收到了包含自身标识符的数据更改，表明该请求已经被处理过，因此会忽略此变更请求，避免重复转发。

​	**环形和星形拓扑的问题是，如果某一个节点发生了故障，在修复之前，会影响其他节点之间复制日志的转发**。可以采用重新配置拓扑结构的方法暂时排除掉故障节点。在大多数部署中，这种重新配置必须手动完成。而对于链接更密集的拓扑(如全部到全部)，消息可以沿着不同的路径传播，避免了单点故障，因而有更好的容错性。

​	但另一方面，**<u>全链接拓扑也存在一些自身的问题。主要是存在某些网络链路比其他链路更快的情况(例如由于不同网络拥塞)，从而导致复制日志之间的覆盖</u>**，如图5-9所示。

​	在图5-9中，客户端A向主节点1的表中首先插入一行，然后客户端B在主节点3上对该行记录进行更新。而在主节点2上，由于网络原因可能出现意外的写日志复制顺序，例如它先接收到了主节点3的更新日志(从主节点2的角度来看，这是对数据库中不存在行的更新操作)，之后才接收到主节点1的插入日志(按道理应该在更新日志之前到达)。

![多主复制 - 图4](https://static.sitestack.cn/projects/ddia/img/fig5-9.png)

​	<u>这里涉及到一个因果关系问题，类似于在本章前面“**前缀一致读**”所看到的：更新操乍一定是依赖于先前完成的插入，因此我们要确保所有节点上一定先接收插入日志，然后再处理更新。在每笔写日志里简单地添加时间戳还不够，**主要因为无法确保时钟完全同步**，因而无法在主节点2上正确地排序所收到日志(参见第8章)</u>。

​	为了使得日志消息正确有序，可以使用一种称为**版本向量**的技术，本章稍后将讨论这种技术(参见本章后面的“检测并发写入”)。需要指出，<u>冲突检测技术在许多多主节点复制系统中的实现还不够完善</u>，例如在撰写本书时， PostgreSQL BDR尚不支持写操作的因果排序，而 MySQL的Tungsten Replicator甚至还没有基本的冲突检测功能。

​	如果正在使用支持多主节点复制的系统，这些问题都值得注意，仔细查阅相关文档，详细测试这些数据库，以确保它确实提供所期望的功能。

## 5.4 无主节点复制

​	到目前为止本章所讨论的复制方法，包括单主节点和多主节点复制，都是基于这样种核心思路，<u>即客户端先向某个节点(主节点)发送写请求，然后数据库系统负责将写请求复制到其他副本。由主节点决定写操作的顺序，从节点按照相同的顺序来应用主节点所发送的写日志</u>。

​	**一些数据存储系统则采用了不同的设计思路：选择放弃主节点，允许任何副本直接接受来自客户端的写请求**。其实最早的数据复制系统就是无主节点的(或称为**去中心复制**，**无中心复制**)，但后来到了关系数据库主导的时代，这个想法被大家选择性遗忘了。当亚马逊内部采用了Dynamo系统之后，无主复制又再次成为一种时髦的数据库架构。Riak、Cassandra和Voldemort都是受Dynamo启发而设计的无主节点、开源数据库系统，这类数据库也被称为Dynamo风格数据库。

​	**对于某些无主节点系统实现，客户端直接将其写请求发送到多副本，而在其他一些实现中，由一个协调者节点代表客户端进行写入，但与主节点的数据库不同，协调者并不负责写入顺序的维护**。我们很快就会看到，这种设计上的差异对数据库的使用方式有着深刻的影响。

### * 5.4.1 节点失效时写入数据库

​	<u>**假设一个三副本数据库，其中一个副本当前不可用(例如正在重启以安装系统更新)。在基于主节点复制模型下，如果要继续处理写操作，则需要执行切换操作**(参阅本章前面的“处理节点失效”)</u>。

​	**对于无主节点配置，则不存在这样的切换操作**。图5-10展示了所发生的情况：用户1234将写请求并行发送到三个副本，有两个可用副本接受写请求，而不可用的副本无法处理该写请求。如果假定三个副本中有两个成功确认写操作，用户1234收到两个确认的回复之后，即可认为写入成功。客户完全可以忽略其中一个副本无法写入的情况。

​	<u>现在设想一下，失效的节点之后重新上线，而客户端又开始从中读取內容。由于节点失效期间发生的任何写入在该节点上都尚未同步，因此读取可能会得到过期的数据</u>。

​	<u>为了解决这个问题，当一个客户端从数据库中读取数据时，它**不是向一个副本发送请求，而是并行地发送到多个副本**。客户端可能会得到不同节点的不同响应，包括某些节点的新值和某些节点的旧值。可以采用**版本号**技术确定哪个值更新</u>(参见本章后面的“检测并发写入”)。

#### * 读修复和反熵

​	**复制模型应确保所有数据最终复制到所有的副本**。当一个失效的节点重新上线之后，它如何赶上中间错过的那些写请求呢?

![无主复制 - 图1](https://static.sitestack.cn/projects/ddia/img/fig5-10.png)

Dynamo风格的数据存储系统经常使用以下两种机制：

+ 读修复

  当客户端并行读取多个副本时，可以检测到过期的返回值。例如，在图5-10中用户2345从副本3获得的是版本6，而从副本1和2得到的是版本7。客户端可以判断副本3一个过期值，然后将新值写入到该副本。这种方法主要**适合那些被频繁读取的场景**。

+ **反熵过程**

  此外，一些数据存储有后台进程不断查找副本之间数据的差异，将任何缺少的数据从一个副本复制到另一个副本。**与基于主节点复制的复制日志不同，此反熵过程并不保证以特定的顺序复制写入，并且会引入明显的同步滞后**。

​	并不是所有的系统都实现了上述两种方案。例如， Voldemort目前没有反熵过程。<u>请注意，当缺少反熵过程的支持时，由于**读时修复只在发生读取时才可能执行修复，那些很少访问的数据有可能在某些副本中已经丢失而无法检测到，从而降低了写的持久性**</u>。

#### * 读写quorum

​	图5-10的例子中，三个副本中如果有两个以上完成处理，写入即可认为成功。如果三个副本中只有一个完成了写请求，会怎样呢？依次类推，究竟多少个副本完成才可以认为写成功?

​	我们知道，<u>成功的写操作要求三个副本中至少两个完成，这意味着至多有一个副本可能包含旧值</u>。<u>因此，**在读取时需要至少向两个副本发起读请求，通过版本号可以确定定至少有一个包含新值**。如果第三个副本出现停机或响应缓慢，则读取仍可以继续并返回最新值</u>。

​	把上述道理推广到一般情况，**<u>如果有n个副本，写入需要w个节点确认，读取必须至少查询r个节点，则只要w+r>n，读取的节点中一定会包含最新值</u>**。例如在前面的例子中，n=3，w=2，r=2。满足上述这些r、w值的读/写操作称之为法定票数读(或仲裁读)或法定票数写(或仲裁写)。**也可以认为r和w是用于判定读、写是否有效的最低票数**。

​	**在Dynamo风格的数据库中，参数n、w和r通常是可配置的**。一个常见的选择是设置n为某奇数(通常为3或5)，w=r=(n+1)/2(向上舍入)。也可以根据自己的需求灵活调整这些配置。<u>例如，对于读多写少的负载，设置w=n和r=1比较合适，这样读取速度更快，但是一个失效的节点就会使得数据库所有写入因无法完成quorum而失败</u>。

> ​	集群中可能存在多于n个节点，但是数据只会保存在所设定的n个节点上。我们可以对数据集进行**分区**，从而支持比节点容纳上限更大的数据集，第6章我们将讨论分区技术。

​	**仲裁条件w+r>n定义了系统可容忍的失效节点数**，如下所示：

+ 当w<n，如果一个节点不可用，仍然可以处理写入。
+ 当r<n，如果一个节点不可用，仍然可以处理读取。
+ 假定n=3，w=2，r=2，则可以容忍一个不可用的节点。
+ 假定n=5，w=3，r=3，则可以容忍两个不可用的节点。如图5-11所示。
+ **<u>通常，读取和写入操作总是并行发送到所有的n个副本。参数w和参数r只是决定要等待的节点数</u>**。即有多少个节点需要返回结果，我们才能判断出结果的正确性。

​	**<u>如果可用节点数小于所需的w或r，则写入或读取就会返回错误</u>**。不可用的原因可能有很多种，包括节点崩溃或者断电而关机，执行操作时岀错(例如磁盘已满而无法写入)，客户端和节点之间的网络中断等。这里，我们只需关心节点是否有返回值，而不需区分出错的具体原因。

![无主复制 - 图2](https://static.sitestack.cn/projects/ddia/img/fig5-11.png)

### * 5.4.2 Quorum 一致性的局限性

​	<u>**如果有n个副本，并且配置w和r，使得w+r>n，可以预期可以读取到一个最新值。之所以这样，是因为成功写入的节点集合和读取的节点集合必然有重合，这样读取的节点中至少有一个具有最新值**</u>(见图5-11)。

​	<u>**通常，设定r和w为简单多数(多于n/2)节点，即可确保w+r>n，且同时容忍多达n/2个节点故障**</u>。但是， quorum不一定非得是多数，**<u>读和写的节点集中有一个重叠的节点才是最关键的</u>**。设定其他的 quorum分配数也是可行的。

​	<u>也可以将w和r设置为较小的数字，从而w+r<=n(即不满足仲裁条件)。此时，读取和写入操作仍会被发送到n个节点，但只需等待更少的节点回应即可返回</u>。

​	由于w和r配置的节点数较小，读取请求当中可能恰好没有包含新值的节点，因此最终可能会返回一个过期的旧值。好的一方面是，这种配置可以获得更低的延迟和更高的可用性，例如网络中断，许多副本变得无法访问，相比而言有更高的概率继续处理读取和写入。只有当可用的副本数已经低于w或r时，数据库才会变得无法读/写，即处于不可用状态。

​	**即使在w+r>n的情况下，也可能存在返回旧值的边界条件**。这主要取决于具体实现，可能的情况包括:

+ 如果采用了sloppy quorum(参阅本章后面的"宽松的quorum与数据回传")，写操作的w节点和读取的r节点可能完全不同，因此无法保证读写请求一定存在重叠的节点。

+ 如果两个写操作同时发生，则无法明确先后顺序。这种情况下，唯一安全的解决方案是合并并发写入(参见本章前面的“处理写冲突”)。如果根据时间戳(最后写入获胜)挑选胜者，则由于时钟偏差问题，某些写入可能会被错误地抛弃。
+ **如果写操作与读操作同时发生，写操作可能仅在一部分副本上完成。此时，读取时返回旧值还是新值存在不确定性**。
+ **如果某些副本上已经写入成功，而其他一些副本发生写入失败(例如磁盘巳满)，且总的成功副本数少于w，那些已成功的副木上不会做回滚**。这意味着尽管这样的写操作被视为失败，后续的读操作仍可能返回新值。
+ **<u>如果具有新值的节点后来发生失效，但恢复数据来自某个旧值，则总的新值副本数会低于w，这就打破了之前的判定条件</u>**。
+ 即使一切工作正常，也会出现一些边界情况，如第9章所介绍的“可线性化与quorun“。

​	**<u>因此，虽然quorum设计上似乎可以保证读取最新值，但现实情况却往往更加复杂</u>**。<u>Dynamo风格的数据库通常是针对最终一致性场景而优化的。我们建议最妤不要把参数w和r视为绝对的保证，而是一种灵活可调的读取新值的概率。</u>

​	例如，<u>这里通常无法得到本章前面的“复制滞后问题”中所罗列的一致性保证，包括**写后读**、**单调读**、**前缀一致读**等，因此前面讨论种种异常同样会发生在这里</u>。**如果确实需要更强的保证，需要考虑事务与共识问题**，接下来的第7章和第9章将对此展开讨论。

#### * 监控旧值

​	从运维角度来看，监视数据库是否返回最新结果非常重要。即使应用程序可以容忍读取旧值，也需要仔细了解复制的当前运行状态。<u>如果已经出现了明显的滞后，它就是个重要的信号提醒我们需要采取必要措施来排查原因</u>(例如网络问题或节点超负荷)。

​	对于主从复制的系统，数据库通常会导出复制滞后的相关指标，可以将其集成到统一监控模块。原理大概是这样，**由于主节点和从节点上写入都遵从相同的顺序，而毎个节点都维护了复制日志执行的当前偏移量。<u>通过对比主节点和从节点当前偏移量的差值，即可衡量该从节点落后于主节点的程度</u>**。

​	然而，<u>对于无主节点复制的系统，并**没有固定的写入顺序**，因而监控就变得更加困难</u>。而且，<u>如果数据库只支持读时修复(不支持反熵)，那么旧值的落后就没有一个上限。例如如果一个值很少被访问，那么所返回的旧值可能非常之古老</u>。

​	目前针对无主节点复制系统已经有一些研究，根据参数n，w和r来预测读到旧值的期望百分比。不过，总体讲还不是很普及。即便如此，**将旧值监控纳入到数据库标准指标集中还是很有必要**。要知道，**最终一致性其实是个非常模糊的保证，从可操作性上讲，量化究竟何为“最终”很有实际价值**。

#### * 宽松的quorum与数据回传

​	<u>配置适当quorum的数据库系统可以容忍某些节点故障，也不需要执行故障切换。它们还可以容忍某些节点变慢，这是因为请求并不需要等待所有n个节点的响应，只需w或r节点响应即可。对于需要高可用和低延迟的场景来说，还可以容忍偶尔读取旧值，所有这些特性使之具有很高的吸引力</u>。

​	但是， quorum并不总如期待的那样提供高容错能力。一个网络中断可以很容易切断一个客户端到多数数据库节点的链接。尽管这些集群节点是活着的，而且其他客户端也确实可以正常链接，但是**对于断掉链接的客户端来讲，情况无疑等价于集群整体失效**。这种情况下，很可能无法满足最低的w和r所要求的节点数，因此导致客户端无法满足quorum要求。

​	在一个大规模集群中(节点数远大于n个)，<u>客户可能在网络中断期间还能连接到某些数据库节点，但这些节点又不是能够满足数据仲裁的那些节点</u>。此时，数据库设计者就面临着一个选择：

+ 如果无法达到w或r所要求quorum，将错误明确地返回给客户端?
+ 或者，我们是否应该接受该写请求，只是将它们暂时写入一些可访问的节点中？注意，<u>这些节点并不在n个节点集合中</u>。

​	**后一种方案称之为放松的仲裁：写入和读取仍然需要w和r个成功的响应，但包含了那些并不在先前指定的n个节点**。打个比方，如果你把不小心把自己锁在房子外面，可能会敲开邻居家的门，请求否可以坐在沙发上暂时休息一下。

​	**一旦网络问题得到解决，临时节点需要把接收到的写入全部发送到原始主节点上**。<u>这就是所谓的数据回传(或暗示移交)</u>。即一旦你找到了房子的钥匙，你的邻居会礼貌地请你离开沙发回到自己的家中。

​	可以看出， **<u>sloppy quorum对于提高写入可用性特别有用：只要有任何w个节点可用，数据库就可以接受新的写入</u>**。<u>然而这意味着，即使满足w+r>n，也不能保证在读取某个键时，一定能读到最新值，因为**新值可能被临时写入n之外的某些节点且尚未回传过来**</u>。

​	因此，**sloppy quorum并非传统意义上quorum。而更像是为了数据持久性而设计的一个保证措施，除非回传结束，否则它无法保证客户端一定能从r个节点读到新值**。

​	**<u>目前，所有Dynamo风格的系统都已经支持sloppy quorum</u>**。在Riak中，默认启用，而在 Cassandra和 Voldemort中则默认关闭。

#### 多数据中心操作

​	我们之前以多数据中心为例介绍了多主节点复制(参见本章前面的“多主节点复制”)。而<u>无主节点复制由于旨在更好地容忍并发写入冲突</u>，网络中断和延迟尖峰等，因此也可适用于多数据中心操作。

​	Cassandra和Voldemort在其默认配置的无主节点模型中都支持跨数据中心操作：<u>副本的数量n是包含所有数据中心的节点总数</u>。配置时，可以指定每个数据中心各有多少副本。每个客户端的写入都会发送到所有副本，但客户端通常只会等待来自本地数据中心内的quorum节点数的确认，这样避免了高延迟和跨数据中心可能的网络异常。<u>尽管可以灵活配置，但对远程数据中心的写入由于延迟很高，通常都被配置为**异步方式**</u>。

​	Riak则将客户端与数据库节点之间的通信限制在一个数据中心内，因此<u>n描述的是以个数据中心内的副本数量</u>。<u>集群之间跨数据中心的复制则在后台异步运行，类似于多主节点复制风格</u>。

### * 5.4.3 检测并发写

​	Dynamo风格的数据库允许多个客户端对相同的主键同时发起写操作，即使采用严格的quorum机制也可能会发生写冲突。这与多主节复制类似(参见本章前面的“处理写冲突”)，此外，由于读时修复或者数据回传也会导致并发写冲突。

​	一个核心问题是，由于网络延迟不稳定或者局部失效，请求在不同的节点上可能会呈现不同的顺序。如图5-12所示，对于包含三个节点的数据系统，客户端A和B同时向主键X发起写请求：

+ 节点1收到来自客户端A的写请求，但由于节点失效，没有收到客户端B的写请求。

+ 节点2首先收到A的写请求，然后是B的写请求。
+ 与节点2相反，节点3首先收到B的写请求，然后是A的写请求

![无主复制 - 图3](https://static.sitestack.cn/projects/ddia/img/fig5-12.png)

​	**如果节点每当收到新的写请求时就简单地覆盖原有的主键，那么这些节点将永久无法达成一致**，如图5-12中的所示，节点2认为X的最终值是B，而其他节点认为值是A。

​	我们知道副本应该收敛于相同的内容，这样才能达成最终一致。但如何才能做到呢？有人可能希望数据副本之间能自动处理，然而非常不幸，<u>目前大多数的系统实现都无法令人满意，如果你不想丢失数据，应用开发者必须了解很多关于数据库内部冲突处理的机制</u>。

​	我们已经在本章前面的“处理写冲突”简要介绍了一些解决冲突的技巧。在总结本章之前，我们来更详细地探讨这个问题。

#### 最后写入者获胜(丢弃并发写入)

​	**一种实现最终收敛的方法是，每个副本总是保存最新值，允许覆盖并丢弃旧值**。那么，<u>假定每个写请求都最终同步到所有副本，只要我们有一个明确的方法来确定哪个写入是最新的，则副本可以最终收敛到相同的值</u>。

​	这个想法其实有些争议，**关键点在于前面所提到关于如何定义“最新”**。在图5-12的例子中，当客户端向数据库节点发送写请求时，一个客户端无法意识到另一个客户端，也不清楚哪一个先发生。其实，争辩哪个先发生没有太大意义，当我们说支持写入并发，也就意味着它们的顺序是不确定的。

​	**即使无法确定写请求的“自然顺序”，我们可以强制对其排序**。例如，<u>为每个写请求附加一个时间戳，然后选择最新即最大的时间戳，丢弃较早时间戳的写入。这个冲突解决算法被称为**最后写入者获胜(last write wins，LWW)**</u>，它是Cassandra仅有的冲突解决方法，而在Riak中，它是可选方案之一。

​	**LWW可以实现最终收敛的目标，但是以牺牲数据持久性为代价**。<u>如果同一个主键有多个并发写，即使这些并发写都向客户端报告成功(因为完成了写入w个副本)，但**最后只有一个写入值会存活下来**，其他的将被系统默默丢弃。此外，LWW甚至可能会删除那些非并发写，我们将在第8章“时间戳与事件顺序”中举例说明</u>。

​	在一些场景如缓存系统，覆盖写是可以接受的。如果覆盖、丟失数据不可接受，则LWW并不是解决冲突很好的选择。

​	**要确保LWW安全无副作用的唯一方法是，<u>只写入一次然后写入值视为不可变</u>，这样就避免了对同一个主键的并发(覆盖)写**。例如， Cassandra的一个推荐使用方法就是采用UUID作为主键，这样每个写操作都针对的不同的、系统唯一的主键。

#### * Happens-before关系和并发

​	如何判断两个操作是否是并发呢？首先为了建立起一个快速的直觉判断，我们先来看一些例子：

+ 图5-9中，两个写入不是并发的：A的插入操作发生在B的增量修改之前，B的递增是基于A插入的行。换句话说，B后发生，其操作建立在A基础之上。A和B属于**因果依赖**关系。
+ 另一个例子，图5-12中的两个写入则是并发的:每个客户端启动写操作时，并不知道另一个客户端是否也在同一个主键上执行操作。因此，操作之间不存在**因果关系**。

​	如果B知道A，或者依赖于A，或者以某种方式在A基础上构建，则称操作A在操作B之前发生。这是定义何为并发的关键。事实上，我们也可以简单地说，**如果两个操作都不在另一个之前发生，那么操作是并发的(或者两者都不知道对方)**。

​	因此，对于两个操作A和B，一共存在三种可能性：A在B之前发生，或者B在A之前发生，或者A和B并发。我们需要的是一个算法来判定两个操作是否并发。<u>如果一个操作发生在另一个操作之前，则后面的操作可以覆盖较早的操作。如果属于并发，就需要解决潜在的冲突问题</u>。

> **并发性、时间和相对性**
>
> ​	<u>通常如果两个操作“同时”发生，则称之为并发，然而事实上，操作是否在时间上重叠并不重要。**由于分布式系统中复杂的时钟同步问题(第8章将会详细讨论)，现实当中，我们很难严格确定它们是否同时发生**</u>。
>
> ​	**<u>为更好地定义并发性，我们并不依赖确切的发生时间，即不管物理的时机如何，如果两个操作并不需要意识到对方，我们即可声称它们是并发操作</u>**。一些人尝试把这个思路与物理学中狭义相对论联系起来，后者引入了“信息传递不能超越光速”的假定，如果两个事件发生的间隔短于光在它们之间的折返，那么这两个事件不可能有相互影响，因此就是并发。
>
> ​	**在计算机系统中，即使光速快到允许一个操作影响到另一个操作，但两个操作仍可能被定义为并发。例如，发生了网络拥塞或中断，可能就会出现两个操作由于网络问题导致一个操作无法感知另一个，因此二者成为并发**。

#### * 确定前后关系

​	我们来看一个确定操作并发性的算法，即两个操作究竟属于并发还是一个发生在另个之前(依赖关系)。简单起见，我们先从只有一个副本的数据库开始，在阐明其原理之后，将其推广到有多个副本的无主节点数据库。

​	图5-13的例子是两个客户端同时向购物篮车加商品(如果觉得这个例子太微不足道，可以类比为，两个空中交管员同时把飞机添加到他们所管理的追踪目标里)。初始时购物车为空。然后两个客户端向数据库共发出五次写入操作:

1. 客户端1首先将牛奶加入购物车。这是第一次写入该主键的值，服务器保存成功然后分配版本1，服务器将值与版本号一起返回给该客户端1。
2. 客户端2将鸡蛋加入购物车，此时它并不知道客户端1已添加了牛奶，而是认为鸡蛋是购物车中的唯一物品。服务器为此写入并分配版本2，然后将鸡蛋和牛奶存储为两个单独的值，最好将这两个值与版本号2返回给客户端2。
3. 同理，客户端1也并不意识上述步骤2，想要将面粉加入购物车，且以为购物车的内容应该是[牛奶，面粉]，将此值与版本号1一起发送到服务器。服务器可以从版本号中知道[牛奶，面粉]的新值要取代先前值[牛奶]，但值[鸡蛋]则是新的并发操作。因此，服务器将版本3分配给[牛奶，面粉]并覆盖版本1的[牛奶]，同时保留版本2的值[鸡蛋]，将二者同时返回给客户端1。

![无主复制 - 图4](https://static.sitestack.cn/projects/ddia/img/fig5-13.png)

4. 同时，客户端2想要加入火腿，也不知道客户端1刚刚加了面粉。其在最后一个响应中从服务器收到的两个值是[牛奶]和[蛋]，现在合并这些值，并添加火腿形成一个新的值[鸡蛋，牛奶，火腿]。它将该值与前一个版本号2一起发送到服务器。服务器检测到版本2会覆盖[鸡蛋]，但与[牛奶，面粉]是同时发生，所以设置为版本4并将所有这些值发送给客户端2。
5. 最后，客户端1想要加培根。它以前在版本3中从服务器接收[牛奶，面粉]和[鸡蛋]，所以合并这些值，添加培根，并将最终值[牛奶，面粉，鸡蛋，培根]连同版本号3来覆盖[牛奶，面粉]，但与[鸡蛋，牛奶，火腿]并发，所以服务器会保留这些并发值。

​	图5-13操作之间的数据流可以通过图5-14形象展示。箭头表示某个操作发生在另一个操作之前，即后面的操作“知道”或是“依赖”于前面的操作。<u>在这个例子中，因为总有另一个操作同时进行，所以每个客户端都没有时时刻刻和服务器上的数据保持同步。但是，新版本值最终会覆盖旧值，且不会发生已写入值的丢失</u>。

![无主复制 - 图5](https://static.sitestack.cn/projects/ddia/img/fig5-14.png)

​	需要注意的是，服务器判断操作是否并发的依据主要依靠**对比版本号**，而并不需要解释新旧值本身(值可以是任何数据结构)。算法的工作流程如下：

+ 服务器为每个主键维护一个版本号，每当主键新值写入时递增版本号，并将新版本号与写入的值一起保存。
+ <u>当客户端读取主键时，服务器将返回所有(未被覆盖的)当前值以及最新的版本号。**且要求写之前，客户必须先发送读请求**</u>。
+ **客户端写主键，写请求必须包含之前读到的版本号、读到的值和新值合并后的集合**。写请求的响应可以像读操作一样，会返回所有当前值，这样就可以像购物车例子那样一步步链接起多个写入的值。
+ **当服务器收到带有特定版本号的写入时，覆盖该版本号或更低版本的所有值(因为知道这些值已经被合并到新传入的值集合中)，但<u>必须保存更高版本号的所有值(因为这些值与当前的写操作属于并发)</u>**。

​	**<u>当写请求包含了前一次读取的版本号时，意味着修改的是基于以前的状态。如果一个写请求没有包含版本号，它将与所有其他写入同时进行，不会覆盖任何已有值，其传入的值将包含在后续读请求的返回值列表当中</u>**。

#### * 合并同时写入的值

​	上述算法可以保证不会发生数据丢弃，但不幸的是，客户端需要做一些额外的工作：即如果多个操作并发发生，则<u>客户端必须通过**合并并发写入的值**来继承旧值</u>。Riak称这些并发值为**兄弟关系**。

​	合并本质上与先前讨论的多节点复制冲突解决类似(参阅本章前面的“处理写冲突”)。一个简单的方法是基于版本号或时间戳(即最后写入获胜)来选择其中的一个值，但这意味着会丢失部分数据。所以，需要在应用程序代码中额外些工作。

​	以购物车为例，合并并发值的合理方式是包含新值和旧值(union操作)。图5-14中，两个客户端最后的值分别是[牛奶，面粉，鸡蛋，熏肉]和[鸡蛋，牛奶，火腿]。注意，虽然牛奶和鸡蛋只是写入了一次，但它在两个客户端中均有出现。合并的最终值应该是[牛奶，面粉，鸡蛋，培根，火腿]，其中去掉了重复值。

​	<u>然而，设想一下人们也可以在购物车中删除商品，此时把并发值都合并起来可能会导致错误的结果：**如果合并了两个客户端的值，且其中有一个商品被某客户端删除掉，则被删除的项目会再次出现在合并的终值中**</u>。<u>为了防止该问题，项目在删除时不能简单地从数据库中删除，**系统必须保留一个对应的版本号以恰当的标记该项目需要在合并时被剔除。这种删除标记被称为墓碑**(之前我们在第3章“哈希索引”日志压缩时提到过)</u>。

​	<u>考虑到在应用代码中合并非常复杂且容易出错，因此可以设计一些专门的数据结构来自动执行合并，例如，Riak支持称为CRDT一系列数据结构(具体参见本章前面的“自动冲突解决”)，以合理的方式高效自动合并，包括支持删除标记</u>。

#### * 版本矢量

​	图5-13中的示例只有一个副本。如果存在多个副本但没有主节点，算法又该如何呢?

​	图5-13使用单个版本号来捕获操作之间的依赖关系，<u>当多个副本同时接受写入时，这是不够的。因此我们需要为每个副本和每个主键均定义一个版本号。每个副本在处理写入时增加自己的版本号，并且跟踪从其他副本看到的版本号。通过这些信息来指示要覆盖哪些值、该保留哪些并发值</u>。

​	<u>所有副本的版本号集合称为**版本矢量**</u>。这种思路还有一些变体，但最有趣的可能是在Riak 2.0中使用的虚线版本矢量。我们无法在此深入其细节，但是它的工作方式与购物车例子所展示的非常相似。

​	与图5-13中版本号类似，当读取数据时，数据库副本会返回版本矢量给客户端，而在随后写入时需要将将版本信息包含在请求当中一起发送到数据库。Riak将版本矢量编码为一个称之为因果上下文的字符串。**版本矢量技术使数据库可以区分究竟应该覆盖写还是保留并发值**。

​	另外，就像单副本的例子一样，应用程序仍然需要执行合并操作。版本矢量可以保证从某一个副本读取值然后写入到另一个副本，而这些值可能会导致在其他副本上衍生出来新的“兄弟”值，但至少不会发生数据丢失且可以正确合并所有并发值。

> **版本矢量和矢量时钟**
>
> 版本矢量有时也被称为矢量时钟，然而两者并不完全相同，细微差别可参阅文献。**<u>简而言之，当需要比较副本状态时，应当采用版本矢量</u>**。

## 5.5 小结

​	本章，我们详细探讨了复制相关的话题。复制或者多副本技术主要服务于以下目的：

+ 高可用性：即使某台机器(或多台机器，或整个数据中心)出现故障，系统也能保持正常运行。
+ 连接断开与容错：允许应用程序在出现网络中断时继续工作。
+ 低延迟：将数据放置在距离用户较近的地方，从而实现更快地交互。
+ 可扩展性：釆用多副本读取，大幅提高系统读操作的吞吐量。

​	在多台机器上保存多份相同的数据副本，看似只是个很简单的目标，但事实上复制技术是一个非常烧脑的问题。需要仔细考虑并发以及所有可能出错的环节，并小心处理故障之后的各种情形。<u>最最基本的，要处理好节点不可用与网络中断问题，这里甚至还没考虑一些更隐蔽的失效场景，例如由于软件bug而导致的无提示的数据损坏</u>。

我们主要讨论了三种多副本方案：

+ 主从复制

  所有的客户端写入操作都发送到某一个节点(主节点)，由该节点负责将数据更改事件发送到其他副本(从节点)。每个副本都可以接收读请求，但内容可能是过期值。

+ 多主节点复制

  系统存在多个主节点，每个都可以接收写请求，客户端将写请求发送到其中的一个主节点上，由该主节点负责将数据更改事件同步到其他主节点和自己的从节点。

+ **无主节点复制**

  <u>客户端将写请求发送到多个节点上，读取时从多个节点上并行读取，以此检测和纠正某些过期数据</u>。

​	每种方法都有其优点和缺点。<u>**主从复制非常流行，主要是因为它很容易理解，也不需要担心冲突问题**。而万一出现节点失效、网络中断或者延迟抖动等情况，多主节点和无主节点复制方案会更加可靠，不过背后的代价则是系统的复杂性和弱一致性保证</u>。

​	复制可以是同步的，也可以是异步的，而一旦发生故障，二者的表现差异会对系统行为产生深远的影响。在系统稳定状态下异步复制性能优秀，但仍须认真考虑一旦岀现复制滞后和节点失效两种场景会导致何种影响。**万一某个主节点发生故障，而一个异步更新的从节点被提升为新的主节点，要意识到最新确认的数据可能有丢失的风险**。

​	我们还分析了由于复制滞后所引起的一些奇怪效应，并讨论了以下一致性模型，来帮助应用程序处理复制滞后：

+ 写后读一致性

  保证用户总能看到自己所提交的最新数据。

+ **单调读**

  **用户在某个时间点读到数据之后，保证此后不会出现比该时间点更早的数据**。

+ **前缀一致读**

  **保证数据之间的因果关系，例如，总是以正确的顺序先读取问题，然后看到回答**。

​	最后，我们讨论了多主节点和无主节点复制方案所引入的并发问题。即由于多个写可能同时发生，继而可能产生冲突。为此，我们研究了一个算法使得数据库系统可以判定某操作是否发生在另一个操作之前，或者是同时发生。接下来，探讨采用合并并发更新值的方法来解决冲突。

​	下一章我们继续研究多节点上数据的分布问题，与本章不同的是，它是针对一个大型数据集而采用**分区技术**。

# 第6章 数据分区

​	第5章讨论了复制技术，即在不同节点上保存相同数据的多个副本。然而，<u>面对一些海量数据集或非常高的査询压力，复制技术还不够，我们还需要将数据拆分成为分区，也称为分片</u>。

> 术语澄清
>
> 这里我们所讨论的分区，在不同系统有着不同的称呼，例如它对应于MongoDB，Elasticsearch和Solrcloud中的shard，Hbase的region，Bigtable中的tablet，Cassandra和Riak中的vnode，以及Couchbase中的 vBucket。总体而言，分区是最普遍的术语。

​	**分区通常是这样定义的，即每一条数据(或者每条记录，每行或每个文档)只属于某个特定分区**。实现的方法有多种，稍后将逐一介绍。<u>实际上，每个分区都可以视为一个完整的小型数据库，虽然数据库可能存在一些跨分区的操作</u>。

​	<u>采用数据分区的主要目的是**提高可扩展性**</u>。不同的分区可以放在一个无共享集群(参阅第二部分关于无共享架构的定义)的不同节点上。<u>这样一个大数据集可以分散在更多的磁盘上，査询负载也随之分布到更多的处理器上</u>。

​	<u>对单个分区进行查询时，每个节点对自己所在分区可以独立执行查询操作，因此**添加更多的节点可以提高査询吞吐量**</u>。超大而复杂的査询尽管比较困难，但也可能做到跨节点的并行处理。

​	分区数据库最初在20世纪80年代由Teradata和Tandem NonStop SQL等率先推出，最近又被一些NoSQL数据库和基于Hadoop的数据仓库重视起来。这些系统有些是为事务型负载设计的，有些是分析型(参阅第3章的“事务处理与分析处理”)。二者的差异会显著影响系统的优化策略，然而分区技术的基本原理则可以普遍适用。

​	本章我们将首先介绍切分大型数据集的若干方法，讨论数据索引如何影响分区。接下来讨论**分区的再平衡**，这对动态添加或删除节点非常重要。最后，我们将介绍数据库如何将请求路由到正确的分区并执行查询。

## 6.1 数据分区与数据复制

​	<u>**分区通常与复制结合使用，即每个分区在多个节点都存有副本**</u>。<u>这意味着某条记录属于特定的分区，而同样的内容会保存在不同的节点上以提高系统的容错性</u>。

​	一个节点上可能存储了多个分区。图6-1展示了主从复制模型与分区组合使用时数据的分布情况。由图可知，每个分区都有自己的主副本，例如被分配给某节点，而从副本则分配在其他一些节点。<u>一个节点可能即是某些分区的主副本，同时又是其他分区的从副本</u>。

​	**第5章所讨论的所有复制相关的原理同样适用于对分区数据的复制**。考虑到分区方案的选择通常独立于复制，因此本章将力求简洁，而省略与复制相关的内容。

## 6.2 键-值数据的分区

​	假设面临海量数据，现在需要切分它们，那么该如何决定哪些记录放在哪些节点上呢？

​	**分区的主要目标是将数据和査询负载均匀分布在所有节点上**。如果节点平均分担负载，那么理论上10个节点应该能够处理10倍的数据量和10倍于单个节点的读写吞吐量(忽略复制)。

![分区与复制 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-1.png)

​	而如果<u>分区不均匀，则会出现某些分区节点比其他分区承担更多的数据量或查询负载，称之为**倾斜**</u>。倾斜会导致分区效率严重下降，在极端情况下，所有的负载可能会集中在一个分区节点上，这就意味着10个节点9个空闲，系统的瓶颈在最繁忙的那个节点上。这种负载严重不成比例的分区即成为**系统热点**。	

​	避免热点最简单的方法是将记录随机分配给所有节点上。这种方法可以比较均匀地分布数据，但是有一个很大的缺点：当试图读取特定的数据时，没有办法知道数据保存在哪个节点上，所以不得不并行查询所有节点。

​	可以改进上述方法。现在我们假设数据是简单的键-值数据模型，这意味着总是可以通过关键字来访问记录。例如，像一个纸质百科全书，可以通过标题来查找某一个条目；而所有的条目按字母序排序，因此可以做到快速查找条目。

### 6.2.1 基于关键字区间分区

​	一种分区方式是为每个分区分配一段连续的关键宇或者关键字区间范围(以最小值和最大值来指示)，如图6-2所示的纸质百科全书的卷目录。如果知道关键字区间的上下限，就可以轻松确定哪个分区包含这些关键字。如果还知道哪个分区分配在哪个节点，就可以直接向该节点发出请求(对于百科全书的例子，就是从书架上直接取到所要的书籍)。

![键值数据的分区 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-2.png)

​	关键字的区间段不一定非要均匀分布，这主要是因为数据本身可能就不均匀。例如，在图6-2中，卷1只包含以A和B开头的单词，但是卷12则包含了T、U、V、Ⅹ、Y和Z开始的单词。如果只是简单地规定每个分区包含两个字母，则可能会导致一些卷比其他卷要大很多。<u>为了更均匀地分布数据，分区边界理应适配数据本身的分布特征</u>。

​	分区边界可以由管理员手动确定，或者由数据库自动选择(我们将在本章后面的“分区再平衡”中更详细地讨论)。采用这种分区策略的系统包括Bigtable， Bigtable的开源版本Hbase，RethinkDB和2.4版本之前MongoDB。

​	<u>毎个分区内可以按照关键字排序保存(参阅第3章的“ SSTables和LSM-Trees”)。这样可以轻松支持区间查询，即将关键字作为一个拼接起来的索引项从而一次查询得到多个相关记录(参阅第3章的“多列索引”)</u>。例如，对于一个保存网络传感器数据的应用系统，选择测量的时间戳(年-月-日-时-分-秒)作为关键字，此时区间查询会非常有用，它可以快速获得某个月份内的所有数据。

​	然而，**基于关键字的区间分区的缺点是某些访问模式会导致热点**。如果关键字是时间戳，则分区对应于一个时间范围，例如每天一个分区。然而，当测量数据从传感器写入数据库时，所有的写入操作都集中在同一个分区(即当天的分区)，这会导致该分区在写入时负载过高，而其他分区始终处于空闲状态的。

​	为了避免上述问题，需要使用时间戳以外的其他内容作为关键字的第一项。例如，可以在时间戳前面加上传感器名称作为前缀，这样首先由传感器名称，然后按时间进行分区。假设同时有许多传感器处于活动状态，则写入负载最终会比较均匀地分布在多个节点上。接下来，当需要获取一个时间范围内、多个传感器的数据时，可以根据传感器名称，各自执行区间查询。

### * 6.2.2 基于关键字晗希值分区

​	对于上述数据倾斜与热点问题，许多分布式系统采用了基于关键字哈希函数的方式来分区。

​	**一个好的哈希函数可以处理数据倾斜并使其均匀分布**。例如一个处理字符串的32位哈希函数，当输入某个宇符串，它会返回一个0和2<sup>32</sup>~1之间近似随机分布的数值。即使输入的字符串非常相似，返回的哈希值也会在上述数字范围内均匀分布。

​	<u>用于数据分区目的的哈希函数不需要在加密方面很强</u>：例如，Cassandra和 MongoDB使用MD5， Voldemort使用 Fowler-Noll-Vo函数。许多编程语言也有内置的简单哈希函数(主要用于哈希表)，但是要注意这些内置的哈希函数可能并不适合分区，例如，Java的`Object.hashcode`和Ruby的`object#hash`，同一个键在不同的进程中可能返回不同的哈希值。

​	一旦找到合适的关键字哈希函数，就可以**为每个分区分配一个哈希范围(而不是直接作用于关键字范围)**，关键字根据其哈希值的范围划分到不同的分区中。如图6-3所示。

![键值数据的分区 - 图2](https://static.sitestack.cn/projects/ddia/img/fig6-3.png)

​	这种方法可以很好地将关键字均匀地分配到多个分区中。<u>分区边界可以是均匀间隔，也可以是伪随机选择(在这种情况下，该技术有时被称为**一致性哈希**)</u>。

​	然而，**通过关键字哈希进行分区，我们丧失了良好的区间查询特性**。<u>即使关键字相邻，但经过哈希之后会分散在不同的分区中，区间查询就失去了原有的有序相邻的特性</u>。<u>在 MongoDB中，如果启用了基于哈希的分片模式，则区间查询会发送到所有的分区上，而Riak、 Couchbase和Voldemort干脆就不支持关键字上的区间查询</u>。

> 一致性哈希
>
> 一致性哈希，由Karger等人首先提出，是一种平均分配负载的方法，最初用于内容分发网络(CDN)等互联网缓存系统。它采用随机选择的分区边界来规避中央控制或分布式共识。请注意，此处的一致性与副本一致性(第5章)或ACID致性(第7章)没有任何关联，它只描述了数据动态平衡的一种方法。
>
> 正如后面“分区再平衡”一节将要介绍的，**这种特殊的分区方法对于数据库实际效果并不是很好，所以目前很少使用(虽然某些数据库的文档仍采用一致性哈希的术语，但其实并不准确)**。为避免混淆，我们此处只用术语哈希分区。

​	<u>Cassandra则在两种分区策略之间做了一个折中。 Cassandra中的表可以声明为由多个列组成的复合主键。复合主键只有第一部分可用于哈希分区，而其他列则用作组合索引来对 Cassandra SSTable中的数据进行排序。因此，它不支持在第一列上进行区间査询，但如果为第一列指定好了固定值，可以对其他列执行高效的区间查询</u>。

​	**组合索引为一对多的关系提供了一个优雅的数据模型**。例如，在社交网站上，一个用户可能会发布很多消息更新。如果更新的关键字设置为`(user_id，update_timestamp)`的组合，那么可以有效地检索由某用户在一段时间内所做的所有更新，且按时间戳排序。<u>不同的用户可以存储在不同的分区上，但是对于某一用户，消息按时间戳顺序存储在一个分区上</u>。

### * 6.2.3 负载倾斜与热点

​	如前所述，基于哈希的分区方法可以减轻热点，但无法做到完全避免。<u>一个极端情况是，所有的读/写操作都是针对同一个关键字，则最终所有请求都将被路由到同一个分区</u>。

​	这种负载或许并不普遍，但也并非不可能：例如，社交媒体网站上，一些名人用户有数百万的粉丝，当其发布一些热点事件时可能会引发一场访问风暴，出现大量的对相同关键字的写操作(其中关键字可能是名人的用户ID，或者人们正在评论的事件ID)。此时，哈希起不到任何帮助作用，因为两个相同ID的哈希值仍然相同。

​	**大多数的系统今天仍然无法自动消除这种高度倾斜的负载，而只能通过应用层来减轻倾斜程**度。例如，如果某个关键字被确认为热点，一个简单的技术就是在关键字的开头或结尾处添加一个随机数。只需一个两位数的十进制随机数就可以将关键字的写操作分布到100个不同的关键字上，从而分配到不同的分区上。

​	<u>但是，随之而来的问题是，之后的任何读取都需要些额外的工作，必须从所有100个关键字中读取数据然后进行合并。因此通常只对少量的热点关键字附加随机数才有意义；而对于写入吞吐量低的绝大多数关键字，这些都意味着不必要的开销。此外，还需要额外的元数据来标记哪些关键字进行了特殊处理</u>。

​	也许将来某一天，数据系统能够自动检测负载倾斜情况，然后自动处理这些倾斜的负载。但截至目前，仍然需要开发者自己结合应用来综合权衡。

## * 6.3 分区与二级索引

​	我们之前所讨论的分区方案都依赖于键-值数据模型。键-值模型相对简单，即都是通过关键字来访冋记录，自然可以根据关键字来确定分区，并将读写请求路由到负责该关键字的分区上。

​	但是，如果涉及二级索引，情况会变得复杂(参阅第3章的“其他索引结构”)。**二级索引通常不能唯一标识一条记录，而是用来加速特定值的查询**，例如查找用户123的所有操作，找到所有含有hogwash的文章，查找所有颜色为红色的汽车等。

​	<u>二级索引是关系数据库的必备特性，在文档数据库中应用也非常普遍</u>。但考虑到其复杂性，许多键-值存储(如 Hbase和Voldemort)并不支持二级索引；但其他一些如Riak则开始增加对二级索引的支持。此外，二级索引技术也是Solr和Elasticsearch等全文索引服务器存在之根本。

​	**二级索引带来的主要挑战是它们不能规整的地映射到分区中**。有两种主要的方法来支持对二级索引进行分区：**基于文档的分区**和**基于词条的分区**。

### * 6.3.1 基于文档分区的二级索引

​	假设一个销售二手车的网站(见图6-4)。每个列表都有一个唯一的文档ID，用此ID对数据库进行分区，例如，ID0到499归分区0，ID500到999划为分区1。

​	现在用户需要搜索汽车，可以持按汽车颜色和厂商进行过滤，所以需要在颜色和制造商上设定二级索引(在文档数据库中这些都是字段；在关系数据库中则是列)。声明这些索引之后，数据库会自动创建索引。例如，每当一辆红色汽车添加到数据库中，数据库分区会自动将其添加到索引条目为“ color:red”的文档列表中。

![分片与次级索引 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-4.png)

​	**<u>在这种索引方法中，每个分区完全独立，各自维护自己的二级索引，且只负责自己分区内的文档而不关心其他分区中数据</u>**。<u>每当需要写数据库时，包括添加，删除或更新文档等，只需要处理包含目标文档ID的那一个分区</u>。因此**文档分区索引也被称为<u>本地索引</u>，而不是全局索引**，后者将在本章后面介绍。

​	<u>但读取时需要注意：除非对文档ID做了特别的处理，否则不太可能所有特定颜色或特定品牌的汽车都放在一个分区中，例如图6-4中，红色汽车就出现在分区0和分区1中。因此，如果想要搜索红色汽车，就**需要将查询发送到所有的分区，然后合并所有返回的结果**</u>。

​	这种査询分区数据库的方法有时也称为**分散/聚集**，显然这种二级索引的查询代价高昂。即使采用了并行査询，也容易导致读延迟显著放大(参阅第1章的“实践中的百分位数”)。尽管如此，它还是广泛用于实践：MongoDB、Riak、 Cassandra、ElasticSearch、 SolrCloud 和 VoltDB都支持基于文档分区二级索引。大多数数据库供应商都建议用户自己来构建合适的分区方案，尽量由单个分区满足二级索引查询，但现实往往难以如愿，尤其是当查询中可能引用多个二级索引时(例如同时指定颜色和制造商两个条件)。

### * 6.3.2 基于词条的二级索引分区

​	另一种方法，我们可以**对所有的数据构建全局索引，而不是每个分区维护自己的本地索引**。而且，为避免成为瓶颈，不能将全局索引存储在一个节点上，否则就破坏了设计分区均衡的目标。所以，**全局索引也必须进行分区，且可以与数据关键字采用不同的分区策略**。

![分片与次级索引 - 图2](https://static.sitestack.cn/projects/ddia/img/fig6-5.png)

​	以图65为例：所有数据分区中的颜色为红色的汽车被收录到在索引`color:red`中，而索引本身也是分区的，例如从a到r开始的颜色放在分区0中，从s到的颜色放在分区1中。类似的，汽车制造商的索引也被分区(两个分区的边界分别是字母f和字母h)。

​	我们将<u>这种索引方案称为**词条分区**，它**以待查找的关键字本身作为索引**</u>。例如颜色`color:red`。<u>名字词条源于全文索引(一种特定类型的二级索引)，term指的是文档中出现的所有单词的集合</u>。

​	<u>和前面讨论的方法一样，可以直接通过关键词来全局划分索引，或者对其取哈希值。直接分区的好处是可以支持高效的区间查询(例如，查询汽车报价在某个值以上)而采用哈希的方式则可以更均匀的划分分区</u>。

​	这种全局的词条分区相比于文档分区索引的主要优点是，**它的读取更为高效，即它不需要釆用 scatter/gather对所有的分区都执行一遍查询，相反，客户端只需要向包含词条的那一个分区发出读请求**。<u>然而全局索引的不利之处在于，**写入速度较慢且非常复杂**，主要因为单个文档的更新时，里面可能会涉及多个二级索引，而二级索引的分区又可能完全不同甚至在不同的节点上，由此势必引入显著的**写放大**</u>。

​	<u>理想情况下，索引应该时刻保持最新，即写入的数据要立即反映在最新的索引上。但是，对词条分区来讲，这需要个跨多个相关分区的分布式事务支持，写入速度会受到极大的影响，所以**现有的数据库都不支持同步更新二级索引**(参阅第7章和第9章)</u>。

​	**实践中，<u>对全局二级索引的更新往往都是异步的(也就意味着，如果在写入之后马上去读索引，那么刚刚发生的更新可能还没有反映在索引中)</u>**。例如， AmazonDynamoDB的二级索引通常可以在1秒之内完成更新，但当底层设施出现故障时，也有可能需要等待很长的时间。其他使用全局索引的系统还包括Riak的搜索功能和Oracle数据仓库，后者允许用户来选择是使用本地还是全局索引。在第12章我们会重新讨论如何实现全局二级索引。

## 6.4 分区再平衡

​	随着时间的推移，数据库可能总会出现某些变化：

+ 查询压力增加，因此需要更多的CPU来处理负载。
+ 数据规模增加，因此需要更多的磁盘和内存来存储数据。
+ 节点可能出现故障，因此需要其他机器来接管失效的节点。

​	<u>所有这些变化都要求数据和请求可以从一个节点转移到另一个节点</u>。这样一个迁移负载的过程称为**再平衡(或者动态平衡)**。无论对于哪种分区方案，分区再平衡通常至少要满足：

+ 平衡之后，负载、数据存储、读写请求等应该在集群范围更均匀地分布。
+ **再平衡执行过程中，数据库应该可以继续正常提供读写服务**。
+ 避免不必要的负载迁移，以加快动态再平衡，并尽量减少网络和磁盘IO影响。

### 6.4.1 动态再平衡的策略

​	将分区对应到节点上存在多种不同的分配策略，这里逐一介绍：

#### * 为什么不用取模？

​	我们在前面提到(见图6-3)，最好将哈希值划分为不同的区间范围，然后将每个区间分配给一个分区。例如，区间[0，b0)对应于分区0，[b0，b1)对应分区1等。

​	也许你会问为什么不直接使用mod(许多编程语言里的取模运算符%)。例如，`hash(key) mod 10`会返回一个介于0和9之间的数字，如果有10个节点，则依次对应节点0到9，这似乎是将每个关键字分配到节点的最简单方法。

​	**对节点数取模方法的问题是，如果节点数N发生了变化，会导致很多关键字需要从现有的节点迁移到另一个节点**。例如，假设hash(key)=123456，假定最初是10个节点，那么这个关键字应该放在节点6(123456 mod 10=6)；当节点数增加到11时，它需要移动到节点3(123456 mod 11=3)；当继续增长到12个节点时，又需要移动到节点0(123456 mod 12=0)。<u>这种频繁的迁移操作大大增加了再平衡的成本</u>。

​	因此我们需要一种减少迁移数据的方法。

#### * 固定数量的分区

​	幸运的是，有一个相当简单的解决方案：**首先，创建远超实际节点数的分区数，然后为每个节点分配多个分区**。例如，对于一个10节点的集群，数据库可以从一开始就逻辑划分为1000个分区，这样大约每个节点承担100个分区。

​	<u>接下来，如果集群中添加了一个新节点，该新节点可以从每个现有的节点上匀走几个分区，直到分区再次达到全局平衡。该过程如图6-6所示。如果从集群中删除节点，则采取相反的均衡措施</u>。

​	<u>选中的整个分区会在节点之间迁移，但分区的总数量仍维持不变，也不会改变关键字到分区的映射关系。**这里唯一要调整的是分区与节点的对应关系**</u>。考虑到节点间通过网络传输数据总是需要些时间，这样调整可以逐步完成，在此期间，旧的分区仍然可以接收读写请求。

![分区再平衡 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-6.png)

​	<u>原则上，也可以将集群中的不同的硬件配置因素考虑进来，即性能更强大的节点将分配更多的分区，从而分担更多的负载</u>。

​	目前，Riak、 Elasticsearch、 couchbase和Voldemort都支持这种动态平衡方法。

​	**使用该策略时，分区的数量往往在数据库创建时就确定好，之后不会改变**。<u>原则上也可以拆分和合并分区(稍后介绍)，但固定数量的分区使得相关操作非常简单，因此许多采用固定分区策略的数据库决定不支持分区拆分功能</u>。所以，在初始化时，已经充分考虑将来扩容增长的需求(未来可能拥有的最大节点数)，设置一个足够大的分区数。而每个分区也有些额外的管理开销，选择过高的数字可能会有副作用。

​	如果数据集的总规模高度不确定或可变(例如，开始非常小，但随着时间的推移可能会变得异常庞大)，此时如何选择合适的分区数就有些困难。每个分区包含的数据量的上限是固定的，实际大小应该与集群中的数据总量成正比。如果分区里的数据量非常大，则每次再平衡和节点故障恢复的代价就很大；但是如果一个分区太小，就会产生太多的开销。分区大小应该“恰到好处”，不要太大，也不能过小，**如果分区数量固定了但总数据量却高度不确定，就难以达到一个最佳取舍点**。

#### * 动态分区

​	对于采用关键字区间分区的数据库，如果边界设置有问题，最终可能会出现所有数据都挤在一个分区而其他分区基本为空，那么设定固定边界、固定数量的分区将非常不便：而手动去重新配置分区边界又非常繁琐。

​	因此，一些数据库如 HBase和 RethinkDB等采用了**动态创建分区**。<u>当分区的数据增长超过一个可配的参数阈值(Hbase上默认值是10GB)，它就拆分为两个分区，每个承担一半的数据量。相反，如果大量数据被删除，并且分区缩小到某个阈值以下，则将其与相邻分区进行合并</u>。该过程类似于B树的分裂操作(参阅第3章的“B-tree”)。

​	**<u>每个分区总是分配给一个节点</u>，而每个节点可以承载多个分区，这点与固定数量的分区一样。当一个大的分区发生分裂之后，可以将其中的一半转移到其他某节点以平衡负载**。<u>对于 HBase，分区文件的传输需要借助HDFS(底层分布式文件系统)</u>。

​	**动态分区的一个优点是分区数量可以自动适配数据总量**。<u>如果只有少量的数据，少量的分区就足够了，这样系统开销很小；如果有大量的数据，每个分区的大小则被限制在一个可配的最大值</u>。

​	但是，需要注意的是，<u>对于一个空的数据库，因为没有任何先验知识可以帮助确定分区的边界，所以会从一个分区开始。可能数据集很小，但直到达到第一个分裂点之前，所有的写入操作都必须由单个节点来处理，而其他节点则处于空闲状态</u>。为了缓解这个问题， HBase和 MongoDB允许在一个空的数据库上配置一组初始分区(这被称为**预分裂**)。对于关键字区间分区，预分裂要求已经知道一些关键字的分布情况。

​	动态分区不仅适用于关键字区间分区，也适用于基于哈希的分区策略。 MongoDB从版本2.4开始，同时支持二者，并且都可以动态分裂分区。

#### 按节点比例分区

​	采用动态分区策略，拆分和合并操作使每个分区的大小维持在设定的最小值和最大值之间，因此分区的数量与数据集的大小成正比关系。另一方面，对于固定数量的分区方式，其每个分区的大小也与数据集的大小成正比。两种情况，分区的数量都与节点数无关。

​	Cassandra和Ketama则采用了第三种方式，<u>使分区数与集群节点数成正比关系</u>。换句话说，**每个节点具有固定数量的分区**。此时，<u>当节点数不变时，每个分区的大小与数据集大小保持正比的增长关系</u>；当节点数增加时，分区则会调整变得更小。较大的数据量通常需要大量的节点来存储，因此这种方法也使每个分区大小保持稳定。

​	**当一个新节点加入集群时，它随机选择固定数量的现有分区进行分裂，然后拿走这些分区的一半数据量，将另一半数据留在原节点**。随机选择可能会带来不太公平的分区分裂，但是当平均分区数量较大时(Cassandra默认情况下，每个节点有256个分区)，新节点最终会从现有节点中拿走相当数量的负载。 Cassandra在3.0时推出了改进算法，可以避免上述不公平的分裂。

​	**随机选择分区边界的前提要求采用基于哈希分区(可以从哈希函数产生的数字范围里设置边界)**。这种方法也最符合本章开头所定义一致性哈希。一些新设计的哈希函数也可以以较低的元数据开销达到类似的效果。

### 6.4.2 自动与手动再平衡操作

​	动态平衡另一个重要问题我们还没有考虑：它是自动执行还是手动方式执行？

​	全自动式的再平衡(即由系统自动决定何时将分区从一个节点迁移到另一个节点，不需要任何管理员的介入)与纯手动方式(即分区到节点的映射由管理员来显式配置)之间，可能还有一个过渡阶段。例如， Couchbase，Riak和 Voldemort会自动生成一个分区分配的建议方案，但需要管理员的确认才能生效。

​	<u>全自动式再平衡会更加方便，它在正常维护之外所增加的操作很少。但是，也有可能出现结果难以预测的情况。再平衡总体讲是个比较昂贵的操作，它需要重新路由请求并将大量数据从一个节点迁移到另一个节点。万一执行过程中间出现异常，会使网络或节点的负载过重，并影响其他请求的性能</u>。

​	**将自动平衡与自动故障检测相结合也可能存在一些风险**。<u>例如，假设某个节点负载过重，对请求的响应暂时受到影响，而其他节点可能会得到结论：该节点已经失效；接下来激活自动平衡来转移其负载。客观上这会加重该节点、其他节点以及网络的负荷，可能会使总体情况变得更糟，**甚至导致级联式的失效扩散**</u>。

​	出于这样的考虑，**让管理员介入到再平衡可能是个更好的选择**。它的确比全自动过程响应慢一些，但它可以有效防止意外发生。

## 6.5 请求路由

​	现在我们已经将数据集分布到多个节点上，但是仍然有一个悬而未决的问题：当客户端需要发送请求时，如何知道应该连接哪个节点？如果发生了分区再平衡，分区与节点的对应关系随之还会变化。为了回答该问题，我们需要一段处理逻辑来感知这些变化，并负责处理客户端的连接，例如想要读/写关键字“foo”，需要连接哪个IP地址和哪个端口号。

​	这其实属于一类典型的**服务发现问题**，服务发现并不限于数据库，任何通过网络访问的系统都有这样的需求，尤其是当服务目标支持高可用时(在多台机器上有冗余配置)。许多公司已经开发了自己的内部服务发现工具，其中很多已经开源。

​	概括来讲，这个问题有以下几种不同的处理策略(分别如图6-7所示的三种情况)：

1. 允许客户端链接任意的节点(例如，采用**循环式的负载均衡器**)。如果某节点恰好拥有所请求的分区，则直接处理该请求；否则，将请求转发到下一个合适的节点，接收答复，并将答复返回给客户端。
2. 将所有客户端的**请求都发送到一个路由层，由后者负责将请求转发到对应的分区节点上**。路由层本身不处理任何请求，它仅充一个分区感知的负载均衡器。
3. 客户端**感知分区和节点分配关系**。此时，客户端可以直接连接到目标节点，而不需要任何中介。

​	<u>**不管哪种方法，核心问题是：作出路由决策的组件(可能是某个节点，路由层或客户端)如何知道分区与节点的对应关系以及其变化情况**</u>?

![请求路由 - 图1](https://static.sitestack.cn/projects/ddia/img/fig6-7.png)

​	这其实是一个很有挑战性的问题，所有参与者都要达成**共识**这一点很重要。否则请求可能被发送到错误的节点，而没有得到正确处理。分布式系统中有专门的共识协议算法，但通常难以正确实现(详见第9章)。

​	许多分布式数据系统依靠独立的协调服务(如 ZooKeeper)跟踪集群范围内的元数据，如图6-8所示。每个节点都向 ZooKeeper中注册自己， ZooKeeper维护了分区到节点的最终映射关系。其他参与者(如路由层或分区感知的客户端)可以向 ZooKeeper订阅此信息。一旦分区发生了改变，或者添加、删除节点，ZooKeeper就会主动通知路由层，这样使路由信息保持最新状态。

​	例如， LinkedIn的 Espresso使 Helix进行集群管理(底层是 ZooKeeper)，实现了图6-8所示的请求路由层。 HBase， SolrCloud和Kafka也使用 ZooKeeper来跟踪分区分配情况。 MongoDB有类似的设计，但它依赖于自己的配置服务器和mongos守护进程来充当路由层。

​	<u>Cassandra和Riak则采用了不同的方法，它们在节点之间使用gossip协议来同步群集状态的变化。请求可以发送到任何节点，由该节点负责将其转发到目标分区节点(图6-7中的方法1)。这种方式增加了数据库节点的复杂性，但是避免了对ZooKeeper之类的外部协调服务的依赖</u>。

![请求路由 - 图2](https://static.sitestack.cn/projects/ddia/img/fig6-8.png)

​	Couchbase并不支持自动再平衡功能，这简化了设计。它通过配置一个名为moxi的路由选择层，向集群节点学习最新的路由变化。

​	<u>当使用路由层或随机选择节点发送请求时，客户端仍然需要知道目标节点的IP地址。IP地址的变化往往没有分区节点变化那么频繁，采用DNS通常就足够了</u>。

### 6.5.1 并行查询执行

​	到目前为止，我们只关注了读取或写入单个关键字这样简单的查询(对于文档分区的二级索引，里面要求分散/聚集查询)。这基本上也是大多数NoSQL分布式数据存储所支持的访问类型。

​	然而对于**大规模并行处理(massively parallel processin，MPP)**这一类主要用于数据分析的关系数据库，在查询类型方面要复杂得多。**典型的数据仓库查询包含多个联合、过滤、分组和聚合操作。<u>MPP査询优化器会将复杂的査询分解成许多执行阶段和分区，以便在集群的不同节点上并行执行</u>。尤其是涉及全表扫描这样的查询操作，可以通过并行执行获益颇多**。

​	数据仓库中快速并行执行查询可以作为单独的话题。考虑到分析业务的重要性，目前它已得到了广泛的商业关注。我们将在第10章中讨论并行查询执行所需的一些技术。有关并行数据库更多相关技术细节请参考文献。

## 6.6 小结

​	本章，我们探讨了将大规模数据集划分成更小子集的多种方法。数据量如果太大，单台机器进行存储和处理就会成为瓶颈，因此需要引入数据分区机制。**分区的目地是通过多台机器均匀分布数据和查询负载，避免出现热点**。这需要选择合适的数据分区方案，**在节点添加或删除时重新动态平衡分区**。

​	我们讨论了两种主要的分区方法：

+ **基于关键字区间的分区**。先对关键字进行排序，每个分区只负责一段包含最小到最大关键字范围的一段关键字。<u>对关键字排序的优点是可以支持高效的区间查询，但是如果应用程序经常访问与排序一致的某段关键字，就会存在热点的风险</u>。釆用这种方法，当分区太大时，通常将其分裂为两个子区间，从而动态地再平衡分区。
+ **哈希分区**。将哈希函数作用于每个关键字，每个分区负责一定范围的哈希值。这种方法打破了原关键字的顺序关系，<u>它的区间查询效率比较低，但可以更均匀地分配负载</u>。采用哈希分区时，通常事先创建好足够多(但固定数量)的分区，让每个节点承担多个分区，当添加或删除节点时将某些分区从一个节点迁移到另个节点，也可以支持动态分区。

​	混合上述两种基本方法也是可行的，例如使用复合键：键的一部分来标识分区，而另部分来记录排序后的顺序。

​	我们还讨论了分区与二级索引，二级索引也需要进行分区，有两种方法：

+ **基于文档来分区二级索引(本地索引)**。二级索引存储在与关键字相同的分区中，这意味着写入时我们只需要更新一个分区，但<u>缺点是读取二级索引时需要在所有分区上执行 scatter/gather</u>。
+ **基于词条来分区二级索引(全局索引)**。它是基于索引的值而进行的独立分区。二级索引中的条目可能包含来自关键字的多个分区里的记录。<u>在写入时，不得不更新二级索引的多个分区；但读取时，则可以从单个分区直接快速提取数据</u>。

​	最后，我们讨论了如何将查询请求路由到正确的分区，包括简单的分区感知负载均衡器，以及复杂的并行査询执行引擎。

​	理论上，毎个分区基本保持独立运行，这也是为什么我们试图将分区数据库分布、扩展到多台机器上。但是，如果写入需要跨多个分区，情况就会格外复杂，例如，如果其中一个分区写入成功，但另一个发生失败，接下来会发生什么？我们将在下面的章节中讨论类似这样的技术挑战。

# * 第7章 事务

​	在一个苛刻的数据存储环境中，会有许多可能出错的情况，例如：

+ 数据库软件或硬件可能会随时失效(包括正在执行写操作的过程中)。
+ 应用程序可能随时崩溃(包括一系列操作执行到中间某一步)。
+ <u>应用与数据库节点之间的链接可能随时会中断，数据库节点之间也存在同样问题</u>。
+ 多个客户端可能同时写入数据库，导致数据覆盖。
+ 客户端可能读到一些无意义的、部分更新的数据。
+ 客户端之间由于边界条件竞争所引入的各种奇怪问题。

​	为了系统高可靠的目标，我们必须处理好上述问题，万一发生类似情况确保不会导致系统级的失效。然而，完善的容错机制需要大量的工作，要仔细考虑各种可能出错的可能，并进行充分的测试才能确保方案切实可靠。

​	<u>近十年来，事务技术一直是简化这些问题的首选机制。事务将应用程序的多个读、写操作捆绑在一起成为一个逻辑操作单元。即事务中的所有读写是一个执行的整体，整个事务要么成功(提交)、要么失败(中止或回滚)</u>。如果失败，应用程序可以安全地重试。这样，由于不需要担心部分失败的情况(无论出于何种原因)，应用层的错误处理就变得简单很多。

​	也许对于那些浸淫此领域多年的读者来说，事务的概念如此简单，但细究起来或许并非如此。事务不是一个天然存在的东西，它是被人为创造出来，目的是简化应用层的编程模型。有了事务，应用程序可以不用考虑某些内部潜在的错误以及复杂的并发性问题，这些都可以交给数据库来负责处理(我们称之为安全性保证)。

​	然而并非每个应用程序都需要事务机制，有时可以弱化事务处理或完全放弃事务(例如，为了实现更高的性能或更高的可用性)。一些安全相关的属性也可能会避免引入事务。

​	那该如何判断是否需要事务呢？为了回答这个问题，我们首先需要确切地理解事务能够提供哪些安全性保证，背后的代价又是什么。事务的概念看似简单，实际上却有许多微妙而关键的细节值得硏究。

​	本章，我们将首先分析可能出错的各种场景，探讨数据库防范这些问题的基本方法和算法设计。特别是在并发控制方面，深入讨论可能各种竞争条件，数据库所提供的多种隔离级别，例如读提交、快照隔离和可串行化等。

​	本章的内容可适用于单节点和分布式场景。在第8章中，我们将重点讨论分布式系统的特殊挑战。

## 7.1 深入理解事务

​	目前几乎所有的关系数据库和一些非关系数据库都支持事务处理。它们大多数都沿用了和IBM于1975年推出的第一个SQL数据库 System R相似的总体设计。尽管在一些具体实现方面有些不同，但事务的概念在这四十年中几乎没有发生变化，换句话说，MySQL、 Postgresql、 Oracle、 SQL Server等系统实现的事务与当年 System R非常相。

​	然而21世纪末，非关系(NoSQL)数据库开始兴起。它们的目标是通过提供新的数据模型(参见第2章)，以及内置的复制(参见第5章)和分区(参见第6章)等手段来改进传统的关系模型。然而事务却成了这场变革的受害者：<u>很多新一代的数据库完全放弃了事务支持，或者将其重新定义，即替换为比以前弱得多的保证</u>。

​	随着这种新型分布式数据库的炒作，很多人开始认为事务与可扩展性是相对立的两面，而大规模系统为了性能与高可用性将不得不牺牲事务的支持。但另一方面，还有一些数据库供应商则坚称事务是“关键应用”和“高价值数据”所必备的重要功能。而笔者看来，这两个观点未免都有些夸大其词。

​	与其他技术一样，事务有其优势，也有其自身的局限性。为了更好地理解事务设计的权衡之道，让我们考虑正常运行和各种极端(但确实存在)情况，详细分析事务可以为我们提供哪些保证。

### 7.1.1 ACID的含义

​	事务所提供的安全保证即大家所熟知的**ACID，分别代表原子性(Atomicity)，一致性(Consistency)，隔离性(Isolation)与持久性(Durability)**，取这四个特性的首字母。最早由 Theoharder和 Andreas Reuter于1983年为精确描述数据库的容错机制而定义。

​	但<u>实际上，各家数据库所实现的ACⅠD并不尽相同。例如，我们稍后就会看到，围绕着“隔离性”就存在很多含糊不清的争议。想法非常美好，细节方见真章。当听到一个系统声称自己“兼容ACID”时，其实你无法确信它究竟能提供了什么样的保证，现在的ACID更像是一个市场营销用语</u>。

​	而不符合ACID标准的系统有时被冠以**BASE**，取另外几个特性的首字母，**即基本可用性(Basically Available)，软状态(Soft State)和最终一致性(Eventualconsistency)**。听下来它似乎比ACID更加模棱两可。BASE唯一可以确定的是“它不是ACID”，此外它几乎没有承诺任何东西。

​	我们还是先搞凊楚原子性，一致性，隔离性和持久性的准确含义，目标是建立对事务思想的清晰而牢固的认识。

#### 原子性

​	通常，原子是指不可分解为更小粒度的东西。这个术语在计算机的不同领域里有着相似但却微妙的差异。例如，<u>多线程编程中，如果某线程执行一个原子操作，这意味着**其他线程是无法看到该操作的中间结果**。它只能处于操作之前或操作之后的状态，而不是两者之间的状态</u>。

​	而ACID中的原子性并不关乎多个操作的并发性，它并没有描述多个线程试图访向相同的数据会发生什么情况，后者其实是由ACID的隔离性所定义(参见本章后面的“隔离性”)。

​	ACID原子性其实描述了客户端发起一个包含多个写操作的请求时可能发生的情况，例如在完成了一部分写入之后，系统发生了故障，包括进程崩溃，网络中断，磁盘变满或者违反了某种完整性约束等；**把多个写操作纳入到一个原子事务，万一出现了上述故障而导致没法完成最终提交时，则事务会中止，并且数据库须丢弃或撤销那些局部完成的更改**。

​	<u>假如没有原子性保证，当多个更新操作中间发生了错误，就需要知道哪些更改已经生效，哪些没有生效，这个寻找过程会非常麻烦。或许应用程序可以重试，但情况类似，并且可能导致重复更新或者不正确的结果。原子性则大大简化了这个问题：如果事务已经中止，应用程序可以确定没有实质发生任何更改，所以可以安全地重试</u>。

​	**因此ACID中原子性所定义的特征是：<u>在出错时中止事务，并将部分完成的写入全部丢弃</u>。也许可中止性比原子性更为准确，不过我们还是沿用原子性这个惯用术语**。

#### 一致性

​	一致性非常重要，但它在不同场景有着不同的具体含义，例如：

+ 第5章我们讨论了副本一致性以及异步复制模型时，引出了最终一致性问题(参见第5章“复制滞后问题”)。
+ 一致性哈希则是某些系统用于动态分区再平衡的方法(参见第6章“一致性哈希”)。
+ CAP理论中，一致性一词用来表示线性化(参见第9章“可线性化”)。
+ 而在ACID中，一致性主要指数据库处于应用程序所期待的“预期状态”。

​	可以看出，同一个词至少有四种不同的含义。

​	**ACID中的一致性的主要是指对数据有特定的预期状态，任何数据更改必须满足这些状态约東(或者恒等条件)**。例如，对于一个账单系统，账户的贷款余额应和借款余额保持平衡。如果某事务从一个有效的状态开始，并且事务中任何更新操作都没有违背约束，那么最后的结果依然符合有效状态。

​	**这种一致性本质上要求应用层来维护状态一致(或者恒等)，应用程序有责任正确地定义事务来保持一致性**。这不是数据库可以保证的事情：即如果提供的数据修改违背了恒等条件，数据库很难检测进而阻止该操作(数据库可以完成针对某些特定类型的恒等约束检查，例如使用外键约束或唯一性约束。<u>但通常主要靠应用程序来定义数据的有效/无效状态，数据库主要负责存储</u>。

​	**原子性，隔离性和持久性是数据库自身的属性，而ACID中的一致性更多是应用层的属性**。<u>应用程序可能借助数据库提供的原子性和隔离性，以达到一致性，但一致性本身并不源于数据库。因此，字母C其实并不应该属于ACID。</u>

#### 隔离性

​	大多数数据库都支持多个客户端同时访问。如果读取和写入的是不同数据，这肯定没有什么问题；但如果访问相同的记录，则可能会遇到并发问题(即带来竞争条件)。

​	一个简单例子如图7-1所示。假设有两个客户端同时增加数据库中的一个计数器。每个客户首先读取当前值，再客户端增加1，然后写回新值(这里假设数据库尚不支持自增操作)。图7-1中，由于有两次相加，计数器应该由42增加到44，但实际上由于竞争条件最终结果却是43。

​	**ACⅠD语义中的隔离性意味着并发执行的多个事务相互隔离，它们不能互相交叉**。<u>经典的数据库教材把隔离定义为**可串行化**，这意味着可以假装它是数据库上运行的唯一事务</u>。**虽然实际上它们可能同时运行，但数据库系统要<u>确保当事务提交时，其结果与串行执行(一个接一个执行)完全相同</u>**。

![事务的棘手概念 - 图1](https://static.sitestack.cn/projects/ddia/img/fig7-1.png)

​	**然而实践中，由于性能问题很少使用串行化隔离**。一些流行的数据库，如Oracle 11g，甚至根本就没有实现它。 Oracle虽然也有声称“串行化”的功能，但它本质上实现的是**快照隔离**，后者提供了比串行化更弱的保证。我们将在本章后面“弱隔离级别”中讨论快照隔离以及其他形式的隔离。

#### 持久性

​	<u>数据库系统本质上是提供一个安全可靠的地方来存储数据而不用担心数据丢失等</u>。持久性就是这样的承诺，它**保证一旦事务提交成功，即使存在硬件故障或数据库崩溃，事务所写入的任何数据也不会消失**。

​	对于单节点数据库，持久性通常意味着数据已被写入非易失性存储设备，如硬盘或SSD。**在写入执行过程中，通常还涉及预写日志等**(参阅第3章“可靠的B-tree”)，这样万一磁盘数据损坏可以进行恢复。而**对于支持远程复制的数据库，持久性则意味着数据已成功复制到多个节点**。<u>**为了实现持久性的保证，数据库必须等到这些写入或复制完成之后才能报告事务成功提交**</u>。

​	正如第1章的“可靠性”所提到的，其实不存在完美的持久性。例如，所有的硬盘和所有的备份如果都同时被(人为)销毁了，那么数据库也无能为力。

> 复制与持久性
>
> 从历史上看，持久性最早意味着是写入磁带来存档，之后演变为写入磁盘或SSD。最近，它又代表着多节点间复制。对比起来，哪种方式更好呢?
>
> 答案是，没有任何一个可以称为完美：
>
> + 如果写入了磁盘但机器发生死机，即使数据没有丢失，在重启机器或者将磁盘转移到另一台机器完成之前，都无法访问任何数据。而基于复制的系统则可以继续可用。
> + 某些故障，例如停电或一个软件bug，可能在某个特定输入时触发了异常进而导致所有节点全部崩溃，甚至删除所有的副本(参阅第1章“可靠性“) 内存中的数据会丟失，因此对于内存数据库，写入磁盘仍然是需要的。
> + **在异步复制系统中，当主节点不可用时，最近的写入操作因为可能没有及时完成同步而导致更新丢失**(参阅第5章“处理节点失效。
> + **当电源突然断电时，特别对于固态硬盘可能发生意外：连fsync之后的数据也不能保证可以正确恢复**。和所有其他类型的软件类似，磁盘的固件也可能存在bug。
> + <u>考虑到数据库存储引擎与文件系统之间复杂而微妙的关系，其中也可能包含难以追踪的bug，并最终导致在系统发生崩溃后，磁盘上的文件也出现损坏</u>。
> + **磁盘上的数据可能会悄无声息地出现损坏但又没有被及时检测到。这种情况如果持续发生，则多个副本甚至最近的备份都可能已经损坏。此时需要从历史备份中恢复数据**。
> + **一项关于固态硬盘的研究发现，在部署的前四年，30%到80%的固态盘至少发生一个坏块。磁盘的坏道率比较低，但整盘发生完全失效的概率却比固态盘高**。
> + 某些固态盘如果遭遇突然断电，可能会在接下来的几周内发生丢失数据情况，温度在里面起了关键性作用。
>
> **现实情况是，没有哪一项技术可以提供绝对的持久性保证**。<u>这些都是帮助降低风险的手段，应该组合使用它们，包括写入磁盘、复制到远程机器以及备份等。因此对任何理论上的“保证”一定要谨慎对待</u>。

> [fsync(2) - Linux man page (die.net)](https://linux.die.net/man/2/fsync)

### 7.1.2 单对象与多对象事务操作

​	回顾一下，<u>ACID中的**原子性**和**隔离性**主要针对客户端在同一事务中包含多个写操作时，数据库所提供的保证</u>：

+ **原子性**

  如果一系列写操作中间发生了错误，则事务必须中止，并且事务中已完成的写入应该被丢弃。换言之，不用担心数据库的部分失败，它总是保证要么全部成功要么全部失败。

+ **隔离性**

  <u>同时运行的事务不应相互干扰</u>。例如，如果某个事务进行多次写入，则另一个事务应该观察到的是其全部完成(或者一个都没完成)的结果，而<u>不应该看到中间的部分结果</u>。

​	这些定义假定在一个事务中会修改多个对象(如行，文档，记录等)。这种多对象事务目的通常是为了在多个数据对象之间保持同步。图7-2展示了一个电子邮件应用的例子。要显示用户的未读邮件数量，可以执行查询如下：

`SELECT COUNT(*) FROM emails WHERE recipient_id =2 AND unread_flag = true`

​	如果电子邮件太多，你会发现査询太慢，然后决定将未读的数量直接保存在一个单独的字段中(虽然这违反了范式要求)。这样毎当收到一个新邮件，需要增加未读计数器；当邮件标记为已读时，还需减少该计数器。

​	在图7-2中，用户2遇到些异常情况：邮箱列表已显示了未读消息，但计数器却还未更新，所显式的数目是0。<u>隔离性将保证用户2看到要么是更新后的电子邮件和更新后的计数器，要么是二者都未更新，而不会是两者不一致</u>。

![事务的棘手概念 - 图2](https://static.sitestack.cn/projects/ddia/img/fig7-2.png)

​	图7-3则说明了对原子性的需求：如果事务执行过程中发生错误，导致邮箱和未读计数器二者不同步。则事务将被中止，且此之前插入的电子邮件将被回滚。

![事务的棘手概念 - 图3](https://static.sitestack.cn/projects/ddia/img/fig7-3.png)

​	<u>多对象事务要求确定知道事务包含了哪些读写操作。对于关系数据库，客户端通常与数据库服务器建立TCP网络连接，因而对于特定的某个连接，SQL语句BEGIN TRANSACTION和COMMIT之间的所有操作都属于同一个事务</u>。

​	**而许多<u>非关系数据库</u>则不会将这些操作组合在一起**。即使他们可能支持多对象API(例如，键-值存储的multi-put API可以在一个操作中更新多个键)，但并不意味着具有事务语义，例如**可能出现某些键更新成功了而其他则发生了失败，最后结果是数据库处于部分更新的状态**。

#### * 单对象写入

​	原子性和隔离性也同样适用于单个对象的更新。例如，假设向数据库写入20KB的JSON文档：

+ 如果发送了第一个10KB之后网络连接中断，数据库是否只存储了无法完整解析的10 KB JSON片段呢?
+ **如果数据库在覆盖磁盘现有数据时发生电源故障，最终是否是新旧值混杂在一起**?
+ **如果另一个客户端在写入的过程中读取该文档，是否会看到部分更新的文档内容**?

​	这些问题着实让人头疼，因此存储引擎几乎必备的设计就是在单节点、单个对象层面上提供原子性和隔离性(比如key-value对)。例如，<u>出现宕机时，基于日志恢复来实现原子性</u>(参阅第3章“可靠的B-tree”)，**对每个对象采用加锁的方式(每次只允许一个线程访问对象)来实现隔离**。

​	<u>某些数据库还提供了高级的原子操作，例如**原子自增**操作，这样就不需要像图7-1那样执行读取-修改-写回，类似地还有**原子比较-设置**操作，即只有当前值没有被他人修改时才执行写入(参阅本章后面的“compare-and-set"</u>)。

​	这些单对象操作可以有效防止多个客户端并发修改同一对象时的更新丢失问题(参阅本章后面的“防止更新丟失”)。但需要注意，它们并不是通常意义上的事务。<u>虽然compare-and-set和其他单对象操作有时也被称为“轻量级事务”，甚至“ACID”，但这里其实存在一些误导性。**通常意义上的事务针对的是多个对象，将多个操作聚合为一个逻辑执行单元**</u>。

#### 多对象事务的必要性

​	许多分布式数据存储系统不支持多对象事务，主要是因为当出现<u>跨分区</u>时，多对象事务非常难以正确实现，同时在高可用或者极致性能的场景下也会带来很多负面影响。

​	但是，分布式数据库实现事务并非不可能，并不存在什么原理上的限制，我们将在第9章讨论分布式事务的实现。

​	但是否所有应用都需要多对象事务呢？是否可能只用键-值数据模型和单对象操作就可以满足应用需求?

​	的确有一些情况，只进行单个对象的插入、更新和删除就足够了。但是，还有许多其他情况要求写入多个不同的对象并进行协调：

+ 对于关系数据模型，表中的某行可能是另一个表中的外键。类似地，在图数据模型中，顶点具有多个边链接到其他的顶点。多对象事务用以确保这些外键引用的有效性，即当插入多个相互引用的记录时，保证外键总是最新、正确的，否则数据更新就变得亳无意义。
+ 对于文档数据模型，如果待更新的字段都在同一个文档中，则可视为单个对象，此时不需要多对象事务。但是，**缺少join支持的文档数据库往往会滋生反规范化**(参阅第2章“关系数据库与文档数据库现状”)，如图7-2所示，当更新这种非规范化数据时，就需要一次更新多个文档。此时多对象事务就可以有效防止非规范化数据之间出现不同步。
+ **对于带有二级索引的数据库(除了纯粹键-值存储以外几乎所有其他系统都支持二级索引)，每次更改值时都需要同步更新索引**。<u>从事务角度来看，这些索引是不同的数据库对象：如果没有事务隔离，就会出现部分索引更新。</u>

​	即使没有事务支持，或许上层应用依然可以工作，然而在没有原子性保证时，错误处理就会异常复杂，而缺乏隔离性则容易出现并发性方面的各种奇怪问题。我们将在本章后面“弱隔离级别”中讨论这些问题，并在第12章中探讨其他的一些方案。

#### 处理错误与中止

​	事务的一个关键特性是，如果发生了意外，所有操作被中止，之后可以安全地重试。**ACID数据库基于这样的一个理念：如果存在违反原子性、隔离性或持久性的风险，则完全放弃整个事务，而不是部分放弃**。

​	<u>然而，并不是所有的系统都遵循上述理念。例如，无主节点复制的数据存储(参阅第5章“无主节点复制”)会在“尽力而为”的基础上尝试多做些工作，可以概括理解为：数据库已经尽其所能，但万一遇到错误，系统并不会撤销已完成的操作，此时需要应用程序来负责从错误中进行恢复</u>。

​	确实我们无法彻底避免错误，然而许多开发人员喜欢只考虑正常的处理路径，而忽视错误处理。比如像流行的 Rails ActiveRecord和Django这样的对象关系映射(ORM)框架，当事务出现异常时不会进行重试而只是简单地抛出堆栈信息，用户虽然得到了错误提示，但所有之前的输入都会被抛弃。这绝对不应该，支持安全的重试机制才是中止流程的重点。

​	重试中止的事务虽然是一个简单有效的错误处理机制，但它并不完美：

+ **如果事务实际已经执行成功，但返回给客户端的消息在网络传输时发生意外(所以在客户端看来事务是失败)，那么重试就会导致重复执行，此时需要额外的应用级重复数据删除机制**。
+ <u>如果错误是由于系统超负荷所导致，则重试事务将使情况变得更糟</u>。为此，可以设定一个重试次数上限，例如指数回退，同时要尝试解决系统过载本身的问题。
+ 由临时性故障(例如死锁，隔离违例，网络闪断和节点切换等)所导致的错误需要重试。但如果出现了永久性故障(例如违反约束)，则重试亳无意义。
+ **如果在数据库之外，事务还产生其他副作用，即使事务被中止，这些副作用可能已事实生效**。例如，假设更新操作还附带发送一封电子邮件，肯定不希望每次重试时都发送邮件。如果想要确保多个不同的系统同时提交或者放弃，可以考虑采用两阶段提交(参阅第9章“原子提交与两阶段提交”)。
+ 如果客户端进程在重试过程中也发生失败，没有其他人继续负责重试，则那些待写入的数据可能会因此而丢失。

## 7.2 弱隔离级别

​	如果两个事务操作的是不同的数据，即不存在数据依赖关系，则它们可以安全地并行执行。**只有出现某个事务修改数据而另一个事务同时要读取该数据，或者两个事务同时修改相同数据时，才会引发并发问题(引入了竞争条件)**。

​	并发性相关的错误很难通过测试发现，这类错误通常只在某些特定时刻才会触发，这种时机相关的问题发生概率低，稳定重现比较困难。并发性也很难对其进行推理分析，特别是对于一个大型应用程序，几乎不可能知道哪些代码正在访问数据库：只有一个用户访问数据时，程序开发就足够困难了，当出现多用户并发时情况会变得更加复杂，每一块数据随时都可能被多个用户所修改。

​	正因如此，数据库一直试图通过事务隔离来对应用开发者隐藏内部的各种并发问题。<u>从理论上讲，隔离是假装没有发生并发，让程序员的生活更轻松，而可串行化隔离意味着数据库保证事务的最终执行结果与串行(即一次一个，没有任何并发)执行结果相同。</u>

​	实现隔离绝不是想象的那么简单。可串行化的隔离会严重影响性能，而许多数据库却不愿意牺牲性能，因而更多倾向于采用较弱的隔离级别，它可以防止某些但并非全部的并发问题。<u>这些弱隔离级别理解起来更为困难，甚至可能会带来一些难以捉摸的隐患，但在实践中还是被广泛使用</u>。

​	弱隔离所引发的并发性错误绝非仅是理论存在，它们已经造成了大量的资金损失，审计部门的调查，以及客户数据破坏等。**对此，有一个流行的说法是“如果你正在处理财务数据，请上ACID系统!”，但这样的建议其实没有太大实际意义，因为很多流行的关系数据库系统(通常被认为是“ACID兼容”)其实也采用的是弱级别隔离，所以它们未必可以阻止类似错误的发生**。

​	与其盲目地相信这些宣传，不如对存在的并发问题以及如何防范有一个全面、深刻的理解。然后，我们就可以使用所掌握的工具和方法来构建正确、可靠的应用。

​	本节将分析几个实际中经常用到的弱级别(非串行化)隔离，并详细讨论可能(或者不可能)发生的竞争条件，有了这些认识之后，可以帮助判断自己的应用更适合什么样的隔离级别。在下一节，我们将详细讨论可串行化。我们主要以示例的方式讨论隔离级别，如果你需要的是严格、正式的定义和分析，可以参考文献。

### 7.2.1 读提交

​	读提交是最基本的事务隔离级别，它只提供以下两个保证：

1. **读数据库时，只能看到已成功提交的数据(防止“脏读”)**。

2. **写数据库时，只会覆盖已成功提交的数据(防止“脏写”)**。

​	这两个保证更深入的介绍如下：

#### 防止脏读

​	假定某个事务已经完成部分数据写入，但事务尚未提交(或中止)，此时另一个事务是否可以看到尚未提交的数据呢？如果是的话，那就是脏读。

​	**读-提交级别的事务隔离必须做到防止发生脏读。这意味着事务的任何写入只有在成功提交之后，才能被其他人观察到(并且所有的写全部可见)**。如图7-4所示，用户1设置了x=3，在用户1的事务未提交之前，用户2的get x操作依旧返回的是旧值2。

![弱隔离级别 - 图1](https://static.sitestack.cn/projects/ddia/img/fig7-4.png)

​	当有以下需求时，需要防止脏读：

+ 如果事务需要更新多个对象，脏读意味着另一个事务可能会看到部分更新，而非全部。例如，图7-2就是一个电子邮件应用的脏读例子，用户看到了新的未读电子邮件，但看不到更新的计数器。观察到部分更新的数据可能会造成用户的困惑，并由此引发一些不必要的后续操作。
+ <u>如果事务发生中止，则所有写入操作都需要回滚(见图7-3)。如果发生了脏读，这意味着它可能会看到一些稍后被回滚的数据，而这些数据并未实际提交到数据库中。之后所引发的后果可能都会变得难以预测</u>。

#### * 防止脏写

​	如果两个事务同时尝试更新相同的对象，会发生什么情况呢？我们不清楚写入的顺序，但可以想象后写的操作会覆盖较早的写入。

​	但是，<u>如果先前的写入是尚未提交事务的一部分，是否还是被覆盖？如果是，那就是脏写</u>。<u>读-提交隔离级别下所提交的事务可以防止脏写，**通常的方式是推迟第二个写请求，直到前面的事务完成提交(或者中止)**</u>。

​	防止脏写可以避免以下并发问题：

+ 如果事务需要更新多个对象，脏写会带来非预期的错误结果。例如，考虑图7-5的二手车销售网站， Alice和Bob两个人试图购买同一辆车。而购买汽车需要两次数据库写入：网站上商品买主需要更新为新买家，销售发票也需要随之更新。对于图7-5的例子，车主被改为Bob(因为他成功地抢先更新了车辆表单)，但发票却发给了 Alice(因为她成功的先执行了发票表单)。读提交隔离要防止这种事故。
+ 但是，读-提交隔离并不能解决图7-1中计数器増量的竟争情况。对于后者，第一次写入确实在第一个事务提交后才执行，虽然不属于脏写，但结果仍然是错误的。在接下来的“防止更新丟失”中，我们将讨论如何安全递增计数器。

![弱隔离级别 - 图2](https://static.sitestack.cn/projects/ddia/img/fig7-5.png)

#### 实现读-提交

​	读-提交隔离非常流行，它是 Oracle 11g、 PostgreSQL、 SQL Server 2012、 MemSQL以及许多其他数据库的默认配置。

​	<u>数据库通常采用**行级锁**来防止脏写</u>：<u>当事务想修改某个对象(例如行或文档)时，它必须首先获得该对象的锁；然后一直**持有锁直到事务提交(或中止)**</u>。<u>给定时刻，只有一个事务可以拿到特定对象的锁，**如果有另一个事务尝试更新同一个对象，则必须等待，直到前面的事务完成了提交(或中止)后，才能获得锁并继续**</u>。**这种锁定是由处于读-提交模式(或更强的隔离级别)数据库自动完成的**。

​	那如何防止脏读呢？一种选择是使用相同的锁，所有试图读取该对象的事务必须先申请锁，事务完成后释放锁。从而确保不会发生读取一个脏的、未提交的值(因为锁在那段期间一直由一个事务所持有)。

​	然而，**读锁的方式在实际中并不可行，因为运行时间较长的写事务会导致许多只读的事务等待太长时间，这会严重影响只读事务的响应延迟，且可操作性差：由于读锁，应用程序任何局部的性能问题会扩散进而影响整个应用，产生连锁反应**。

​	因此，<u>大多数数据库注采用了图7-4所示的方法来防止脏读：**对于每个待更新的对象，数据库都会维护其旧值和当前持锁事务将要设置的新值两个版本**</u>。**<u>在事务提交之前，所有其他读操作都读取旧值；仅当写事务提交之后，才会切换到读取新值**</u>。

### * 7.2.2 快照级别隔离与可重复读

​	表面上看读-提交级别隔离，可能会认为它已经满足了事务所需要一切特征：它支持中止(原子性所必须的)，可以防止读取不完整的结果，并且防止并发写的混合。事实上，这些确实非常有用，相比没有事务的系统，它的确提供了更多的保证。

​	但是，在使用此隔离级别时，仍然有很多场景可能导致并发错误。如图7-6所示。

![弱隔离级别 - 图3](https://static.sitestack.cn/projects/ddia/img/fig7-6.png)

​	假设Alce银行有1000美元的存款，分为两个账户，每个500美元。现在有这样一笔转账交易从账户1转100美元到账户2。如果在她提交转账请求之后而银行数据库系统执行转账的过程中间，来查看两个账户的余额，她有可能会看到账号2在收到转账之前的余额(500美元)，和账户1在完成转账之后的余额(400美元)。对于Alice来说，貌似她的账户总共只有900美元，有100美元消失了。

​	这种异常现象被称为**不可重复读取(nonrepeatable read)**或**读倾斜(read skew)**。如果Alice在交易结束时再次读取账户1的余额，她将看到不同的值(600美元)。读倾斜在读提交隔离语义下是可以接受的， Alice所看到的账户余额的确都是账户当时的最新值。

> 倾斜(skew)这个词有些滥用了，我们以前使用它是由于热点而导致负载不平衡(参阅第6章“倾斜的负载与减轻热点”)，而这里则主要是指时间异常。

​	对于 Alice这个例子，这并非一个永久性问题，例如几秒钟之后当她重新加载银行页面，可能就能看到一致的账户余额。但是，还有些场景则不能容忍这种暂时的不一致：

+ **备份场景**

  备份任务要复制整个数据库，这可能需要数小时才能完成。<u>在备份过程中，可以继续写入数据库。因此，得到镜像里可能包含部分旧版本数据和部分新版本数据。如果从这样的备份进行恢复，最终就导致了永久性的不一致(例如那些消失的存款)</u>。

+ **分析査询与完整性检查场景**

  有时査询可能会扫描几乎大半个数据库。这类查询在分析业务中很常见(参阅第3章“事务处理或分析”)，亦或定期的数据完整性检査(即监视数据损坏情况)。如果这些查询在不同时间点观察数据库，可能会返回无意义的结果。

​	<u>**快照级别隔离**这是阶级上述问题最常见的手段</u>。**其总体想法是，<u>每个事务都从数据库的一致性快照中读取，事务一开始所看到是最近提交的数据，即使数据随后可能被另一个事务更改，但保证每个事务都只看到该特定时间点的旧数据</u>**。

​	<u>快照级别隔离对于长时间运行的只读查询(如备份和分析)非常有用。如果数据在执行査询的同时还在发生变化，那么査询结果对应的物理含义就难以理清</u>。而如果查询的是数据库在某时刻点所冻结的一致性快照，则査询结果的含义非常明确。

​	**快照级别隔离非常流行，目前 PostgreSQL， MySQL的 InnoDB存储引擎，Oracle，SQLrver等都已经支持**。

#### * 实现快照级别隔离

​	与读-提交隔离类似，快照级别隔离的实现通常**采用写锁来防止脏写**(参阅本章前面的“实现读-提交”)，这意味着<u>正在进行**写操作**的事务会阻止同一对象上的其他事务</u>。但是，**读取则不需要加锁**。从性能角度看，**快照级别隔离的一个关键点是读操作不会阻止写操作，反之亦然**。这使得数据库可以在处理正常写入的同时，在一致性快照上执行长时间的只读查询，且两者之间没有任何锁的竞争。

​	为了实现快照级别隔离，数据库采用了一种类似于图7-4中防止脏读但却更为通用的机制。<u>考虑到多个正在进行的事务可能会在不同的时间点查看数据库状态，所以**数据库保留了对象多个不同的提交版本**，这种技术因此也被称为**多版本并发控制( Multi-Version Concurrency Control， MVCC )**</u>。

​	**如果只是为了提供读-提交级别隔离，而不是完整的快照级别隔离，则只保留对象的两个版本就足够了：一个已提交的旧版本和尚未提交的新版本。**所以，<u>**支持快照级别隔离的存储引擎往往直接采用MVCC来实现读-提交隔离**</u>。

+ 典型的做法是，在读-提交级别下，**对每一个不同的查询单独创建一个快照**；
+ 而快照级别隔离则是**使用一个快照来运行整个事务**。

​	图7-7说明了 PostgreSQL 如何实现基于MVCC的快照级别隔离(其他实现基本类似)。

+ **<u>当事务开始时，首先赋予一个唯一的、单调递增的事务ID(txid)</u>**。
+ <u>每当事务向数据库写入新内容时，**所写的数据都会被标记写入者的事务ID**</u>。

![弱隔离级别 - 图4](https://static.sitestack.cn/projects/ddia/img/fig7-7.png)

​	<u>表中的每一行都有一个 **created by字段**，其中包含了**创建该行的事务ID**。每一行还有个 **deleted by字段**，初始为空</u>。**如果事务要删除某行，该行实际上并未从数据库中删除，而只是将 deleted_by字段设置为请求删除的事务ID(仅仅标记为删除)**。<u>事后，当确定没有其他事务引用该标记删除的行时，数据库的垃圾回收进程才去真正删除并释放存储空间</u>。

​	<u>这样一笔**更新操作**在内部会被转换为**一个删除操作加一个创建操作**</u>。例如，图7-7中，事务13从账户2中扣除100美元，余额从500美元减为400美元。 accounts表里会出现两行账户2：一个余额为$500但标记为删除的行(由事务13删除)，另一个余额为$400，由事务13创建。

#### * 一致性快照的可见性规则

​	当事务读数据库时，通过事务ID可以决定哪些对象可见，哪些不可见。要想对上层应用维护好快照的一致性，需要精心定义数据的可见性规则。例如：

1. **<u>每笔事务开始时，数据库列出所有当时尚在进行中的其他事务(即尚未提交或中止)，然后忽略这些事务完成的部分写入(尽管之后可能会被提交)，即不可见</u>**。
2. **<u>所有中止事务所做的修改全部不可见</u>**。
3. **较晚事务ID(即晚于当前事务)所倣的任何修改不可见，不管这些事务是否完成了提交**。
4. 除此之外，其他所有的写入都对应用杳询可见。

​	以上规则可以适用于**创建操作**和**删除操作**。<u>在图7-7中，当事务12从账户2读取时，它看到的是$500的余额，这是因为删除操作是由稍后事务13所产生的(依据规则3，事务12对事务13所做的删除不可见)，同理，400美元余额的创建操作也不可见</u>。

​	换句话说，仅当以下两个条件都成立，则该数据对象对事务可见：

+ **事务开始的时刻，创建该对象的事务已经完成了提交**。
+ **对象没有被标记为删除；或者即使标记了，<u>但删除事务在当前事务开始时还没有完成提交</u>**。

​	<u>长时间运行的事务可能会使用快照很长时间，从其他事务的角度来看，它可能在持续访问正在被覆盖或删除的内容。由于**没有就地更新，而是每次修改总创建一个新版本**，因此数据库可以以较小的运行代价来维护一致性快照</u>。

#### * 索引与快照级别隔离

​	接下来一个问题是，这种多版本数据库该如何支持索引呢？<u>一种方案是索引直接指向对象的所有版本，然后想办法过滤对当前事务不可见的那些版本。当后台的垃圾回收进程决定删除某个旧对象版本时，对应的索引条目也需要随之删除</u>。

​	在实践中，有许多细节决定了多版本并发控制的实际性能表现。例如，**可以把同一对象的不同版本放在一个内存页面上， PostgreSQL采取这样的优化措施来避免更新索引**。

​	**CouchDB、 Datomic和LMDB则使用另一种方法。它们主体结构是B-tree(参阅第3章“B-tree”)，但采用了一种追加/写时复制的技术，当需要更新时，不会修改现有的页面，而总是创建一个新的修改副本**，拷贝必要的内容，然后让父结点，或者递归向上直到树的root结点都指向新创建的结点。那些不受更新影响的页面都不需要复制，保持不变并被父结点所指向。

​	<u>这种**采用追加式的B-tree，每个写入事务(或一批事务)都会创建一个新的B-tree root**，代表该时刻数据库的一致性快照</u>。这时就没有必要根据事务ID再去过滤掉某些对象，每笔写入都会修改现有的B-tree，因为之后的查询可以直接作用于特定快照B-tree(有利于査询性能)。采用这种方法依然需要后台进程来执行压缩和垃圾回收。

#### * 可重复读与命名混淆

​	**快照级别隔离对于只读事务特别有效**。但是，具体到实现，许多数据库却对它有着不同的命名。 <u>**Oracle称之为可串行化， PostgreSQL和 MySQL则称为可重复读**</u>。

​	这种命名混淆的原因是SQL标准并没有定义**快照级别隔离**，而仍然是基于老的 System R 1975年所定义的隔离级别)，而当时还没有出现快照级别隔离。标准定义的是“可重复读”，这看起来比较接近于快照级别隔离，所以 PostgreSQL和 MySQL称它们的快照级别隔为“可重复读”，这符合标准要求(即合规性)。

​	**然而必须指出，SQL标准对隔离级别的定义还是存在一些缺陷，<u>某些定义模棱两可不够精确，且不能做到与实现无关</u>**。尽管有几个数据库实现了可重复读，表面上看符合标准，但它们实际所提供的保证却大相径庭。可重复读有一个更为严谨的定义，然而大多数实现并没有遵循它。最后还要指出， **IBM DB2的“可重复读”实则是可串行化级别隔离**。

​	现在的结果是，我们已经搞不清楚“可重复读”究竟代表什么了。

### * 7.2.3 防止更新丢失

​	总结一下，**我们所讨论的读-提交和快照级别隔离主要都是为了解决只读事务遇到并发写时可以看到什么**(虽然中间也涉及脏写问题)，总体而言我们还没有触及另一种情况，即两个写事务并发，而<u>脏写只是写并发的一个特例</u>。

​	写事务并发还会带来其他一些值得关注的冲突问题，最著名的就是更新丢失问题，前面图7-1正是这样的一个例子。

​	更新丟失可能发生在这样一个操作场景中：应用程序从数据库读取某些值，根据应用逻辑做出修改，然后写回新值(**read-modify-write过程**)。当有两个事务在同样的数据对象上执行类似操作时，<u>由于隔离性，第二个写操作并不包括第一个事务修改后的值，最终会导致第一个事务的修改值可能会丢失</u>。这种冲突还可能在其他不同的场景下发生，例如:

+ 递增计数器，或更新账户余额(需要读取当前值，计算新值并写回更新后的值)。
+ 对某复杂对象的一部分内容执行修改，例如对JSON文档中一个列表添加新元素(需要读取并解析文档，执行更改并写回修改后的文档)。
+ 两个用户同时编辑wiki页面，且毎个用户都尝试将整个页面发送到服务器，覆盖数据库中现有内容以使更改生效。

​	并发写事务冲突是一个普遍问题，目前有多种可行的解决方案。

#### * 原子写操作

​	许多数据库提供了**原子更新操作**，以避免在应用层代码完成“读-修改-写回”操作，如果支持的话，通常这就是最好的解决方案。例如，以下指令在多数关系数据库中都是并发安全的。

`UPDATE counters SET value = value + 1 WHERE key = 'foo'`

​	类似地，像 MongoDB这样的文档数据库支持对JSON文档的某部分进行本地修改的原子操作， Redis也提供了对特定数据结构(如优先级队列)修改的原子操作。然而，<u>并非所有的应用更新操作都可以以原子操作的方式来表达，例如维基页面的更新涉及各种文本编辑</u>。**无论如何，如果原子操作可行，那么它就是推荐的最佳方式**。

+ <u>原子操作通常釆用对读取对象加**独占锁**的方式来实现，这样在更新被提交之前不会其他事务可以读它。这种技术有时被称为**游标稳定性**</u>。
+ <u>另一种实现方式是强制所有的原子操作都在**单线程**上执行</u>。

​	不过，<u>基于对象关系映射(ORM)框架可以很容易地就产生出来非“读-修改-写回”的应用层代码，导致无法使用数据库所提供的原子操作。假如你清楚知道自己在做什么，或许这并不会引发什么问题，但往往这种情况会埋下很多难以发现的潜在错误</u>。

#### * 显式加锁

​	如果数据库不支持内置原子操作，另一种和防止更新丢失的方法是由应用程序显式**锁**定待更新的对象。然后，应用程序可以执行“读-修改-写回”这样的操作序列，此时如果有其他事务尝试同时读取对象，则必须等待当前正在执行的序列全部完成。

​	例如，考虑一个多人游戏，其中几个玩家可以同时移动同一个数字。只靠原子操作可能还不够，因为应用程序还需要确保玩家的移动还需遵守其他游戏规则，这涉及一些应用层逻辑，不可能将其剥离转移给数据库层在査询时执行。此时，可以使用锁来防止两名玩家同时操作相同的棋子，参见示例7-1。

​	示例7-1：显式锁定行以防丢失更新

```sql
BEGIN TRANSACTION;
SELECT * FROM figures
    WHERE name = 'robot' AND game_id = 222
FOR UPDATE;
-- Check whether move is valid, then update the position
-- of the piece that was returned by the previous SELECT.
UPDATE figures SET position = 'c4' WHERE id = 1234;
COMMIT;
```

​	**<u>FOR UPDATE指令指示数据库对返回的所有结果行要加锁</u>**。

​	首先该方法是可行的，但要做到这一点，需要仔细考虑清楚应用层的逻辑。很多代码会忘记在必要的地方加锁，结果很容易引入**竞争冲突**。

#### * 自动检测更新丢失

​	<u>**原子操作**和**锁**都是通过强制“读-修改-写回”操作序列串行执行来防止丢失更新</u>。另一种思路则是**先让他们并发执行，但如果事务管理器检测到了更新丢失风险，则会中止当前事务，并强制回退到安全的“读-修改-写回”方式**。

​	<u>该方法的一个优点是数据库完全可以借助**快照级别隔离**来高效地执行检査</u>。的确，PostgreSQL的可重复读， Oracle的可串行化以及 SQL Server的快照级别隔离等，都可以自动检测何时发生了更新丢失，然后会中止违规的那个事务。但是， <u>MySQL InnoDB的可重复读却并不支持检测更新丢失</u>。**有一些观点认为，数据库必须防止更新丟失，要不然就不能宣称符合快照级别隔离，如果基于这样的定义，那么MySQL就属于没有完全支持快照级别隔离**。

​	更新丟失检测是一个非常赞的功能，应用层代码因此不用依赖于某些特殊的数据库功能。开发者可能会不小心忘记使用锁或原子操作，但更新丢失检测会自动生效，有效地避免这类错误。

#### * 原子比较和设置

​	在不提供事务支持的数据库中，有时你会发现它们支持原子“比较和设置”操作(之前“单对象写入”有提到)。使用该操作可以避免更新丢失，即**只有在上次读取的数据没有发生变化时才允许更新**；<u>**如果已经发生了变化，则回退到“读-修改-写回”方式**</u>。

​	例如，为了防止两个用户同时更新同一个wiki页面，可以尝试下面的操作，这样只有当页面从上次读取之后没发生变化时，才会执行当前的更新:

```sql
-- This may or may not be safe, depending on the database implementation
UPDATE wiki_pages SET content = '新内容'
  WHERE id = 1234 AND content = '旧内容';
```

​	**如果内容已经有了变化且值与“旧内容”不匹配，则更新将失败，需要应用层再次检查并在必要时进行重试**。需要注意，<u>**如果 WHERE语句是运行在数据库的某个旧的快照上，即使另一个并发写入正在运行，条件可能仍然为真，最终可能无法防止更新丢失问题。所以在使用之前，应该首先仔细检査“比较-设置”操作的安全运行条件**</u>。

#### * 冲突解决与复制

​	对于支持多副本的数据库(参见第5章)，防止丢失更新还需要考虑另一个维度：由于多节点上的数据副本，不同的节点可能会并发修改数据，因此必须采取一些额外的措施来防止丢失更新。

​	**<u>加锁</u>和<u>原子修改</u>都有个前提即只有一个最新的数据副本**。<u>然而，对于**多主节点**或者**无主节点**的**多副本数据库**，由于支持多个并发写，且通常以异步方式来同步更新，所以会出现多个最新的数据副本。**此时加锁和原子比较将不再适用**(我们将在第9章“线性化”详细讨论这个问题)</u>。

​	正如第5章“检测并发写”所描述的，<u>多副本数据库通常支持多个**并发写**，然后**保留多个冲突版本(互称为兄弟)**，之后由应用层逻辑或依靠特定的数据结构来解决、合并多版本</u>。

​	**如果操作可交换(顺序无关，在不同的副本上以不同的顺序执行时仍然得到相同的结果)，则原子操作在多副本情况下也可以工作**。例如，计数器递增或向集合中添加元素等都是典型的可交换操作。这也是Riak 2.0新数据类型的设计思路，<u>当一个值被不同的客户端同时更新时，Riak自动**将更新合并**在一起，**避免发生更新丢失**。</u>

​	而<u>“**最后写入获胜(LWW)**”(详见第5章)冲突解决方法则容易丢失更新</u>。**不幸的是，目前LWW是许多多副本数据库的默认配置**。

### * 7.2.4 写倾斜与幻读

​	当多个事务同时写入同一对象时引发了两种竞争条件，即前面章节所讨论的**脏写**和**更新丢失**。为了避免数据不一致，需要借助数据库的一些内置机制，或者采取**手动加锁**、执行**原子操作**等。

​	然而，这还不是并发写所引发的全部问题。本节马上将看到更为微妙的写冲突的例子。

​	首先，设想这样一个例子：你正在开发一个应用程序来帮助医生管理医院的轮班。通常，医院会安排多个医生值班，医生也可以申请调整班次(例如他们自己生病了)，但前提是确保至少一位医生还在该班次中值班。

​	现在情况是， Alice和Bob是两位值班医生。两人碰巧都感到身体不适，因而都决定请假。不幸的是，他们几乎同一时刻点击了调班按钮。接下来发生的事情如图7-8所示。

​	<u>每笔事务总是首先检查是否至少有两名医生目前在值班。如果是的话，则有一名医生可以安全离开。由于数据库正在使用**快照级别隔离**，两个检查都返回有两名医生，所以两个事务都安仝地进入到下一个阶段。接下来 Alice更新自己的值班记录为离开，同样，Bob也更新自己的记录。两个事务都成功提交，最后的结果却是没有任何医生在值班，显然这违背了至少一名医生值班的业务要求</u>。

#### * 定义写倾斜

​	这种异常情况称为**写倾斜**。<u>它既不是一种脏写，也不是更新丢失</u>，两笔事务更新的是两个不同的对象(分别是 Alice和Bob的值班记录)。这里的写冲突并不那么直接，但很显然这的确是某种竞争状态：试想，如果两笔事务是串行执行，则第二个医生的申请肯定被拒绝；只有同时执行两个事务时才会引发该异常。

![弱隔离级别 - 图5](https://static.sitestack.cn/projects/ddia/img/fig7-8.png)

​	<u>**可以将写倾斜视为一种更广义的更新丢失问题**</u>。

+ 即**<u>如果两个事务读取相同的一组对象，然后更新其中一部分：不同的事务可能更新不同的对象，则可能发生写倾斜</u>**；
+ 而**不同的事务如果更新的是<u>同一个对象</u>，则可能发生<u>脏写</u>或<u>更新丢失</u>(具体取决于时间窗口)**。

​	我们已经给出了多种防范更新丢失的手段。然而对于写倾斜，可选的方案则有很多限制：

+ **由于涉及多个对象，<u>单对象的原子操作</u>不起作用**。

+ <u>基于快照级别隔离来实现更新丢失自动检测</u>也有问题：<u>目前所有的数据库实现包括 PostgreSQL 的可重复读， MySQL/InnoDB可重复读， Oracle可串行化以及SQL Server的**快照级别隔离级别都不支持检测写倾斜问题**</u>。**自动防止写倾斜要求真正的可串行化隔离**(参阅本章后面“可串行化”)。

+ 某些数据库支持自定义约束条件，然后由数据库代为检查、执行约束(例如，唯一性，外键约束或限制一些特定值)。<u>但是，至少一名医生值班这样的要求涉及对多个对象进行约束，**目前大多数数据库不支持这种类型约束**，所以取决于具体的数据库，开发者可能可以采用触发器或物化视图来自己实现类似约束</u>。

+ **如果不能使用可串行化级别隔离，一个次优的选择是对事务依赖的行来显式的加锁**。对于上述医生值班的例子，可以这样：

  ```sql
  BEGIN TRANSACTION;
  SELECT * FROM doctors
    WHERE on_call = TRUE 
    AND shift_id = 1234 FOR UPDATE;
  UPDATE doctors
    SET on_call = FALSE
    WHERE name = 'Alice' 
    AND shift_id = 1234;
  COMMIT;
  ```

  **“FOR UPDATE”语句会通知数据库对返回的所有结果行自动加锁**。

> [mysql --- select ...for update - D_戴同学 - 博客园 (cnblogs.com)](https://www.cnblogs.com/daijiabao/p/11284934.html)
>
> [mysql innodb中 select for update 和直接update 的差别是什么？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/340712418)
>
> [MySQL 共享锁 (lock in share mode)，排他锁 (for update) | Laravel China 社区 (learnku.com)](https://learnku.com/articles/12800/lock-in-share-mode-mysql-shared-lock-exclusive-lock-for-update)

#### * 更多写倾斜的例子

​	写倾斜可能看起来很晦涩拗口，可一旦深刻意识到问题的本质，就会注意到还有更多可能发生的场景。下面就是一些例子：

+ 会议室预订系统

  假设要求同一时间、同个会议室不能被预订两次。当有人想要预订时，首先检查是否有冲突的预订(即对同一房间的预订存在时间范围重叠)，如果没有，则提交申请，参见示例7-2。

  示例7-2：会议室预订系统，<u>试图避免重复预订(但在快照级别隔离下不安全)</u>

  ```sql
  BEGIN TRANSACTION;
  -- Check for any existing bookings that overlap with the period of noon-lpm
  SELECT COUNT(*) FROM bookings
  WHERE room_id = 123 AND 
      end_time > '2015-01-01 12:00' AND start_time < '2015-01-01 13:00';
  -- If the previous query returned zero :
  INSERT INTO bookings(room_id, start_time, end_time, user_id)
    VALUES (123, '2015-01-01 12:00', '2015-01-01 13:00', 666);
  COMMIT;
  ```

  需要指出，**快照级别隔离**无法阻止并发用户预订同一个会议室。<u>为了保证预订不会产生冲突，需要**可串行化的隔离**</u>。

+ 多人游戏

  在示例7-1中，我们使用加锁来防止更新丢失(即两个玩家不能同时移动同一个数字)。但是，锁并不能阻止玩家将两个不同的数字移动到棋盘上的同一个位置上，或者其他可能违反游戏规则的移动。这取决于具体的游戏规则，可能需要更多的条件约束，否则很容易发生写倾斜。

+ 声明一个用户名

  网站通常要求每个用户有唯一的用户名，两个用户可能同时尝试创建相同的用户名。可以采用事务的方式首先来检查名称是否被使用，如果没有，则使用该名称创建账户。但是，和之前的例子类似，在**快照级别隔离**下这是不安全的。不过，对于该例子，一个简单的方案是采用**唯一性约束**(第二个事务由于违反约束，会中止创建相同用户名)。

+ 防止双重开支

  <u>支付或积分相关的服务通常需要检查用户的花费不能超过其限额</u>。**一种方式是在用户的账户中插入一个临时的支出项目，然后对于所有项目，检查总开销并对比限额。由于写倾斜问题，可能会同时插入两个支出项目，<u>两个交易各自都不超额，也不会注意到对方，但加在一起则会超额</u>**。

#### * 为何产生写倾斜

​	上述所有写倾斜的例子都遵循以下类似的模式：

1. 首先输入一些匹配条件，即**采用 SELECT 查询所有满足条件的行**(例如，至少有两名医生正在值班，同一时刻房间没有预订，棋盘的某位置没有出现数字，用户名还没有被占用，账户里还有余额等。
2. **根据查询的结果，应用层代码来决定下一步的操作**(有可能继续，或者报告错误并中止)。

3. **如果应用程序决定继续执行，它将发起数据库写入(INSERT， UPDATE或DELETE)并提交事务**。而<u>**这个写操作会改变步骤2做出决定的前提条件**</u>。换句话说，**<u>如果提交写入之后再重复执行步骤1的 SELECT 查询，就会返回完全不同的结果，原因是刚刚的写操作改变了决定的前提条件</u>**(现在只有一名医生在值班，现在会议室已被预订，现在棋盘位置已经出现了数字，现在用户名已被占用，现在余额巳经不足等)。

​	上述步骤可能有不同的执行顺序，例如，可以先写入，然后是 SELECT 查询，最后根据查询来决定是否提交或者放弃。

​	对于医生值班的例子，步骤3中所修改的行恰好是步骤1查询结果的一部分，因此如果先修改值班记录并加锁(SELECT FOR UPDATE)，再查询可以保证事务安全，避免写倾斜。<u>然而，对于其他例子则并不适用，它们检査的是不满足给定搜索条件的行(预期结果为空)，接下来添加符合条件的行。**如果步骤1的查询根本没有返回任何行，则 SELECT FOR UPDATE也就无从加锁**</u>。

+ <u>这种**在一个事务中的写入改变了另一个事务查询结果**的现象，称为<big>**幻读**</big></u>。

+ **快照级别隔离**可以避免**只读查询**时的**幻读**，但是对于我们上面所讨论那些**读-写事务**，它却无法解决棘手的**写倾斜**问题。

#### * 实体化冲突

​	**如果问题的关键是杳询结果中没有对象(空)可以加锁，或许可以人为引入一些可加锁的对象**?

​	例如，对于会议室预订的例子，<u>构造一个时间-房间表，表的每一行对应于特定时间段(例如最小15分钟间隔)的特定房间</u>。我们提前，例如对接下来的6个月，创建好所有可能的房间与时间的组合。

​	现在，预订事务可以查询并锁定(SELECT FOR UPDATE)表中与查询房间和时间段所对应的行。加锁之后，即可检查是否有重叠，然后像之前一样插入新的预订。<u>注意，这种附加表格并不存储预订相关的信息，它**仅仅用于方便加锁**，防止同一房间和时间范围内的重复预订</u>。

​	<u>这种方法称为**实体化冲突(或物化冲突)**，它把**幻读**问题转变为针对数据库中一组具体行的锁冲突问题</u>。

​	然而，弄清楚如何实现实体化往往也具有挑战性，实现过程也容易出错，这种把一个并发控制机制降级为数据模型的思路总是不够优雅。出于这些原因，**除非万不得已，没有其他可选方案，我们不推荐采用<u>实体化冲突</u>**。而在**大多数情况下，<u>可串行化隔离</u>方案更为可行**。

## * 7.3 串行化

​	我们已经分析了很多容易出现竟争条件的例子。采用**读-提交**和**快照隔离**可以防止其中一部分，但并非对所有情况都有效，例如**写倾斜**和**幻读**所导致的棘手问题。最后你会发现面临以下挑战：

+ 隔离级别通常难以理解，而且<u>不同的数据库的实现不尽一致</u>(例如“可重复读取”的含义在各家数据库的差别很大)。
+ 如果去检杳应用层的代码，往往很难判断它在特定的隔离级别下是否安全，特别是对于大型应用系统，几乎<u>无法预测所有可能并发情况</u>。
+ 同时，还缺乏好的工具来帮助检测竟争状况。<u>理论上，静态分析可能有所帮助，但更多的还只是学术研究缺乏实用性</u>。**测试并发性问题往往效率很低，一切取决于时机，它只有在特定的情景下才会出现，存在很大的不确定性**。

​	而这些都不是新问题，自20世纪70年代引入**弱隔离级别**以后，这种情况就一直存在。长久以来，研究人员给出的答案都很简单：**采用可串行化隔离**!

​	<u>可串行化隔离通常被认为是最强的隔离级别</u>。**它保证即使事务可能会并行执行，但<u>最终的结果与毎次一个即串行执行结果相同</u>**。这意味着，如果事务在单独运行时表现正确，那么它们在并发运行时结果仍然正确，换句话说，数据库可以防止所有可能的竞争条件。

​	如果串行化隔离比其他各种弱隔离级别好得多，那么为什么没有广泛使用呢？要回答这个问题，我们需要看看可串行化究竟是什么，以及如何执行。目前大多数提供可串行化的数据库都使用了以下三种技术之一，我们将依次探讨：

+ 严格按照串行顺序执行(参阅本章后面的“实际的串行执行”)。
+ **<u>两阶段锁定(参阅本章后面的“两阶段加锁”)，几十年来这几乎是唯一可行的选择</u>**。
+ **乐观并发控制技术**，例如**可串行化的快照隔离**(参阅本章后面的“可串行化的快照隔离“)。

​	我们首先限定在单节点背景下讨论这些技术。在接下来的第9章，我们将其推广到分布式系统多节点场景。

### 7.3.1 实际串行执行

​	解决并发问题最直接的方法是**避免并发**：即在一个线程上按顺序方式每次只执行一个事务。这样我们完全回避了诸如检测、防止事务冲突等问题，其对应的隔离级别一定是严格串行化的。

​	看上去这是个很直白的想法，但数据库设计人员直到2007年前后才完全确信，采用单线程循环来执行事务是可行的。如果多线程并发在过去的30年中一只被认为是提升性能的关键，那么现在转向单线程执行，这意味着什么呢?

​	有以下两方面的进展促使我们重新做出思考：

+ <u>内存越来越便宜</u>，现在许多应用可以将整个活动数据集都加载到内存中(参阅第3章“将所有内容加载到内存”)。当事务所需的所有数据都在内存中时，事务的执行速度要比等待磁盘I/O快得多。
+ 数据库设计人员意识到<u>OLTP事务通常执行很快</u>，只产生少量的读写操作(参阅第3章“事务处理或分析”)。相比之下，运行时间较长的分析查询则通常是只读的，可以在一致性快照(使用快照隔离)上运行，而不需要运行在串行主循环里。

​	**VoltDB/H-Store、 Redis和Datomic等采用串行方式执行事务**。单线程执行有时可能会比支持并发的系统效率更高，尤其是可以**避免锁开销**。但是，**其吞吐量上限是单个CPU核的吞吐量**。<u>为了充分利用单线程，相比于传统形式，事务也需要做出相应调整</u>。

#### * 采用存储过程封装事务

​	**在数据库的早期应用阶段，采用事务机制是希望能囊括用户的所有操作序列**。例如，预订机票涉及多个步骤(搜索路线，票价和可用座位，决定行程，在行程的某个航班上预订座位，输入乘客信息，最后是付款)。数据库设计者认为，如果整个过程是一个事务，那么就可以方便地原子化执行。

​	然而，人类做出决定并回应的速度通常比较慢。如果数据库事务总是需要等待来自用户的输入，同时还要支持潜在大量并发需求，那么系统大部分时间将处于空闲状态。这样数据库无法高效运行，所以<u>几乎所有的OLTP应用程序都避免在事务执行中等待用户交互从而使事务非常简洁</u>。<u>对于Web，这意味着事务会在一个HTTP请求中提交，而不会跨越多个请求。新的事务往往需要开启新的HTTP请求</u>。

​	即使把人为交互从关键路径移除掉，**事务总体沿用的依然是交互式客户端/服务器风格，一次一个请求语句**。应用程序来提交査询，读取结果，可能会根据前一个査询的结果来进行其他査询，依此类推。请求与结果在应用层代码(某台机器)和数据库服务器(另一台机器)之间来回交互。

​	<u>对于这种交互式的事务处理，大量时间花费在应用程序与数据库之间的网络通信。如果不允许事务并发，而是一次仅处理一个，那么吞吐量非常低，数据库总是在等待应用提交下一个请求。在这种类型的数据库中，为了获得足够的吞吐性能，需要能够**同时处理多个事务**</u>。

​	<u>出于这个原因，采用**单线程串行执行**的系统往往不支持**交互式**的**多语句事务**。应用程序必须提交整个事务代码作为**存储过程**打包发送到数据库</u>。这之间的差异如图7-9所示。把事务所需的所有数据全部加载在内存中，使存储过程高效执行，而无需等待网络或磁盘I/O。

![可序列化 - 图1](https://static.sitestack.cn/projects/ddia/img/fig7-9.png)

#### * 存储过程的优缺点

​	关系型数据库支持存储过程已经有很长一段时间了，自1999年以来巳是SQL标准(SQL/PSM)。然而由于各种原因，存储过程的声誉有所下降：

+ **每家数据库厂商都有自己的存储过程语言**(Oracle的PL/SQL， SQL Server的T-SQL， PostgreSQL的PL/pgSQL等)。<u>这些语言并没有跟上通用编程语言的发展，如果从今天的角度来看，这些语义都相当丑陋、过时，而且缺乏如今大多数编程语言所常用的函数库</u>。
+ **<u>在数据库中运行代码难以管理：与应用服务器相比，调试更加困难，版本控制与部署复杂，测试不便，并且不容易和指标监控系统集成</u>**。
+ 因为数据库实例往往被多个应用服务器所共享，所以数倨库通常比应用服务器要求更高的性能。**<u>数据库中一个设计不好的存储过程(例如，消耗大量内存或CPU时间)要比同样低效的应用服务器代码带来更大的麻烦</u>**。

​	不过这些问题也是可以克服的。最新的存储过程已经放弃了PL/SQL，而是使用现有的通用编程语言，例如 VoltDB使用Java或Groovy，Datomic使用Java或 Clojure，而Redis使用Lua。

​	**<u>存储过程与内存式数据存储使得单线程上执行所有事务变得可行。它们不需要等待I/O，避免加锁开销等复杂的并发控制机制，可以得到相当不错的吞吐量</u>**。

​	<u>ⅤoltDB还借助存储过程来执行**复制**：它并非将事务的执行结果从一个节点复制到另一个节点，而是**在每个副本上都执行相同的存储过程**</u>。因此， VoltDB要求存储过程必须是确定性的(即不同的节点上运行时，结果必须完全相同)。如果事务需要获得当前的日期和时间等，必须通过专门的确定性API来实现。

#### * 分区

​	串行执行所有事务使得并发控制更加简单，但是数据库的吞吐量被限制在单机单个CPU核。虽然只读事务可以在单独的快照上执行，但是<u>对于那些**高写入**需求的应用程序，**单线程事务处理**很容易成为严重的瓶颈</u>。

​	为了扩展到多个CPU核和多节点，可以对数据进行分区(参见第6章)， VoltDB支持这种配置模式。<u>如果你能找到一个方法来对数据集进行分区，使得**每个事务只在单个分区内读写数据**，这样**每个分区都可以有自己的事务处理线程且独立运行**。此时为**每个CPU核分配一个分区**，则数据库的**总体事务吞吐量**可以到达与CPU核的数量成**线性**比例关系</u>。

​	但是，对于跨分区的事务，数据库必须在涉及的所有分区之间协调事务。<u>存储过程需要跨越所有分区**加锁**执行，以确保整个系统的**可串行化**</u>。

​	<u>由于跨分区事务具有额外的协调开销，其性能比单分区内要慢得多</u>。 VoltDB报告的跨区事务吞吐量大约只有1000次/秒，这比单分区吞吐量低了好几个数量级，而且没法通过增加更多机器的方式来扩展性能。

​	<u>事务是否能只在单分区上执行很大程度上取决于应用层的数据结构</u>。简单的键-值数据比较容易切分，而**带有多个二级索引的数据则需要大量的跨区协调(参阅第6章“分区与二级索引”)，因此不太合适**。

#### * 串行执行小结

​	当满足以下约束条件时，串行执行事务可以实现串行化隔离：

+ **事务必须简短而高效**，否则一个缓慢的事务会影响到所有其他事务的执行性能。
+ **仅限于活动数据集完全可以加载到内存的场景**。有些很少访问的数据可能会被移到磁盘，但万一单线程事务需要访问它，就会严重拖累性能。
+ **写入吞吐量必须足够低**，才能在单个CPU核上处理；否则就需要采用分区，**最好没有跨分区事务**。
+ <u>**跨分区事务虽然也可以支持，但是占比必须很小**</u>。

### * 7.3.2 两阶段加锁

​	近三十年来，可以说数据库只有一种被广泛使用的串行化算法，那就是**两阶段加锁(two-phase locking， 2PL)**。

> 2PL不是2PC
>
> 虽然两阶段加锁(2PL)听起来和两阶段提交( two-phase commit，2PC)很相近，但它们是完全不同的东西。我们将在第9章讨论2PC。

​	之前我们看到，可以使用**加锁**的方法来**防止脏写**(参阅本章前面的“防止脏写”)：即如果两个事务同时尝试写入同一个对象时，以加锁的方式来确保第二个写入等待前面事务完成(包括中止或提交)。

​	<u>两阶段加锁方法类似，但锁的强制性更高。多个事务可以同时读取同一对象，但只要出现**任何写操作**(包括修改或删除)，则必须加锁以**独占**访问</u>：

+ 如果事务A已经读取了某个对象，此时事务B想要写入该对象，那么B必须等到A提交或中止之才能继续。以确保B不会在事务A执行的过程中间去修改对象。
+ <u>如果事务A已经修改了对象，此时事务B想要**读取**该对象，则B必须等到A提交或中止之后才能继续。**对于2PL，不会出现读到旧值的情况**</u>(参见图7-1的示例)。

​	因此<u>**2PL不仅在并发写操作之间互斥，读取也会和修改产生互斥**</u>。快照级别隔离的口号“读写互不干扰”(参阅本章前面的“实现快照级别隔离”)非常准确地点明了它和两阶段加锁的关键区别。另一方面，<u>因为**2PL提供了串行化**，所以它可以防止前面讨论的所有竞争条件，包括**更新丢失**和**写倾斜**</u>。

#### * 实现两阶段加锁

​	目前，2PL已经用于 MySQL(InnoDB)和 SQL Server中的“可串行化隔离”，以及DB2中的“可重复读隔离“。

​	此时数捃库的每个对象都有一个读写锁来隔离读写操作。即锁可以处于**共享模式**或**独占模式**。基本用法如下：

+ **如果事务要读取对象，必须先以共享模式获得锁**。可以有多个事务同时获得一个对象的共享锁，但是如果某个事务已经获得了对象的独占锁，则所有其他事务必须等待。
+ **如果事务要修改对象，必须以独占模式获取锁**。不允许多个事务同时持有该锁(包括共享或独占模式)，换言之，如果对象上已被加锁，则修改事务必须等待。
+ **<u>如果事务首先读取对象，然后尝试写入对象，则需要将共享锁升级为独占锁</u>**。<u>升级锁的流程等价于直接获得独占锁</u>。
+ **<u>事务获得锁之后，一直持有锁直到事务结束(包括提交或中止)</u>**。<u>这也是名字“两阶段”的来由，在第一阶段即事务执行之前要获取锁，第二阶段(即事务结束时)则释放锁</u>。

​	由于使用了这么多的锁机制，所以很容易出现死锁现象，例如事务A可能在等待事务B释放它的锁，而事务B在等待事务A释放所持有的锁。<u>数据库系统会**自动检测**事务之间的**死锁情况**，并强行中止其中的一个以打破僵局，这样另一个可以继续向前执行。而**被中止的事务需要由应用层来重试**</u>。

#### * 两阶段加锁的性能

​	两阶段加锁的主要缺点，或者说自1970年以来并不被所有人接纳的主要原因在于性能：**<u>其事务吞吐量和查询响应时间相比于其他弱隔离级别下降非常多</u>**。

​	部分原因在于锁的获取和释放本身的开销，但更重要的是**其降低了事务的并发性**。<u>按2PL的设计，两个并发事务如果试图做任何可能导致竞争条件的事情，其中一个必须等待对方完成</u>。

​	传统的关系数据库并不限制事务的执行时间，且当初是为和人类输入等交互式应用而设计的。结合2PL，最终结果是，当一个事务还需要等待另一个事务时，那么最终的等待时间几乎是没有上限的。即使可以保证自己的事务足够简短、高效，但<u>一旦出现多个事务同时访问同一对象，会形成一个等待队列，事务就必须等待队列前面所有其他事务完成之后才能继续</u>。

​	因此，在2PL模式下数据库的访问延迟具有非常大的不确定性，如果工作负载存在严重竞争，以百分比方式观察延迟指标会发现非常缓慢(参阅第1章“描述性能”)。试想这种情况，**某个事务本身很慢，或者是由于需要访冋大量数据而获得了许多锁，则它还会导致系统的其他部分都停顿下来**。如果应用需要稳定如一的性能，这种不确定性就是致命的。

​	**同样是基于加锁方式的读-提交隔离也可能发生死锁**，然而在2PL下，取决于事务的访问模式，死锁可能变得更为频繁。因而导致另一个性能问题，即如果事务由于死锁而被强行中止，应用层就必须从头重试，假如死锁过于频繁，则最后的性能和效率必然大打折扣。

#### * 谓词锁

​	对于加锁，我们还忽略了一个微妙但重要的细节。如本章前面“写倾斜与幻读”中的幻读问题，即一个事务改变另一个事务的查询结果。**可串行化隔离也必须防止幻读问题**。

​	以会议室预订为例，如果事务在查询某个时间段内一个房间的预订情况(参见示例7-2)，则<u>另一个事务不能同时去插入或更新同一时间段内该房间的预订情况，但它可以修改其他房间的预订情况，或者在不影响当前查询的情况下，修改该房间的其他时间段预订</u>。

​	如何实现呢？技术上讲，我们需要引入一种谓词锁(或者属性谓词锁)。它的作用类似于之前描述的共享/独占锁，而区别在于，**它并不属于某个特定的对象(如表的某一行)，而是<u>作用于满足某些搜索条件的所有查询对象</u>**，例如:

```sql
SELECT * FROM bookings
WHERE room_id = 123 AND
      end_time > '2018-01-01 12:00' AND 
      start_time < '2018-01-01 13:00';
```

​	谓词锁会限制如下访问:

+ <u>如果事务A想要读取某些满足匹配条件的对象，例如采用 SELECT 查询，它必须以**共享模式**获得査询条件的谓词锁</u>。**如果另一个事务B正持有任何一个匹配对象的互斥锁，那么A必须等到B释放锁之后才能继续执行查询**。
+ <u>如果事务A想要插入、更新或删除任何对象，则必须首先检查所有旧值和新值是否与现有的任何谓词锁匹配(即冲突)</u>。**如果事务B持有这样的谓词锁，那么A必须等到B完成提交(或中止)后才能继续**。

​	这里的关键点在于，<u>**谓词锁**甚至可以保护数据库中那些尚不存在但可能马上会被插入的对象**(幻读**)。**将两阶段加锁与谓词锁结合使用，数据库可以防止所有形式的写倾斜以及其他竞争条件，隔离变得真正可串行化**</u>。

#### * 索引区间锁

​	不幸的是，**谓词锁性能不佳**：如果活动事务中存在许多锁，那么检査匹配这些锁就变得非常耗时。因此，**大多数使用2PL的数据库实际上实现的是<u>索引区间锁(或者next-key locking)</u>**，<u>本质上它是对谓词锁的简化或者近似</u>。

​	<u>简化谓词锁的方式是**将其保护的对象扩大化**，首先这肯定是**安全**的</u>。例如，如果一个谓词锁保护的是查询条件是：房间123，时间段是中午至下午1点，则一种方式是通过扩大时间段来简化，即保护123房间的所有时间段；或者另一种方式是扩大房间，即保护中午至下午1点之间的所有房间(而不仅是123号房间)。<u>这样，**任何与原始谓词锁冲突的操作肯定也和近似之后的区间锁相冲突**</u>。

​	对于房间预订数据库，通常会在room_id列上创建索引，和/或在 start_time和end_time上有索引(否则前面的查询在大型数据库上会很慢)：

+ <u>假设索引位于 room_id上，数据库使用此索引查找123号房间的当前预订情况。现在，数据库可以简单地**将共享锁附加到此索引条目**，表明事务已搜索了123号房间的所有时间段预订</u>。
+ <u>或者，如果数据库使用基于时间的索引来査找预订，则可以**将共享锁附加到该索引中的一系列值**，表示事务已经搜索了该时间段内的所有值(例如直到2020年1月1日)</u>。

​	**无论哪种方式，查询条件的近似值都附加到某个索引上**。

​	<u>接下来，如果另一个事务想要插入、更新或删除同一个房间和/或重叠时间段的预订，则肯定需要更新这些索引，一定就会与共享锁冲突，因此会自动处于等待状态直到共享锁释放</u>。

​	这样就有效防止了**写倾斜**和**幻读**问题。的确，<u>**索引区间锁不像谓词锁那么精确**(会锁定更大范围的对象，而超出了串行化所要求的部分)，但由于**开销低得多**，可以认为是一种很好的折衷方案</u>。

​	<u>如果没有合适的**索引**可以施加**区间锁**，则数据库可以回退到对**整个表施加共享锁**</u>。这种方式的性能肯定不好，它甚至会阻止所有其他事务的写操作，但的确可以保证安全性。

### * 7.3.3 可串行化的快照隔离

​	本章已经给大家展示了数据库并发方面很多让人纠结、黯淡的一面。

+ <u>两阶段加锁虽然可以保证串行化，但性能差强人意且无法扩展(由于串行执行)</u>；
+ <u>弱级别隔离虽然性能不错，但容易引发各种边界条件(如**更新丢失**，**写倾斜**，**幻读**等)</u>。

​	那么，串行化隔离与性能是不是从根本上就是互相冲突而无法兼得吗？

​	或许并非如此。最近一种称为**可串行化的快照隔离(Serializable Snapshot Isolation，SSⅠ)**算法看起来让人眼前一亮。**它提供了完整的可串行性保证，而性能相比于快照隔离损失很小**。SSI算法面世至今不过数年，它于2008年被首次提出，后来成为Michael Cahil博士论文研究主题。

​	<u>目前，SSI可用于单节点数据库(PostgreSQL 9.1之后的可串行化隔离)或者分布式数据库(如 FoundationDB釆用了类似的算法)。相比于其他并发控制机制，SSI尚需在实践中证明其性能。即使如此，它很有可能成为未来数据库的标配。</u>

> [Postgresql可串行化快照隔离浅析 - 尚码园 (shangmayuan.com)](https://www.shangmayuan.com/a/aeed06ae6dbb42feb138a047.html)
>
> [Jepsen发现PostgreSQL重大Bug：在单个PostgreSQL实例上以可串行化隔离执行的事务实际上是不可串行的 (jdon.com)](https://www.jdon.com/54407)
>
> [解读年度数据库PostgreSQL：如何处理并发控制（一）_数据和云的博客-CSDN博客](https://blog.csdn.net/enmotech/article/details/94683664)

#### * 悲观与乐观的并发控制

​	<u>两阶段加锁是一种典型的**悲观并发控制**机制</u>。<u>它基于这样的设计原则：如果某些操作可能出错(例如与其他并发事务发生了锁冲突)，那么直接放弃，采用等待方式直到绝对安全。这**和多线程编程中互斥锁是一致的**</u>。

​	某种意义上讲，**串行执行**是种极端悲观的选择：<u>事务执行期间，等价于事务对**整个数据库(或数据库的一个分区)**持有**互斥锁**</u>。而我们只能假定事务执行得足够快、持锁时间足够短，来稍稍弥补这种悲观色彩。

​	相比之下，<u>**可串行化的快照隔离**则是一种**乐观并发控制**</u>。<u>在这种情况下，如果可能发生潜在冲突，事务会继续执行而不是中止，寄希望一切相安无事；而当事务提交时(只有可串行化的事务被允许提交)，数据库会检查是否确实发生了冲突(即违反了隔离性原则)，如果是的话，**中止事务并接下来重试**</u>。

​	**乐观并发控制**其实是一个古老的想法，关于其优点和缺点已经争论了很长时间：

+ **如果冲突很多，则性能不佳(许多事务试图访问相同的对象)，大量的事务必须中止**。
+ **如果系统已接近其最大吞吐量，反复重试事务会使系统性能变得更差**。

​	但是，如果系统还有足够的性能提升空间，且如果事务之间的竞争不大，乐观并发控制会比悲观方式高效很多。通过可交换的原子操作还可以减少一些竞争情况。例如，如果多个事务同时试图增加某个计数器，那么不管以什么样的顺序去增加(只要同一事务不去读计数器)，最后的结果总是等价的，并发提交多个增量操作是可行的。

​	顾名思义，**SSI基于快照隔离**，也就是说，<u>事务中的**所有读取操作**都是基于数据库的一致性快照</u>(请参阅本章前面的“快照隔离”和“可重复读”)。这是与早期的乐观并发控制主要区别。<u>在快照隔离的基础上，SSI新增加了相关算法来**检测写入之间的串行化冲突从而决定中止哪些事务**。</u>

#### * 基于过期的条件做决定

​	我们在讨论**写倾斜**(参阅本章前面的“写倾斜与幻读”)时，介绍了这样一种使用场景：事务首先査询某些数据，根据査询的结果来决定采取后续操作，例如修改数据。<u>而在快照隔离情况下，数椐可能在査询期间就已经被其他事务修改，导致原事务在提交时决策的依据信息已出现变化</u>。

​	换句话说，事务是基于某些前提条件而决定采取行动，在事务开始时条件成立，例如“目前有两名医生值班”，而当事务要提交时，数据可能已经发生改变，条件已不再成立。

​	当应用程序执行查询时(例如“当前有多少医生在值班?”)，数据库本身无法预知应用层逻辑如何使用这些查询结果。安全起见，数据库假定对查询结果(决策的前提条件)的任何变化都应使写事务失效。换言之，**<u>查询与写事务</u>之间可能存在<u>因果依赖关系</u>**。<u>**为了提供可串行化的隔离，数据库必须检测事务是否会修改其他事务的查询结果，并在此情况下中止写事务**</u>。

​	数据库如何知道查询结果是否发生了改变呢？可以分以下两种情况：

+ <u>**读取是否作用于一个(即将)过期的MVCC对象(读取之前已经有未提交的写入)**</u>。
+ <u>**检查写入是否影响即将完成的读取(读取之后，又有新的写入)**</u>。

#### * 检测是否读取了过期的MVCC对象

​	回想一下，**快照隔离通常采用多版本并发控制技术(MVCC，见图7-10)来实现**。<u>当事务从MⅤCC数据库一致性快照读取时，它会忽略那些在创建快照时**尚未提交**的事务**写入**</u>。例如图7-10中，事务42(修改 Alice的值班状态)未被提交，因此事务43中Alice查询到的oncall是true；当事务43提交时，事务42已经完成了提交。**换言之从快照读取时被忽略的写入已经生效，并且直接导致事务43做决定的前提已不再成立**。

![可序列化 - 图2](https://static.sitestack.cn/projects/ddia/img/fig7-10.png)

​	**为防止这种异常，<u>数据库需要跟踪那些由于MVCC可见性规则而被忽略的写操作</u>**。**<u>当事务提交时，数据库会检查是否存在一些当初被忽略的写操作现在已经完成了提交，如果是则必须中止当前事务</u>**。

​	为什么要等到提交：当检测到读旧值，为何不立即中止事务43呢？可以考虑这些情况：

+ 首先，如果事务43是个只读事务，没有任何<u>写倾斜</u>风险，就不需要中止；<u>而事务43读取数据库时，数据库还不知道事务是否稍后有任何写操作</u>。
+ 此外，事务43提交时，<u>有可能事务42发生了中止或者还处于未提交状态</u>，因此读取的并非是过期值。

​	**<u>通过减少不必要的中止，SSI可以高效支持那些需要在一致性快照中运行很长时间的读事务</u>**。

#### * 检测写是否影晌了之前的读

​	第二种要考虑的情况是，<u>在读取数据之后，另一个事务修改了数据</u>。如图7-11所示。

![可序列化 - 图3](https://static.sitestack.cn/projects/ddia/img/fig7-11.png)

​	在“两阶段加锁”中，我们讨论了<u>索引区间锁</u>(参阅本章前面的“索引区间锁”)。**它可以锁定与某个查询条件匹配的所有行**，例如 `WHERE shift_id = 1234`。这里使用了类似的技术，只有一点差异：**<u>SSI锁不会阻塞其他事务</u>**。

​	在图7-11中，事务42和事务43都在查询轮班1234期间的值班医生。<u>如果在 shift_id上建有**索引**，数据库可以通过索引条目1234来记录事务42和事务43都查询了相同的结果。如果没有索引，可以在**表级别**跟踪此信息。**该额外记录只需保留很小一段时间，当并发的所有事务都处理完成(提交或中止)之后，就可以丢弃**</u>。

​	**当另一个事务尝试修改时，它首先检查索引，从而确定是否最近存在一些读目标数据的其他事务**。<u>这个过程类似于在受影响的字段范围上获取写锁，但它并**不会阻塞读取**，而是**直到读事务提交时**才进一步通知他们：所读到的数据现在已经发生了变化</u>。

​	图7-11中，**事务43和事务42会互相通知对方先前的读已经过期**。<u>虽然事务43的修改的确影响了事务42，但事务43当时并未提交(修改未生效)，而事务42首先尝试提交，所以可以成功；**随后当事务43试图提交时，来自42的冲突写已经提交生效，事务43不得不中止**</u>。

#### * 可串行化快照隔离的性能

​	有许多工程方面的细节会直接影响算法在实践中的效果。例如， 一个需要**权衡考虑的是关于跟踪事务读、写的粒度**。

+ 如果非常详细地跟踪毎个事务的操作，确实可以准确推测有哪些事务受到影响、需要中止，但是记录元数据的开销可能很大；
+ 而粗粒度的记录则速度占优，但可能会扩大受影响的事务范围。

​	<u>有时，读取过期的数据并不会造成太大影响，这完全取决于所处的具体场景</u>。有时可以确信执行的最终结果是可串行化的， PostgreSQL采用这样的信条来减少不必要的中止。

​	**<u>与两阶段加锁相比，可串行化快照隔离的一大优点是事务不需要等待其他事务所持有的锁</u>**。**这一点和快照隔离一样，<u>读写通常不会互相阻塞</u>**。这样的设计使得査询延迟更加稳定、可预测。<u>特别是，在一致性快照上执行**只读查询不需要任何锁**，这对于读密集的负载非常有吸引力</u>。

​	<u>与串行执行相比，**可串行化快照隔离可以突破单个CPU核的限制**</u>。 FoundationDB将冲突检测分布在多台机器上，从而提髙总体吞吐量。即使数据可能跨多台机器进行分区，事务也可以在多个分区上读、写数据并保证可串行化隔离。

​	<u>**需要指出，事务中止的比例会显著影响SSI的性能表现**</u>。例如，一个运行很长时间的事务，读取和写入了大量数据，因而产生冲突并中止的概率就会增大，所以**<u>SSI要求读-写型事务要简短(而长时间执行的只读事务则没有此限制)</u>**。但**总体讲，相比于<u>两阶段加锁</u>与<u>串行执行</u>，<u>SSI</u>更能容忍那些执行缓慢的事务**。

## * 7.4 小结

​	事务作为一个抽象层，使得应用程序可以忽略数据库内部一些复杂的并发问题，以及某些硬件、软件故障，从而简化应用层的处理逻辑，大量的错误可以转化为简单的事务中止和应用层重试。

​	本章，我们例举了很多事务能够预防的问题。尽管并非所有的应用程序都会面临这些可题，例如那些简单的访问模式，只读或者只写，可能根本无需事务的帮助。但对于复杂的访问模式，事务可以大大简化需要考虑的潜在错误情况。

​	如果没有事务，各种错误情况(如进程崩溃、网络中断、停电、磁盘已满、并发竞争等)会导致数据可能出现各种不一致。例如，反规格化数据模式和相关操作会导致与源数据不同步。假如没有事务，处理那些复杂交互访问最后所导致的数据库混乱就会异常痛苦。

​	本章，我们深入探讨了并发控制这一主题。介绍了多个广泛使用的隔离级别，特别是**读-提交**，**快照隔离(或可重复读取)**与**可串行化**。通过分析如何处理边界条件来阐述这些隔离级别的要点：

+ 脏读

  客户端读到了其他客户端尚未提交的写入。读-提交以及更强的隔离级别可以防止脏读。

+ **脏写**

  **客户端覆盖了另一个客户端尚未提交的写入**。<u>几乎所有的数据库实现都可以防止脏写</u>。

+ **读倾斜(不可重复读)**

  客户在不同的时间点看到了不同值。快照隔离是最用的防范手段，即事务总是在某个时间点的一致性快照中读取数据。通常采用多版本并发控制(MVCC)来实现快照隔离。

+ 更新丢失

  两个客户端同时执行读-修改-写入操作序列，出现了**其中一个覆盖了另一个的写入**，但又没有包含对方最新值的情况，最终导致了部分修改数据发生了丢失。<u>快照隔离的一些实现可以自动防止这种异常，而另一些则需要手动锁定查询结果(SELECT FOR UPDATE)</u>。

+ **写倾斜**

  **事务首先査询数据，根据返回的结果而作出某些决定，然后修改数据库。<u>当事务提交时，支持决定的前提条件已不再成立</u>。<u>只有可串行化的隔离才能防止这种异常</u>**。

+ **幻读**

  **事务读取了某些符合査询条件的对象，同时另一个客户端执行写入，改变了先前的査询结果**。<u>**快照隔离可以防止简单的幻读，但写倾斜情况则需要特殊处理，例如采用区间范围锁**</u>。

​	<u>**弱隔离级别**可以防止上面的某些异常，但还需要应用开发人员手动处理其他复杂情况(例如，显式加锁)。只有**可串行化**的隔离可以防止所有这些问题</u>。我们主要讨论了实现可串行化隔离的三种不同方法：

+ 严格串行执行事务

  如果毎个事务的执行速度非常快，且单个CPU核可以满足事务的吞吐量要求，严格串行执行是一个非常简单有效的方案。

+ **两阶段加锁**

  **几十年来，这一直是实现可串行化的标准方式，但还是有很多系统出于性能原因而放弃使用它**。

+ **可串行化的快照隔离(SSⅠ)**

  **一种最新的算法，可以避免前面方法的大部分缺点。它秉持乐观预期的原则，允许多个事务并发执行而不互相阻塞；仅当事务尝试提交时，才检查可能的冲突，如果发现违背了串行化，则某些事务会被中止**。

​	本章中的示例都采用关系数据模型。但是，正如本章前面的“多对象事务的需求”所描述的，**无论对哪种数据模型，事务都是非常有用的数据库功能**。

​	本章所介绍的算法、方案主要针对单节点。对于分布式数据库，还会面临更多、更复杂的挑战，我们将在接下来的两章中继续展开讨论。

# 第8章 分布式系统的挑战

P258
