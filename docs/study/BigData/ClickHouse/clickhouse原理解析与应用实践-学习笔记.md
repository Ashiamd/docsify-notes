# clickhouse原理解析与应用实践-学习笔记

# 1. ClickHouse的前世今生

## 1.1 传统BI系统之殇

> + [ERP系统_百度百科 (baidu.com)](https://baike.baidu.com/item/ERP系统/1569198?fr=aladdin)
>
>   ERP系统是企业资源计划（Enterprise Resource Planning）的简称，是指建立在信息技术基础上，集信息技术与先进管理思想于一身，以系统化的[管理思想](https://baike.baidu.com/item/管理思想/2555826)，为企业员工及决策层提供决策手段的管理平台。
>
> + [客户关系管理（管理学词汇CRM）_百度百科 (baidu.com)](https://baike.baidu.com/item/客户关系管理/254554?fromtitle=CRM&fromid=165070&fr=aladdin)
>
>   客户关系管理是指[企业](https://baike.baidu.com/item/企业/707680)为提高核心竞争力，利用相应的信息技术以及[互联网技术](https://baike.baidu.com/item/互联网技术/617749)协调企业与顾客间在[销售](https://baike.baidu.com/item/销售/239410)、[营销](https://baike.baidu.com/item/营销/150434)和服务上的交互，从而提升其[管理方式](https://baike.baidu.com/item/管理方式/260899)，向客户提供创新式的个性化的客户交互和服务的过程。其最终目标是吸引新客户、保留老客户以及将已有客户转为忠实[客户](https://baike.baidu.com/item/客户/1598984)，增加市场。
>
> + [OLTP_百度百科 (baidu.com)](https://baike.baidu.com/item/OLTP/5019563?fr=aladdin)
>
>   On-Line Transaction Processing[联机事务处理](https://baike.baidu.com/item/联机事务处理)过程(OLTP)，也称为**面向交易的处理过程**，其基本特征是前台接收的用户数据可以立即传送到计算中心进行处理，并在很短的时间内给出处理结果，是对用户操作快速响应的方式之一。
>
> + [数据孤岛_百度百科 (baidu.com)](https://baike.baidu.com/item/数据孤岛/10305414?fr=aladdin)
>
>   数据孤岛在企业信息化中,还有很多类似的描述,如"数据的污染"等比较形象的说法，专业人士把数据孤岛分为物理性和逻辑性两种。**物理性的数据孤岛指的是，数据在不同部门相互独立存储，独立维护，彼此间相互孤立，形成了物理上的孤岛**。
>
> + [数据仓库_百度百科 (baidu.com)](https://baike.baidu.com/item/数据仓库/381916?fr=aladdin)
>
>   数据仓库，英文名称为Data Warehouse，可简写为[DW](https://baike.baidu.com/item/DW/1264123)或DWH。数据仓库，是为[企业](https://baike.baidu.com/item/企业/707680)所有级别的决策制定过程，提供所有类型数据支持的战略[集合](https://baike.baidu.com/item/集合)。它是单个数据存储，出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。
>
> + [商业智能_百度百科 (baidu.com)](https://baike.baidu.com/item/商业智能/406141?fromtitle=BI&fromid=4579902&fr=aladdin)
>
>   商业智能（Business Intelligence，简称：BI），又称商业智慧或商务智能，指用现代[数据仓库技术](https://baike.baidu.com/item/数据仓库技术/10292797)、线上分析处理技术、数据挖掘和数据展现技术进行数据分析以实现商业价值。
>
>   商业智能的概念在1996年最早由[加特纳](https://baike.baidu.com/item/加特纳)集团（Gartner Group）提出，加特纳集团将商业智能定义为：商业智能描述了一系列的概念和方法，通过应用基于事实的支持系统来辅助商业决策的制定。商业智能技术提供使企业迅速分析数据的技术和方法，包括收集、管理和分析数据，将这些数据转化为有用的信息，然后分发到企业各处。
>
> + [联机分析处理_百度百科 (baidu.com)](https://baike.baidu.com/item/联机分析处理?fromtitle=OLAP&fromid=1049009)
>
>   联机分析处理OLAP是一种[软件技术](https://baike.baidu.com/item/软件技术/3313113)，它使分析人员能够迅速、一致、交互地从各个方面观察信息，以达到深入理解数据的目的。它具有FASMI(Fast Analysis of Shared Multidimensional Information)，即共享多维信息的快速分析的特征。其中F是快速性(Fast)，指系统能在数秒内对用户的多数分析要求做出反应；A是可分析性(Analysis)，指用户无需编程就可以定义新的专门计算，将其作为分析的一部 分，并以用户所希望的方式给出报告；M是多维性(Multi—dimensional)，指提供对[数据分析](https://baike.baidu.com/item/数据分析/6577123)的多维视图和分析；I是信息性(Information)，指能及时获得信息，并且管理大容量信息。

​	诸如ERP、CRM这类系统，可以看作<u>线下工作线上化</u>的过程，这也是IT时代的主要特征之一，通常我们把这类系统称为**联机事务处理（OLTP）**系统。

​	为了解决**数据孤岛**的问题，人们提出了**数据仓库**的概念。即通过引入一个专门用于分析类场景的数据库，将分散的数据统一汇聚到一处。借助数据仓库的概念，用户第一次拥有了站在企业全局鸟瞰一切数据的视角。

​	随着这个概念被进一步完善，<u>一类统一面向数据仓库，专注于提供数据分析、决策类功能的系统</u>与解决方案应运而生。最终于20世纪90年代，有人第一次提出了**BI（商业智能）**系统的概念。<u>自此以后，人们通常用BI一词指代这类分析系统</u>。相对于联机事务处理系统，我们把这类BI系统称为**联机分析（OLAP）系统**。

​	**传统BI系统**的设计初衷虽然很好，但实际的应用效果却不能完全令人满意。

+ 传统BI系统<u>对企业的信息化水平要求较高</u>。按照传统BI系统的设计思路，通常只有中大型企业才有能力实施。因为它的定位是站在企业视角通盘分析并辅助决策的，所以如果企业的信息化水平不高，基础设施不完善，想要实施BI系统根本无从谈起。这已然把许多潜在用户挡在了门外。
+ 狭小的受众制约了传统BI系统发展的生命力。传统BI系统的<u>主要受众是企业中的管理层或决策层</u>。这类用户虽然通常具有较高的话语权和决策权，但用户基数相对较小。同时他们对系统的参与度和使用度不高，久而久之这类所谓的BI系统就沦为了领导视察、演示的“特供系统”了。

+ <u>冗长的研发过程</u>滞后了需求的响应时效。传统BI系统需要大量IT人员的参与，用户的任何想法，哪怕小到只是想把一张用饼图展示的页面换成柱状图展示，可能都需要依靠IT人员来实现。一个分析需求从由用户大脑中产生到最终实现，可能需要几周甚至几个月的时间。这种严重的滞后性仿佛将人类带回到了飞鸽传书的古代。

## 1.2 现代BI系统的新思潮

> + [saas平台_百度百科 (baidu.com)](https://baike.baidu.com/item/saas平台/2147529?fr=aladdin)	=>	软件服务化(Software as a Service)
>
>   [SaaS](https://baike.baidu.com/item/SaaS)平台是运营saas软件的平台。SaaS提供商为企业搭建信息化所需要的所有[网络基础设施](https://baike.baidu.com/item/网络基础设施/5183560)及软件、硬件运作平台，并负责所有前期的实施、后期的维护等一系列服务，企业无需购买软硬件、建设[机房](https://baike.baidu.com/item/机房/5066792)、招聘IT人员，即可通过[互联网](https://baike.baidu.com/item/互联网/199186)使用信息系统。SaaS 是一种软件布局模型，其应用专为[网络](https://baike.baidu.com/item/网络/143243)交付而设计，便于用户通过互联网托管、部署及接入。
>
> + [数据立方_百度百科 (baidu.com)](https://baike.baidu.com/item/数据立方/13237096?fromtitle=数据立方体&fromid=9851963&fr=aladdin)
>
>   我们以B+树的结构建立了字段的索引，每个B+树结构的字段索引相当于一个数据平面，这样一个全局数据表与其多个重要字段的索引就组成了一个类似于立方体的数据组织结构，我们称之为“ 数据立方(DataCube)”。

​	SaaS模式的兴起，为传统企业软件系统的商业模式带来了新的思路，这是一次新的技术普惠。一方面，<u>SaaS模式将之前只服务于中大型企业的软件系统放到了互联网，扩展了它的受众；另一方面，由于互联网用户的基本特征和软件诉求，又倒逼了这些软件系统在方方面面进行革新与升级</u>。

​	技术普惠，导致现代BI系统在设计思路上发生了天翻地覆的变化。

+ 变轻：无需强制捆绑企业级数据仓库，仅Excel也可进行数据分析
+ 多元化：简单图形界面操作，无需IT专业人士操作
+ 用户体验更好：虽然仍私有化部署在企业内部，但是应答更快、操作更简单。

​	如果说SaaS化这波技术普惠为现代BI系统带来了新的思路与契机，那么背后的技术创新则保障了其思想的落地。**在传统BI系统的体系中，背后是传统的关系型数据库技术（OLTP数据库）**。

​	为了能够解决海量数据下分析查询的性能问题，人们绞尽脑汁，在数据仓库的基础上衍生出众多概念，例如：对数据进行分层，通过层层递进形成数据集市，从而减少最终查询的数据体量；提出**数据立方体**的概念，通过对数据进行预先处理，以空间换时间，提升查询性能。然而无论如何努力，设计的局限始终是无法突破的瓶颈。

​	<u>OLTP技术由诞生的那一刻起就注定不是为数据分析而生的</u>，于是很多人将目光投向了新的方向。

​	2003年起，Google陆续发表的三篇论文开启了大数据的技术普惠，Hadoop生态由此开始一发不可收拾，数据分析开启了新纪元。<u>从某种角度来看，以使用Hadoop生态为代表的这类非传统关系型数据库技术所实现的BI系统，可以称为现代BI系统</u>。换装了大马力发动机的现代BI系统在面对海量数据分析的场景时，显得更加游刃有余。然而Hadoop技术也不是银弹，在现代BI系统的构建中仍然面临诸多挑战。在海量数据下要实现多维分析的实时应答，仍旧困难重重。（现代BI系统的典型应用场景是**多维分析**，某些时候可以直接使用OLAP指代这类场景。）

## 1.3 OLAP常见架构分类

> [ClickHouse原理解析与应用实践-朱凯-微信读书 (qq.com)](https://weread.qq.com/web/reader/03532e3071e24e90035375dk6f4322302126f4922f45dec)	=>	图文来源

​	OLAP名为联机分析，又可以称为多维分析，是由关系型数据库之父埃德加·科德（Edgar Frank Codd）于1993年提出的概念。

​	为了更好地理解多维分析的概念，可以使用一个立方体的图像具象化操作。如下图所示，对于一张销售明细表，数据立方体可以进行如下操作。

+ 下钻：从高层次向低层次明细数据穿透。例如从“省”下钻到“市”，从“湖北省”穿透到“武汉”和“宜昌”。

+ 上卷：和下钻相反，从低层次向高层次汇聚。例如从“市”汇聚成“省”，将“武汉”“宜昌”汇聚成“湖北”。

+ 切片：观察立方体的一层，将一个或多个维度设为单个固定值，然后观察剩余的维度，例如将商品维度固定为“足球”。

+ 切块：与切片类似，只是将单个固定值变成多个值。例如将商品维度固定成“足球”“篮球”和“乒乓球”。

+ 旋转：旋转立方体的一面，如果要将数据映射到一张二维表，那么就要进行旋转，这就等同于行列置换。

![img](https://res.weread.qq.com/wrepub/epub_31608464_2)

为了实现上述这些操作，将常见的OLAP架构大致分成三类（ROLAP、MOLAP、HOLAP）。

+ ROLAP（Relational OLAP，关系型OLAP）

  顾名思义，它直接使用关系模型构建，数据模型常使用星型模型或者雪花模型。这是最先能够想到，也是最为直接的实现方法。因为<u>OLAP概念在最初提出的时候，就是建立在关系型数据库之上的</u>。**多维分析的操作，可以直接转换成SQL查询**。例如，通过上卷操作查看省份的销售额，就可以转换成类似下面的SQL语句：

  `SELECT SUM（价格）FROM 销售数据表 GROUP BY 省`

+ MOLAP（Multidimensional OLAP，多维型OLAP）

  它的出现是为了**缓解ROLAP性能问题**。MOLAP使用多维数组的形式保存数据，其核心思想是借助预先聚合结果，使用空间换取时间的形式最终提升查询性能。也就是说，用更多的存储空间换得查询时间的减少。**其具体的实现方式是依托立方体模型的概念**。

  **首先，对需要分析的数据进行建模，框定需要分析的维度字段；然后，通过预处理的形式，对各种维度进行组合并事先聚合；最后，将聚合结果以某种索引或者缓存的形式保存起来（通常只保留聚合后的结果，不存储明细数据）**。这样一来，在随后的查询过程中，就可以直接利用结果返回数据。

  但是这种架构也并不完美。**维度预处理可能会导致数据的膨胀**。

  这里可以做一次简单的计算，以图1-1中所示的销售明细表为例。如果数据立方体包含了5个维度（字段），那么维度组合的方式则有2<sup>5</sup> （2<sup>n</sup> ，n=维度个数）个。例如，省和市两个维度的组合就有<湖北，武汉>、<湖北、宜昌>、<武汉、湖北>、<宜昌、湖北>等。可想而知，当维度数据基数较高的时候，（高基数意味着重复相同的数据少。）其立方体预聚合后的数据量可能会达到10到20倍的膨胀。一张千万级别的数据表，就有可能膨胀到亿级别的体量。人们意识到这个问题之后，虽然也实现了一些能够降低膨胀率的优化手段，但并不能完全避免。

  另外，由于**使用了预处理的形式，数据立方体会有一定的滞后性，不能实时进行数据分析**。而且，**立方体只保留了聚合后的结果数据，导致无法查询明细数据**。

+ HOLAP（Hybrid OLAP，混合架构的OLAP）

  这种思路可以理解成ROLAP和MOLAP两者的集成。这里不再展开，我们重点关注ROLAP和MOLAP。

## 1.4 OLAP实现技术的演进

​	在介绍了OLAP几种主要的架构之后，再来看看它们背后技术的演进过程。我把这个演进过程简单划分成两个阶段。

1. 传统关系型数据库阶段

   在这个阶段中，OLAP主要基于以Oracle、MySQL为代表的一众关系型数据实现。

   + **在ROLAP架构下，直接使用这些数据库作为存储与计算的载体**；
   + **在MOLAP架构下，则借助物化视图的形式实现数据立方体**。

   在这个时期，不论是ROLAP还是MOLAP，在数据体量大、维度数目多的情况下都存在严重的性能问题，甚至存在根本查询不出结果的情况。

2. 大数据技术阶段

   由于大数据处理技术的普及，人们开始使用大数据技术重构ROLAP和MOLAP。

   + 以ROLAP架构为例，传统关系型数据库就被Hive和SparkSQL这类新兴技术所取代。虽然，以Spark为代表的分布式计算系统，相比Oracle这类传统数据库而言，在面向海量数据的处理性能方面已经优秀很多，但是直接把它们作为面向终端用户的在线查询系统还是太慢了。我们的用户普遍缺乏耐心，如果一个查询响应需要<u>几十秒甚至数分钟</u>才能返回，那这套方案就完全行不通。
   + 再看MOLAP架构，MOLAP背后也转为依托MapReduce或Spark这类新兴技术，将其作为立方体的计算引擎，加速立方体的构建过程。其预聚合结果的存储载体也转向HBase这类高性能分布式数据库。大数据技术阶段，主流MOLAP架构已经能够在亿万级数据的体量下，实现<u>毫秒级</u>的查询响应时间。尽管如此，**MOLAP架构依然存在维度爆炸、数据同步实时性不高的问题**。

​	不难发现，虽然OLAP在经历了大数据技术的洗礼之后，其各方面性能已经有了脱胎换骨式的改观，但不论是ROLAP还是MOLAP，仍然存在各自的痛点。

​	如果单纯从模型角度考虑，很明显ROLAP架构更胜一筹。因为关系模型拥有最好的“群众基础”，也更简单且容易理解。它直接面向明细数据查询，由于不需要预处理，也就自然没有预处理带来的负面影响（维度组合爆炸、数据实时性、更新问题）。那是否存在这样一种技术，它既使用ROLAP模型，同时又拥有比肩MOLAP的性能呢？

## 1.5 一匹横空出世的黑马

​	上文曾提及，以Spark为代表的新一代ROLAP方案虽然可以一站式处理海量数据，但<u>无法真正做到实时应答和高并发</u>，它更适合作为一个后端的查询系统。而<u>新一代的MOLAP方案虽然解决了大部分查询性能的瓶颈问题，能够做到实时应答，但数据膨胀和预处理等问题依然没有被很好解决</u>。除了上述两类方案之外，也有一种另辟蹊径的选择，即摒弃ROLAP和MOALP转而<u>使用搜索引擎来实现OLAP查询，ElasticSearch是这类方案的代表</u>。<u>ElasticSearch支持实时更新，在**百万级**别数据的场景下可以做到实时聚合查询</u>，但是随着数据体量的继续增大，它的查询性能也将捉襟见肘。

​	难道真的是鱼与熊掌不可兼得了吗？直到有一天，在查阅一份Spark性能报告的时候，我不经意间看到了一篇性能对比的博文。Spark的对手是一个我从来没有见过的陌生名字，在10亿条测试数据的体量下，Spark这个我心目中的绝对王者，居然被对手打得落花流水，查询响应时间竟然比对手慢数90%之多。而对手居然只使用了一台配有i5 CPU、16GB内存和SSD磁盘的普通PC电脑。我揉了揉眼睛，定了定神，这不是做梦。ClickHouse就这样进入了我的视野。

### 1.5.1 天下武功唯快不破

​	我对ClickHouse的最初印象极为深刻，其具有**ROLAP**、**在线实时查询**、**完整的DBMS**、**列式存储**、**不需要任何数据预处理**、**支持批量更新**、**拥有非常完善的SQL支持和函数**、**支持高可用**、**不依赖Hadoop复杂生态**、**开箱即用**等许多特点。特别是它那夸张的查询性能，我想大多数刚接触ClickHouse的人也一定会因为它的性能指标而动容。在一系列官方公布的基准测试对比中，ClickHouse都遥遥领先对手，这其中不乏一些我们耳熟能详的名字。

​	所有用于对比的数据库都使用了相同配置的服务器，在单个节点的情况下，对一张拥有133个字段的数据表分别在1000万、1亿和10亿三种数据体量下执行基准测试，基准测试的范围涵盖43项SQL查询。在1亿数据集体量的情况下，ClickHouse的平均响应速度是Vertica的2.63倍、InfiniDB的17倍、MonetDB的27倍、Hive的126倍、MySQL的429倍以及Greenplum的10倍。详细的测试结果可以查阅[Performance comparison of database management systems (clickhouse.tech)](https://clickhouse.tech/benchmark/dbms/)。

### 1.5.2 社区活跃

​	基本保持着每个月发布一次版本的更新频率。

## 1.6 ClickHouse的发展历程

### 1.6.1 顺理成章的MySQL时期

​	作为一款在线流量分析产品，对其功能的要求自然是分析流量了。早期的Yandex.Metrica以提供固定报表的形式帮助用户进行分析，例如分析访问者使用的设备、访问者来源的分布之类。其实这也是早期分析类产品的典型特征之一，分析维度和场景是固定的，新的分析需求往往需要IT人员参与。

​	<u>从技术角度来看，当时还处于关系型数据库称霸的时期，所以Yandex在内部其他产品中使用了MySQL数据库作为统计信息系统的底层存储软件</u>。Yandex.Metrica的第一版架构顺理成章延续了这套内部稳定成熟的MySQL方案，并将其作为它的数据存储和分析引擎的解决方案。

​	因为Yandex内部的这套MySQL方案<u>使用了MyISAM表引擎</u>，所以Yandex.Metrica也延续了表引擎的选择。这类分析场景更关注数据写入和查询的性能，不关心事务操作（MyISAM表引擎不支持事务特性）。<u>相比InnoDB表引擎，MyISAM表引擎在分析场景中具有更好的性能</u>。

​	**业内有一个常识性的认知，按顺序存储的数据会拥有更高的查询性能**。<u>因为读取顺序文件会用更少的磁盘寻道和旋转延迟时间（这里主要指机械磁盘），同时顺序读取也能利用操作系统层面文件缓存的预读功能，所以数据库的查询性能与数据在物理磁盘上的存储顺序息息相关。然而这套MySQL方案无法做到顺序存储</u>。

​	MyISAM表引擎使用B+树结构存储索引，而数据则使用另外单独的存储文件（InnoDB表引擎使用B+树同时存储索引和数据，数据直接挂载在叶子节点中）。如果只考虑单线程的写入场景，并且在写入过程中不涉及数据删除或者更新操作，那么数据会依次按照写入的顺序被写入文件并落至磁盘。然而现实的场景不可能如此简单。

​	流量的数据采集链路是这样的：<u>网站端的应用程序首先通过Yandex提供的站点SDK实时采集数据并发送到远端的接收系统，再由接收系统将数据写入MySQL集群</u>。整个过程都是实时进行的，并且数据接收系统是一个**分布式**系统，所以它们会**并行、随机**将数据写入MySQL集群。这最终导致了数据在磁盘中是完全随机存储的，并且会**产生大量的磁盘碎片**。

​	市面上一块典型的7200转SATA磁盘的IOPS（每秒能处理的请求数）仅为100左右，也就是说每秒只能执行100次随机读取。假设一次随机读取返回10行数据，那么查询100000行记录则需要至少100秒，这种响应时间显然是不可接受的。

​	**RAID可以提高磁盘IOPS性能，但并不能解决根本问题。SSD随机读取性能很高，但是考虑到硬件成本和集群规模，不可能全部采取SSD存储**。

​	随着时间的推移，MySQL中的数据越来越多（截至2011年，存储的数据超过5800亿行）。虽然Yandex又额外做了许多优化，成功地将90%的分析报告控制在26秒内返回，但是这套技术方案越来越显得力不从心。

### 1.6.2 * 另辟蹊径的Metrage时期

> + [LSM 算法的原理是什么？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/19887265)
>
> + [Log Structured Merge Trees(LSM) 原理 - LSM - 软件开发 - 深度开源 (open-open.com)](https://www.open-open.com/lib/view/open1424916275249.html)
>
>   + LSM比传统B+树、ISAM有更好的写操作吞吐量（通过消去随机的本地更新操作来实现该目标）
>
>   + 传统注重写性能的方法，即**文件存储**（顺序写代替随机写：kafka等），但是读数据前需要维护**倒排索引**来加速读过程
>
>   + 满足复杂读场景（性能好）的传统方案：
>
>     + 二分查找: 将文件数据有序保存，使用二分查找来完成特定key的查找。
>     + 哈希：用哈希将数据分割为不同的bucket
>     + B+树：使用B+树 或者 ISAM 等方法，可以减少外部文件的读取
>     + 外部文件： 将数据保存为日志，并创建一个hash或者查找树映射相应的文件
>
>     上诉方案能够提高读性能，但是丢失了文件系统的写性能。
>
>     并且维护Hash、B+树时，时常需要更新文件系统中的特定部分（随机读写），这类操作较慢，也是我们需要尽量减少的。
>
>   所以这就是 LSM 被发明的原理， <u>LSM 使用一种不同于上述四种的方法，保持了日志文件写性能，以及微小的读操作性能损失。**本质上就是让所有的操作顺序化，而不是像散弹枪一样随机读写**</u>。
>
>   + 其他具体的描述看原文.....
>
>   + 关于LSM的思考：
>
>     + LSM有更好的写性能，同时LSM还有其它一些好处。 
>
>       **sstable文件是不可修改的，这让对他们的锁操作非常简单**。一般来说，唯一的竞争资源就是 memtable，相对来说需要相对复杂的锁机制来管理在不同的级别。
>
>     + 所以最后的问题很可能是**以写为导向的压力预期**如何。
>
>       如果你对LSM带来的写性能的提高很敏感，这将会很重要。大型互联网企业似乎很看中这个问题。 <u>Yahoo 提出因为事件日志的增加和手机数据的增加，工作场景为从 read-heavy 到 read-write。许多传统数据库产品似乎更青睐读优化文件结构。</u>
>
>     + 因为可用的内存的增加，通过操作系统提供的大文件缓存，读操作自然会被优化。
>
>       **写性能（内存不可提高）因此变成了主要的关注点**，所以采取其它的方法，<u>硬件提升为读性能做的更多，相对于写来说。因此选择一个**写优化的文件结构**很有意义</u>。
>
>     理所当然的，LSM的实现，像LevelDB和Cassandra提供了更好的写性能，相对于单树结构的策略。
>
>   + 上述文章内的**总结**（原文自带的总结，非原创）：
>
>     ​	所以， LSM 是日志和传统的单文件索引（B+ tree，Hash Index）的中立，他提供一个机制来管理更小的独立的索引文件(sstable)。
>
>     ​	**通过管理一组索引文件而不是单一的索引文件，LSM 将B+树等结构昂贵的随机IO变的更快，而代价就是读操作要处理大量的索引文件(sstable)而不是一个，另外还是一些IO被合并操作消耗。**
>
>     ​	如果还有不明白的，这还有一些其它的好的介绍。 [here](https://www.open-open.com/misc/goto?guid=4959627134870208906) and [here](https://www.open-open.com/misc/goto?guid=4958194564427775209)
>
> + [LSM、B 树、B+树、B*对比 - 简书 (jianshu.com)](https://www.jianshu.com/p/3fb899684392) => 图文并茂，推荐阅读（<= 下面文字来自原文）
>
>   ​	LSM树（Log-Structured MergeTree）存储引擎和B+树存储引擎一样，同样支持增、删、读、改、顺序扫描操作。而且通过批量存储技术规避磁盘随机写入问题。当然凡事有利有弊，LSM树和B+树相比，LSM树牺牲了部分读性能，用来大幅提高写性能。
>
>   + LSM树核心思想的核心就是放弃部分读能力，换取写入的最大化能力。LSM Tree ，这个概念就是结构化合并树的意思，它的核心思路其实非常简单，就是假定内存足够大，因此不需要每次有数据更新就必须将数据写入到磁盘中，而可以先将最新的数据驻留在内存中，等到积累到足够多之后，再使用归并排序的方式将内存内的数据合并追加到磁盘队尾(因为所有待排序的树都是有序的，可以通过合并排序的方式快速合并到一起)。
>
>   + 日志结构的合并树（LSM-tree）是一种基于硬盘的数据结构，与B+tree相比，能显著地减少硬盘磁盘臂的开销，并能在较长的时间提供对文件的高速插入（删除）。**然而LSM-tree在某些情况下，特别是在查询需要快速响应时性能不佳。**通常LSM-tree适用于索引插入比检索更频繁的应用系统。
>
>   + LSM树和B+树的差异主要在于读性能和写性能进行权衡。在牺牲的同时寻找其余补救方案：
>
>     1. LSM具有批量特性，存储延迟。当写读比例很大的时候（写比读多），LSM树相比于B树有更好的性能。因为**随着insert操作，为了维护B+树结构，节点分裂。读磁盘的随机读写概率会变大，性能会逐渐减弱**。
>
>     2. B树的写入过程：对B树的写入过程是一次原位写入的过程，主要分为两个部分，首先是查找到对应的块的位置，然后将新数据写入到刚才查找到的数据块中，然后再查找到块所对应的磁盘物理位置，将数据写入去。当然，在内存比较充足的时候，因为B树的一部分可以被缓存在内存中，所以查找块的过程有一定概率可以在内存内完成，不过为了表述清晰，我们就假定内存很小，只够存一个B树块大小的数据吧。可以看到，在上面的模式中，需要两次随机寻道（一次查找，一次原位写），才能够完成一次数据的写入，代价还是很高的。
>
>     3. LSM优化方式：
>
>        a. **Bloom filter**: 就是个带随机概率的bitmap,可以快速的告诉你，某一个小的有序结构里有没有指定的那个数据的。于是就可以不用二分查找，而只需简单的计算几次就能知道数据是否在某个小集合里啦。效率得到了提升，但付出的是空间代价。
>         b. compact:小树合并为大树:因为小树性能有问题，所以要有个进程不断地将小树合并到大树上，这样大部分的老数据查询也可以直接使用log2N的方式找到，不需要再进行(N/m)*log2n的查询了

​	由于MySQL带来的局限性，Yandex自研了一套全新的系统并命名为Metrage。Metrage在设计上与MySQL完全不同，它选择了另外一条截然不同的道路。

+ 首先，**在数据模型层面，它使用Key-Value模型（键值对）**代替了关系模型；
+ 其次，**在索引层面，它使用LSM树代替了B+树**；
+ 最后，**在数据处理层面，由实时查询的方式改为了预处理的方式**。 

​	LSM树也是一种非常流行的索引结构，发源于Google的BigTable，**现在最具代表性的使用LSM树索引结构的系统是HBase**。

​	LSM本质上可以看作将原本的一棵大树拆成了许多棵小树，每一批次写入的数据都会经历如下过程。

+ 首先，会在内存中构建出一棵小树，构建完毕即算写入成功（这里会通过**预写日志**的形式，防止因内存故障而导致的数据丢失）。写入动作只发生在内存中，不涉及磁盘操作，所以极大地提升了数据写入性能。
+ 其次，**小树在构建的过程中会进行排序**，这样就保证了数据的有序性。
+ 最后，当内存中小树的数量达到某个阈值时，就会借助后台线程将小树刷入磁盘并生成一个小的数据段。**在每个数据段中，数据局部有序**。也正因为数据有序，所以能够进一步使用稀疏索引来优化查询性能。

​	借助LSM树索引，可使得Metrage引擎在软硬件层面同时得到优化（**磁盘顺序读取、预读缓存、稀疏索引**等），最终有效提高系统的综合性能。

​	如果仅拥有索引结构的优化，还不足以从根本上解决性能问题。Metrage设计的第二个重大转变是通过**预处理**的方式，<u>将需要分析的数据预先聚合</u>。这种做法类似数据立方体的思想，

+ 首先对分析的具体场景实施立方体建模，框定所需的维度和度量以形成数据立方体；
+ 接着预先计算立方体内的所有维度组合；
+ 最后将聚合的结果数据按照Key-Value的形式存储。

​	这样一来，对于固定分析场景，就可以直接利用数据立方体的聚合结果立即返回相关数据。这套系统的实现思路和现今的一些MOLAP系统如出一辙。

​	通过上述一系列的转变，Metrage为Yandex.Metrica的性能带来了革命性提升。截至2015年，在Metrage内存储了超过3万亿行的数据，其集群规模超过了60台服务器，查询性能也由先前的26秒降低到了惊人的1秒以内。

​	**然而，使用立方体这类预先聚合的思路会带来一个新的问题，那就是维度组合爆炸，因为需要预先对所有的维度组合进行计算**。

​	那么**维度组合的方式具体有多少种呢？它的计算公式是2<sup>N</sup> （N=维度数量）**。

​	可以做一次简单的计算，例如5个维度的组合方式会有2<sup>5</sup></sup> =32种，而9个维度的组合方式则会多达2<sup>9</sup> =512种，这是一种指数级的增长方式。**维度组合的爆炸会直接导致数据膨胀，有时候这种膨胀可能会多达10～20倍**。

### 1.6.3 * 自我突破的OLAPServer时期

​	<small>如果说Metrage系统是弥补Yandex.Metrica性能瓶颈的产物，那么OLAPServer系统的诞生，则是产品形态升级倒逼的结果。在<u>Yandex.Metrica的产品初期，它只支持**固定报表的分析功能**</u>。随着时间的推移，这种固定化的分析形式早已不能满足用户的诉求，于是Yandex.Metrica计划推出自定义分析报告的功能。然而Metrage系统却无法满足这类自定义的分析场景，因为它需要预先聚合，并且只提供了内置的40多种固定分析场景。单独为每一个用户提供面向个人的预聚合功能显然是不切实际的。<u>在这种背景下，Yandex.Metrica的研发团队只有寻求自我突破，于是自主研发了OLAPServer系统</u>。</small>

​	OLAPServer系统被设计成**专门处理自定义报告**这类临时性分析需求的系统，与Metrage系统形成互补的关系。

​	结合之前两个阶段的建设经验，OLAPServer在设计思路上可以说是取众家之长。

+ 在**数据模型**方面，它又换回了**关系模型**，因为相比Key-Value模型，关系模型拥有更好的描述能力。使用SQL作为查询语言，也将会拥有更好的“群众基础”。
+ 而在存储结构和索引方面，它结合了MyISAM和LSM树最精华的部分。
  + 在**存储结构**上，它与MyISAM表引擎类似，**分为了索引文件和数据文件两个部分**。
  + 在**索引**方面，它并没有完全沿用LSM树，而是**使用了LSM树所使用到的稀疏索引**。
  + 在**数据文件的设计**上，则**沿用了LSM树中数据段的思想，即数据段内数据有序，借助稀疏索引定位数据段**。
  + 在有了上述基础之后，OLAPServer又进一步引入了**列式存储**的思想，**将索引文件和数据文件按照列字段的粒度进行了拆分，每个列字段各自独立存储，以此进一步减少数据读取的范围**。

​	虽然OLAPServer在实时聚合方面的性能相比MySQL有了质的飞跃，但从功能的完备性角度来看，OLAPServer还是差了一个量级。

​	**如果说MySQL可以称为数据库管理系统（DBMS），那么OLAPServer只能称为数据库**。

​	因为OLAPServer的定位只是和Metrage形成互补，所以它缺失了一些基本的功能。例如，**它只有一种数据类型，即固定长度的数值类型，且没有DBMS应有的基本管理功能（DDL查询等）**。

### 1.6.4 * 水到渠成的ClickHouse时代

​	现在，一个新的选择题摆在了Yandex.Metrica研发团队的面前，**实时聚合还是预先聚合**？

​	预先聚合方案在查询性能方面带来了质的提升，成功地将之前的报告查询时间从26秒降低到了1秒以内，但同时它也带来了新的难题。

1. 由于预先聚合**只能支持固定的分析场景**，所以它无法满足自定义分析的需求。
2. **维度组合爆炸会导致数据膨胀**，这样会造成不必要的计算和存储开销。因为用户并不一定会用到所有维度的组合，那些没有被用到的组合将会成为浪费的开销。

3. **流量数据是在线实时接收的，所以预聚合还需要考虑如何及时更新数据**。

​	经过这么一分析，预先聚合的方案看起来似乎也没有那么完美。这是否表示实时聚合的方案更优呢？实时聚合方案意味着一切查询都是动态、实时的，从用户发起查询的那一刻起，整个过程需要在一秒内完成并返回，而在这个查询过程的背后，可能会涉及数亿行数据的处理。如果做不到这么快的响应速度，那么这套方案就不可行，因为用户都讨厌等待。很显然，如果查询性可以得到保障，实时聚合会是一个更为简洁的架构。由于OLAPServer的成功使用经验，选择倾向于实时聚合这一方。

​	OLAPServer在查询性能方面并不比Metrage差太多，在查询的灵活性方面反而更胜一筹。于是Yandex.Metrica研发团队以OLAPServer为基础进一步完善，以实现一个完备的数据库管理系统（DBMS）为目标，最终打造出了ClickHouse，并于2016年开源。纵览Yandex.Metrica背后技术的发展历程，ClickHouse的出现似乎是一个水到渠成的结果。

​	ClickHouse的发展历程如表1-1所示。

​	表1-1 ClickHouse的发展历程

| 发展历程                 | OLAP架构                      | Yandex.Metrica产品形态 |
| ------------------------ | ----------------------------- | ---------------------- |
| 顺理成章的MySQL时期      | ROLAP                         | 固定报告               |
| 另辟蹊径的Metrage时期    | MOLAP                         | 固定报告               |
| 自我突破的OLAPServer时期 | HOLAP（Metrage + OLAPServer） | 自助报告               |
| 水到渠成的ClickHouse时代 | ROLAP                         | 自助报告               |

## 1.7 ClickHouse的名称含义

​	经过上一节的介绍，大家知道了ClickHouse由雏形发展至今一共经历了四个阶段。

​	它的初始设计目标是服务自己公司的一款名叫Yandex.Metrica的产品。Metrica是一款Web流量分析工具，基于前方探针采集行为数据，然后进行一系列的数据分析，类似数据仓库的OLAP分析。而<u>在采集数据的过程中，一次页面click（点击），会产生一个event（事件）</u>。

​	至此，整个系统的逻辑就十分清晰了，那就是**基于页面的点击事件流，面向数据仓库进行OLAP分析**。所以**ClickHouse的全称是Click Stream，Data WareHouse，简称ClickHouse**。

## 1.8 ClickHouse适用的场景

​	因为ClickHouse在诞生之初是为了服务Yandex自家的Web流量分析产品Yandex.Metrica，所以**在存储数据超过20万亿行的情况下，ClickHouse做到了90%的查询都能够在1秒内返回的惊人之举**。随后，ClickHouse进一步被应用到Yandex内部大大小小数十个其他的分析场景中。可以说ClickHouse具备了人们对一款高性能OLAP数据库的美好向往，所以它基本能够胜任各种数据分析类的场景，并且随着数据体量的增大，它的优势也会变得越为明显。

​	**ClickHouse非常适用于商业智能领域（也就是我们所说的BI领域），除此之外，它也能够被广泛应用于广告流量、Web、App流量、电信、金融、电子商务、信息安全、网络游戏、物联网等众多其他领域**。

## 1.9 ClickHouse不适用的场景

​	ClickHouse作为一款高性能OLAP数据库，虽然足够优秀，但也不是万能的。我们不应该把它用于任何OLTP事务性操作的场景，因为它有以下几点不足。

+ 不支持事务
+ 不擅长根据主键按行粒度进行查询（虽然支持），故不应该把ClickHouse当作Key-Value数据库使用
+ 不擅长按行删除数据（虽然支持）

​	这些弱点并不能视为ClickHouse的缺点，事实上其他同类高性能的OLAP数据库同样也不擅长上述的这些方面。因为对于一款OLAP数据库而言，上述这些能力并不是重点，只能说这是为了极致查询性能所做的权衡。

## 1.10 有谁在使用ClickHouse

​	除了Yandex自己以外，ClickHouse还被众多商业公司或研究组织成功地运用到了它们的生产环境。

+ 欧洲核子研究中心（CERN）将它用于保存强对撞机试验后记录下的数十亿事件的测量数据，并成功将先前查找数据的时间由几个小时缩短到几秒。
+ 著名的CDN服务厂商CloudFlare将ClickHouse用于HTTP的流量分析。
+ 国内的头条、阿里、腾讯和新浪等一众互联网公司对ClickHouse也都有涉猎。

> 更多详情可以参考ClickHouse官网的案例介绍（https://clickhouse.yandex/ ）。

## 1.11 本章小结

​	本章开宗明义，介绍了作为一线从业者的我（本书作者）在经历BI系统从传统转向现代的过程中的所思所想，以及如何在机缘巧合之下发现了令人印象深刻的ClickHouse。本章抽丝剥茧，揭开了ClickHouse诞生的缘由。

​	原来ClickHouse背后的研发团队是来自俄罗斯的Yandex公司，Yandex为了支撑Web流量分析产品Yandex.Metrica，在历经MySQL、Metrage和OLAPServer三种架构之后，集众家之所长，打造出了ClickHouse。

​	在下一章，我们将进一步详细介绍ClickHouse的架构，对它的核心特点进行深入探讨。

# 2. ClickHouse架构概述

> + MPP架构（Massively Parallel Processing）
>
>   + [MPP(大规模并行处理)简介_Mr_不想起床的博客-CSDN博客_mpp架构](https://blog.csdn.net/qq_42189083/article/details/80610092) => 图文并茂，推荐阅读
>
>     1. 低硬件成本：完全使用 x86 架构的 PC Server，不需要昂贵的 Unix 服务器和磁盘阵列；
>     2. **集群架构与部署：完全并行的 MPP + Shared Nothing 的分布式架构，采用 Non-Master 部署，节点对等的扁平结构；**
>     3. 海量数据分布压缩存储：可处理 PB 级别以上的结构化数据，采用 hash分布、random 存储策略进行数据存储；同时采用先进的压缩算法，减少存储数据所需的空间，可以将所用空间减少 1~20 倍，并相应地提高 I/O 性能；
>     4. 数据加载高效性：基于策略的数据加载模式，集群整体加载速度可达2TB/h；
>     5. 高扩展、高可靠：支持集群节点的扩容和缩容，支持全量、增量的备份/恢复;
>     6. 高可用、易维护：数据通过副本提供冗余保护，自动故障探测和管理，自动同步元数据和业务数据。提供图形化工具，以简化管理员对数据库的管理工作；
>     7. 高并发：读写不互斥，支持数据的边加载边查询，单个节点并发能力大于 300 用户；
>     8. 行列混合存储：提供行列混合存储方案，从而提高了列存数据库特殊查询场景的查询响应耗时；
>     9. 标准化：支持SQL92 标准，支持 C API、ODBC、JDBC、ADO.NET 等接口规范。
>
>     综合而言，Hadoop和MPP两种技术的特定和适用场景为：
>
>     + **Hadoop在处理非结构化和半结构化数据上具备优势，尤其适合海量数据批处理等应用要求**
>
>     + **MPP适合替代现有关系数据机构下的大数据处理，具有较高的效率**
>
>     ​	MPP适合多维度数据自助分析、数据集市等；Hadoop适合海量数据存储查询、批量数据ETL、机构化数据分析(日志分析、文本分析)等。
>
>     ​	由上述对比可预见未来大数据存储与处理趋势：MPPDB+Hadoop混搭使用，用MPP处理PB级别的、高质量的结构化数据，同时为应用提供丰富的SQL和事物支持能力；用Hadoop实现半结构化、非结构化数据处理。这样可以同时满足结构化、半结构化和非结构化数据的高效处理需求。
>
>   + [MPP架构是什么？看这一篇就行了。。_feiying-CSDN博客_mpp架构](https://blog.csdn.net/feiyingwang/article/details/100258668) => 硬件、操作系统层面分析，推荐阅读
>
>     + SMP- Symmetric Multi-Processor 对称多处理器结构
>     + NUMA -Non-Uniform Memory Access 非一致存储访问结构
>     + MPP -Massive-Parallel Processing 海量并行处理架构
>
>   + [MPP数据库简介 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/94938447)
>
> + 对等网络
>
>   + [对等网络 - WCF | Microsoft Docs](https://docs.microsoft.com/zh-cn/dotnet/framework/wcf/feature-details/peer-to-peer-networking)
>
>   + [对等网络（对等网络）_百度百科 (baidu.com)](https://baike.baidu.com/item/对等网络/5482934?fr=aladdin)
>
>     ​	在P2P网络环境中，彼此连接的多台计算机之间都处于对等的地位，各台计算机有相同的功能，无主从之分，一台计算机既可作为服务器，设定共享资源供网络中其他计算机所使用，又可以作为工作站，整个网络一般来说不依赖专用的集中服务器，也没有专用的工作站。网络中的每一台计算机既能充当网络服务的请求者，又对其它计算机的请求做出响应，提供资源、服务和内容。通常这些资源和服务包括：信息的共享和交换、计算资源（如CPU计算能力共享）、存储共享（如缓存和磁盘空间的使用）、网络共享、打印机共享等。
>
>     ​	对等网络是一种网络结构的思想。它与目前网络中占据主导地位的客户端/服务器（Client/Server）结构（也就是WWW所采用的结构方式）的一个本质区别是，整个网络结构中不存在中心节点（或中心服务器）。**在P2P结构中，每一个节点（peer）大都同时具有信息消费者、信息提供者和信息通讯等三方面的功能。从计算模式上来说，P2P打破了传统的Client/Server (C/S)模式，在网络中的每个节点的地位都是对等的。每个节点既充当服务器，为其他节点提供服务，同时也享用其他节点提供的服务**。

​	随着业务的迅猛增长，**Yandex.Metrica目前已经成为世界第三大Web流量分析平台，每天处理超过200亿个跟踪事件**。能够拥有如此惊人的体量，在它背后提供支撑的ClickHouse功不可没。**ClickHouse已经为Yandex.Metrica存储了超过20万亿行的数据，90%的自定义查询能够在1秒内返回，其集群规模也超过了400台服务器**。虽然ClickHouse起初只是为了Yandex.Metrica而研发的，但由于它出众的性能，目前也被广泛应用于Yandex内部其他数十个产品上。

​	初识ClickHouse的时候，我曾产生这样的感觉：它仿佛违背了物理定律，没有任何缺点，是一个不真实的存在。一款高性能、高可用OLAP数据库的一切诉求，ClickHouse似乎都能满足，这股神秘的气息引起了我极大的好奇。

​	但是刚从Hadoop生态转向ClickHouse的时候，我曾有诸多的不适应，因为它和我们往常使用的技术“性格”迥然不同。如果把数据库比作汽车，那么ClickHouse俨然就是一辆手动挡的赛车。它在很多方面不像其他系统那样高度自动化。

​	ClickHouse的一些概念也与我们通常的理解有所不同，特别是**在分片和副本方面，有些时候数据的分片甚至需要手动完成**。在进一步深入使用ClickHouse之后，我渐渐地理解了这些设计的目的。**某些看似不够自动化的设计，反过来却在使用中带来了极大的灵活性**。

​	**与Hadoop生态的其他数据库相比，ClickHouse更像一款“传统”MPP架构的数据库，它没有采用Hadoop生态中常用的主从架构，而是使用了多主对等网络结构，同时它也是基于关系模型的ROLAP方案**。

​	这一章就让我们抽丝剥茧，看看ClickHouse都有哪些核心特性。

## 2.1 ClickHouse的核心特性

### 2.1.1 完备的DBMS功能

​	ClickHouse拥有完备的管理功能，所以它称得上是一个DBMS（Database Management System，数据库管理系统），而不仅是一个数据库。作为一个DBMS，它具备了一些基本功能，如下所示。

+ DDL（数据定义语言）：可以动态地创建、修改或删除数据库、表和视图，而无须重启服务。

+ DML（数据操作语言）：可以动态查询、插入、修改或删除数据。

+ 权限控制：可以按照用户粒度设置数据库或者表的操作权限，保障数据的安全性。

+ 数据备份与恢复：提供了数据备份导出与导入恢复机制，满足生产环境的要求。

+ 分布式管理：提供集群模式，能够自动管理多个数据库节点。

​	这里只列举了一些最具代表性的功能，但已然足以表明为什么Click House称得上是DBMS了。

### 2.1.2 列式存储与数据压缩

​	列式存储和数据压缩，对于一款高性能数据库来说是必不可少的特性。一个非常流行的观点认为，如果你想让查询变得更快，最简单且有效的方法是**减少数据扫描范围和数据传输时的大小**，而列式存储和数据压缩就可以帮助我们实现上述两点。**列式存储和数据压缩通常是伴生的，因为一般来说列式存储是数据压缩的前提**。

​	按列存储与按行存储相比，前者可以有效减少查询时所需扫描的数据量，这一点可以用一个示例简单说明。假设一张数据表A拥有50个字段A1～A50，以及100行数据。现在需要查询前5个字段并进行数据分析，则可以用如下SQL实现：

```sql
SELECT A1，A2，A3，A4，A5 FROM A
```

​	如果数据按行存储，数据库首先会逐行扫描，并获取每行数据的所有50个字段，再从每一行数据中返回A1～A5这5个字段。不难发现，尽管只需要前面的5个字段，但由于数据是按行进行组织的，实际上还是扫描了所有的字段。如果数据按列存储，就不会发生这样的问题。由于数据按列组织，数据库可以直接获取A1～A5这5列的数据，从而避免了多余的数据扫描。

​	按列存储相比按行存储的另一个优势是对数据压缩的友好性。同样可以用一个示例简单说明压缩的本质是什么。假设有两个字符串abcdefghi和bcdefghi，现在对它们进行压缩，如下所示：

```shell
压缩前：abcdefghi_bcdefghi
压缩后：abcdefghi_(9,8)
```

​	可以看到，压缩的本质是按照一定步长对数据进行匹配扫描，当发现重复部分的时候就进行编码转换。例如上述示例中的(9,8)，表示如果从下划线开始向前移动9个字节，会匹配到8个字节长度的重复项，即这里的bcdefghi。

​	真实的压缩算法自然比这个示例更为复杂，但压缩的实质就是如此。数据中的重复项越多，则压缩率越高；压缩率越高，则数据体量越小；而数据体量越小，则数据在网络中的传输越快，对网络带宽和磁盘IO的压力也就越小。既然如此，那**怎样的数据最可能具备重复的特性呢？答案是属于同一个列字段的数据，因为它们拥有相同的<u>数据类型</u>和<u>现实语义</u>，重复项的可能性自然就更高**。

​	**ClickHouse就是一款使用列式存储的数据库，数据按列进行组织，属于同一列的数据会被保存在一起，列与列之间也会由不同的文件分别保存（这里主要指MergeTree表引擎，表引擎会在后续章节详细介绍）**。数据默认使用LZ4算法压缩，<u>在Yandex.Metrica的生产环境中，数据总体的压缩比可以达到8:1</u>（未压缩前17PB，压缩后2PB）。**列式存储除了降低IO和存储的压力之外，还为向量化执行做好了铺垫**。

### 2.1.3 * 向量化执行引擎

> + [ClickHouse到底是什么？凭啥这么牛逼！]([ClickHouse到底是什么？凭啥这么牛逼！_芋艿V-CSDN博客](https://blog.csdn.net/github_38592071/article/details/117309063))	=>	下面图片出处，其实这个文章也就是书上内容，正好我笔记要图片。
>
> + [数据库计算引擎的优化技术：向量化执行与代码生成 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/100933389)
>
>   **向量化执行**
>
>   向量化执行的思想就是**均摊开销**：
>
>   假设每次通过 operator tree 生成一行结果的开销是 C 的话，经典模型的计算框架总开销就是 C * N，其中 N 为参与计算的总行数，如果把计算引擎每次生成一行数据的模型改为每次生成一批数据的话，因为每次调用的开销是相对恒定的，所以计算框架的总开销就可以减小到C * N / M，其中 M 是每批数据的行数，这样每一行的开销就减小为原来的 1 / M，当 M 比较大时，计算框架的开销就不会成为系统瓶颈了。
>
>   除此之外，向量化执行还能给 compiler 带来更多的优化空间，因为引入向量化之后，实际上是将原来数据库运行时的一个大 for 循环拆成了两层 for 循环，内层的 for 循环通常会比较简单，对编译器来说也存在更大的优化可能性。举例来说，对于一个实现两个 int 相加的 expression，在向量化之前，其实现可能是这样的：
>
>   ```cpp
>   class ExpressionIntAdd extends Expression {
>           Datum eval(Row input) {
>                   int left = input.getInt(leftIndex);
>                   int right = input.getInt(rightIndex);
>                   return new Datum(left+right);
>           }
>   }
>   ```
>
>   在向量化之后，其实现可能会变为这样：
>
>   ```cpp
>   class VectorExpressionIntAdd extends VectorExpression {
>           int[] eval(int[] left, int[] right) {
>                   int[] ret = new int[input.length];
>                   for(int i = 0; i < input.length; i++) {
>                           ret[i] = new Datum(left[i] + right[i]);
>                   }
>                   return ret;
>           }
>   }
>   ```
>
>   <u>显然对比向量化之前的版本，向量化之后的版本不再是每次只处理一条数据，而是每次能处理一批数据，而且这种向量化的计算模式在计算过程中也具有更好的数据局部性</u>。
>
> + [SIMD_百度百科 (baidu.com)](https://baike.baidu.com/item/SIMD/3412835?fr=aladdin)
>
>   SIMD全称Single Instruction Multiple Data，单指令多数据流，能够[复制](https://baike.baidu.com/item/复制/33067)多个[操作数](https://baike.baidu.com/item/操作数/7658270)，并把它们打包在大型[寄存器](https://baike.baidu.com/item/寄存器/187682)的一组[指令集](https://baike.baidu.com/item/指令集/238130)。
>
>   + 概念：以同步方式，在同一时间内执行同一条指令。
>
>   + 性能上优势：
>
>     以加法指令为例，
>
>     + 单指令单数据（[SISD](https://baike.baidu.com/item/SISD)）的CPU对加法指令译码后，执行部件先访问内存，取得第一个[操作数](https://baike.baidu.com/item/操作数)；之后再一次访问内存，取得第二个操作数；随后才能进行求和运算。
>     + 而在SIMD型的CPU中，指令译码后**几个执行部件同时访问内存**，一次性获得所有操作数进行运算。这个特点使SIMD特别适合于多媒体应用等数据密集型运算。
>
>     如：[AMD](https://baike.baidu.com/item/AMD)公司引以为豪的3D NOW! 技术实质就是SIMD，这使K6-2、雷鸟、毒龙处理器在音频解码、视频回放、3D游戏等应用中显示出优异的性能。
>
> + [SSE 4.2_百度百科 (baidu.com)](https://baike.baidu.com/item/SSE 4.2/138412?fromtitle=SSE4.2&fromid=4680786&fr=aladdin)
>
>   英特尔表示，采用SSE 4.2[指令集](https://baike.baidu.com/item/指令集)后，XML的解析速度最高是原来的3.8倍，而[指令周期](https://baike.baidu.com/item/指令周期)节省可以达到2.7倍。此外，在ATA领域，SSE 4.2指令集对于大规模数据集中处理和提高通信效率都会发挥应有的作用，这些对于企业IT应用显然是有帮助的。当然，SSE 4.2指令集只有在软件对其支持后才会生产效果，相信Nehalem-EP上市，相关的优化与升级届时就会出现。

​	坊间有句玩笑，即“能用钱解决的问题，千万别花时间”。而业界也有种调侃如出一辙，即“能升级硬件解决的问题，千万别优化程序”。有时候，你千辛万苦优化程序逻辑带来的性能提升，还不如直接升级硬件来得简单直接。这虽然只是一句玩笑不能当真，但硬件层面的优化确实是最直接、最高效的提升途径之一。**向量化执行就是这种方式的典型代表，这项寄存器硬件层面的特性，为上层应用程序的性能带来了指数级的提升**。

​	**向量化执行，可以简单地看作一项消除程序中循环的优化**。这里用一个形象的例子比喻。小胡经营了一家果汁店，虽然店里的鲜榨苹果汁深受大家喜爱，但客户总是抱怨制作果汁的速度太慢。小胡的店里只有一台榨汁机，每次他都会从篮子里拿出一个苹果，放到榨汁机内等待出汁。如果有8个客户，每个客户都点了一杯苹果汁，那么小胡需要重复循环8次上述的榨汁流程，才能榨出8杯苹果汁。如果制作一杯果汁需要5分钟，那么全部制作完毕则需要40分钟。为了提升果汁的制作速度，小胡想出了一个办法。他将榨汁机的数量从1台增加到了8台，这么一来，他就可以从篮子里一次性拿出8个苹果，分别放入8台榨汁机同时榨汁。此时，小胡只需要5分钟就能够制作出8杯苹果汁。为了制作n杯果汁，非向量化执行的方式是用1台榨汁机重复循环制作n次，而向量化执行的方式是用n台榨汁机只执行1次。

​	**为了实现向量化执行，需要利用CPU的SIMD指令**。SIMD的全称是Single Instruction Multiple Data，即用单条指令操作多条数据。现代计算机系统概念中，它是**通过数据并行以提高性能的一种实现方式**（其他的还有指令级并行和线程级并行），它的**原理是在CPU寄存器层面实现数据的并行操作**。

​	在计算机系统的体系结构中，存储系统是一种层次结构。典型服务器计算机的存储层次结构如图2-1所示。一个实用的经验告诉我们，存储媒介距离CPU越近，则访问数据的速度越快。

![img](https://img-blog.csdnimg.cn/img_convert/5944e32c6d7b4376b041bf8e1af52e6b.png)

​	从上图中可以看到，从左向右，**距离CPU越远，则数据的访问速度越慢**。从寄存器中访问数据的速度，是从内存访问数据速度的300倍，是从磁盘中访问数据速度的3000万倍。所以<u>利用CPU向量化执行的特性，对于程序的性能提升意义非凡</u>。

​	<u>ClickHouse目前利用SSE4.2指令集实现向量化执行</u>。

### 2.1.4 关系模型与SQL查询

​	相比HBase和Redis这类NoSQL数据库，**ClickHouse使用关系模型描述数据并提供了传统数据库的概念（数据库、表、视图和函数等）**。与此同时，**ClickHouse完全使用SQL作为查询语言（支持GROUP BY、ORDER BY、JOIN、IN等大部分标准SQL）**，这使得它平易近人，容易理解和学习。因为关系型数据库和SQL语言，可以说是软件领域发展至今应用最为广泛的技术之一，拥有极高的“群众基础”。也正因为ClickHouse提供了标准协议的SQL查询接口，使得现有的第三方分析可视化系统可以轻松与它集成对接。**在SQL解析方面，ClickHouse是大小写敏感的，这意味着SELECT a和SELECT A所代表的语义是不同的**。

​	关系模型相比文档和键值对等其他模型，拥有更好的描述能力，也能够更加清晰地表述实体间的关系。<u>更重要的是，在OLAP领域，已有的大量数据建模工作都是基于关系模型展开的（星型模型、雪花模型乃至宽表模型）</u>。**ClickHouse使用了关系模型，所以将构建在传统关系型数据库或数据仓库之上的系统迁移到ClickHouse的成本会变得更低，可以直接沿用之前的经验成果。**

### 2.1.5 多样化的表引擎

​	也许因为Yandex.Metrica的最初架构是基于MySQL实现的，所以在ClickHouse的设计中，能够察觉到一些MySQL的影子，表引擎的设计就是其中之一。与MySQL类似，ClickHouse也将存储部分进行了抽象，把存储引擎作为一层独立的接口。**截至本书完稿时，ClickHouse共拥有合并树、内存、文件、接口和其他6大类20多种表引擎。**其中每一种表引擎都有着各自的特点，用户可以根据实际业务场景的要求，选择合适的表引擎使用。

​	**通常而言，一个通用系统意味着更广泛的适用性，能够适应更多的场景。但通用的另一种解释是平庸，因为它无法在所有场景内都做到极致**。

​	在软件的世界中，并不会存在一个能够适用任何场景的通用系统，为了突出某项特性，势必会在别处有所取舍。其实世间万物都遵循着这样的道理，就像信天翁和蜂鸟，虽然都属于鸟类，但它们各自的特点却铸就了完全不同的体貌特征。信天翁擅长远距离飞行，环绕地球一周只需要1至2个月的时间。因为它能够长时间处于滑行状态，5天才需要扇动一次翅膀，心率能够保持在每分钟100至200次之间。而蜂鸟能够垂直悬停飞行，每秒可以挥动翅膀70～100次，飞行时的心率能够达到每分钟1000次。

​	如果用数据库的场景类比信天翁和蜂鸟的特点，那么信天翁代表的可能是<u>使用普通硬件就能实现高性能的设计思路，数据按粗粒度处理，通过批处理的方式执行</u>；而蜂鸟代表的可能是<u>按细粒度处理数据的设计思路，需要高性能硬件的支持</u>。

​	将表引擎独立设计的好处是显而易见的，通过特定的表引擎支撑特定的场景，十分灵活。对于简单的场景，可直接使用简单的引擎降低成本，而复杂的场景也有合适的选择。

### 2.1.6 * 多线程与分布式

> + **计算移动比数据移动更加划算**
> + [对ClickHouse分片和分区的简单理解 - 简书 (jianshu.com)](https://www.jianshu.com/p/178a01e0ae6e)
>   + **分区**是表的分区，具体的DDL操作关键词是 `PARTITION BY`，指的是一个表按照某一列数据（比如日期）进行分区，对应到最终的结果就是不同分区的数据会写入不同的文件中。
>   + **分片**复用了数据库的分区，相当于在原有的分区下，作为第二层分区， 是在不同节点/机器上的体现。

​	ClickHouse几乎具备现代化高性能数据库的所有典型特征，对于可以提升性能的手段可谓是一一用尽，对于多线程和分布式这类被广泛使用的技术，自然更是不在话下。

​	**如果说向量化执行是通过数据级并行的方式提升了性能，那么多线程处理就是通过线程级并行的方式实现了性能的提升**。

​	相比基于底层硬件实现的向量化执行SIMD，线程级并行通常由更高层次的软件层面控制。现代计算机系统早已普及了多处理器架构，所以现今市面上的服务器都具备良好的多核心多线程处理能力。**由于SIMD不适合用于带有较多分支判断的场景，ClickHouse也大量使用了多线程技术以实现提速，以此和向量化执行形成互补**。

​	如果一个篮子装不下所有的鸡蛋，那么就多用几个篮子来装，这就是分布式设计中分而治之的基本思想。同理，如果一台服务器性能吃紧，那么就利用多台服务的资源协同处理。为了实现这一目标，首先需要在数据层面实现数据的分布式。

​	因为**==在分布式领域，存在一条金科玉律——计算移动比数据移动更加划算==**。

​	**在各服务器之间，通过网络传输数据的成本是高昂的，所以相比移动数据，更为聪明的做法是预先将数据分布到各台服务器，将数据的计算查询直接下推到数据所在的服务器。**

​	**ClickHouse在数据存取方面，既支持分区（纵向扩展，利用多线程原理），也支持分片（横向扩展，利用分布式原理），可以说是将多线程和分布式的技术应用到了极致**。

### 2.1.7 * 多主架构

​	**HDFS、Spark、HBase和Elasticsearch这类分布式系统，都采用了Master-Slave主从架构，由一个管控节点作为Leader统筹全局。**

​	**而ClickHouse则采用Multi-Master多主架构，集群中的每个节点角色对等，客户端访问任意一个节点都能得到相同的效果。这种多主的架构有许多优势，例如对等的角色使系统架构变得更加简单，不用再区分主控节点、数据节点和计算节点，集群中的所有节点功能相同。所以它天然规避了单点故障的问题，非常适合用于多数据中心、异地多活的场景**。

### 2.1.8 在线查询

​	<small>ClickHouse经常会被拿来与其他的分析型数据库作对比，比如Vertica、SparkSQL、Hive和Elasticsearch等，它与这些数据库确实存在许多相似之处。例如，它们都可以支撑海量数据的查询场景，都拥有分布式架构，都支持列存、数据分片、计算下推等特性。这其实也侧面说明了ClickHouse在设计上确实吸取了各路奇技淫巧。与其他数据库相比，ClickHouse也拥有明显的优势。例如，Vertica这类商用软件价格高昂；SparkSQL与Hive这类系统无法保障90%的查询在1秒内返回，在大数据量下的复杂查询可能会需要分钟级的响应时间；而Elasticsearch这类搜索引擎在处理亿级数据聚合查询时则显得捉襟见肘。</small>

​	<small>正如ClickHouse的“广告词”所言，其他的开源系统太慢，商用的系统太贵，只有Clickouse在成本与性能之间做到了良好平衡，即又快又开源。ClickHouse当之无愧地阐释了“在线”二字的含义，即便是在复杂查询的场景下，它也能够做到极快响应，且无须对数据进行任何预处理加工。</small>

### 2.1.9 * 数据分片与分布式查询

​	数据分片是将数据进行横向切分，这是一种在面对海量数据的场景下，解决存储和查询瓶颈的有效手段，是一种分治思想的体现。ClickHouse支持分片，而分片则依赖集群。每个集群由1到多个分片组成，而每个分片则对应了ClickHouse的1个服务节点。**分片的数量上限取决于节点数量（1个分片只能对应1个服务节点）**。

​	ClickHouse并不像其他分布式系统那样，拥有高度自动化的分片功能。**ClickHouse提供了本地表（Local Table）与分布式表（Distributed Table）的概念。一张本地表等同于一份数据的分片。而分布式表本身不存储任何数据，它是本地表的访问代理，其作用类似分库中间件。借助分布式表，能够代理访问多个数据分片，从而实现分布式查询**。

​	这种设计类似数据库的分库和分表，十分灵活。例如在业务系统上线的初期，数据体量并不高，此时数据表并不需要多个分片。所以使用单个节点的本地表（单个数据分片）即可满足业务需求，待到业务增长、数据量增大的时候，再通过新增数据分片的方式分流数据，并通过分布式表实现分布式查询。这就好比一辆手动挡赛车，它将所有的选择权都交到了使用者的手中。

## 2.2 ClickHouse的架构设计

### 2.2.1 * Column与Field

> [ClickHouse到底是什么？凭啥这么牛逼！_芋艿V-CSDN博客](https://blog.csdn.net/github_38592071/article/details/117309063)	=>	图片出处。

​	**Column和Field是ClickHouse数据最基础的映射单元**。作为一款百分之百的列式存储数据库，**ClickHouse按列存储数据，内存中的一列数据由一个Column对象表示**。Column对象分为接口和实现两个部分，在IColumn接口对象中，定义了对数据进行各种关系运算的方法，例如插入数据的insertRangeFrom和insertFrom方法、用于分页的cut，以及用于过滤的filter方法等。而<u>这些方法的具体实现对象则根据数据类型的不同，由相应的对象实现，例如ColumnString、ColumnArray和ColumnTuple等</u>。

​	**在大多数场合，ClickHouse都会以整列的方式操作数据**，但凡事也有例外。**如果需要操作单个具体的数值（也就是单列中的一行数据），则需要使用Field对象，Field对象代表一个单值**。

​	**与Column对象的泛化设计思路不同，Field对象使用了聚合的设计模式。在Field对象内部聚合了Null、UInt64、String和Array等13种数据类型及相应的处理逻辑**。

![img](https://img-blog.csdnimg.cn/img_convert/a6eeb469aa1fb90577049c5cbdba7ce7.png)

### 2.2.2 * DataType

> ***泛化*是把特殊代码转换成通用目的代码的过程**

​	**数据的序列化和反序列化工作由DataType负责**。IDataType接口定义了许多正反序列化的方法，它们成对出现，例如serializeBinary和deserializeBinary、serializeTextJSON和deserializeTextJSON等，涵盖了常用的二进制、文本、JSON、XML、CSV和Protobuf等多种格式类型。IDataType也使用了泛化的设计模式，具体方法的实现逻辑由对应数据类型的实例承载，例如DataTypeString、DataTypeArray及DataTypeTuple等。

​	**DataType虽然负责序列化相关工作，但它并不直接负责数据的读取，而是转由从Column或Field对象获取**。

​	在DataType的实现类中，聚合了相应数据类型的Column对象和Field对象。例如，DataTypeString会引用字符串类型的ColumnString，而DataTypeArray则会引用数组类型的ColumnArray，以此类推。

### 2.2.3 * Block与Block流

​	**ClickHouse内部的数据操作是面向Block对象进行的，并且采用了流的形式**。虽然Column和Filed组成了数据的基本映射单元，但对应到实际操作，它们还缺少了一些必要的信息，比如数据的类型及列的名称。于是ClickHouse设计了Block对象，Block对象可以看作数据表的子集。

​	**Block对象的本质是由数据对象、数据类型和列名称组成的三元组，即Column、DataType及列名称字符串**。

​	Column提供了数据的读取能力，而DataType知道如何正反序列化，所以Block在这些对象的基础之上实现了进一步的抽象和封装，从而简化了整个使用的过程，仅通过Block对象就能完成一系列的数据操作。**在具体的实现过程中，Block并没有直接聚合Column和DataType对象，而是通过ColumnWithTypeAndName对象进行间接引用**。

​	有了Block对象这一层封装之后，对Block流的设计就是水到渠成的事情了。流操作有两组顶层接口：IBlockInputStream负责数据的读取和关系运算，IBlockOutputStream负责将数据输出到下一环节。**Block流也使用了泛化的设计模式，对数据的各种操作最终都会转换成其中一种流的实现**。IBlockInputStream接口定义了读取数据的若干个read虚方法，而具体的实现逻辑则交由它的实现类来填充。

​	IBlockInputStream接口总共有60多个实现类，它们涵盖了ClickHouse数据摄取的方方面面。这些实现类大致可以分为三类：

1. 第一类**用于处理数据定义的DDL操作**，例如DDLQueryStatusInputStream等；
2. 第二类**用于处理关系运算的相关操作**，例如LimitBlockInput-Stream、JoinBlockInputStream及AggregatingBlockInputStream等；
3. 第三类则是**与表引擎呼应**，每一种表引擎都拥有与之对应的BlockInputStream实现，例如MergeTreeBaseSelect-BlockInputStream（MergeTree表引擎）、TinyLogBlockInputStream（TinyLog表引擎）及KafkaBlockInputStream（Kafka表引擎）等。

​	IBlockOutputStream的设计与IBlockInputStream如出一辙。IBlockOutputStream接口同样也定义了若干写入数据的write虚方法。它的实现类比IBlockInputStream要少许多，一共只有20多种。这些实现类基本用于表引擎的相关处理，负责将数据写入下一环节或者最终目的地，例如MergeTreeBlockOutputStream、TinyLogBlockOutputStream及StorageFileBlock-OutputStream等。

### 2.2.4 Table

> + [AST-抽象语法树_百度百科 (baidu.com)](https://baike.baidu.com/item/抽象语法树/6129952?fr=aladdin)
>
>   ​	在[计算机科学](https://baike.baidu.com/item/计算机科学)中，抽象语法树（Abstract Syntax Tree，AST），或简称**[语法树](https://baike.baidu.com/item/语法树/7031301)**（Syntax tree），是[源代码](https://baike.baidu.com/item/源代码)[语法](https://baike.baidu.com/item/语法)结构的一种抽象表示。它以树状的形式表现[编程语言](https://baike.baidu.com/item/编程语言)的语法结构，树上的每个节点都表示源代码中的一种结构。

​	**在数据表的底层设计中并没有所谓的Table对象，它直接使用IStorage接口指代数据表**。

​	表引擎是ClickHouse的一个显著特性，不同的表引擎由不同的子类实现，例如IStorageSystemOneBlock（系统表）、StorageMergeTree（合并树表引擎）和StorageTinyLog（日志表引擎）等。IStorage接口定义了DDL（如ALTER、RENAME、OPTIMIZE和DROP等）、read和write方法，它们分别负责数据的定义、查询与写入。在数据查询时，IStorage负责根据AST查询语句的指示要求，返回指定列的原始数据。后续对数据的进一步加工、计算和过滤，则会统一交由Interpreter解释器对象处理。对Table发起的一次操作通常都会经历这样的过程，接收AST查询语句，根据AST返回指定列的数据，之后再将数据交由Interpreter做进一步处理。

### 2.2.5 Parser与Interpreter

​	**Parser和Interpreter是非常重要的两组接口：**

+ **Parser分析器负责创建AST对象；**
+ **而Interpreter解释器则负责解释AST，并进一步创建查询的执行管道**。

​	它们与IStorage一起，串联起了整个数据查询的过程。

​	Parser分析器可以将一条SQL语句以递归下降的方法解析成AST语法树的形式。不同的SQL语句，会经由不同的Parser实现类解析。例如，有负责解析DDL查询语句的ParserRenameQuery、ParserDropQuery和ParserAlterQuery解析器，也有负责解析INSERT语句的ParserInsertQuery解析器，还有负责SELECT语句的ParserSelectQuery等。

​	Interpreter解释器的作用就像Service服务层一样，起到串联整个查询过程的作用，它会根据解释器的类型，聚合它所需要的资源。首先它会解析AST对象；然后执行“业务逻辑”（例如分支判断、设置参数、调用接口等）；最终返回IBlock对象，以线程的形式建立起一个查询执行管道。

### 2.2.6 * Functions与Aggregate Functions

​	**ClickHouse主要提供两类函数——普通函数和聚合函数**。普通函数由IFunction接口定义，拥有数十种函数实现，例如FunctionFormatDateTime、FunctionSubstring等。除了一些常见的函数（诸如四则运算、日期转换等）之外，也不乏一些非常实用的函数，例如网址提取函数、IP地址脱敏函数等。

+ **普通函数是没有状态的，函数效果作用于每行数据之上**。
+ 当然，**在函数具体执行的过程中，并不会一行一行地运算，而是采用向量化的方式直接作用于一整列数据**。

​	**聚合函数由IAggregateFunction接口定义，相比无状态的普通函数，聚合函数是有状态的**。以COUNT聚合函数为例，其AggregateFunctionCount的状态使用整型UInt64记录。**聚合函数的状态支持序列化与反序列化，所以能够在分布式节点之间进行传输，以实现增量计算**。

### 2.2.7 * Cluster与Replication

​	**ClickHouse的集群由分片（Shard）组成，而每个分片又通过副本（Replica）组成**。这种分层的概念，在一些流行的分布式系统中十分普遍。例如，在Elasticsearch的概念中，一个索引由分片和副本组成，副本可以看作一种特殊的分片。如果一个索引由5个分片组成，副本的基数是1，那么这个索引一共会拥有10个分片（每1个分片对应1个副本）。

​	如果你用同样的思路来理解ClickHouse的分片，那么很可能会在这里栽个跟头。ClickHouse的某些设计总是显得独树一帜，而集群与分片就是其中之一。这里有几个与众不同的特性。

1. **ClickHouse的1个节点只能拥有1个分片，也就是说如果要实现1分片、1副本，则至少需要部署2个服务节点。**
2. **分片只是一个逻辑概念，其物理承载还是由副本承担的。**

​	代码清单2-1所示是ClickHouse的一份集群配置示例，从字面含义理解这份配置的语义，可以理解为自定义集群ch_cluster拥有1个shard（分片）和1个replica（副本），且该副本由10.37.129.6服务节点承载。

​	代码清单2-1 自定义集群ch_cluster的配置示例

```xml
<ch_cluster>
  <shard>
    <replica>
      <host>10.37.129.6</host>
      <port>9000</port>
    </replica>
  </shard>
</ch_cluster>
```

​	**从本质上看，这组1分片、1副本的配置在ClickHouse中只有1个物理副本，所以它正确的语义应该是1分片、0副本。分片更像是逻辑层的分组，在物理存储层面则统一使用副本代表分片和副本**。所以真正表示1分片、1副本语义的配置，应该改为1个分片和2个副本，如代码清单2-2所示。

​	代码清单2-2 1分片、1副本的集群配置

```xml
<ch_cluster>
  <shard>
    <replica>
      <host>10.37.129.6</host>
      <port>9000</port>
    </replica>
    <replica>
      <host>10.37.129.7</host>
      <port>9000</port>
    </replica>
  </shard>
</ch_cluster>
```

​	副本与分片将在第10章详细介绍。

## 2.3 ClickHouse为何如此之快

​	<small>很多用户心中一直会有这样的疑问，为什么ClickHouse这么快？前面的介绍对这个问题已经做出了科学合理的解释。比方说，因为ClickHouse是**列式存储数据库**，所以快；也因为ClickHouse使用了**向量化引擎**，所以快。这些解释都站得住脚，但是依然不能消除全部的疑问。因为这些技术并不是秘密，世面上有很多数据库同样使用了这些技术，但是依然没有ClickHouse这么快。所以我想从另外一个角度来探讨一番ClickHouse的秘诀到底是什么。</small>

​	<small>首先向各位读者抛出一个疑问：在设计软件架构的时候，做设计的原则应该是自顶向下地去设计，还是应该自下而上地去设计呢？在传统观念中，或者说在我的观念中，自然是自顶向下的设计，通常我们都被教导要做好顶层设计。而**ClickHouse的设计则采用了自下而上的方式**。ClickHouse的原型系统早在2008年就诞生了，在诞生之初它并没有宏伟的规划。相反它的目的很单纯，就是希望能以最快的速度进行GROUP BY查询和过滤。他们是如何实践自下而上设计的呢？</small>

### 2.3.1 着眼硬件，先想后做

​	首先从硬件功能层面着手设计，在设计伊始就至少需要想清楚如下几个问题。

+ 我们将要使用的硬件水平是怎样的？包括CPU、内存、硬盘、网络等。
+ 在这样的硬件上，我们需要达到怎样的性能？包括延迟、吞吐量等。
+ 我们准备使用怎样的数据结构？包括String、HashTable、Vector等。
+ 选择的这些数据结构，在我们的硬件上会如何工作？

​	如果能想清楚上面这些问题，那么在动手实现功能之前，就已经能够计算出粗略的性能了。所以，基于将硬件功效最大化的目的，ClickHouse会在内存中进行GROUP BY，并且使用HashTable装载数据。

​	<u>与此同时，他们非常在意CPU L3级别的缓存，因为一次L3的缓存失效会带来70～100ns的延迟。这意味着在单核CPU上，它会浪费4000万次/秒的运算；而在一个32线程的CPU上，则可能会浪费5亿次/秒的运算。所以别小看这些细节，一点一滴地将它们累加起来，数据是非常可观的。正因为注意了这些细节，所以ClickHouse在基准查询中能做到1.75亿次/秒的数据扫描性能</u>。

### 2.3.2 * 算法在前，抽象在后

​	常有人念叨：“有时候，选择比努力更重要。”确实，路线选错了再努力也是白搭。在ClickHouse的底层实现中，经常会面对一些重复的场景，例如字符串子串查询、数组排序、使用HashTable等。如何才能实现性能的最大化呢？算法的选择是重中之重。<u>以字符串为例，有一本专门讲解字符串搜索的书，名为“Handbook of Exact StringMatching Algorithms”，列举了35种常见的字符串搜索算法。各位猜一猜ClickHouse使用了其中的哪一种？答案是一种都没有。这是为什么呢？因为性能不够快</u>。

​	在字符串搜索方面，针对不同的场景，ClickHouse最终选择了这些算法：

+ 对于常量，使用Volnitsky算法；
+ 对于非常量，使用CPU的向量化执行SIMD，暴力优化；
+ 正则匹配使用re2和hyperscan算法。

​	**性能是算法选择的首要考量指标**。

### 2.3.3 勇于尝鲜，不行就换

​	除了字符串之外，其余的场景也与它类似，ClickHouse会**使用最合适、最快的算法**。如果世面上出现了号称性能强大的新算法，ClickHouse团队会立即将其纳入并进行验证。如果效果不错，就保留使用；如果性能不尽人意，就将其抛弃。

### 2.3.4 特定场景，特殊优化

​	针对同一个场景的不同状况，选择使用不同的实现方式，尽可能将性能最大化。关于这一点，其实在前面介绍字符串查询时，针对不同场景选择不同算法的思路就有体现了。类似的例子还有很多，例如去重计数uniqCombined函数，会根据数据量的不同选择不同的算法：<u>当数据量较小的时候，会选择Array保存；当数据量中等的时候，会选择HashSet；而当数据量很大的时候，则使用HyperLogLog算法</u>。

​	<u>对于数据结构比较清晰的场景，会通过代码生成技术实现循环展开，以减少循环次数。接着就是大家熟知的大杀器——向量化执行了</u>。**SIMD被广泛地应用于文本转换、数据过滤、数据解压和JSON转换等场景。相较于单纯地使用CPU，利用寄存器暴力优化也算是一种降维打击了**。

### 2.3.5 持续测试，持续改进

​	如果只是单纯地在上述细节上下功夫，还不足以构建出如此强大的ClickHouse，还需要拥有一个能够持续验证、持续改进的机制。由于Yandex的天然优势，ClickHouse经常会使用真实的数据进行测试，这一点很好地保证了测试场景的真实性。与此同时，ClickHouse也是我见过的发版速度最快的开源软件了，差不多每个月都能发布一个版本。没有一个可靠的持续集成环境，这一点是做不到的。正因为拥有这样的发版频率，ClickHouse才能够快速迭代、快速改进。

​	所以ClickHouse的黑魔法并不是一项单一的技术，而是一种**自底向上的、追求极致性能的设计思路**。这就是它如此之快的秘诀。

## 2.4 本章小结

​	本章我们快速浏览了世界第三大Web流量分析平台Yandex.Metrica背后的支柱ClickHouse的核心特性和逻辑架构。通过对核心特性部分的展示，ClickHouse如此强悍的缘由已初见端倪，**列式存储**、**向量化执行引擎**和**表引擎**都是它的撒手锏。在架构设计部分，则进一步展示了ClickHouse的一些设计思路，例如Column、Field、Block和Cluster。了解这些设计思路，能够帮助我们更好地理解和使用ClickHouse。最后又从另外一个角度探讨了ClickHouse如此之快的秘诀。下一章将介绍如何安装、部署ClickHouse。

# 3. 安装与部署

> + [使用教程 | ClickHouse文档](https://clickhouse.tech/docs/zh/getting-started/tutorial/)
> + [ClickHouse docker-compose初试_lzshlzsh的专栏-CSDN博客](https://blog.csdn.net/lzshlzsh/article/details/104296325)

## 3.1 ClickHouse的安装过程

### 3.1.1 环境准备

### 3.1.2 安装ClickHouse

1. 安装执行

2. 目录结构

   程序在安装的过程中会自动构建整套目录结构，接下来分别说明它们的作用。

   首先是核心目录部分：

   1. `/etc/clickhouse-server`：服务端的配置文件目录，包括全局配置config.xml和用户配置users.xml等。
   2. `/var/lib/clickhouse`：默认的数据存储目录（通常会修改默认路径配置，将数据保存到大容量磁盘挂载的路径）。
   3. `/var/log/clickhouse-server`：默认保存日志的目录（通常会修改路径配置，将日志保存到大容量磁盘挂载的路径）。

   接着是配置文件部分：

   1. `/etc/security/limits.d/clickhouse.conf`：文件句柄数量的配置，默认值如下所示：

      ```shell
      # cat /etc/security/limits.d/clickhouse.conf
      clickhouse soft nofile 262144
      clickhouse hard nofile 262144
      ```

      该配置也可以通过config.xml的max_open_files修改。

   2. `/etc/cron.d/clickhouse-server`：cron定时任务配置，用于恢复因异常原因中断的ClickHouse服务进程，其默认的配置如下：

      ```shell
      # cat /etc/cron.d/clickhouse-server
      # */10 * * * * root (which service > /dev/null 2>&1 && (service clickhouse-server
      condstart ||:)) || /etc/init.d/clickhouse-server condstart > /dev/null 2>&1
      ```

      可以看到，在默认的情况下，每隔10秒就会使用condstart尝试启动一次ClickHouse服务，而condstart命令的启动逻辑如下所示。

      ```shell
      is_running || service_or_func start
      ```

      如果ClickHouse服务正在运行，则跳过；如果没有运行，则通过start启动。

   最后是一组在/usr/bin路径下的可执行文件：

   1. clickhouse：主程序的可执行文件。

   2. clickhouse-client：一个指向ClickHouse可执行文件的软链接，供客户端连接使用。

   3. clickhouse-server：一个指向ClickHouse可执行文件的软链接，供服务端启动使用。

   4. clickhouse-compressor：内置提供的压缩工具，可用于数据的正压反解。

3. 启动服务

4. 版本升级

## 3.2 客户端的访问接口

​	**ClickHouse的底层访问接口支持TCP和HTTP两种协议**。

+ 其中，TCP协议拥有更好的性能，其默认端口为**9000**，主要用于集群间的内部通信及CLI客户端；
+ 而HTTP协议则拥有更好的兼容性，可以通过REST服务的形式被广泛用于JAVA、Python等编程语言的客户端，其默认端口为**8123**。

​	通常而言，并不建议用户直接使用底层接口访问ClickHouse，更为推荐的方式是通过CLI和JDBC这些封装接口，因为它们更加简单易用。

### 3.2.1 CLI

### 3.2.2 JDBC

​	ClickHouse支持标准的JDBC协议，底层基于HTTP接口通信。使用下面的Maven依赖，即可为Java程序引入官方提供的数据库驱动包：

```xml
<dependency>
  <groupId>ru.yandex.clickhouse</groupId>
  <artifactId>clickhouse-jdbc</artifactId>
  <version>0.2.4</version>
</dependency>
```

​	该驱动有两种使用方式。

1. 标准形式

   ​	标准形式是我们常用的方式，通过JDK原生接口获取连接，其关键参数如下：

   + JDBC Driver Class为`ru.yandex.clickhouse.ClickHouseDriver`；
   + JDBC URL为`jdbc:clickhouse://<host>:<port>[/<database>]`。

   ​	接下来是一段伪代码用例：

   ```java
   // 初始化驱动
   Class.forName("ru.yandex.clickhouse.ClickHouseDriver");
   // url
   String url = "jdbc:clickhouse://ch5.nauu.com:8123/default";
   // 用户名密码
   String user = "default";
   String password = "";
   // 登录
   Connection con = DriverManager.getConnection(url, username, password);
   Statement stmt = con.createStatement();
   // 查询
   ResultSet rs = stmt.executeQuery("SELECT 1");
   rs.next();
   System.out.printf("res "+rs.getInt(1));
   ```

2. 高可用模式

   ​	高可用模式允许设置多个host地址，每次会从可用的地址中随机选择一个进行连接，其URL声明格式如下：

   ```shell
   jdbc:clickhouse://<first-host>:<port>,<second-host>:<port>[,…]/<database>
   ```

   ​	在高可用模式下，需要通过BalancedClickhouseDataSource对象获取连接，接下来是一段伪代码用例：

   ```java
   //多个地址使用逗号分隔
   String url1 = "jdbc:clickhouse://ch8.nauu.com:8123,ch5.nauu.com:8123/default";
   //设置JDBC参数
   ClickHouseProperties clickHouseProperties = new ClickHouseProperties();
   clickHouseProperties.setUser("default");
   //声明数据源
   BalancedClickhouseDataSource balanced = new BalancedClickhouseDataSource(url1,
   clickHouseProperties);
   //对每个host进行ping操作, 排除不可用的dead连接
   balanced.actualize();
   //获得JDBC连接
   Connection con = balanced.getConnection();
   Statement stmt = con.createStatement();
   //查询
   ResultSet rs = stmt.executeQuery("SELECT 1 , hostName()");
   rs.next();
   System.out.println("res "+rs.getInt(1)+","+rs.getString(2));
   ```

   ​	由于篇幅所限，所以本小节只介绍了两个典型的封装接口，即CLI（基于TCP）和JDBC（基于HTTP）。但ClickHouse的访问接口并不仅限于此、它还拥有原生的C++、ODBC接口及众多第三方的集成接口（Python、NodeJS、Go、PHP等），如果想进一步了解可参阅官方手册。

## 3.3 内置的实用工具

​	ClickHouse除了提供基础的服务端与客户端程序之外，还内置了clickhouse-local和clickhouse-benchmark两种实用工具，现在分别说明它们的作用。

### 3.3.1 clickhouse-local

​	clickhouse-local可以独立运行大部分SQL查询，不需要依赖任何ClickHouse的服务端程序，它可以理解成是ClickHouse服务的单机版微内核，是一个轻量级的应用程序。clickhouse-local只能够使用File表引擎（关于表引擎的更多介绍在后续章节展开），它的数据与同机运行的ClickHouse服务也是完全隔离的，相互之间并不能访问。

### 3.3.2 clickhouse-benchmark

​	clickhouse-benchmark是基准测试的小工具，它可以自动运行SQL查询，并生成相应的运行指标报告。

## 3.4 本章小结

​	本章首先介绍了基于离线RPM包安装ClickHouse的整个过程。接着介绍了ClickHouse的两种访问接口，其中TCP端口拥有更好的访问性能，而HTTP端口则拥有更好的兼容性。但是在日常应用的过程中，更推荐使用基于它们之上实现的封装接口。所以接下来，我们又分别介绍了两个典型的封装接口，其中CLI接口是基于TCP封装的，它拥有交互式和非交互式两种运行模式。而JDBC接口是基于HTTP封装的，是一种标准的数据库访问接口。最后介绍了ClickHouse内置的几种实用工具。从下一章开始将正式介绍ClickHouse的功能，首先会从数据定义开始。

# 4. 数据定义

​	对于一款可以处理海量数据的分析系统而言，支持DML查询实属难能可贵。有人曾笑言：解决问题的最好方法就是恰好不需要。在海量数据的场景下，许多看似简单的操作也会变得举步维艰，所以一些系统会选择做减法以规避一些难题。而ClickHouse支持较完备的DML语句，包括INSERT、SELECT、UPDATE和DELETE。虽然UPDATE和DELETE可能存在性能问题，但这些能力的提供确实丰富了各位架构师手中的筹码，在架构设计时也能多几个选择。

​	作为一款完备的DBMS（数据库管理系统），ClickHouse提供了DDL与DML的功能，并支持大部分标准的SQL。也正因如此，ClickHouse十分容易入门。如果你是一个拥有其他数据库（如MySQL）使用经验的老手，通过上一章的介绍，在搭建好数据库环境之后，再凭借自身经验摸索几次，很快就能够上手使用ClickHouse了。但是作为一款异军突起的OLAP数据库黑马，ClickHouse有着属于自己的设计目标，高性能才是它的根本，所以也不能完全以对传统数据库的理解度之。比如，ClickHouse在基础数据类型方面，虽然相比常规数据库更为精练，但同时它又提供了实用的复合数据类型，而这些是常规数据库所不具备的。再比如，**ClickHouse所提供的DDL与DML查询，在部分细节上也与其他数据库有所不同（例如UPDATE和DELETE是借助ALTER变种实现的）**。

​	所以系统学习并掌握ClickHouse中数据定义的方法是很有必要的，这能够帮助我们更深刻地理解和使用ClickHouse。本章将详细介绍ClickHouse的数据类型及DDL的相关操作，在章末还会讲解部分DML操作。

## 4.1 ClickHouse的数据类型

​	作为一款分析型数据库，ClickHouse提供了许多数据类型，它们可以划分为**基础类型**、**复合类型**和**特殊类型**。其中基础类型使ClickHouse具备了描述数据的基本能力，而另外两种类型则使ClickHouse的数据表达能力更加丰富立体。

### 4.1.1 基础类型

​	基础类型只有**数值**、**字符串**和**时间**三种类型，没有Boolean类型，但可以使用整型的0或1替代。

1. 数值类型

   1. Int

      ​	在普遍观念中，常用Tinyint、Smallint、Int和Bigint指代整数的不同取值范围。而ClickHouse则直接使用Int8、Int16、Int32和Int64指代4种大小的Int类型，其末尾的数字正好表明了占用字节的大小（8位=1字节），具体信息如表4-1所示。

      ​	表4-1 有符号整数类型的具体信息

      | 名称  | 大小（字节） | 范围                                     | 普遍观念 |
      | ----- | ------------ | ---------------------------------------- | -------- |
      | Int8  | 1            | -128~127                                 | Tinyint  |
      | Int16 | 2            | -32768~32767                             | Smallint |
      | Int32 | 4            | -2147483648~2147483647                   | Int      |
      | Int64 | 8            | -9223372036854775808~9223372036854775807 | Bigint   |

      ​	ClickHouse支持无符号的整数，使用前缀U表示，具体信息如表4-2所示。

      ​	表4-2 无符号整数类型的具体信息

      | 名称   | 大小（字节） | 范围                   | 普遍观念          |
      | ------ | ------------ | ---------------------- | ----------------- |
      | UInt8  | 1            | 0~255                  | Tinyint Unsigned  |
      | UInt16 | 2            | 0~65535                | Smallint Unsigned |
      | UInt32 | 4            | 0~4294967295           | Int Unsigned      |
      | UInt64 | 8            | 0~18446744073709551615 | Bigint Unsigned   |

   2. Float

      ​	与整数类似，ClickHouse直接使用Float32和Float64代表单精度浮点数以及双精度浮点数，具体信息如表4-3所示。
      
      ​	表4-3 浮点数类型的具体信息
      
      | 名称    | 大小（字节） | 有效精度(位数) | 普遍观念 |
      | ------- | ------------ | -------------- | -------- |
      | Float32 | 4            | 7              | Float    |
      | Float64 | 8            | 16             | Double   |
      
      ​	在使用浮点数的时候，应当要意识到它是有限精度的。假如，分别对Float32和Float64写入超过有效精度的数值，下面我们看看会发生什么。例如，将拥有20位小数的数值分别写入Float32和Float64，此时结果就会出现数据误差：
      
      ```shell
      ```
      
      可以发现，Float32从小数点后第8位起及Float64从小数点后第17位起，都产生了数据溢出。
      
      ClickHouse的浮点数支持**正无穷**、**负无穷**以及**非数字**的表达方式。
      
      + 正无穷：
      
        ```shell
        ```
      
      + 负无穷：
      
        ```shell
        
        ```
      
      + 非数字：
      
        ```shell
        ```
      
   3. Decimal

      ​	如果要求更高精度的数值运算，则需要使用定点数。ClickHouse提供了Decimal32、Decimal64和Decimal128三种精度的定点数。可以通过两种形式声明定点：简写方式有Decimal32(S)、Decimal64(S)、Decimal128(S)三种，原生方式为Decimal(P,S)，其中：

      + P代表精度，决定总位数（整数部分+小数部分），取值范围是1～38；
      + S代表规模，决定小数位数，取值范围是0～P。

      ​	简写方式与原生方式的对应关系如表4-4所示。

      ​	表4-4 定点数类型的具体信息

      | 名称          | 等效声明          | 范围                        |
      | ------------- | ----------------- | --------------------------- |
      | Decimal32(S)  | Decimal(1~9，S)   | -1\*10^(9-S)到1\*10^(9-S)   |
      | Decimal64(S)  | Decimal(10~18，S) | -1\*10^(18-S)到1\*10^(18-S) |
      | Decimal128(S) | Decimal(19~38，S) | -1\*10^(38-S)到1\*10^(38-S) |

      .....

      ....

      .....

      需要补全内容。。

2. 字符串类型

   ​	字符串类型可以细分为String、FixedString和UUID三类。从命名来看仿佛不像是由一款数据库提供的类型，反而更像是一门编程语言的设计。

   1. String

      ​	字符串由String定义，长度不限。因此在使用String的时候无须声明大小。它完全代替了传统意义上数据库的Varchar、Text、Clob和Blob等字符类型。String类型不限定字符集，因为它根本就没有这个概念，所以可以将任意编码的字符串存入其中。但是为了程序的规范性和可维护性，在同一套程序中应该遵循使用统一的编码，例如“统一保持UTF-8编码”就是一种很好的约定。

   2. FixedString

      ​	FixedString类型和传统意义上的Char类型有些类似，对于一些字符有明确长度的场合，可以使用固定长度的字符串。定长字符串通过FixedString(N)声明，其中N表示字符串长度。但与Char不同的是，FixedString使用null字节填充末尾字符，而Char通常使用空格填充。比如在下面的例子中，字符串‘abc’虽然只有3位，但长度却是5，因为末尾有2位空字符填充：

      ```shell
      ```

   3. UUID

      ​	UUID是一种数据库常见的主键类型，在ClickHouse中直接把它作为一种数据类型。UUID共有32位，它的格式为8-4-4-4-12。如果一个UUID类型的字段在写入数据时没有被赋值，则会依照格式使用0填充，例如：

      ```shell
      ```

      ​	可以看到，第二行没有被赋值的UUID被0填充了。

3. 时间类型

   ​	时间类型分为DateTime、DateTime64和Date三类。ClickHouse目前没有时间戳类型。时间类型最高的精度是秒，也就是说，**如果需要处理毫秒、微秒等大于秒分辨率的时间，则只能借助UInt类型实现**。

   ....

   ....

   ...

   需要补充内容，，，

### 4.1.2 复合类型

### 4.1.3 特殊类型

## 4.2 如何定义数据表



