# clickhouse原理解析与应用实践-学习笔记-01

# 1. ClickHouse的前世今生

## 1.1 传统BI系统之殇

> + [ERP系统_百度百科 (baidu.com)](https://baike.baidu.com/item/ERP系统/1569198?fr=aladdin)
>
>   ERP系统是企业资源计划（Enterprise Resource Planning）的简称，是指建立在信息技术基础上，集信息技术与先进管理思想于一身，以系统化的[管理思想](https://baike.baidu.com/item/管理思想/2555826)，为企业员工及决策层提供决策手段的管理平台。
>
> + [客户关系管理（管理学词汇CRM）_百度百科 (baidu.com)](https://baike.baidu.com/item/客户关系管理/254554?fromtitle=CRM&fromid=165070&fr=aladdin)
>
>   客户关系管理是指[企业](https://baike.baidu.com/item/企业/707680)为提高核心竞争力，利用相应的信息技术以及[互联网技术](https://baike.baidu.com/item/互联网技术/617749)协调企业与顾客间在[销售](https://baike.baidu.com/item/销售/239410)、[营销](https://baike.baidu.com/item/营销/150434)和服务上的交互，从而提升其[管理方式](https://baike.baidu.com/item/管理方式/260899)，向客户提供创新式的个性化的客户交互和服务的过程。其最终目标是吸引新客户、保留老客户以及将已有客户转为忠实[客户](https://baike.baidu.com/item/客户/1598984)，增加市场。
>
> + [OLTP_百度百科 (baidu.com)](https://baike.baidu.com/item/OLTP/5019563?fr=aladdin)
>
>   On-Line Transaction Processing[联机事务处理](https://baike.baidu.com/item/联机事务处理)过程(OLTP)，也称为**面向交易的处理过程**，其基本特征是前台接收的用户数据可以立即传送到计算中心进行处理，并在很短的时间内给出处理结果，是对用户操作快速响应的方式之一。
>
> + [数据孤岛_百度百科 (baidu.com)](https://baike.baidu.com/item/数据孤岛/10305414?fr=aladdin)
>
>   数据孤岛在企业信息化中,还有很多类似的描述,如"数据的污染"等比较形象的说法，专业人士把数据孤岛分为物理性和逻辑性两种。**物理性的数据孤岛指的是，数据在不同部门相互独立存储，独立维护，彼此间相互孤立，形成了物理上的孤岛**。
>
> + [数据仓库_百度百科 (baidu.com)](https://baike.baidu.com/item/数据仓库/381916?fr=aladdin)
>
>   数据仓库，英文名称为Data Warehouse，可简写为[DW](https://baike.baidu.com/item/DW/1264123)或DWH。数据仓库，是为[企业](https://baike.baidu.com/item/企业/707680)所有级别的决策制定过程，提供所有类型数据支持的战略[集合](https://baike.baidu.com/item/集合)。它是单个数据存储，出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。
>
> + [商业智能_百度百科 (baidu.com)](https://baike.baidu.com/item/商业智能/406141?fromtitle=BI&fromid=4579902&fr=aladdin)
>
>   商业智能（Business Intelligence，简称：BI），又称商业智慧或商务智能，指用现代[数据仓库技术](https://baike.baidu.com/item/数据仓库技术/10292797)、线上分析处理技术、数据挖掘和数据展现技术进行数据分析以实现商业价值。
>
>   商业智能的概念在1996年最早由[加特纳](https://baike.baidu.com/item/加特纳)集团（Gartner Group）提出，加特纳集团将商业智能定义为：商业智能描述了一系列的概念和方法，通过应用基于事实的支持系统来辅助商业决策的制定。商业智能技术提供使企业迅速分析数据的技术和方法，包括收集、管理和分析数据，将这些数据转化为有用的信息，然后分发到企业各处。
>
> + [联机分析处理_百度百科 (baidu.com)](https://baike.baidu.com/item/联机分析处理?fromtitle=OLAP&fromid=1049009)
>
>   联机分析处理OLAP是一种[软件技术](https://baike.baidu.com/item/软件技术/3313113)，它使分析人员能够迅速、一致、交互地从各个方面观察信息，以达到深入理解数据的目的。它具有FASMI(Fast Analysis of Shared Multidimensional Information)，即共享多维信息的快速分析的特征。其中F是快速性(Fast)，指系统能在数秒内对用户的多数分析要求做出反应；A是可分析性(Analysis)，指用户无需编程就可以定义新的专门计算，将其作为分析的一部 分，并以用户所希望的方式给出报告；M是多维性(Multi—dimensional)，指提供对[数据分析](https://baike.baidu.com/item/数据分析/6577123)的多维视图和分析；I是信息性(Information)，指能及时获得信息，并且管理大容量信息。

​	诸如ERP、CRM这类系统，可以看作<u>线下工作线上化</u>的过程，这也是IT时代的主要特征之一，通常我们把这类系统称为**联机事务处理（OLTP）**系统。

​	为了解决**数据孤岛**的问题，人们提出了**数据仓库**的概念。即通过引入一个专门用于分析类场景的数据库，将分散的数据统一汇聚到一处。借助数据仓库的概念，用户第一次拥有了站在企业全局鸟瞰一切数据的视角。

​	随着这个概念被进一步完善，<u>一类统一面向数据仓库，专注于提供数据分析、决策类功能的系统</u>与解决方案应运而生。最终于20世纪90年代，有人第一次提出了**BI（商业智能）**系统的概念。<u>自此以后，人们通常用BI一词指代这类分析系统</u>。相对于联机事务处理系统，我们把这类BI系统称为**联机分析（OLAP）系统**。

​	**传统BI系统**的设计初衷虽然很好，但实际的应用效果却不能完全令人满意。

+ 传统BI系统<u>对企业的信息化水平要求较高</u>。按照传统BI系统的设计思路，通常只有中大型企业才有能力实施。因为它的定位是站在企业视角通盘分析并辅助决策的，所以如果企业的信息化水平不高，基础设施不完善，想要实施BI系统根本无从谈起。这已然把许多潜在用户挡在了门外。
+ 狭小的受众制约了传统BI系统发展的生命力。传统BI系统的<u>主要受众是企业中的管理层或决策层</u>。这类用户虽然通常具有较高的话语权和决策权，但用户基数相对较小。同时他们对系统的参与度和使用度不高，久而久之这类所谓的BI系统就沦为了领导视察、演示的“特供系统”了。

+ <u>冗长的研发过程</u>滞后了需求的响应时效。传统BI系统需要大量IT人员的参与，用户的任何想法，哪怕小到只是想把一张用饼图展示的页面换成柱状图展示，可能都需要依靠IT人员来实现。一个分析需求从由用户大脑中产生到最终实现，可能需要几周甚至几个月的时间。这种严重的滞后性仿佛将人类带回到了飞鸽传书的古代。

## 1.2 现代BI系统的新思潮

> + [saas平台_百度百科 (baidu.com)](https://baike.baidu.com/item/saas平台/2147529?fr=aladdin)	=>	软件服务化(Software as a Service)
>
>   [SaaS](https://baike.baidu.com/item/SaaS)平台是运营saas软件的平台。SaaS提供商为企业搭建信息化所需要的所有[网络基础设施](https://baike.baidu.com/item/网络基础设施/5183560)及软件、硬件运作平台，并负责所有前期的实施、后期的维护等一系列服务，企业无需购买软硬件、建设[机房](https://baike.baidu.com/item/机房/5066792)、招聘IT人员，即可通过[互联网](https://baike.baidu.com/item/互联网/199186)使用信息系统。SaaS 是一种软件布局模型，其应用专为[网络](https://baike.baidu.com/item/网络/143243)交付而设计，便于用户通过互联网托管、部署及接入。
>
> + [数据立方_百度百科 (baidu.com)](https://baike.baidu.com/item/数据立方/13237096?fromtitle=数据立方体&fromid=9851963&fr=aladdin)
>
>   我们以B+树的结构建立了字段的索引，每个B+树结构的字段索引相当于一个数据平面，这样一个全局数据表与其多个重要字段的索引就组成了一个类似于立方体的数据组织结构，我们称之为“ 数据立方(DataCube)”。

​	SaaS模式的兴起，为传统企业软件系统的商业模式带来了新的思路，这是一次新的技术普惠。一方面，<u>SaaS模式将之前只服务于中大型企业的软件系统放到了互联网，扩展了它的受众；另一方面，由于互联网用户的基本特征和软件诉求，又倒逼了这些软件系统在方方面面进行革新与升级</u>。

​	技术普惠，导致现代BI系统在设计思路上发生了天翻地覆的变化。

+ 变轻：无需强制捆绑企业级数据仓库，仅Excel也可进行数据分析
+ 多元化：简单图形界面操作，无需IT专业人士操作
+ 用户体验更好：虽然仍私有化部署在企业内部，但是应答更快、操作更简单。

​	如果说SaaS化这波技术普惠为现代BI系统带来了新的思路与契机，那么背后的技术创新则保障了其思想的落地。**在传统BI系统的体系中，背后是传统的关系型数据库技术（OLTP数据库）**。

​	为了能够解决海量数据下分析查询的性能问题，人们绞尽脑汁，在数据仓库的基础上衍生出众多概念，例如：对数据进行分层，通过层层递进形成数据集市，从而减少最终查询的数据体量；提出**数据立方体**的概念，通过对数据进行预先处理，以空间换时间，提升查询性能。然而无论如何努力，设计的局限始终是无法突破的瓶颈。

​	<u>OLTP技术由诞生的那一刻起就注定不是为数据分析而生的</u>，于是很多人将目光投向了新的方向。

​	2003年起，Google陆续发表的三篇论文开启了大数据的技术普惠，Hadoop生态由此开始一发不可收拾，数据分析开启了新纪元。<u>从某种角度来看，以使用Hadoop生态为代表的这类非传统关系型数据库技术所实现的BI系统，可以称为现代BI系统</u>。换装了大马力发动机的现代BI系统在面对海量数据分析的场景时，显得更加游刃有余。然而Hadoop技术也不是银弹，在现代BI系统的构建中仍然面临诸多挑战。在海量数据下要实现多维分析的实时应答，仍旧困难重重。（现代BI系统的典型应用场景是**多维分析**，某些时候可以直接使用OLAP指代这类场景。）

## 1.3 OLAP常见架构分类

> [ClickHouse原理解析与应用实践-朱凯-微信读书 (qq.com)](https://weread.qq.com/web/reader/03532e3071e24e90035375dk6f4322302126f4922f45dec)	=>	图文来源

​	OLAP名为联机分析，又可以称为多维分析，是由关系型数据库之父埃德加·科德（Edgar Frank Codd）于1993年提出的概念。

​	为了更好地理解多维分析的概念，可以使用一个立方体的图像具象化操作。如下图所示，对于一张销售明细表，数据立方体可以进行如下操作。

+ 下钻：从高层次向低层次明细数据穿透。例如从“省”下钻到“市”，从“湖北省”穿透到“武汉”和“宜昌”。

+ 上卷：和下钻相反，从低层次向高层次汇聚。例如从“市”汇聚成“省”，将“武汉”“宜昌”汇聚成“湖北”。

+ 切片：观察立方体的一层，将一个或多个维度设为单个固定值，然后观察剩余的维度，例如将商品维度固定为“足球”。

+ 切块：与切片类似，只是将单个固定值变成多个值。例如将商品维度固定成“足球”“篮球”和“乒乓球”。

+ 旋转：旋转立方体的一面，如果要将数据映射到一张二维表，那么就要进行旋转，这就等同于行列置换。

![img](https://res.weread.qq.com/wrepub/epub_31608464_2)

为了实现上述这些操作，将常见的OLAP架构大致分成三类（ROLAP、MOLAP、HOLAP）。

+ ROLAP（Relational OLAP，关系型OLAP）

  顾名思义，它直接使用关系模型构建，数据模型常使用星型模型或者雪花模型。这是最先能够想到，也是最为直接的实现方法。因为<u>OLAP概念在最初提出的时候，就是建立在关系型数据库之上的</u>。**多维分析的操作，可以直接转换成SQL查询**。例如，通过上卷操作查看省份的销售额，就可以转换成类似下面的SQL语句：

  `SELECT SUM（价格）FROM 销售数据表 GROUP BY 省`

+ MOLAP（Multidimensional OLAP，多维型OLAP）

  它的出现是为了**缓解ROLAP性能问题**。MOLAP使用多维数组的形式保存数据，其核心思想是借助预先聚合结果，使用空间换取时间的形式最终提升查询性能。也就是说，用更多的存储空间换得查询时间的减少。**其具体的实现方式是依托立方体模型的概念**。

  **首先，对需要分析的数据进行建模，框定需要分析的维度字段；然后，通过预处理的形式，对各种维度进行组合并事先聚合；最后，将聚合结果以某种索引或者缓存的形式保存起来（通常只保留聚合后的结果，不存储明细数据）**。这样一来，在随后的查询过程中，就可以直接利用结果返回数据。

  但是这种架构也并不完美。**维度预处理可能会导致数据的膨胀**。

  这里可以做一次简单的计算，以图1-1中所示的销售明细表为例。如果数据立方体包含了5个维度（字段），那么维度组合的方式则有2<sup>5</sup> （2<sup>n</sup> ，n=维度个数）个。例如，省和市两个维度的组合就有<湖北，武汉>、<湖北、宜昌>、<武汉、湖北>、<宜昌、湖北>等。可想而知，当维度数据基数较高的时候，（高基数意味着重复相同的数据少。）其立方体预聚合后的数据量可能会达到10到20倍的膨胀。一张千万级别的数据表，就有可能膨胀到亿级别的体量。人们意识到这个问题之后，虽然也实现了一些能够降低膨胀率的优化手段，但并不能完全避免。

  另外，由于**使用了预处理的形式，数据立方体会有一定的滞后性，不能实时进行数据分析**。而且，**立方体只保留了聚合后的结果数据，导致无法查询明细数据**。

+ HOLAP（Hybrid OLAP，混合架构的OLAP）

  这种思路可以理解成ROLAP和MOLAP两者的集成。这里不再展开，我们重点关注ROLAP和MOLAP。

## 1.4 OLAP实现技术的演进

​	在介绍了OLAP几种主要的架构之后，再来看看它们背后技术的演进过程。我把这个演进过程简单划分成两个阶段。

1. 传统关系型数据库阶段

   在这个阶段中，OLAP主要基于以Oracle、MySQL为代表的一众关系型数据实现。

   + **在ROLAP架构下，直接使用这些数据库作为存储与计算的载体**；
   + **在MOLAP架构下，则借助物化视图的形式实现数据立方体**。

   在这个时期，不论是ROLAP还是MOLAP，在数据体量大、维度数目多的情况下都存在严重的性能问题，甚至存在根本查询不出结果的情况。

2. 大数据技术阶段

   由于大数据处理技术的普及，人们开始使用大数据技术重构ROLAP和MOLAP。

   + 以ROLAP架构为例，传统关系型数据库就被Hive和SparkSQL这类新兴技术所取代。虽然，以Spark为代表的分布式计算系统，相比Oracle这类传统数据库而言，在面向海量数据的处理性能方面已经优秀很多，但是直接把它们作为面向终端用户的在线查询系统还是太慢了。我们的用户普遍缺乏耐心，如果一个查询响应需要<u>几十秒甚至数分钟</u>才能返回，那这套方案就完全行不通。
   + 再看MOLAP架构，MOLAP背后也转为依托MapReduce或Spark这类新兴技术，将其作为立方体的计算引擎，加速立方体的构建过程。其预聚合结果的存储载体也转向HBase这类高性能分布式数据库。大数据技术阶段，主流MOLAP架构已经能够在亿万级数据的体量下，实现<u>毫秒级</u>的查询响应时间。尽管如此，**MOLAP架构依然存在维度爆炸、数据同步实时性不高的问题**。

​	不难发现，虽然OLAP在经历了大数据技术的洗礼之后，其各方面性能已经有了脱胎换骨式的改观，但不论是ROLAP还是MOLAP，仍然存在各自的痛点。

​	如果单纯从模型角度考虑，很明显ROLAP架构更胜一筹。因为关系模型拥有最好的“群众基础”，也更简单且容易理解。它直接面向明细数据查询，由于不需要预处理，也就自然没有预处理带来的负面影响（维度组合爆炸、数据实时性、更新问题）。那是否存在这样一种技术，它既使用ROLAP模型，同时又拥有比肩MOLAP的性能呢？

## 1.5 一匹横空出世的黑马

​	上文曾提及，以Spark为代表的新一代ROLAP方案虽然可以一站式处理海量数据，但<u>无法真正做到实时应答和高并发</u>，它更适合作为一个后端的查询系统。而<u>新一代的MOLAP方案虽然解决了大部分查询性能的瓶颈问题，能够做到实时应答，但数据膨胀和预处理等问题依然没有被很好解决</u>。除了上述两类方案之外，也有一种另辟蹊径的选择，即摒弃ROLAP和MOALP转而<u>使用搜索引擎来实现OLAP查询，ElasticSearch是这类方案的代表</u>。<u>ElasticSearch支持实时更新，在**百万级**别数据的场景下可以做到实时聚合查询</u>，但是随着数据体量的继续增大，它的查询性能也将捉襟见肘。

​	难道真的是鱼与熊掌不可兼得了吗？直到有一天，在查阅一份Spark性能报告的时候，我不经意间看到了一篇性能对比的博文。Spark的对手是一个我从来没有见过的陌生名字，在10亿条测试数据的体量下，Spark这个我心目中的绝对王者，居然被对手打得落花流水，查询响应时间竟然比对手慢数90%之多。而对手居然只使用了一台配有i5 CPU、16GB内存和SSD磁盘的普通PC电脑。我揉了揉眼睛，定了定神，这不是做梦。ClickHouse就这样进入了我的视野。

### 1.5.1 天下武功唯快不破

​	我对ClickHouse的最初印象极为深刻，其具有**ROLAP**、**在线实时查询**、**完整的DBMS**、**列式存储**、**不需要任何数据预处理**、**支持批量更新**、**拥有非常完善的SQL支持和函数**、**支持高可用**、**不依赖Hadoop复杂生态**、**开箱即用**等许多特点。特别是它那夸张的查询性能，我想大多数刚接触ClickHouse的人也一定会因为它的性能指标而动容。在一系列官方公布的基准测试对比中，ClickHouse都遥遥领先对手，这其中不乏一些我们耳熟能详的名字。

​	所有用于对比的数据库都使用了相同配置的服务器，在单个节点的情况下，对一张拥有133个字段的数据表分别在1000万、1亿和10亿三种数据体量下执行基准测试，基准测试的范围涵盖43项SQL查询。在1亿数据集体量的情况下，ClickHouse的平均响应速度是Vertica的2.63倍、InfiniDB的17倍、MonetDB的27倍、Hive的126倍、MySQL的429倍以及Greenplum的10倍。详细的测试结果可以查阅[Performance comparison of database management systems (clickhouse.tech)](https://clickhouse.tech/benchmark/dbms/)。

### 1.5.2 社区活跃

​	基本保持着每个月发布一次版本的更新频率。

## 1.6 ClickHouse的发展历程

### 1.6.1 顺理成章的MySQL时期

​	作为一款在线流量分析产品，对其功能的要求自然是分析流量了。早期的Yandex.Metrica以提供固定报表的形式帮助用户进行分析，例如分析访问者使用的设备、访问者来源的分布之类。其实这也是早期分析类产品的典型特征之一，分析维度和场景是固定的，新的分析需求往往需要IT人员参与。

​	<u>从技术角度来看，当时还处于关系型数据库称霸的时期，所以Yandex在内部其他产品中使用了MySQL数据库作为统计信息系统的底层存储软件</u>。Yandex.Metrica的第一版架构顺理成章延续了这套内部稳定成熟的MySQL方案，并将其作为它的数据存储和分析引擎的解决方案。

​	因为Yandex内部的这套MySQL方案<u>使用了MyISAM表引擎</u>，所以Yandex.Metrica也延续了表引擎的选择。这类分析场景更关注数据写入和查询的性能，不关心事务操作（MyISAM表引擎不支持事务特性）。<u>相比InnoDB表引擎，MyISAM表引擎在分析场景中具有更好的性能</u>。

​	**业内有一个常识性的认知，按顺序存储的数据会拥有更高的查询性能**。<u>因为读取顺序文件会用更少的磁盘寻道和旋转延迟时间（这里主要指机械磁盘），同时顺序读取也能利用操作系统层面文件缓存的预读功能，所以数据库的查询性能与数据在物理磁盘上的存储顺序息息相关。然而这套MySQL方案无法做到顺序存储</u>。

​	MyISAM表引擎使用B+树结构存储索引，而数据则使用另外单独的存储文件（InnoDB表引擎使用B+树同时存储索引和数据，数据直接挂载在叶子节点中）。如果只考虑单线程的写入场景，并且在写入过程中不涉及数据删除或者更新操作，那么数据会依次按照写入的顺序被写入文件并落至磁盘。然而现实的场景不可能如此简单。

​	流量的数据采集链路是这样的：<u>网站端的应用程序首先通过Yandex提供的站点SDK实时采集数据并发送到远端的接收系统，再由接收系统将数据写入MySQL集群</u>。整个过程都是实时进行的，并且数据接收系统是一个**分布式**系统，所以它们会**并行、随机**将数据写入MySQL集群。这最终导致了数据在磁盘中是完全随机存储的，并且会**产生大量的磁盘碎片**。

​	市面上一块典型的7200转SATA磁盘的IOPS（每秒能处理的请求数）仅为100左右，也就是说每秒只能执行100次随机读取。假设一次随机读取返回10行数据，那么查询100000行记录则需要至少100秒，这种响应时间显然是不可接受的。

​	**RAID可以提高磁盘IOPS性能，但并不能解决根本问题。SSD随机读取性能很高，但是考虑到硬件成本和集群规模，不可能全部采取SSD存储**。

​	随着时间的推移，MySQL中的数据越来越多（截至2011年，存储的数据超过5800亿行）。虽然Yandex又额外做了许多优化，成功地将90%的分析报告控制在26秒内返回，但是这套技术方案越来越显得力不从心。

### 1.6.2 * 另辟蹊径的Metrage时期

> + [LSM 算法的原理是什么？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/19887265)
>
> + [Log Structured Merge Trees(LSM) 原理 - LSM - 软件开发 - 深度开源 (open-open.com)](https://www.open-open.com/lib/view/open1424916275249.html)
>
>   + LSM比传统B+树、ISAM有更好的写操作吞吐量（通过消去随机的本地更新操作来实现该目标）
>
>   + 传统注重写性能的方法，即**文件存储**（顺序写代替随机写：kafka等），但是读数据前需要维护**倒排索引**来加速读过程
>
>   + 满足复杂读场景（性能好）的传统方案：
>
>     + 二分查找: 将文件数据有序保存，使用二分查找来完成特定key的查找。
>     + 哈希：用哈希将数据分割为不同的bucket
>     + B+树：使用B+树 或者 ISAM 等方法，可以减少外部文件的读取
>     + 外部文件： 将数据保存为日志，并创建一个hash或者查找树映射相应的文件
>
>     上诉方案能够提高读性能，但是丢失了文件系统的写性能。
>
>     并且维护Hash、B+树时，时常需要更新文件系统中的特定部分（随机读写），这类操作较慢，也是我们需要尽量减少的。
>
>   所以这就是 LSM 被发明的原理， <u>LSM 使用一种不同于上述四种的方法，保持了日志文件写性能，以及微小的读操作性能损失。**本质上就是让所有的操作顺序化，而不是像散弹枪一样随机读写**</u>。
>
>   + 其他具体的描述看原文.....
>
>   + 关于LSM的思考：
>
>     + LSM有更好的写性能，同时LSM还有其它一些好处。 
>
>       **sstable文件是不可修改的，这让对他们的锁操作非常简单**。一般来说，唯一的竞争资源就是 memtable，相对来说需要相对复杂的锁机制来管理在不同的级别。
>
>     + 所以最后的问题很可能是**以写为导向的压力预期**如何。
>
>       如果你对LSM带来的写性能的提高很敏感，这将会很重要。大型互联网企业似乎很看中这个问题。 <u>Yahoo 提出因为事件日志的增加和手机数据的增加，工作场景为从 read-heavy 到 read-write。许多传统数据库产品似乎更青睐读优化文件结构。</u>
>
>     + 因为可用的内存的增加，通过操作系统提供的大文件缓存，读操作自然会被优化。
>
>       **写性能（内存不可提高）因此变成了主要的关注点**，所以采取其它的方法，<u>硬件提升为读性能做的更多，相对于写来说。因此选择一个**写优化的文件结构**很有意义</u>。
>
>     理所当然的，LSM的实现，像LevelDB和Cassandra提供了更好的写性能，相对于单树结构的策略。
>
>   + 上述文章内的**总结**（原文自带的总结，非原创）：
>
>     ​	所以， LSM 是日志和传统的单文件索引（B+ tree，Hash Index）的中立，他提供一个机制来管理更小的独立的索引文件(sstable)。
>
>     ​	**通过管理一组索引文件而不是单一的索引文件，LSM 将B+树等结构昂贵的随机IO变的更快，而代价就是读操作要处理大量的索引文件(sstable)而不是一个，另外还是一些IO被合并操作消耗。**
>
>     ​	如果还有不明白的，这还有一些其它的好的介绍。 [here](https://www.open-open.com/misc/goto?guid=4959627134870208906) and [here](https://www.open-open.com/misc/goto?guid=4958194564427775209)
>
> + [LSM、B 树、B+树、B*对比 - 简书 (jianshu.com)](https://www.jianshu.com/p/3fb899684392) => 图文并茂，推荐阅读（<= 下面文字来自原文）
>
>   ​	LSM树（Log-Structured MergeTree）存储引擎和B+树存储引擎一样，同样支持增、删、读、改、顺序扫描操作。而且通过批量存储技术规避磁盘随机写入问题。当然凡事有利有弊，LSM树和B+树相比，LSM树牺牲了部分读性能，用来大幅提高写性能。
>
>   + LSM树核心思想的核心就是放弃部分读能力，换取写入的最大化能力。LSM Tree ，这个概念就是结构化合并树的意思，它的核心思路其实非常简单，就是假定内存足够大，因此不需要每次有数据更新就必须将数据写入到磁盘中，而可以先将最新的数据驻留在内存中，等到积累到足够多之后，再使用归并排序的方式将内存内的数据合并追加到磁盘队尾(因为所有待排序的树都是有序的，可以通过合并排序的方式快速合并到一起)。
>
>   + 日志结构的合并树（LSM-tree）是一种基于硬盘的数据结构，与B+tree相比，能显著地减少硬盘磁盘臂的开销，并能在较长的时间提供对文件的高速插入（删除）。**然而LSM-tree在某些情况下，特别是在查询需要快速响应时性能不佳。**通常LSM-tree适用于索引插入比检索更频繁的应用系统。
>
>   + LSM树和B+树的差异主要在于读性能和写性能进行权衡。在牺牲的同时寻找其余补救方案：
>
>     1. LSM具有批量特性，存储延迟。当写读比例很大的时候（写比读多），LSM树相比于B树有更好的性能。因为**随着insert操作，为了维护B+树结构，节点分裂。读磁盘的随机读写概率会变大，性能会逐渐减弱**。
>
>     2. B树的写入过程：对B树的写入过程是一次原位写入的过程，主要分为两个部分，首先是查找到对应的块的位置，然后将新数据写入到刚才查找到的数据块中，然后再查找到块所对应的磁盘物理位置，将数据写入去。当然，在内存比较充足的时候，因为B树的一部分可以被缓存在内存中，所以查找块的过程有一定概率可以在内存内完成，不过为了表述清晰，我们就假定内存很小，只够存一个B树块大小的数据吧。可以看到，在上面的模式中，需要两次随机寻道（一次查找，一次原位写），才能够完成一次数据的写入，代价还是很高的。
>
>     3. LSM优化方式：
>
>        a. **Bloom filter**: 就是个带随机概率的bitmap,可以快速的告诉你，某一个小的有序结构里有没有指定的那个数据的。于是就可以不用二分查找，而只需简单的计算几次就能知道数据是否在某个小集合里啦。效率得到了提升，但付出的是空间代价。
>         b. compact:小树合并为大树:因为小树性能有问题，所以要有个进程不断地将小树合并到大树上，这样大部分的老数据查询也可以直接使用log2N的方式找到，不需要再进行(N/m)*log2n的查询了

​	由于MySQL带来的局限性，Yandex自研了一套全新的系统并命名为Metrage。Metrage在设计上与MySQL完全不同，它选择了另外一条截然不同的道路。

+ 首先，**在数据模型层面，它使用Key-Value模型（键值对）**代替了关系模型；
+ 其次，**在索引层面，它使用LSM树代替了B+树**；
+ 最后，**在数据处理层面，由实时查询的方式改为了预处理的方式**。 

​	LSM树也是一种非常流行的索引结构，发源于Google的BigTable，**现在最具代表性的使用LSM树索引结构的系统是HBase**。

​	LSM本质上可以看作将原本的一棵大树拆成了许多棵小树，每一批次写入的数据都会经历如下过程。

+ 首先，会在内存中构建出一棵小树，构建完毕即算写入成功（这里会通过**预写日志**的形式，防止因内存故障而导致的数据丢失）。写入动作只发生在内存中，不涉及磁盘操作，所以极大地提升了数据写入性能。
+ 其次，**小树在构建的过程中会进行排序**，这样就保证了数据的有序性。
+ 最后，当内存中小树的数量达到某个阈值时，就会借助后台线程将小树刷入磁盘并生成一个小的数据段。**在每个数据段中，数据局部有序**。也正因为数据有序，所以能够进一步使用稀疏索引来优化查询性能。

​	借助LSM树索引，可使得Metrage引擎在软硬件层面同时得到优化（**磁盘顺序读取、预读缓存、稀疏索引**等），最终有效提高系统的综合性能。

​	如果仅拥有索引结构的优化，还不足以从根本上解决性能问题。Metrage设计的第二个重大转变是通过**预处理**的方式，<u>将需要分析的数据预先聚合</u>。这种做法类似数据立方体的思想，

+ 首先对分析的具体场景实施立方体建模，框定所需的维度和度量以形成数据立方体；
+ 接着预先计算立方体内的所有维度组合；
+ 最后将聚合的结果数据按照Key-Value的形式存储。

​	这样一来，对于固定分析场景，就可以直接利用数据立方体的聚合结果立即返回相关数据。这套系统的实现思路和现今的一些MOLAP系统如出一辙。

​	通过上述一系列的转变，Metrage为Yandex.Metrica的性能带来了革命性提升。截至2015年，在Metrage内存储了超过3万亿行的数据，其集群规模超过了60台服务器，查询性能也由先前的26秒降低到了惊人的1秒以内。

​	**然而，使用立方体这类预先聚合的思路会带来一个新的问题，那就是维度组合爆炸，因为需要预先对所有的维度组合进行计算**。

​	那么**维度组合的方式具体有多少种呢？它的计算公式是2<sup>N</sup> （N=维度数量）**。

​	可以做一次简单的计算，例如5个维度的组合方式会有2<sup>5</sup> =32种，而9个维度的组合方式则会多达2<sup>9</sup> =512种，这是一种指数级的增长方式。**维度组合的爆炸会直接导致数据膨胀，有时候这种膨胀可能会多达10～20倍**。

### 1.6.3 * 自我突破的OLAPServer时期

​	<small>如果说Metrage系统是弥补Yandex.Metrica性能瓶颈的产物，那么OLAPServer系统的诞生，则是产品形态升级倒逼的结果。在<u>Yandex.Metrica的产品初期，它只支持**固定报表的分析功能**</u>。随着时间的推移，这种固定化的分析形式早已不能满足用户的诉求，于是Yandex.Metrica计划推出自定义分析报告的功能。然而Metrage系统却无法满足这类自定义的分析场景，因为它需要预先聚合，并且只提供了内置的40多种固定分析场景。单独为每一个用户提供面向个人的预聚合功能显然是不切实际的。<u>在这种背景下，Yandex.Metrica的研发团队只有寻求自我突破，于是自主研发了OLAPServer系统</u>。</small>

​	OLAPServer系统被设计成**专门处理自定义报告**这类临时性分析需求的系统，与Metrage系统形成互补的关系。

​	结合之前两个阶段的建设经验，OLAPServer在设计思路上可以说是取众家之长。

+ 在**数据模型**方面，它又换回了**关系模型**，因为相比Key-Value模型，关系模型拥有更好的描述能力。使用SQL作为查询语言，也将会拥有更好的“群众基础”。
+ 而在存储结构和索引方面，它结合了MyISAM和LSM树最精华的部分。
  + 在**存储结构**上，它与MyISAM表引擎类似，**分为了索引文件和数据文件两个部分**。
  + 在**索引**方面，它并没有完全沿用LSM树，而是**使用了LSM树所使用到的稀疏索引**。
  + 在**数据文件的设计**上，则**沿用了LSM树中数据段的思想，即数据段内数据有序，借助稀疏索引定位数据段**。
  + 在有了上述基础之后，OLAPServer又进一步引入了**列式存储**的思想，**将索引文件和数据文件按照列字段的粒度进行了拆分，每个列字段各自独立存储，以此进一步减少数据读取的范围**。

​	虽然OLAPServer在实时聚合方面的性能相比MySQL有了质的飞跃，但从功能的完备性角度来看，OLAPServer还是差了一个量级。

​	**如果说MySQL可以称为数据库管理系统（DBMS），那么OLAPServer只能称为数据库**。

​	因为OLAPServer的定位只是和Metrage形成互补，所以它缺失了一些基本的功能。例如，**它只有一种数据类型，即固定长度的数值类型，且没有DBMS应有的基本管理功能（DDL查询等）**。

### 1.6.4 * 水到渠成的ClickHouse时代

​	现在，一个新的选择题摆在了Yandex.Metrica研发团队的面前，**实时聚合还是预先聚合**？

​	预先聚合方案在查询性能方面带来了质的提升，成功地将之前的报告查询时间从26秒降低到了1秒以内，但同时它也带来了新的难题。

1. 由于预先聚合**只能支持固定的分析场景**，所以它无法满足自定义分析的需求。
2. **维度组合爆炸会导致数据膨胀**，这样会造成不必要的计算和存储开销。因为用户并不一定会用到所有维度的组合，那些没有被用到的组合将会成为浪费的开销。

3. **流量数据是在线实时接收的，所以预聚合还需要考虑如何及时更新数据**。

​	经过这么一分析，预先聚合的方案看起来似乎也没有那么完美。这是否表示实时聚合的方案更优呢？实时聚合方案意味着一切查询都是动态、实时的，从用户发起查询的那一刻起，整个过程需要在一秒内完成并返回，而在这个查询过程的背后，可能会涉及数亿行数据的处理。如果做不到这么快的响应速度，那么这套方案就不可行，因为用户都讨厌等待。很显然，如果查询性可以得到保障，实时聚合会是一个更为简洁的架构。由于OLAPServer的成功使用经验，选择倾向于实时聚合这一方。

​	OLAPServer在查询性能方面并不比Metrage差太多，在查询的灵活性方面反而更胜一筹。于是Yandex.Metrica研发团队以OLAPServer为基础进一步完善，以实现一个完备的数据库管理系统（DBMS）为目标，最终打造出了ClickHouse，并于2016年开源。纵览Yandex.Metrica背后技术的发展历程，ClickHouse的出现似乎是一个水到渠成的结果。

​	ClickHouse的发展历程如表1-1所示。

​	表1-1 ClickHouse的发展历程

| 发展历程                 | OLAP架构                      | Yandex.Metrica产品形态 |
| ------------------------ | ----------------------------- | ---------------------- |
| 顺理成章的MySQL时期      | ROLAP                         | 固定报告               |
| 另辟蹊径的Metrage时期    | MOLAP                         | 固定报告               |
| 自我突破的OLAPServer时期 | HOLAP（Metrage + OLAPServer） | 自助报告               |
| 水到渠成的ClickHouse时代 | ROLAP                         | 自助报告               |

## 1.7 ClickHouse的名称含义

​	经过上一节的介绍，大家知道了ClickHouse由雏形发展至今一共经历了四个阶段。

​	它的初始设计目标是服务自己公司的一款名叫Yandex.Metrica的产品。Metrica是一款Web流量分析工具，基于前方探针采集行为数据，然后进行一系列的数据分析，类似数据仓库的OLAP分析。而<u>在采集数据的过程中，一次页面click（点击），会产生一个event（事件）</u>。

​	至此，整个系统的逻辑就十分清晰了，那就是**基于页面的点击事件流，面向数据仓库进行OLAP分析**。所以**ClickHouse的全称是Click Stream，Data WareHouse，简称ClickHouse**。

## 1.8 ClickHouse适用的场景

​	因为ClickHouse在诞生之初是为了服务Yandex自家的Web流量分析产品Yandex.Metrica，所以**在存储数据超过20万亿行的情况下，ClickHouse做到了90%的查询都能够在1秒内返回的惊人之举**。随后，ClickHouse进一步被应用到Yandex内部大大小小数十个其他的分析场景中。可以说ClickHouse具备了人们对一款高性能OLAP数据库的美好向往，所以它基本能够胜任各种数据分析类的场景，并且随着数据体量的增大，它的优势也会变得越为明显。

​	**ClickHouse非常适用于商业智能领域（也就是我们所说的BI领域），除此之外，它也能够被广泛应用于广告流量、Web、App流量、电信、金融、电子商务、信息安全、网络游戏、物联网等众多其他领域**。

## 1.9 ClickHouse不适用的场景

​	ClickHouse作为一款高性能OLAP数据库，虽然足够优秀，但也不是万能的。我们不应该把它用于任何OLTP事务性操作的场景，因为它有以下几点不足。

+ 不支持事务
+ 不擅长根据主键按行粒度进行查询（虽然支持），故**不应该把ClickHouse当作Key-Value数据库使用**
+ 不擅长按行删除数据（虽然支持）

​	这些弱点并不能视为ClickHouse的缺点，事实上其他同类高性能的OLAP数据库同样也不擅长上述的这些方面。因为对于一款OLAP数据库而言，上述这些能力并不是重点，只能说这是为了极致查询性能所做的权衡。

## 1.10 有谁在使用ClickHouse

​	除了Yandex自己以外，ClickHouse还被众多商业公司或研究组织成功地运用到了它们的生产环境。

+ 欧洲核子研究中心（CERN）将它用于保存强对撞机试验后记录下的数十亿事件的测量数据，并成功将先前查找数据的时间由几个小时缩短到几秒。
+ 著名的CDN服务厂商CloudFlare将ClickHouse用于HTTP的流量分析。
+ 国内的头条、阿里、腾讯和新浪等一众互联网公司对ClickHouse也都有涉猎。

> 更多详情可以参考ClickHouse官网的案例介绍（https://clickhouse.yandex/ ）。

## 1.11 本章小结

​	本章开宗明义，介绍了作为一线从业者的我（本书作者）在经历BI系统从传统转向现代的过程中的所思所想，以及如何在机缘巧合之下发现了令人印象深刻的ClickHouse。本章抽丝剥茧，揭开了ClickHouse诞生的缘由。

​	原来ClickHouse背后的研发团队是来自俄罗斯的Yandex公司，Yandex为了支撑Web流量分析产品Yandex.Metrica，在历经MySQL、Metrage和OLAPServer三种架构之后，集众家之所长，打造出了ClickHouse。

​	在下一章，我们将进一步详细介绍ClickHouse的架构，对它的核心特点进行深入探讨。

# 2. ClickHouse架构概述

> + MPP架构（Massively Parallel Processing）
>
>   + [MPP(大规模并行处理)简介_Mr_不想起床的博客-CSDN博客_mpp架构](https://blog.csdn.net/qq_42189083/article/details/80610092) => 图文并茂，推荐阅读
>
>     1. 低硬件成本：完全使用 x86 架构的 PC Server，不需要昂贵的 Unix 服务器和磁盘阵列；
>     2. **集群架构与部署：完全并行的 MPP + Shared Nothing 的分布式架构，采用 Non-Master 部署，节点对等的扁平结构；**
>     3. 海量数据分布压缩存储：可处理 PB 级别以上的结构化数据，采用 hash分布、random 存储策略进行数据存储；同时采用先进的**压缩**算法，减少存储数据所需的空间，可以将所用空间减少 1~20 倍，并相应地提高 I/O 性能；
>     4. 数据加载高效性：基于策略的数据加载模式，集群整体加载速度可达2TB/h；
>     5. 高扩展、高可靠：支持集群节点的扩容和缩容，支持全量、增量的备份/恢复;
>     6. 高可用、易维护：数据通过副本提供冗余保护，自动故障探测和管理，自动同步元数据和业务数据。提供图形化工具，以简化管理员对数据库的管理工作；
>     7. 高并发：读写不互斥，支持数据的边加载边查询，单个节点并发能力大于 300 用户；
>     8. 行列混合存储：提供行列混合存储方案，从而提高了列存数据库特殊查询场景的查询响应耗时；
>     9. 标准化：支持SQL92 标准，支持 C API、ODBC、JDBC、ADO.NET 等接口规范。
>
>     综合而言，Hadoop和MPP两种技术的特定和适用场景为：
>
>     + **Hadoop在处理非结构化和半结构化数据上具备优势，尤其适合海量数据批处理等应用要求**
>
>     + **MPP适合替代现有关系数据机构下的大数据处理，具有较高的效率**
>
>     ​	MPP适合多维度数据自助分析、数据集市等；Hadoop适合海量数据存储查询、批量数据ETL、机构化数据分析(日志分析、文本分析)等。
>
>     ​	由上述对比可预见未来大数据存储与处理趋势：MPPDB+Hadoop混搭使用，用MPP处理PB级别的、高质量的结构化数据，同时为应用提供丰富的SQL和事物支持能力；用Hadoop实现半结构化、非结构化数据处理。这样可以同时满足结构化、半结构化和非结构化数据的高效处理需求。
>
>   + [MPP架构是什么？看这一篇就行了。。_feiying-CSDN博客_mpp架构](https://blog.csdn.net/feiyingwang/article/details/100258668) => 硬件、操作系统层面分析，推荐阅读
>
>     + SMP- Symmetric Multi-Processor 对称多处理器结构
>     + NUMA -Non-Uniform Memory Access 非一致存储访问结构
>     + MPP -Massive-Parallel Processing 海量并行处理架构
>
>   + [MPP数据库简介 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/94938447)
>
> + 对等网络
>
>   + [对等网络 - WCF | Microsoft Docs](https://docs.microsoft.com/zh-cn/dotnet/framework/wcf/feature-details/peer-to-peer-networking)
>
>   + [对等网络（对等网络）_百度百科 (baidu.com)](https://baike.baidu.com/item/对等网络/5482934?fr=aladdin)
>
>     ​	在P2P网络环境中，彼此连接的多台计算机之间都处于对等的地位，各台计算机有相同的功能，无主从之分，一台计算机既可作为服务器，设定共享资源供网络中其他计算机所使用，又可以作为工作站，整个网络一般来说不依赖专用的集中服务器，也没有专用的工作站。网络中的每一台计算机既能充当网络服务的请求者，又对其它计算机的请求做出响应，提供资源、服务和内容。通常这些资源和服务包括：信息的共享和交换、计算资源（如CPU计算能力共享）、存储共享（如缓存和磁盘空间的使用）、网络共享、打印机共享等。
>
>     ​	对等网络是一种网络结构的思想。它与目前网络中占据主导地位的客户端/服务器（Client/Server）结构（也就是WWW所采用的结构方式）的一个本质区别是，整个网络结构中不存在中心节点（或中心服务器）。**在P2P结构中，每一个节点（peer）大都同时具有信息消费者、信息提供者和信息通讯等三方面的功能。从计算模式上来说，P2P打破了传统的Client/Server (C/S)模式，在网络中的每个节点的地位都是对等的。每个节点既充当服务器，为其他节点提供服务，同时也享用其他节点提供的服务**。

​	随着业务的迅猛增长，**Yandex.Metrica目前已经成为世界第三大Web流量分析平台，每天处理超过200亿个跟踪事件**。能够拥有如此惊人的体量，在它背后提供支撑的ClickHouse功不可没。**ClickHouse已经为Yandex.Metrica存储了超过20万亿行的数据，90%的自定义查询能够在1秒内返回，其集群规模也超过了400台服务器**。虽然ClickHouse起初只是为了Yandex.Metrica而研发的，但由于它出众的性能，目前也被广泛应用于Yandex内部其他数十个产品上。

​	初识ClickHouse的时候，我曾产生这样的感觉：它仿佛违背了物理定律，没有任何缺点，是一个不真实的存在。一款高性能、高可用OLAP数据库的一切诉求，ClickHouse似乎都能满足，这股神秘的气息引起了我极大的好奇。

​	但是刚从Hadoop生态转向ClickHouse的时候，我曾有诸多的不适应，因为它和我们往常使用的技术“性格”迥然不同。如果把数据库比作汽车，那么ClickHouse俨然就是一辆手动挡的赛车。它在很多方面不像其他系统那样高度自动化。

​	ClickHouse的一些概念也与我们通常的理解有所不同，特别是**在分片和副本方面，有些时候数据的分片甚至需要手动完成**。在进一步深入使用ClickHouse之后，我渐渐地理解了这些设计的目的。**某些看似不够自动化的设计，反过来却在使用中带来了极大的灵活性**。

​	**与Hadoop生态的其他数据库相比，ClickHouse更像一款“传统”MPP架构的数据库，它没有采用Hadoop生态中常用的主从架构，而是使用了多主对等网络结构，同时它也是基于关系模型的ROLAP方案**。

​	这一章就让我们抽丝剥茧，看看ClickHouse都有哪些核心特性。

## 2.1 ClickHouse的核心特性

### 2.1.1 完备的DBMS功能

​	ClickHouse拥有完备的管理功能，所以它称得上是一个DBMS（Database Management System，数据库管理系统），而不仅是一个数据库。作为一个DBMS，它具备了一些基本功能，如下所示。

+ DDL（数据定义语言）：可以动态地创建、修改或删除数据库、表和视图，而无须重启服务。

+ DML（数据操作语言）：可以动态查询、插入、修改或删除数据。

+ 权限控制：可以按照用户粒度设置数据库或者表的操作权限，保障数据的安全性。

+ 数据备份与恢复：提供了数据备份导出与导入恢复机制，满足生产环境的要求。

+ 分布式管理：提供集群模式，能够自动管理多个数据库节点。

​	这里只列举了一些最具代表性的功能，但已然足以表明为什么ClickHouse称得上是DBMS了。

### 2.1.2 列式存储与数据压缩

​	列式存储和数据压缩，对于一款高性能数据库来说是必不可少的特性。一个非常流行的观点认为，如果你想让查询变得更快，最简单且有效的方法是**减少数据扫描范围和数据传输时的大小**，而列式存储和数据压缩就可以帮助我们实现上述两点。**列式存储和数据压缩通常是伴生的，因为一般来说列式存储是数据压缩的前提**。

​	按列存储与按行存储相比，前者可以有效减少查询时所需扫描的数据量，这一点可以用一个示例简单说明。假设一张数据表A拥有50个字段A1～A50，以及100行数据。现在需要查询前5个字段并进行数据分析，则可以用如下SQL实现：

```sql
SELECT A1，A2，A3，A4，A5 FROM A
```

​	如果数据按行存储，数据库首先会逐行扫描，并获取每行数据的所有50个字段，再从每一行数据中返回A1～A5这5个字段。不难发现，尽管只需要前面的5个字段，但由于数据是按行进行组织的，实际上还是扫描了所有的字段。如果数据按列存储，就不会发生这样的问题。由于数据按列组织，数据库可以直接获取A1～A5这5列的数据，从而避免了多余的数据扫描。

​	按列存储相比按行存储的另一个优势是对数据压缩的友好性。同样可以用一个示例简单说明压缩的本质是什么。假设有两个字符串abcdefghi和bcdefghi，现在对它们进行压缩，如下所示：

```shell
压缩前：abcdefghi_bcdefghi
压缩后：abcdefghi_(9,8)
```

​	可以看到，压缩的本质是按照一定步长对数据进行匹配扫描，当发现重复部分的时候就进行编码转换。例如上述示例中的(9,8)，表示如果从下划线开始向前移动9个字节，会匹配到8个字节长度的重复项，即这里的bcdefghi。

​	真实的压缩算法自然比这个示例更为复杂，但压缩的实质就是如此。数据中的重复项越多，则压缩率越高；压缩率越高，则数据体量越小；而数据体量越小，则数据在网络中的传输越快，对网络带宽和磁盘IO的压力也就越小。既然如此，那**怎样的数据最可能具备重复的特性呢？答案是属于同一个列字段的数据，因为它们拥有相同的<u>数据类型</u>和<u>现实语义</u>，重复项的可能性自然就更高**。

​	**ClickHouse就是一款使用列式存储的数据库，数据按列进行组织，属于同一列的数据会被保存在一起，列与列之间也会由不同的文件分别保存（这里主要指MergeTree表引擎，表引擎会在后续章节详细介绍）**。数据默认使用LZ4算法压缩，<u>在Yandex.Metrica的生产环境中，数据总体的压缩比可以达到8:1</u>（未压缩前17PB，压缩后2PB）。**列式存储除了降低IO和存储的压力之外，还为向量化执行做好了铺垫**。

### 2.1.3 * 向量化执行引擎

> + [ClickHouse到底是什么？凭啥这么牛逼！]([ClickHouse到底是什么？凭啥这么牛逼！_芋艿V-CSDN博客](https://blog.csdn.net/github_38592071/article/details/117309063))	=>	下面图片出处，其实这个文章也就是书上内容，正好我笔记要图片。
>
> + [数据库计算引擎的优化技术：向量化执行与代码生成 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/100933389)
>
>   **向量化执行**
>
>   向量化执行的思想就是**均摊开销**：
>
>   假设每次通过 operator tree 生成一行结果的开销是 C 的话，经典模型的计算框架总开销就是 C * N，其中 N 为参与计算的总行数，如果把计算引擎每次生成一行数据的模型改为每次生成一批数据的话，因为每次调用的开销是相对恒定的，所以计算框架的总开销就可以减小到C * N / M，其中 M 是每批数据的行数，这样每一行的开销就减小为原来的 1 / M，当 M 比较大时，计算框架的开销就不会成为系统瓶颈了。
>
>   除此之外，向量化执行还能给 compiler 带来更多的优化空间，因为引入向量化之后，实际上是将原来数据库运行时的一个大 for 循环拆成了两层 for 循环，内层的 for 循环通常会比较简单，对编译器来说也存在更大的优化可能性。举例来说，对于一个实现两个 int 相加的 expression，在向量化之前，其实现可能是这样的：
>
>   ```cpp
>   class ExpressionIntAdd extends Expression {
>           Datum eval(Row input) {
>                   int left = input.getInt(leftIndex);
>                   int right = input.getInt(rightIndex);
>                   return new Datum(left+right);
>           }
>   }
>   ```
>
>   在向量化之后，其实现可能会变为这样：
>
>   ```cpp
>   class VectorExpressionIntAdd extends VectorExpression {
>           int[] eval(int[] left, int[] right) {
>                   int[] ret = new int[input.length];
>                   for(int i = 0; i < input.length; i++) {
>                           ret[i] = new Datum(left[i] + right[i]);
>                   }
>                   return ret;
>           }
>   }
>   ```
>
>   <u>显然对比向量化之前的版本，向量化之后的版本不再是每次只处理一条数据，而是每次能处理一批数据，而且这种向量化的计算模式在计算过程中也具有更好的数据局部性</u>。
>
> + [SIMD_百度百科 (baidu.com)](https://baike.baidu.com/item/SIMD/3412835?fr=aladdin)
>
>   SIMD全称Single Instruction Multiple Data，单指令多数据流，能够[复制](https://baike.baidu.com/item/复制/33067)多个[操作数](https://baike.baidu.com/item/操作数/7658270)，并把它们打包在大型[寄存器](https://baike.baidu.com/item/寄存器/187682)的一组[指令集](https://baike.baidu.com/item/指令集/238130)。
>
>   + 概念：以同步方式，在同一时间内执行同一条指令。
>
>   + 性能上优势：
>
>     以加法指令为例，
>
>     + 单指令单数据（[SISD](https://baike.baidu.com/item/SISD)）的CPU对加法指令译码后，执行部件先访问内存，取得第一个[操作数](https://baike.baidu.com/item/操作数)；之后再一次访问内存，取得第二个操作数；随后才能进行求和运算。
>     + 而在SIMD型的CPU中，指令译码后**几个执行部件同时访问内存**，一次性获得所有操作数进行运算。这个特点使SIMD特别适合于多媒体应用等数据密集型运算。
>
>     如：[AMD](https://baike.baidu.com/item/AMD)公司引以为豪的3D NOW! 技术实质就是SIMD，这使K6-2、雷鸟、毒龙处理器在音频解码、视频回放、3D游戏等应用中显示出优异的性能。
>
> + [SSE 4.2_百度百科 (baidu.com)](https://baike.baidu.com/item/SSE 4.2/138412?fromtitle=SSE4.2&fromid=4680786&fr=aladdin)
>
>   英特尔表示，采用SSE 4.2[指令集](https://baike.baidu.com/item/指令集)后，XML的解析速度最高是原来的3.8倍，而[指令周期](https://baike.baidu.com/item/指令周期)节省可以达到2.7倍。此外，在ATA领域，SSE 4.2指令集对于大规模数据集中处理和提高通信效率都会发挥应有的作用，这些对于企业IT应用显然是有帮助的。当然，SSE 4.2指令集只有在软件对其支持后才会生产效果，相信Nehalem-EP上市，相关的优化与升级届时就会出现。

​	坊间有句玩笑，即“能用钱解决的问题，千万别花时间”。而业界也有种调侃如出一辙，即“能升级硬件解决的问题，千万别优化程序”。有时候，你千辛万苦优化程序逻辑带来的性能提升，还不如直接升级硬件来得简单直接。这虽然只是一句玩笑不能当真，但硬件层面的优化确实是最直接、最高效的提升途径之一。**向量化执行就是这种方式的典型代表，这项寄存器硬件层面的特性，为上层应用程序的性能带来了指数级的提升**。

​	**向量化执行，可以简单地看作一项消除程序中循环的优化**。这里用一个形象的例子比喻。小胡经营了一家果汁店，虽然店里的鲜榨苹果汁深受大家喜爱，但客户总是抱怨制作果汁的速度太慢。小胡的店里只有一台榨汁机，每次他都会从篮子里拿出一个苹果，放到榨汁机内等待出汁。如果有8个客户，每个客户都点了一杯苹果汁，那么小胡需要重复循环8次上述的榨汁流程，才能榨出8杯苹果汁。如果制作一杯果汁需要5分钟，那么全部制作完毕则需要40分钟。为了提升果汁的制作速度，小胡想出了一个办法。他将榨汁机的数量从1台增加到了8台，这么一来，他就可以从篮子里一次性拿出8个苹果，分别放入8台榨汁机同时榨汁。此时，小胡只需要5分钟就能够制作出8杯苹果汁。为了制作n杯果汁，非向量化执行的方式是用1台榨汁机重复循环制作n次，而向量化执行的方式是用n台榨汁机只执行1次。

​	**为了实现向量化执行，需要利用CPU的SIMD指令**。SIMD的全称是Single Instruction Multiple Data，即用单条指令操作多条数据。现代计算机系统概念中，它是**通过数据并行以提高性能的一种实现方式**（其他的还有指令级并行和线程级并行），它的**原理是在CPU寄存器层面实现数据的并行操作**。

​	在计算机系统的体系结构中，存储系统是一种层次结构。典型服务器计算机的存储层次结构如图2-1所示。一个实用的经验告诉我们，存储媒介距离CPU越近，则访问数据的速度越快。

![img](https://img-blog.csdnimg.cn/img_convert/5944e32c6d7b4376b041bf8e1af52e6b.png)

​	从上图中可以看到，从左向右，**距离CPU越远，则数据的访问速度越慢**。从寄存器中访问数据的速度，是从内存访问数据速度的300倍，是从磁盘中访问数据速度的3000万倍。所以<u>利用CPU向量化执行的特性，对于程序的性能提升意义非凡</u>。

​	<u>ClickHouse目前利用SSE4.2指令集实现向量化执行</u>。

### 2.1.4 关系模型与SQL查询

​	相比HBase和Redis这类NoSQL数据库，**ClickHouse使用关系模型描述数据并提供了传统数据库的概念（数据库、表、视图和函数等）**。与此同时，**ClickHouse完全使用SQL作为查询语言（支持GROUP BY、ORDER BY、JOIN、IN等大部分标准SQL）**，这使得它平易近人，容易理解和学习。因为关系型数据库和SQL语言，可以说是软件领域发展至今应用最为广泛的技术之一，拥有极高的“群众基础”。也正因为ClickHouse提供了标准协议的SQL查询接口，使得现有的第三方分析可视化系统可以轻松与它集成对接。**在SQL解析方面，ClickHouse是大小写敏感的，这意味着SELECT a和SELECT A所代表的语义是不同的**。

​	关系模型相比文档和键值对等其他模型，拥有更好的描述能力，也能够更加清晰地表述实体间的关系。<u>更重要的是，在OLAP领域，已有的大量数据建模工作都是基于关系模型展开的（星型模型、雪花模型乃至宽表模型）</u>。**ClickHouse使用了关系模型，所以将构建在传统关系型数据库或数据仓库之上的系统迁移到ClickHouse的成本会变得更低，可以直接沿用之前的经验成果。**

### 2.1.5 多样化的表引擎

​	也许因为Yandex.Metrica的最初架构是基于MySQL实现的，所以在ClickHouse的设计中，能够察觉到一些MySQL的影子，表引擎的设计就是其中之一。与MySQL类似，ClickHouse也将存储部分进行了抽象，把存储引擎作为一层独立的接口。**截至本书完稿时，ClickHouse共拥有合并树、内存、文件、接口和其他6大类20多种表引擎。**其中每一种表引擎都有着各自的特点，用户可以根据实际业务场景的要求，选择合适的表引擎使用。

​	**通常而言，一个通用系统意味着更广泛的适用性，能够适应更多的场景。但通用的另一种解释是平庸，因为它无法在所有场景内都做到极致**。

​	在软件的世界中，并不会存在一个能够适用任何场景的通用系统，为了突出某项特性，势必会在别处有所取舍。其实世间万物都遵循着这样的道理，就像信天翁和蜂鸟，虽然都属于鸟类，但它们各自的特点却铸就了完全不同的体貌特征。信天翁擅长远距离飞行，环绕地球一周只需要1至2个月的时间。因为它能够长时间处于滑行状态，5天才需要扇动一次翅膀，心率能够保持在每分钟100至200次之间。而蜂鸟能够垂直悬停飞行，每秒可以挥动翅膀70～100次，飞行时的心率能够达到每分钟1000次。

​	如果用数据库的场景类比信天翁和蜂鸟的特点，那么信天翁代表的可能是<u>使用普通硬件就能实现高性能的设计思路，数据按粗粒度处理，通过批处理的方式执行</u>；而蜂鸟代表的可能是<u>按细粒度处理数据的设计思路，需要高性能硬件的支持</u>。

​	将表引擎独立设计的好处是显而易见的，通过特定的表引擎支撑特定的场景，十分灵活。对于简单的场景，可直接使用简单的引擎降低成本，而复杂的场景也有合适的选择。

### 2.1.6 * 多线程与分布式

> + <u>**计算移动比数据移动更加划算**</u>
> + [对ClickHouse分片和分区的简单理解 - 简书 (jianshu.com)](https://www.jianshu.com/p/178a01e0ae6e)
>   + **分区**是表的分区，具体的DDL操作关键词是 `PARTITION BY`，指的是一个表按照某一列数据（比如日期）进行分区，对应到最终的结果就是不同分区的数据会写入不同的文件中。
>   + **分片**复用了数据库的分区，相当于在原有的分区下，作为第二层分区， 是在不同节点/机器上的体现。

​	ClickHouse几乎具备现代化高性能数据库的所有典型特征，对于可以提升性能的手段可谓是一一用尽，对于多线程和分布式这类被广泛使用的技术，自然更是不在话下。

​	**如果说向量化执行是通过数据级并行的方式提升了性能，那么多线程处理就是通过线程级并行的方式实现了性能的提升**。

​	相比基于底层硬件实现的向量化执行SIMD，线程级并行通常由更高层次的软件层面控制。现代计算机系统早已普及了多处理器架构，所以现今市面上的服务器都具备良好的多核心多线程处理能力。**由于SIMD不适合用于带有较多分支判断的场景，ClickHouse也大量使用了多线程技术以实现提速，以此和向量化执行形成互补**。

​	如果一个篮子装不下所有的鸡蛋，那么就多用几个篮子来装，这就是分布式设计中分而治之的基本思想。同理，如果一台服务器性能吃紧，那么就利用多台服务的资源协同处理。为了实现这一目标，首先需要在数据层面实现数据的分布式。

​	因为**==在分布式领域，存在一条金科玉律——<u>计算移动比数据移动更加划算</u>==**。

​	**<u>在各服务器之间，通过网络传输数据的成本是高昂的，所以相比移动数据，更为聪明的做法是预先将数据分布到各台服务器，将数据的计算查询直接下推到数据所在的服务器</u>。**

​	**ClickHouse在数据存取方面，既支持分区（纵向扩展，利用多线程原理），也支持分片（横向扩展，利用分布式原理），可以说是将多线程和分布式的技术应用到了极致**。

![img](https://upload-images.jianshu.io/upload_images/21274876-eaf1f19566246fa1.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

### 2.1.7 * 多主架构

​	**HDFS、Spark、HBase和Elasticsearch这类分布式系统，都采用了Master-Slave主从架构，由一个管控节点作为Leader统筹全局。**

​	**而ClickHouse则采用Multi-Master多主架构，集群中的每个节点角色对等，客户端访问任意一个节点都能得到相同的效果。这种多主的架构有许多优势，例如对等的角色使系统架构变得更加简单，不用再区分主控节点、数据节点和计算节点，集群中的所有节点功能相同。所以它天然规避了单点故障的问题，非常适合用于多数据中心、异地多活的场景**。

### 2.1.8 在线查询

​	<small>ClickHouse经常会被拿来与其他的分析型数据库作对比，比如Vertica、SparkSQL、Hive和Elasticsearch等，它与这些数据库确实存在许多相似之处。例如，它们都可以支撑海量数据的查询场景，都拥有分布式架构，都支持列存、数据分片、计算下推等特性。这其实也侧面说明了ClickHouse在设计上确实吸取了各路奇技淫巧。与其他数据库相比，ClickHouse也拥有明显的优势。例如，Vertica这类商用软件价格高昂；SparkSQL与Hive这类系统无法保障90%的查询在1秒内返回，在大数据量下的复杂查询可能会需要分钟级的响应时间；而Elasticsearch这类搜索引擎在处理亿级数据聚合查询时则显得捉襟见肘。</small>

​	<small>正如ClickHouse的“广告词”所言，其他的开源系统太慢，商用的系统太贵，只有Clickouse在成本与性能之间做到了良好平衡，即又快又开源。ClickHouse当之无愧地阐释了“在线”二字的含义，即便是在复杂查询的场景下，它也能够做到极快响应，且无须对数据进行任何预处理加工。</small>

### 2.1.9 * 数据分片与分布式查询

​	数据分片是将数据进行横向切分，这是一种在面对海量数据的场景下，解决存储和查询瓶颈的有效手段，是一种分治思想的体现。ClickHouse支持分片，而分片则依赖集群。每个集群由1到多个分片组成，而每个分片则对应了ClickHouse的1个服务节点。**分片的数量上限取决于节点数量（1个分片只能对应1个服务节点）**。

​	ClickHouse并不像其他分布式系统那样，拥有高度自动化的分片功能。**ClickHouse提供了本地表（Local Table）与分布式表（Distributed Table）的概念。一张本地表等同于一份数据的分片。而分布式表本身不存储任何数据，它是本地表的访问代理，其作用类似分库中间件。借助分布式表，能够代理访问多个数据分片，从而实现分布式查询**。

​	这种设计类似数据库的分库和分表，十分灵活。例如在业务系统上线的初期，数据体量并不高，此时数据表并不需要多个分片。所以使用单个节点的本地表（单个数据分片）即可满足业务需求，待到业务增长、数据量增大的时候，再通过新增数据分片的方式分流数据，并通过分布式表实现分布式查询。这就好比一辆手动挡赛车，它将所有的选择权都交到了使用者的手中。

## 2.2 ClickHouse的架构设计

### 2.2.1 * Column与Field

> [ClickHouse到底是什么？凭啥这么牛逼！_芋艿V-CSDN博客](https://blog.csdn.net/github_38592071/article/details/117309063)	=>	图片出处。

​	**Column和Field是ClickHouse数据最基础的映射单元**。作为一款百分之百的列式存储数据库，**ClickHouse按列存储数据，内存中的一列数据由一个Column对象表示**。Column对象分为接口和实现两个部分，在IColumn接口对象中，定义了对数据进行各种关系运算的方法，例如插入数据的insertRangeFrom和insertFrom方法、用于分页的cut，以及用于过滤的filter方法等。而<u>这些方法的具体实现对象则根据数据类型的不同，由相应的对象实现，例如ColumnString、ColumnArray和ColumnTuple等</u>。

​	**在大多数场合，ClickHouse都会以整列的方式操作数据**，但凡事也有例外。**如果需要操作单个具体的数值（也就是单列中的一行数据），则需要使用Field对象，Field对象代表一个单值**。

​	**与Column对象的泛化设计思路不同，Field对象使用了聚合的设计模式。在Field对象内部聚合了Null、UInt64、String和Array等13种数据类型及相应的处理逻辑**。

![img](https://img-blog.csdnimg.cn/img_convert/a6eeb469aa1fb90577049c5cbdba7ce7.png)

### 2.2.2 * DataType

> ***泛化*是把特殊代码转换成通用目的代码的过程**

​	**数据的序列化和反序列化工作由DataType负责**。IDataType接口定义了许多正反序列化的方法，它们成对出现，例如serializeBinary和deserializeBinary、serializeTextJSON和deserializeTextJSON等，涵盖了常用的二进制、文本、JSON、XML、CSV和Protobuf等多种格式类型。IDataType也使用了泛化的设计模式，具体方法的实现逻辑由对应数据类型的实例承载，例如DataTypeString、DataTypeArray及DataTypeTuple等。

​	**DataType虽然负责序列化相关工作，但它并不直接负责数据的读取，而是转由从Column或Field对象获取**。

​	<u>在DataType的实现类中，聚合了相应数据类型的Column对象和Field对象</u>。例如，DataTypeString会引用字符串类型的ColumnString，而DataTypeArray则会引用数组类型的ColumnArray，以此类推。

### 2.2.3 * Block与Block流

​	**ClickHouse内部的数据操作是面向Block对象进行的，并且采用了流的形式**。虽然Column和Filed组成了数据的基本映射单元，但对应到实际操作，它们还缺少了一些必要的信息，比如数据的类型及列的名称。于是ClickHouse设计了Block对象，Block对象可以看作数据表的子集。

​	**Block对象的本质是由数据对象、数据类型和列名称组成的三元组，即Column、DataType及列名称字符串**。

​	Column提供了数据的读取能力，而DataType知道如何正反序列化，所以Block在这些对象的基础之上实现了进一步的抽象和封装，从而简化了整个使用的过程，仅通过Block对象就能完成一系列的数据操作。**在具体的实现过程中，Block并没有直接聚合Column和DataType对象，而是通过ColumnWithTypeAndName对象进行间接引用**。

​	有了Block对象这一层封装之后，对Block流的设计就是水到渠成的事情了。流操作有两组顶层接口：IBlockInputStream负责数据的读取和关系运算，IBlockOutputStream负责将数据输出到下一环节。**Block流也使用了泛化的设计模式，对数据的各种操作最终都会转换成其中一种流的实现**。IBlockInputStream接口定义了读取数据的若干个read虚方法，而具体的实现逻辑则交由它的实现类来填充。

​	IBlockInputStream接口总共有60多个实现类，它们涵盖了ClickHouse数据摄取的方方面面。这些实现类大致可以分为三类：

1. 第一类**用于处理数据定义的DDL操作**，例如DDLQueryStatusInputStream等；
2. 第二类**用于处理关系运算的相关操作**，例如LimitBlockInputStream、JoinBlockInputStream及AggregatingBlockInputStream等；
3. 第三类则是**与表引擎呼应**，每一种表引擎都拥有与之对应的BlockInputStream实现，例如MergeTreeBaseSelect-BlockInputStream（MergeTree表引擎）、TinyLogBlockInputStream（TinyLog表引擎）及KafkaBlockInputStream（Kafka表引擎）等。

​	IBlockOutputStream的设计与IBlockInputStream如出一辙。IBlockOutputStream接口同样也定义了若干写入数据的write虚方法。它的实现类比IBlockInputStream要少许多，一共只有20多种。这些实现类基本用于表引擎的相关处理，负责将数据写入下一环节或者最终目的地，例如MergeTreeBlockOutputStream、TinyLogBlockOutputStream及StorageFileBlock-OutputStream等。

### 2.2.4 Table

> + [AST-抽象语法树_百度百科 (baidu.com)](https://baike.baidu.com/item/抽象语法树/6129952?fr=aladdin)
>
>   ​	在[计算机科学](https://baike.baidu.com/item/计算机科学)中，抽象语法树（Abstract Syntax Tree，AST），或简称**[语法树](https://baike.baidu.com/item/语法树/7031301)**（Syntax tree），是[源代码](https://baike.baidu.com/item/源代码)[语法](https://baike.baidu.com/item/语法)结构的一种抽象表示。它以树状的形式表现[编程语言](https://baike.baidu.com/item/编程语言)的语法结构，树上的每个节点都表示源代码中的一种结构。

​	**在数据表的底层设计中并没有所谓的Table对象，它直接使用IStorage接口指代数据表**。

​	表引擎是ClickHouse的一个显著特性，不同的表引擎由不同的子类实现，例如IStorageSystemOneBlock（系统表）、StorageMergeTree（合并树表引擎）和StorageTinyLog（日志表引擎）等。IStorage接口定义了DDL（如ALTER、RENAME、OPTIMIZE和DROP等）、read和write方法，它们分别负责数据的定义、查询与写入。在数据查询时，IStorage负责根据AST查询语句的指示要求，返回指定列的原始数据。后续对数据的进一步加工、计算和过滤，则会统一交由Interpreter解释器对象处理。对Table发起的一次操作通常都会经历这样的过程，接收AST查询语句，根据AST返回指定列的数据，之后再将数据交由Interpreter做进一步处理。

### 2.2.5 Parser与Interpreter

​	**Parser和Interpreter是非常重要的两组接口：**

+ **Parser分析器负责创建AST对象；**
+ **而Interpreter解释器则负责解释AST，并进一步创建查询的执行管道**。

​	它们与IStorage一起，串联起了整个数据查询的过程。

​	Parser分析器可以将一条SQL语句以递归下降的方法解析成AST语法树的形式。不同的SQL语句，会经由不同的Parser实现类解析。例如，有负责解析DDL查询语句的ParserRenameQuery、ParserDropQuery和ParserAlterQuery解析器，也有负责解析INSERT语句的ParserInsertQuery解析器，还有负责SELECT语句的ParserSelectQuery等。

​	Interpreter解释器的作用就像Service服务层一样，起到串联整个查询过程的作用，它会根据解释器的类型，聚合它所需要的资源。首先它会解析AST对象；然后执行“业务逻辑”（例如分支判断、设置参数、调用接口等）；最终返回IBlock对象，以线程的形式建立起一个查询执行管道。

### 2.2.6 * Functions与Aggregate Functions

​	**ClickHouse主要提供两类函数——普通函数和聚合函数**。普通函数由IFunction接口定义，拥有数十种函数实现，例如FunctionFormatDateTime、FunctionSubstring等。除了一些常见的函数（诸如四则运算、日期转换等）之外，也不乏一些非常实用的函数，例如网址提取函数、IP地址脱敏函数等。

+ **普通函数是没有状态的，函数效果作用于每行数据之上**。
+ 当然，**在函数具体执行的过程中，并不会一行一行地运算，而是采用向量化的方式直接作用于一整列数据**。

​	**聚合函数由IAggregateFunction接口定义，相比无状态的普通函数，聚合函数是有状态的**。以COUNT聚合函数为例，其AggregateFunctionCount的状态使用整型UInt64记录。**聚合函数的状态支持序列化与反序列化，所以能够在分布式节点之间进行传输，以实现增量计算**。

### 2.2.7 * Cluster与Replication

​	**ClickHouse的集群由分片（Shard）组成，而每个分片又通过副本（Replica）组成**。这种分层的概念，在一些流行的分布式系统中十分普遍。例如，在Elasticsearch的概念中，一个索引由分片和副本组成，副本可以看作一种特殊的分片。如果一个索引由5个分片组成，副本的基数是1，那么这个索引一共会拥有10个分片（每1个分片对应1个副本）。

​	如果你用同样的思路来理解ClickHouse的分片，那么很可能会在这里栽个跟头。ClickHouse的某些设计总是显得独树一帜，而集群与分片就是其中之一。这里有几个与众不同的特性。

1. **ClickHouse的1个节点只能拥有1个分片，也就是说如果要实现1分片、1副本，则至少需要部署2个服务节点。**
2. **分片只是一个逻辑概念，其物理承载还是由副本承担的。**

​	代码清单2-1所示是ClickHouse的一份集群配置示例，从字面含义理解这份配置的语义，可以理解为自定义集群ch_cluster拥有1个shard（分片）和1个replica（副本），且该副本由10.37.129.6服务节点承载。

​	代码清单2-1 自定义集群ch_cluster的配置示例

```xml
<ch_cluster>
  <shard>
    <replica>
      <host>10.37.129.6</host>
      <port>9000</port>
    </replica>
  </shard>
</ch_cluster>
```

​	**从本质上看，这组1分片、1副本的配置在ClickHouse中只有1个物理副本，所以它正确的语义应该是1分片、0副本。分片更像是逻辑层的分组，在物理存储层面则统一使用副本代表分片和副本**。所以真正表示1分片、1副本语义的配置，应该改为1个分片和2个副本，如代码清单2-2所示。

​	代码清单2-2 1分片、1副本的集群配置

```xml
<ch_cluster>
  <shard>
    <replica>
      <host>10.37.129.6</host>
      <port>9000</port>
    </replica>
    <replica>
      <host>10.37.129.7</host>
      <port>9000</port>
    </replica>
  </shard>
</ch_cluster>
```

​	副本与分片将在第10章详细介绍。

## 2.3 ClickHouse为何如此之快

​	<small>很多用户心中一直会有这样的疑问，为什么ClickHouse这么快？前面的介绍对这个问题已经做出了科学合理的解释。比方说，因为ClickHouse是**列式存储数据库**，所以快；也因为ClickHouse使用了**向量化引擎**，所以快。这些解释都站得住脚，但是依然不能消除全部的疑问。因为这些技术并不是秘密，世面上有很多数据库同样使用了这些技术，但是依然没有ClickHouse这么快。所以我想从另外一个角度来探讨一番ClickHouse的秘诀到底是什么。</small>

​	<small>首先向各位读者抛出一个疑问：在设计软件架构的时候，做设计的原则应该是自顶向下地去设计，还是应该自下而上地去设计呢？在传统观念中，或者说在我的观念中，自然是自顶向下的设计，通常我们都被教导要做好顶层设计。而**ClickHouse的设计则采用了自下而上的方式**。ClickHouse的原型系统早在2008年就诞生了，在诞生之初它并没有宏伟的规划。相反它的目的很单纯，就是希望能以最快的速度进行GROUP BY查询和过滤。他们是如何实践自下而上设计的呢？</small>

### 2.3.1 着眼硬件，先想后做

​	首先从硬件功能层面着手设计，在设计伊始就至少需要想清楚如下几个问题。

+ 我们将要使用的硬件水平是怎样的？包括CPU、内存、硬盘、网络等。
+ 在这样的硬件上，我们需要达到怎样的性能？包括延迟、吞吐量等。
+ 我们准备使用怎样的数据结构？包括String、HashTable、Vector等。
+ 选择的这些数据结构，在我们的硬件上会如何工作？

​	如果能想清楚上面这些问题，那么在动手实现功能之前，就已经能够计算出粗略的性能了。所以，基于将硬件功效最大化的目的，ClickHouse会在内存中进行GROUP BY，并且使用HashTable装载数据。

​	<u>与此同时，他们非常在意CPU L3级别的缓存，因为一次L3的缓存失效会带来70～100ns的延迟。这意味着在单核CPU上，它会浪费4000万次/秒的运算；而在一个32线程的CPU上，则可能会浪费5亿次/秒的运算。所以别小看这些细节，一点一滴地将它们累加起来，数据是非常可观的。正因为注意了这些细节，所以ClickHouse在基准查询中能做到1.75亿次/秒的数据扫描性能</u>。

### 2.3.2 * 算法在前，抽象在后

​	常有人念叨：“有时候，选择比努力更重要。”确实，路线选错了再努力也是白搭。在ClickHouse的底层实现中，经常会面对一些重复的场景，例如字符串子串查询、数组排序、使用HashTable等。如何才能实现性能的最大化呢？算法的选择是重中之重。<u>以字符串为例，有一本专门讲解字符串搜索的书，名为“Handbook of Exact StringMatching Algorithms”，列举了35种常见的字符串搜索算法。各位猜一猜ClickHouse使用了其中的哪一种？答案是一种都没有。这是为什么呢？因为性能不够快</u>。

​	在字符串搜索方面，针对不同的场景，ClickHouse最终选择了这些算法：

+ 对于常量，使用Volnitsky算法；
+ 对于非常量，使用CPU的向量化执行SIMD，暴力优化；
+ 正则匹配使用re2和hyperscan算法。

​	**性能是算法选择的首要考量指标**。

### 2.3.3 勇于尝鲜，不行就换

​	除了字符串之外，其余的场景也与它类似，ClickHouse会**使用最合适、最快的算法**。如果世面上出现了号称性能强大的新算法，ClickHouse团队会立即将其纳入并进行验证。如果效果不错，就保留使用；如果性能不尽人意，就将其抛弃。

### 2.3.4 特定场景，特殊优化

​	针对同一个场景的不同状况，选择使用不同的实现方式，尽可能将性能最大化。关于这一点，其实在前面介绍字符串查询时，针对不同场景选择不同算法的思路就有体现了。类似的例子还有很多，例如去重计数uniqCombined函数，会根据数据量的不同选择不同的算法：<u>当数据量较小的时候，会选择Array保存；当数据量中等的时候，会选择HashSet；而当数据量很大的时候，则使用HyperLogLog算法</u>。

​	<u>对于数据结构比较清晰的场景，会通过代码生成技术实现循环展开，以减少循环次数。接着就是大家熟知的大杀器——向量化执行了</u>。**SIMD被广泛地应用于文本转换、数据过滤、数据解压和JSON转换等场景。相较于单纯地使用CPU，利用寄存器暴力优化也算是一种降维打击了**。

### 2.3.5 持续测试，持续改进

​	如果只是单纯地在上述细节上下功夫，还不足以构建出如此强大的ClickHouse，还需要拥有一个能够持续验证、持续改进的机制。由于Yandex的天然优势，ClickHouse经常会使用真实的数据进行测试，这一点很好地保证了测试场景的真实性。与此同时，ClickHouse也是我见过的发版速度最快的开源软件了，差不多每个月都能发布一个版本。没有一个可靠的持续集成环境，这一点是做不到的。正因为拥有这样的发版频率，ClickHouse才能够快速迭代、快速改进。

​	所以ClickHouse的黑魔法并不是一项单一的技术，而是一种**自底向上的、追求极致性能的设计思路**。这就是它如此之快的秘诀。

## 2.4 本章小结

​	本章我们快速浏览了世界第三大Web流量分析平台Yandex.Metrica背后的支柱ClickHouse的核心特性和逻辑架构。通过对核心特性部分的展示，ClickHouse如此强悍的缘由已初见端倪，**列式存储**、**向量化执行引擎**和**表引擎**都是它的撒手锏。在架构设计部分，则进一步展示了ClickHouse的一些设计思路，例如Column、Field、Block和Cluster。了解这些设计思路，能够帮助我们更好地理解和使用ClickHouse。最后又从另外一个角度探讨了ClickHouse如此之快的秘诀。下一章将介绍如何安装、部署ClickHouse。

# 3. 安装与部署

> + [使用教程 | ClickHouse文档](https://clickhouse.tech/docs/zh/getting-started/tutorial/)
> + [ClickHouse docker-compose初试_lzshlzsh的专栏-CSDN博客](https://blog.csdn.net/lzshlzsh/article/details/104296325)

## 3.1 ClickHouse的安装过程

### 3.1.1 环境准备

### 3.1.2 安装ClickHouse

1. 安装执行

2. 目录结构

   程序在安装的过程中会自动构建整套目录结构，接下来分别说明它们的作用。

   首先是核心目录部分：

   1. `/etc/clickhouse-server`：服务端的配置文件目录，包括全局配置config.xml和用户配置users.xml等。
   2. `/var/lib/clickhouse`：默认的数据存储目录（通常会修改默认路径配置，将数据保存到大容量磁盘挂载的路径）。
   3. `/var/log/clickhouse-server`：默认保存日志的目录（通常会修改路径配置，将日志保存到大容量磁盘挂载的路径）。

   接着是配置文件部分：

   1. `/etc/security/limits.d/clickhouse.conf`：文件句柄数量的配置，默认值如下所示：

      ```shell
      # cat /etc/security/limits.d/clickhouse.conf
      clickhouse soft nofile 262144
      clickhouse hard nofile 262144
      ```

      该配置也可以通过config.xml的max_open_files修改。

   2. `/etc/cron.d/clickhouse-server`：cron定时任务配置，用于恢复因异常原因中断的ClickHouse服务进程，其默认的配置如下：

      ```shell
      # cat /etc/cron.d/clickhouse-server
      # */10 * * * * root (which service > /dev/null 2>&1 && (service clickhouse-server
      condstart ||:)) || /etc/init.d/clickhouse-server condstart > /dev/null 2>&1
      ```

      可以看到，在默认的情况下，每隔10秒就会使用condstart尝试启动一次ClickHouse服务，而condstart命令的启动逻辑如下所示。

      ```shell
      is_running || service_or_func start
      ```

      如果ClickHouse服务正在运行，则跳过；如果没有运行，则通过start启动。

   最后是一组在/usr/bin路径下的可执行文件：

   1. clickhouse：主程序的可执行文件。

   2. clickhouse-client：一个指向ClickHouse可执行文件的软链接，供客户端连接使用。

   3. clickhouse-server：一个指向ClickHouse可执行文件的软链接，供服务端启动使用。

   4. clickhouse-compressor：内置提供的压缩工具，可用于数据的正压反解。

3. 启动服务

4. 版本升级

## 3.2 客户端的访问接口

​	**ClickHouse的底层访问接口支持TCP和HTTP两种协议**。

+ 其中，TCP协议拥有更好的性能，其默认端口为**9000**，主要用于集群间的内部通信及CLI客户端；
+ 而HTTP协议则拥有更好的兼容性，可以通过REST服务的形式被广泛用于JAVA、Python等编程语言的客户端，其默认端口为**8123**。

​	通常而言，并不建议用户直接使用底层接口访问ClickHouse，更为推荐的方式是通过CLI和JDBC这些封装接口，因为它们更加简单易用。

### 3.2.1 CLI

### 3.2.2 JDBC

​	ClickHouse支持标准的JDBC协议，底层基于HTTP接口通信。使用下面的Maven依赖，即可为Java程序引入官方提供的数据库驱动包：

```xml
<dependency>
  <groupId>ru.yandex.clickhouse</groupId>
  <artifactId>clickhouse-jdbc</artifactId>
  <version>0.2.4</version>
</dependency>
```

​	该驱动有两种使用方式。

1. 标准形式

   ​	标准形式是我们常用的方式，通过JDK原生接口获取连接，其关键参数如下：

   + JDBC Driver Class为`ru.yandex.clickhouse.ClickHouseDriver`；
   + JDBC URL为`jdbc:clickhouse://<host>:<port>[/<database>]`。

   ​	接下来是一段伪代码用例：

   ```java
   // 初始化驱动
   Class.forName("ru.yandex.clickhouse.ClickHouseDriver");
   // url
   String url = "jdbc:clickhouse://ch5.nauu.com:8123/default";
   // 用户名密码
   String user = "default";
   String password = "";
   // 登录
   Connection con = DriverManager.getConnection(url, username, password);
   Statement stmt = con.createStatement();
   // 查询
   ResultSet rs = stmt.executeQuery("SELECT 1");
   rs.next();
   System.out.printf("res "+rs.getInt(1));
   ```

2. 高可用模式

   ​	高可用模式允许设置多个host地址，每次会从可用的地址中随机选择一个进行连接，其URL声明格式如下：

   ```shell
   jdbc:clickhouse://<first-host>:<port>,<second-host>:<port>[,…]/<database>
   ```

   ​	在高可用模式下，需要通过BalancedClickhouseDataSource对象获取连接，接下来是一段伪代码用例：

   ```java
   //多个地址使用逗号分隔
   String url1 = "jdbc:clickhouse://ch8.nauu.com:8123,ch5.nauu.com:8123/default";
   //设置JDBC参数
   ClickHouseProperties clickHouseProperties = new ClickHouseProperties();
   clickHouseProperties.setUser("default");
   //声明数据源
   BalancedClickhouseDataSource balanced = new BalancedClickhouseDataSource(url1,
   clickHouseProperties);
   //对每个host进行ping操作, 排除不可用的dead连接
   balanced.actualize();
   //获得JDBC连接
   Connection con = balanced.getConnection();
   Statement stmt = con.createStatement();
   //查询
   ResultSet rs = stmt.executeQuery("SELECT 1 , hostName()");
   rs.next();
   System.out.println("res "+rs.getInt(1)+","+rs.getString(2));
   ```

   ​	由于篇幅所限，所以本小节只介绍了两个典型的封装接口，即CLI（基于TCP）和JDBC（基于HTTP）。但ClickHouse的访问接口并不仅限于此、它还拥有原生的C++、ODBC接口及众多第三方的集成接口（Python、NodeJS、Go、PHP等），如果想进一步了解可参阅官方手册。

## 3.3 内置的实用工具

​	ClickHouse除了提供基础的服务端与客户端程序之外，还内置了clickhouse-local和clickhouse-benchmark两种实用工具，现在分别说明它们的作用。

### 3.3.1 clickhouse-local

​	clickhouse-local可以独立运行大部分SQL查询，不需要依赖任何ClickHouse的服务端程序，它可以理解成是ClickHouse服务的单机版微内核，是一个轻量级的应用程序。clickhouse-local只能够使用File表引擎（关于表引擎的更多介绍在后续章节展开），它的数据与同机运行的ClickHouse服务也是完全隔离的，相互之间并不能访问。

### 3.3.2 clickhouse-benchmark

​	clickhouse-benchmark是基准测试的小工具，它可以自动运行SQL查询，并生成相应的运行指标报告。

## 3.4 本章小结

​	本章首先介绍了基于离线RPM包安装ClickHouse的整个过程。接着介绍了ClickHouse的两种访问接口，其中TCP端口拥有更好的访问性能，而HTTP端口则拥有更好的兼容性。但是在日常应用的过程中，更推荐使用基于它们之上实现的封装接口。所以接下来，我们又分别介绍了两个典型的封装接口，其中CLI接口是基于TCP封装的，它拥有交互式和非交互式两种运行模式。而JDBC接口是基于HTTP封装的，是一种标准的数据库访问接口。最后介绍了ClickHouse内置的几种实用工具。从下一章开始将正式介绍ClickHouse的功能，首先会从数据定义开始。

# 4. 数据定义

​	对于一款可以处理海量数据的分析系统而言，支持DML查询实属难能可贵。有人曾笑言：解决问题的最好方法就是恰好不需要。在海量数据的场景下，许多看似简单的操作也会变得举步维艰，所以一些系统会选择做减法以规避一些难题。而ClickHouse支持较完备的DML语句，包括INSERT、SELECT、UPDATE和DELETE。虽然UPDATE和DELETE可能存在性能问题，但这些能力的提供确实丰富了各位架构师手中的筹码，在架构设计时也能多几个选择。

​	作为一款完备的DBMS（数据库管理系统），ClickHouse提供了DDL与DML的功能，并支持大部分标准的SQL。也正因如此，ClickHouse十分容易入门。如果你是一个拥有其他数据库（如MySQL）使用经验的老手，通过上一章的介绍，在搭建好数据库环境之后，再凭借自身经验摸索几次，很快就能够上手使用ClickHouse了。但是作为一款异军突起的OLAP数据库黑马，ClickHouse有着属于自己的设计目标，高性能才是它的根本，所以也不能完全以对传统数据库的理解度之。比如，ClickHouse在基础数据类型方面，虽然相比常规数据库更为精练，但同时它又提供了实用的复合数据类型，而这些是常规数据库所不具备的。再比如，**ClickHouse所提供的DDL与DML查询，在部分细节上也与其他数据库有所不同（例如UPDATE和DELETE是借助ALTER变种实现的）**。

​	所以系统学习并掌握ClickHouse中数据定义的方法是很有必要的，这能够帮助我们更深刻地理解和使用ClickHouse。本章将详细介绍ClickHouse的数据类型及DDL的相关操作，在章末还会讲解部分DML操作。

## 4.1 ClickHouse的数据类型

​	作为一款分析型数据库，ClickHouse提供了许多数据类型，它们可以划分为**基础类型**、**复合类型**和**特殊类型**。其中基础类型使ClickHouse具备了描述数据的基本能力，而另外两种类型则使ClickHouse的数据表达能力更加丰富立体。

### 4.1.1 基础类型

​	基础类型只有**数值**、**字符串**和**时间**三种类型，没有Boolean类型，但可以使用整型的0或1替代。

1. 数值类型

   1. Int

      ​	在普遍观念中，常用Tinyint、Smallint、Int和Bigint指代整数的不同取值范围。而ClickHouse则直接使用Int8、Int16、Int32和Int64指代4种大小的Int类型，其末尾的数字正好表明了占用字节的大小（8位=1字节），具体信息如表4-1所示。

      ​	表4-1 有符号整数类型的具体信息

      | 名称  | 大小（字节） | 范围                                     | 普遍观念 |
      | ----- | ------------ | ---------------------------------------- | -------- |
      | Int8  | 1            | -128~127                                 | Tinyint  |
      | Int16 | 2            | -32768~32767                             | Smallint |
      | Int32 | 4            | -2147483648~2147483647                   | Int      |
      | Int64 | 8            | -9223372036854775808~9223372036854775807 | Bigint   |

      ​	ClickHouse支持无符号的整数，使用前缀U表示，具体信息如表4-2所示。

      ​	表4-2 无符号整数类型的具体信息

      | 名称   | 大小（字节） | 范围                   | 普遍观念          |
      | ------ | ------------ | ---------------------- | ----------------- |
      | UInt8  | 1            | 0~255                  | Tinyint Unsigned  |
      | UInt16 | 2            | 0~65535                | Smallint Unsigned |
      | UInt32 | 4            | 0~4294967295           | Int Unsigned      |
      | UInt64 | 8            | 0~18446744073709551615 | Bigint Unsigned   |

   2. Float

      ​	与整数类似，ClickHouse直接使用Float32和Float64代表单精度浮点数以及双精度浮点数，具体信息如表4-3所示。
      
      ​	表4-3 浮点数类型的具体信息
      
      | 名称    | 大小（字节） | 有效精度(位数) | 普遍观念 |
      | ------- | ------------ | -------------- | -------- |
      | Float32 | 4            | 7              | Float    |
      | Float64 | 8            | 16             | Double   |
      
      ​	在使用浮点数的时候，应当要意识到它是有限精度的。假如，分别对Float32和Float64写入超过有效精度的数值，下面我们看看会发生什么。例如，将拥有20位小数的数值分别写入Float32和Float64，此时结果就会出现数据误差：
      
      ```shell
      :) SELECT toFloat32('0.12345678901234567890') as a , toTypeName(a)
      ┌──────a─┬─toTypeName(toFloat32('0.12345678901234567890'))─┐
      │ 0.12345679 │ Float32 │
      └────────┴───────────────────────────────┘
      :) SELECT toFloat64('0.12345678901234567890') as a , toTypeName(a)
      ┌────────────a─┬─toTypeName(toFloat64('0.12345678901234567890'))─┐
      │ 0.12345678901234568 │ Float64 │
      └─────────────┴──────────────────────────────┘
      ```
      
      可以发现，Float32从小数点后第8位起及Float64从小数点后第17位起，都产生了数据溢出。
      
      ClickHouse的浮点数支持**正无穷**、**负无穷**以及**非数字**的表达方式。
      
      + 正无穷：
      
        ```shell
        :) SELECT 0.8/0
        ┌─divide(0.8, 0)─┐
        │ inf │
        └──────────┘
        ```
      
      + 负无穷：
      
        ```shell
        :) SELECT -0.8/0
        ┌─divide(-0.8, 0)─┐
        │ -inf │
        └───────────┘
        ```
      
      + 非数字：
      
        ```shell
        :) SELECT 0/0
        ┌─divide(0, 0)──┐
        │ nan │
        └──────────┘
        ```
      
   3. Decimal

      ​	如果要求更高精度的数值运算，则需要使用定点数。ClickHouse提供了Decimal32、Decimal64和Decimal128三种精度的定点数。可以通过两种形式声明定点：简写方式有Decimal32(S)、Decimal64(S)、Decimal128(S)三种，原生方式为Decimal(P,S)，其中：

      + P代表精度，决定总位数（整数部分+小数部分），取值范围是1～38；
      + S代表规模，决定小数位数，取值范围是0～P。

      ​	简写方式与原生方式的对应关系如表4-4所示。

      ​	表4-4 定点数类型的具体信息

      | 名称          | 等效声明          | 范围                        |
      | ------------- | ----------------- | --------------------------- |
      | Decimal32(S)  | Decimal(1~9，S)   | -1\*10^(9-S)到1\*10^(9-S)   |
      | Decimal64(S)  | Decimal(10~18，S) | -1\*10^(18-S)到1\*10^(18-S) |
      | Decimal128(S) | Decimal(19~38，S) | -1\*10^(38-S)到1\*10^(38-S) |

      ​	在使用两个不同精度的定点数进行四则运算的时候，它们的小数点位数S会发生变化。<u>在进行加法运算时，S取最大值</u>。例如下面的查询，toDecimal64(2,4)与toDecimal32(2,2)相加后S=4：

      ```shell
      :) SELECT toDecimal64(2,4) + toDecimal32(2,2)
      ┌─plus(toDecimal64(2, 4), toDecimal32(2, 2))─┐
      │ 4.0000 │
      └───────────────────────────┘
      ```
      
      ​	<u>在进行减法运算时，其规则与加法运算相同，S同样会取最大值</u>。例如toDecimal32(4,4)与toDecimal64(2,2)相减后S=4：
      
      ```shell
      :) SELECT toDecimal32(4,4) - toDecimal64(2,2)
      ┌─minus(toDecimal32(4, 4), toDecimal64(2, 2))┐
      │ 2.0000 │
      └────────────────────────────┘
      ```
      
      ​	<u>在进行乘法运算时，S取两者S之和</u>。例如下面的查询，toDecimal64(2,4)与toDecimal32(2,2)相乘后S=4+2=6：
      
      ```shell
      :) SELECT toDecimal64(2,4) * toDecimal32(2,2)
      ┌─multiply(toDecimal64(2, 4), toDecimal32(2, 2))┐
      │ 4.000000 │
      └─────────────────────────────┘
      ```
      
      ​	<u>在进行除法运算时，S取被除数的值，此时要求被除数S必须大于除数S，否则会报错</u>。例如toDecimal64(2,4)与toDecimal32(2,2)相除后S=4：
      
      ```shell
      :) SELECT toDecimal64(2,4) / toDecimal32(2,2)
      ┌─divide(toDecimal64(2, 4), toDecimal32(2, 2))┐
      │ 1.0000 │
      └───────────────────────────┘
      ```
      
      ​	最后进行一番总结：对于不同精度定点数之间的四则运算，其精度S的变化会遵循表4-5所示的规则。
      
      ​	表4-5 定点数四则运算后，精度变化的规则：
      
      | 名称 | 规则                           |
      | ---- | ------------------------------ |
      | 加法 | S = max(S1, S2)                |
      | 减法 | S = max(S1, S2)                |
      | 乘法 | S = S1 + S2 (S1范围 >= S2范围) |
      | 除法 | S = S1 (S1为被除数，S1/S2)     |
      
      ​	在使用定点数时还有一点值得注意：<u>由于现代计算器系统只支持32位和64位CPU，所以Decimal128是在软件层面模拟实现的，它的速度会明显慢于Decimal32与Decimal64</u>。

2. 字符串类型

   ​	字符串类型可以细分为String、FixedString和UUID三类。从命名来看仿佛不像是由一款数据库提供的类型，反而更像是一门编程语言的设计。

   1. String

      ​	字符串由String定义，长度不限。因此在使用String的时候无须声明大小。它完全代替了传统意义上数据库的Varchar、Text、Clob和Blob等字符类型。String类型**不限定字符集**，因为它根本就没有这个概念，所以可以将任意编码的字符串存入其中。<u>但是为了程序的规范性和可维护性，在同一套程序中应该遵循使用统一的编码，例如“统一保持UTF-8编码”就是一种很好的约定</u>。

   2. FixedString

      ​	FixedString类型和传统意义上的Char类型有些类似，对于一些字符有明确长度的场合，可以使用<u>固定长度的字符串</u>。定长字符串通过FixedString(N)声明，其中N表示字符串长度。<u>但与Char不同的是，FixedString使用null字节填充末尾字符，而Char通常使用空格填充</u>。比如在下面的例子中，字符串‘abc’虽然只有3位，但长度却是5，因为末尾有2位空字符填充：

      ```shell
      :) SELECT toFixedString('abc',5) , LENGTH(toFixedString('abc',5)) AS LENGTH
      ┌─toFixedString('abc', 5)─┬─LENGTH─┐
      │ abc │ 5 │
      └────────────────┴──────┘
      ```

   3. UUID

      ​	UUID是一种数据库常见的主键类型，在ClickHouse中直接把它作为一种数据类型。UUID共有32位，它的格式为8-4-4-4-12。<u>如果一个UUID类型的字段在写入数据时没有被赋值，则会依照格式使用0填充</u>，例如：

      ```shell
      CREATE TABLE UUID_TEST (
      c1 UUID,
      c2 String
      ) ENGINE = Memory;
      --第一行UUID有值
      INSERT INTO UUID_TEST SELECT generateUUIDv4(),'t1'
      --第二行UUID没有值
      INSERT INTO UUID_TEST(c2) VALUES('t2')
      :) SELECT * FROM UUID_TEST
      ┌─────────────────────c1─┬─c2─┐
      │ f36c709e-1b73-4370-a703-f486bdd22749 │ t1 │
      └───────────────────────┴────┘
      ┌─────────────────────c1─┬─c2─┐
      │ 00000000-0000-0000-0000-000000000000 │ t2 │
      └───────────────────────┴────┘
      ```
      
      ​	可以看到，第二行没有被赋值的UUID被0填充了。

3. 时间类型

   ​	时间类型分为DateTime、DateTime64和Date三类。ClickHouse目前没有时间戳类型。时间类型最高的精度是秒，也就是说，**如果需要处理毫秒、微秒等大于秒分辨率的时间，则只能借助UInt类型实现**。

   1. DateTime

      DateTime类型包含时、分、秒信息，精确到秒，支持使用字符串形式写入：

      ```shell
      CREATE TABLE Datetime_TEST (
      c1 Datetime
      ) ENGINE = Memory
      --以字符串形式写入
      INSERT INTO Datetime_TEST VALUES('2019-06-22 00:00:00')
      SELECT c1, toTypeName(c1) FROM Datetime_TEST
      ┌──────────c1─┬─toTypeName(c1)─┐
      │ 2019-06-22 00:00:00 │ DateTime │
      └─────────────┴───────────┘
      ```
   
   2. DateTime64
   
      DateTime64可以记录亚秒，它在DateTime之上增加了精度的设置，例如：
   
      ```shell
      CREATE TABLE Datetime64_TEST (
      c1 Datetime64(2)
      ) ENGINE = Memory
      --以字符串形式写入
      INSERT INTO Datetime64_TEST VALUES('2019-06-22 00:00:00')
      SELECT c1, toTypeName(c1) FROM Datetime64_TEST
      ┌─────────────c1─┬─toTypeName(c1)─┐
      │ 2019-06-22 00:00:00.00 │ DateTime │
      └───────────────┴──────────┘
      ```
   
   3. Date
   
      Date类型不包含具体的时间信息，只精确到天，它同样也支持字符串形式写入：
   
      ```shell
      CREATE TABLE Date_TEST (
      c1 Date
      ) ENGINE = Memory
      --以字符串形式写入
      INSERT INTO Date_TEST VALUES('2019-06-22')
      SELECT c1, toTypeName(c1) FROM Date_TEST
      ┌─────────c1─┬─toTypeName(c1)─┐
      │ 2019-06-22 │ Date │
      └───────────┴──────────┘
      ```

### 4.1.2 复合类型

​	除了基础数据类型之外，ClickHouse还提供了数组、元组、枚举和嵌套四类复合类型。这些类型通常是其他数据库原生不具备的特性。拥有了复合类型之后，ClickHouse的数据模型表达能力更强了。

1. Array

   数组有两种定义形式，常规方式array(T)：

   ```shell
   SELECT array(1, 2) as a , toTypeName(a)
   ┌─a───┬─toTypeName(array(1, 2))─┐
   │ [1,2] │ Array(UInt8) │
   └─────┴────────────────┘
   ```

   或者简写方式[T]：

   ```shell
   SELECT [1, 2]
   ```

   通过上述的例子可以发现，在查询时并不需要主动声明数组的元素类型。因为**ClickHouse的数组拥有类型推断的能力，推断依据：以最小存储代价为原则，即使用最小可表达的数据类型**。例如在上面的例子中，array(1,2)会通过自动推断将UInt8作为数组类型。<u>但是数组元素中如果存在Null值，则元素类型将变为Nullable</u>，例如：

   ```shell
   SELECT [1, 2, null] as a , toTypeName(a)
   ┌─a──────┬─toTypeName([1, 2, NULL])─┐
   │ [1,2,NULL] │ Array(Nullable(UInt8)) │
   └────────┴─────────────────┘
   ```

   细心的读者可能已经发现，<u>在同一个数组内可以包含多种数据类型，例如数组[1,2.0]是可行的。但各类型之间必须兼容，例如数组[1,'2']则会报错</u>。

   <u>在定义表字段时，数组需要指定明确的元素类型</u>，例如：

   ```shell
   CREATE TABLE Array_TEST (
   c1 Array(String)
   ) engine = Memory
   ```

2. Tuple

   元组类型由1～n个元素组成，每个元素之间允许设置不同的数据类型，且彼此之间不要求兼容。<u>元组同样支持类型推断，其推断依据仍然以最小存储代价为原则</u>。与数组类似，元组也可以使用两种方式定义，常规方式tuple(T)：

   ```shell
   SELECT tuple(1,'a',now()) AS x, toTypeName(x)
   ┌─x─────────────────┬─toTypeName(tuple(1, 'a', now()))─┐
   │ (1,'a','2019-08-28 21:36:32') │ Tuple(UInt8, String, DateTime) │
   └───────────────────┴─────────────────────┘
   ```

   或者简写方式（T）：

   ```shell
   SELECT (1,2.0,null) AS x, toTypeName(x)
   ┌─x──────┬─toTypeName(tuple(1, 2., NULL))───────┐
   │ (1,2,NULL) │ Tuple(UInt8, Float64, Nullable(Nothing)) │
   └───────┴──────────────────────────┘
   ```

   在定义表字段时，元组也需要指定明确的元素类型：

   ```shell
   CREATE TABLE Tuple_TEST (
   c1 Tuple(String,Int8)
   ) ENGINE = Memory;
   ```

   元素类型和泛型的作用类似，可以进一步保障数据质量。在数据写入的过程中会进行类型检查。例如，写入`INSERT INTO Tuple_TEST VALUES(('abc',123))`是可行的，而写入`INSERT INTO Tuple_TEST VALUES(('abc','efg'))`则会报错。

3. Enum

   ​	ClickHouse支持枚举类型，这是一种在定义常量时经常会使用的数据类型。ClickHouse提供了Enum8和Enum16两种枚举类型，它们除了取值范围不同之外，别无二致。<u>枚举固定使用(String:Int)Key/Value键值对的形式定义数据，所以Enum8和Enum16分别会对应(String:Int8)和(String:Int16)</u>，例如：

   ```shell
   CREATE TABLE Enum_TEST (
   c1 Enum8('ready' = 1, 'start' = 2, 'success' = 3, 'error' = 4)
   ) ENGINE = Memory;
   ```

   ​	在定义枚举集合的时候，有几点需要注意。首先，Key和Value是不允许重复的，要保证唯一性。其次，Key和Value的值都不能为Null，但Key允许是空字符串。在写入枚举数据的时候，只会用到Key字符串部分，例如：

   ```shell
   INSERT INTO Enum_TEST VALUES('ready');
   INSERT INTO Enum_TEST VALUES('start');
   ```

   ​	数据在写入的过程中，会对照枚举集合项的内容逐一检查。如果Key字符串不在集合范围内则会抛出异常，比如执行下面的语句就会出错：

   ```shell
   INSERT INTO Enum_TEST VALUES('stop');
   ```

   ​	<u>可能有人会觉得，完全可以使用String代替枚举，为什么还需要专门的枚举类型呢？这是出于性能的考虑。因为虽然枚举定义中的Key属于String类型，但是**在后续对枚举的所有操作中（包括排序、分组、去重、过滤等），会使用Int类型的Value值**</u>。

4. Nested

   ​	嵌套类型，顾名思义是一种嵌套表结构。<u>一张数据表，可以定义任意多个嵌套类型字段，但每个字段的嵌套层级只支持一级，即嵌套表内不能继续使用嵌套类型</u>。对于简单场景的层级关系或关联关系，使用嵌套类型也是一种不错的选择。例如，下面的nested_test是一张模拟的员工表，它的所属部门字段就使用了嵌套类型：

   ```sql
   CREATE TABLE nested_test (
     name String,
     age UInt8 ,
     dept Nested(
       id UInt8,
       name String
     )
   ) ENGINE = Memory;
   ```

   ​	ClickHouse的嵌套类型和传统的嵌套类型不相同，导致在初次接触它的时候会让人十分困惑。以上面这张表为例，如果按照它的字面意思来理解，会很容易理解成nested_test与dept是一对一的包含关系，其实这是错误的。不信可以执行下面的语句，看看会是什么结果：

   ```shell
   INSERT INTO nested_test VALUES ('nauu',18, 10000, '研发部');
   Exception on client:
   Code: 53. DB::Exception: Type mismatch in IN or VALUES section. Expected:
   Array(UInt8). Got: UInt64
   ```

   ​	注意上面的异常信息，它提示期望写入的是一个Array数组类型。

   ​	现在大家应该明白了，<u>嵌套类型本质是一种多维数组的结构。嵌套表中的每个字段都是一个数组，并且行与行之间数组的长度无须对齐</u>。所以需要把刚才的INSERT语句调整成下面的形式：

   ```sql
   INSERT INTO nested_test VALUES ('bruce' , 30 , [10000,10001,10002], ['研发部','技
   术支持中心','测试部']);
   --行与行之间,数组长度无须对齐
   INSERT INTO nested_test VALUES ('bruce' , 30 , [10000,10001], ['研发部','技术支持中
   心']);
   ```

   ​	需要注意的是，<u>在同一行数据内每个数组字段的长度必须相等</u>。例如，在下面的示例中，由于行内数组字段的长度没有对齐，所以会抛出异常：

   ```sql
   INSERT INTO nested_test VALUES ('bruce' , 30 , [10000,10001], ['研发部','技术支持中
   心',
   '测试部']);
   DB::Exception: Elements 'dept.id' and 'dept.name' of Nested data structure 'dept'
   (Array columns) have different array sizes..
   ```

   ​	在访问嵌套类型的数据时需要使用点符号，例如：

   ```shell
   SELECT name, dept.id, dept.name FROM nested_test
   ┌─name─┬─dept.id──┬─dept.name─────────────┐
   │ bruce │ [16,17,18] │ ['研发部','技术支持中心','测试部'] │
   └────┴───────┴────────────────────┘
   ```

### 4.1.3 特殊类型

​	ClickHouse还有一类不同寻常的数据类型，我将它们定义为特殊类型。

1. Nullable

   ​	准确来说，Nullable并不能算是一种独立的数据类型，它更像是一种辅助的修饰符，需要与基础数据类型一起搭配使用。<u>Nullable类型与Java8的Optional对象有些相似，它表示某个基础数据类型可以是Null值</u>。其具体用法如下所示：

   ```shell
   CREATE TABLE Null_TEST (
   c1 String,
   c2 Nullable(UInt8)
   ) ENGINE = TinyLog;
   通过Nullable修饰后c2字段可以被写入Null值：
   INSERT INTO Null_TEST VALUES ('nauu',null)
   INSERT INTO Null_TEST VALUES ('bruce',20)
   SELECT c1 , c2 ,toTypeName(c2) FROM Null_TEST
   ┌─c1───┬───c2─┬─toTypeName(c2)─┐
   │ nauu │ NULL │ Nullable(UInt8) │
   │ bruce │ 20 │ Nullable(UInt8) │
   └─────┴──────┴───────────┘
   ```

   ​	**在使用Nullable类型的时候还有两点值得注意：首先，它只能和基础类型搭配使用，不能用于数组和元组这些复合类型，也不能作为索引字段；其次，应该慎用Nullable类型，包括Nullable的数据表，不然会使查询和写入性能变慢**。<u>因为在正常情况下，每个列字段的数据会被存储在对应的[Column].bin文件中。如果一个列字段被Nullable类型修饰后，会额外生成一个[Column].null.bin文件专门保存它的Null值。这意味着在读取和写入数据时，需要一倍的额外文件操作</u>。

2. Domain

   ​	域名类型分为IPv4和IPv6两类，本质上它们是对整型和字符串的进一步封装。IPv4类型是基于UInt32封装的，它的具体用法如下所示：

   ```shell
   CREATE TABLE IP4_TEST (
   url String,
   ip IPv4
   ) ENGINE = Memory;
   INSERT INTO IP4_TEST VALUES ('www.nauu.com','192.0.0.0')
   SELECT url , ip ,toTypeName(ip) FROM IP4_TEST
   ┌─url──────┬─────ip─┬─toTypeName(ip)─┐
   │ www.nauu.com │ 192.0.0.0 │ IPv4 │
   └────────┴───────┴──────────┘
   ```

   ​	细心的读者可能会问，直接使用字符串不就行了吗？为何多此一举呢？我想至少有如下两个原因。

   1. 出于便捷性的考量，例如IPv4类型支持格式检查，格式错误的IP数据是无法被写入的，例如：

      ```shell
      INSERT INTO IP4_TEST VALUES ('www.nauu.com','192.0.0')
      Code: 441. DB::Exception: Invalid IPv4 value.
      ```

   2. 出于性能的考量，同样以IPv4为例，IPv4使用UInt32存储，相比String更加紧凑，占用的空间更小，查询性能更快。IPv6类型是基于FixedString(16)封装的，它的使用方法与IPv4别无二致，此处不再赘述。

   ​	<u>在使用Domain类型的时候还有一点需要注意，虽然它从表象上看起来与String一样，但Domain类型并不是字符串，所以它不支持隐式的自动类型转换</u>。如果需要返回IP的字符串形式，则需要显式调用IPv4NumToString或IPv6NumToString函数进行转换。

## 4.2 如何定义数据表

​	在知晓了ClickHouse的主要数据类型之后，接下来我们开始介绍DDL操作及定义数据的方法。DDL查询提供了数据表的创建、修改和删除操作，是最常用的功能之一。

### 4.2.1 数据库

​	数据库起到了命名空间的作用，可以有效规避命名冲突的问题，也为后续的数据隔离提供了支撑。任何一张数据表，都必须归属在某个数据库之下。创建数据库的完整语法如下所示：

```sql
CREATE DATABASE IF NOT EXISTS db_name [ENGINE = engine]
```

​	其中，IF NOT EXISTS表示如果已经存在一个同名的数据库，则会忽略后续的创建过程；[ENGINE=engine]表示数据库所使用的引擎类型（是的，你没看错，<u>数据库也支持设置引擎</u>）。

​	数据库目前一共支持5种引擎，如下所示。

+ <u>Ordinary：默认引擎，在绝大多数情况下我们都会使用默认引擎，使用时无须刻意声明。在此数据库下可以使用任意类型的表引擎</u>。
+ Dictionary：字典引擎，此类数据库会自动为所有数据字典创建它们的数据表，关于数据字典的详细介绍会在第5章展开。
+ Memory：<u>内存引擎，用于存放临时数据。此类数据库下的数据表只会停留在内存中，不会涉及任何磁盘操作，当服务重启后数据会被清除</u>。
+ Lazy：日志引擎，此类数据库下只能使用Log系列的表引擎，关于Log表引擎的详细介绍会在第8章展开。
+ MySQL：MySQL引擎，此类数据库下会自动拉取远端MySQL中的数据，并为它们创建MySQL表引擎的数据表，关于MySQL表引擎的详细介绍会在第8章展开。

​	在绝大多数情况下都只需使用默认的数据库引擎。例如执行下面的语句，即能够创建属于我们的第一个数据库：

```shell
CREATE DATABASE DB_TEST
```

​	默认数据库的实质是物理磁盘上的一个文件目录，所以在语句执行之后，ClickHouse便会在安装路径下创建DB_TEST数据库的文件目录：

```shell
# pwd
/chbase/data
# ls
DB_TEST default system
```

​	与此同时，在metadata路径下也会一同创建用于恢复数据库的DB_TEST.sql文件：

```shell
# pwd
/chbase/data/metadata
# ls
DB_TEST DB_TEST.sql default system
```

​	使用SHOW DATABASES查询，即能够返回ClickHouse当前的数据库列表：

```shell
SHOW DATABASES
┌─name───┐
│ DB_TEST │
│ default │
│ system │
└───────┘
```

​	使用USE查询可以实现在多个数据库之间进行切换，而通过SHOW TABLES查询可以查看当前数据库的数据表列表。删除一个数据库，则需要用到下面的DROP查询。

```sql
DROP DATABASE [IF EXISTS] db_name
```

### 4.2.2 数据表

​	ClickHouse数据表的定义语法，是在标准SQL的基础之上建立的，所以熟悉数据库的读者们在看到接下来的语法时，应该会感到很熟悉。ClickHouse目前提供了三种最基本的建表方法，其中，第一种是常规定义方法，它的完整语法如下所示：

```sql
CREATE TABLE [IF NOT EXISTS] [db_name.]table_name (
  name1 [type] [DEFAULT|MATERIALIZED|ALIAS expr],
  name2 [type] [DEFAULT|MATERIALIZED|ALIAS expr],
  省略…
) ENGINE = engine
```

​	使用`[db_name.]`参数可以为数据表指定数据库，如果不指定此参数，则默认会使用default数据库。例如执行下面的语句：

```sql
CREATE TABLE hits_v1 (
  Title String,
  URL String ,
  EventTime DateTime
) ENGINE = Memory;
```

​	上述语句将会在default默认的数据库下创建一张内存表。注意末尾的ENGINE参数，它被用于指定数据表的引擎。表引擎决定了数据表的特性，也决定了数据将会被如何存储及加载。例如示例中使用的Memory表引擎，是ClickHouse最简单的表引擎，数据只会被保存在内存中，在服务重启时数据会丢失。我们会在后续章节详细介绍数据表引擎，此处暂不展开。

​	第二种定义方法是复制其他表的结构，具体语法如下所示：

```sql
CREATE TABLE [IF NOT EXISTS] [db_name1.]table_name AS [db_name2.] table_name2
[ENGINE = engine]
```

​	这种方式<u>支持在不同的数据库之间复制表结构</u>，例如下面的语句：

```sql
--创建新的数据库
CREATE DATABASE IF NOT EXISTS new_db
--将default.hits_v1的结构复制到new_db.hits_v1
CREATE TABLE IF NOT EXISTS new_db.hits_v1 AS default.hits_v1 ENGINE = TinyLog
```

​	上述语句将会把`default.hits_v1`的表结构原样复制到`new_db.hits_v1`，并且**ENGINE表引擎可以与原表不同**。

​	第三种定义方法是通过SELECT子句的形式创建，它的完整语法如下：

```sql
CREATE TABLE [IF NOT EXISTS] [db_name.]table_name ENGINE = engine AS SELECT …
```

​	在这种方式下，不仅会根据SELECT子句建立相应的表结构，同时还会将SELECT子句查询的数据顺带写入，例如执行下面的语句：

```sql
CREATE TABLE IF NOT EXISTS hits_v1_1 ENGINE = Memory AS SELECT * FROM hits_v1
```

​	上述语句会将`SELECT * FROM hits_v1`的查询结果一并写入数据表。

​	ClickHouse和大多数数据库一样，使用DESC查询可以返回数据表的定义结构。如果想删除一张数据表，则可以使用下面的DROP语句：

```sql
DROP TABLE [IF EXISTS] [db_name.]table_name
```

### 4.2.3 默认值表达式

​	表字段支持三种默认值表达式的定义方法，分别是DEFAULT、MATERIALIZED和ALIAS。<u>无论使用哪种形式，表字段一旦被定义了默认值，它便不再强制要求定义数据类型，因为ClickHouse会根据默认值进行类型推断</u>。**如果同时对表字段定义了数据类型和默认值表达式，则以明确定义的数据类型为主**，例如下面的例子：

```sql
CREATE TABLE dfv_v1 (
  id String,
  c1 DEFAULT 1000,
  c2 String DEFAULT c1
) ENGINE = TinyLog
```

​	c1字段没有定义数据类型，默认值为整型1000；c2字段定义了数据类型和默认值，且默认值等于c1，现在写入测试数据：

```sql
INSERT INTO dfv_v1(id) VALUES ('A000')
```

​	在写入之后执行以下查询：

```shell
SELECT c1, c2, toTypeName(c1), toTypeName(c2) from dfv_v1
┌──c1─┬─c2──┬─toTypeName(c1)─┬─toTypeName(c2)─┐
│ 1000 │ 1000 │ UInt16 │ String │
└────┴────┴───────────┴──────────┘
```

​	由查询结果可以验证，默认值的优先级符合我们的预期，其中c1字段根据默认值被推断为UInt16；而c2字段由于同时定义了数据类型和默认值，所以它最终的数据类型来自明确定义的String。

​	默认值表达式的三种定义方法之间也存在着不同之处，可以从如下三个方面进行比较。

1. 数据写入：<u>在数据写入时，只有DEFAULT类型的字段可以出现在INSERT语句中</u>。而MATERIALIZED和ALIAS都不能被显式赋值，它们只能依靠计算取值。例如试图为MATERIALIZED类型的字段写入数据，将会得到如下的错误。

   ```shell
   DB::Exception: Cannot insert column URL, because it is MATERIALIZED column..
   ```

2. 数据查询：在数据查询时，<u>只有DEFAULT类型的字段可以通过SELECT *返回</u>。而MATERIALIZED和ALIAS类型的字段不会出现在SELECT *查询的返回结果集中。

3. 数据存储：在数据存储时，<u>只有DEFAULT和MATERIALIZED类型的字段才支持持久化</u>。如果使用的表引擎支持物理存储（例如TinyLog表引擎），那么这些列字段将会拥有物理存储。而<u>ALIAS类型的字段不支持持久化，它的取值总是需要依靠计算产生，数据不会落到磁盘</u>。

   ​	可以使用ALTER语句修改默认值，例如：

   ```sql
   ALTER TABLE [db_name.]table MODIFY COLUMN col_name DEFAULT value
   ```

   ​	<u>修改动作并不会影响数据表内先前已经存在的数据。但是默认值的修改有诸多限制，例如在合并树表引擎中，它的主键字段是无法被修改的；而某些表引擎则完全不支持修改（例如TinyLog）</u>。

### 4.2.4 * 临时表

​	ClickHouse也有临时表的概念，创建临时表的方法是在普通表的基础之上添加TEMPORARY关键字，它的完整语法如下所示：

```sql
CREATE TEMPORARY TABLE [IF NOT EXISTS] table_name (
  name1 [type] [DEFAULT|MATERIALIZED|ALIAS expr],
  name2 [type] [DEFAULT|MATERIALIZED|ALIAS expr],
)
```

​	相比普通表而言，临时表有如下两点特殊之处：

+ <u>它的生命周期是会话绑定的，所以它只支持Memory表引擎，如果会话结束，数据表就会被销毁</u>；
+ <u>临时表不属于任何数据库，所以在它的建表语句中，既没有数据库参数也没有表引擎参数</u>。

​	针对第二个特殊项，读者心中难免会产生一个疑问：既然临时表不属于任何数据库，如果临时表和普通表名称相同，会出现什么状况呢？接下来不妨做个测试。首先在DEFAULT数据库创建测试表并写入数据：

```sql
CREATE TABLE tmp_v1 (
  title String
) ENGINE = Memory;
INSERT INTO tmp_v1 VALUES ('click')
```

​	接着创建一张名称相同的临时表并写入数据：

```sql
CREATE TEMPORARY TABLE tmp_v1 (createtime Datetime)
INSERT INTO tmp_v1 VALUES (now())
```

​	现在查询tmp_v1看看会发生什么：

```sql
SELECT * FROM tmp_v1
┌──────createtime─┐
│ 2019-08-30 10:20:29 │
└─────────────┘
```

​	通过返回结果可以得出结论：**临时表的优先级是大于普通表的。当两张数据表名称相同的时候，会优先读取临时表的数据**。

​	<u>在ClickHouse的日常使用中，通常不会刻意使用临时表。它更多被运用在ClickHouse的内部，是数据在集群间传播的载体</u>。

### 4.2.5 * 分区表

​	**数据分区（partition）**和**数据分片（shard）**是完全不同的两个概念。数据分区是针对本地数据而言的，是数据的一种纵向切分。而数据分片是数据的一种横向切分（第10章会详细介绍）。**数据分区对于一款OLAP数据库而言意义非凡：借助数据分区，在后续的查询过程中能够跳过不必要的数据目录，从而提升查询的性能**。<u>合理地利用分区特性，还可以变相实现数据的更新操作，因为数据分区支持删除、替换和重置操作。假设数据表按照月份分区，那么数据就可以按月份的粒度被替换更新</u>。

​	<u>分区虽好，但不是所有的表引擎都可以使用这项特性，目前只有**合并树（MergeTree）家族系列的表引擎**才支持数据分区</u>。接下来通过一个简单的例子演示分区表的使用方法。首先由PARTITION BY指定分区键，例如下面的数据表partition_v1使用了日期字段作为分区键，并将其格式化为年月的形式：

```sql
CREATE TABLE partition_v1 (
  ID String,
  URL String,
  EventTime Date
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(EventTime)
ORDER BY ID
```

​	接着写入不同月份的测试数据：

```sql
INSERT INTO partition_v1 VALUES
('A000','www.nauu.com', '2019-05-01'),
('A001','www.brunce.com', '2019-06-02')
```

​	最后通过system.parts系统表，查询数据表的分区状态：

```sql
SELECT table,partition,path from system.parts WHERE table = 'partition_v1'
┌─table─────┬─partition─┬─path─────────────────────────┐
│ partition_v1 │ 201905 │ /chbase/data/default/partition_v1/201905_1_1_0/│
│ partition_v1 │ 201906 │ /chbase/data/default/partition_v1/201906_2_2_0/│
└─────────┴────────┴─────────────────────────────┘
```

​	可以看到，partition_v1按年月划分后，目前拥有两个数据分区，且每个分区都对应一个独立的文件目录，用于保存各自部分的数据。

​	合理设计分区键非常重要，通常会按照数据表的查询场景进行针对性设计。例如在刚才的示例中数据表按年月分区，如果后续的查询按照分区键过滤，例如：

```sql
SELECT * FROM partition_v1 WHERE EventTime ='2019-05-01'
```

​	那么在后续的查询过程中，可以利用分区索引跳过6月份的分区目录，只加载5月份的数据，从而带来查询的性能提升。

​	当然，**使用不合理的分区键也会适得其反，分区键不应该使用粒度过细的数据字段**。<u>例如，按照小时分区，将会带来分区数量的急剧增长，从而导致性能下降</u>。关于数据分区更详细的原理说明，将会在第6章进行。

### 4.2.6 * 视图

​	ClickHouse拥有普通和物化两种视图，其中物化视图拥有独立的存储，而普通视图只是一层简单的查询代理。创建普通视图的完整语法如下所示：

```sql
CREATE VIEW [IF NOT EXISTS] [db_name.]view_name AS SELECT ...
```

​	<u>普通视图不会存储任何数据，它只是一层单纯的SELECT查询映射，起着简化查询、明晰语义的作用，对查询性能不会有任何增强</u>。假设有一张普通视图view_tb_v1，它是基于数据表tb_v1创建的，那么下面的两条SELECT查询是完全等价的：

```sql
--普通表
SELECT * FROM tb_v1
-- tb_v1的视图
SELECT * FROM view_tb_v1
```

​	**物化视图支持表引擎，数据保存形式由它的表引擎决定**，创建物化视图的完整语法如下所示：

```sql
CREATE [MATERIALIZED] VIEW [IF NOT EXISTS] [db.]table_name [TO[db.]name] [ENGINE
= engine] [POPULATE] AS SELECT ...
```

​	**物化视图创建好之后，如果源表被写入新数据，那么物化视图也会同步更新**。POPULATE修饰符决定了物化视图的初始化策略：<u>如果使用了POPULATE修饰符，那么在创建视图的过程中，会连带将源表中已存在的数据一并导入，如同执行了SELECT INTO一般；反之，如果不使用POPULATE修饰符，那么物化视图在创建之后是没有数据的，它只会同步在此之后被写入源表的数据</u>。**物化视图目前并不支持同步删除，如果在源表中删除了数据，物化视图的数据仍会保留**。

​	**物化视图本质是一张特殊的数据表**，例如使用SHOW TABLE查看数据表的列表：

```sql
SHOW TABLES
┌─name────────┐
│ .inner.view_test2 │
│ .inner.view_test3 │
└───────────┘
```

​	由上可以发现，物化视图也在其中，它们是使用了`.inner.`特殊前缀的数据表，所以删除视图的方法是直接使用DROP TABLE查询，例如：

```sql
DROP TABLE view_name
```

## 4.3 数据表的基本操作

​	<u>目前只有MergeTree、Merge和Distributed这三类表引擎支持ALTER查询</u>，如果现在还不明白这些表引擎的作用也不必担心，目前只需简单了解这些信息即可，后面会有专门章节对它们进行介绍。

### 4.3.1 追加新字段

​	假如需要对一张数据表追加新的字段，可以使用如下语法：

`ALTER TABLE tb_name ADD COLUMN [IF NOT EXISTS] name [type] [default_expr] [AFTER name_after]`

​	例如，在数据表的末尾增加新字段：

`ALTER TABLE testcol_v1 ADD COLUMN OS String DEFAULT 'mac'`

​	或是通过AFTER修饰符，在指定字段的后面增加新字段：

`ALTER TABLE testcol_v1 ADD COLUMN IP String AFTER ID`

​	<u>对于数据表中已经存在的旧数据而言，新追加的字段会使用默认值补全</u>。

### 4.3.2 修改数据类型

​	如果需要改变表字段的数据类型或者默认值，需要使用下面的语法：

```sql
ALTER TABLE tb_name MODIFY COLUMN [IF EXISTS] name [type] [default_expr]
```

​	<u>修改某个字段的数据类型，实质上会调用相应的**toType**转型方法</u>。<u>如果当前的类型与期望的类型不能兼容，则修改操作将会失败</u>。例如，将String类型的IP字段修改为IPv4类型是可行的：

```sql
ALTER TABLE testcol_v1 MODIFY COLUMN IP IPv4
```

​	而尝试将String类型转为UInt类型就会出现错误：

```shell
ALTER TABLE testcol_v1 MODIFY COLUMN OS UInt32
DB::Exception: Cannot parse string 'mac' as UInt32: syntax error at begin of
string.
```

### 4.3.3 修改备注

​	做好信息备注是保持良好编程习惯的美德之一，所以如果你还没有为列字段添加备注信息，那么就赶紧行动吧。追加备注的语法如下所示：

```sql
ALTER TABLE tb_name COMMENT COLUMN [IF EXISTS] name 'some comment'
```

​	例如，为ID字段增加备注：

```sql
ALTER TABLE testcol_v1 COMMENT COLUMN ID '主键ID'
```

​	使用DESC查询可以看到上述增加备注的操作已经生效：

```sql
DESC testcol_v1
┌─name─────┬─type──┬─comment─┐
│ ID │ String │ 主键ID │
└─────────┴─────┴──────┘
```

### 4.3.4 删除已有字段

​	假如要删除某个字段，可以使用下面的语句：

```sql
ALTER TABLE tb_name DROP COLUMN [IF EXISTS] name
```

​	例如，执行下面的语句删除URL字段：

```sql
ALTER TABLE testcol_v1 DROP COLUMN URL
```

​	上述列字段在被删除之后，它的数据也会被连带删除。进一步来到testcol_v1的数据目录查验，会发现URL的数据文件已经被删除了;

### * 4.3.5 移动数据表

​	在Linux系统中，mv命令的本意是将一个文件从原始位置A移动到目标位置B，但是如果位置A与位置B相同，则可以变相实现重命名的作用。ClickHouse的RENAME查询就与之有着异曲同工之妙，RENAME语句的完整语法如下所示：

```sql
RENAME TABLE [db_name11.]tb_name11 TO [db_name12.]tb_name12,
[db_name21.]tb_name21 TO [db_name22.]tb_name22, ...
```

​	RENAME可以修改数据表的名称，如果将原始数据库与目标数据库设为不同的名称，那么就可以实现数据表在两个数据库之间移动的效果。例如在下面的例子中，testcol_v1从default默认数据库被移动到了db_test数据库，同时数据表被重命名为testcol_v2：

```sql
RENAME TABLE default.testcol_v1 TO db_test.testcol_v2
```

​	**需要注意的是，<u>数据表的移动只能在单个节点的范围内</u>。换言之，数据表移动的目标数据库和原始数据库必须处在同一个服务节点内，而不能是集群中的远程节点**。

### 4.3.6 清空数据表

​	假设需要将表内的数据全部清空，而不是直接删除这张表，则可以使用TRUNCATE语句，它的完整语法如下所示：

```sql
TRUNCATE TABLE [IF EXISTS] [db_name.]tb_name
```

​	例如执行下面的语句，就能将db_test.testcol_v2的数据一次性清空：

```sql
TRUNCATE TABLE db_test.testcol_v2
```

## 4.4 数据分区的基本操作

​	了解并善用数据分区益处颇多，熟练掌握它的使用方法，可以为后续的程序设计带来极大的灵活性和便利性，**目前只有MergeTree系列的表引擎支持数据分区**。

### 4.4.1 查询分区信息

​	ClickHouse内置了许多system系统表，用于查询自身的状态信息。其中parts系统表专门用于查询数据表的分区信息。例如执行下面的语句，就能够得到数据表partition_v2的分区状况：

```sql
SELECT partition_id,name,table,database FROM system.parts WHERE table =
'partition_v2'
┌─partition_id─┬─name───────┬─table─────┬─database┐
│ 201905 │ 201905_1_1_0_6 │ partition_v2 │ default │
│ 201910 │ 201910_3_3_0_6 │ partition_v2 │ default │
│ 201911 │ 201911_4_4_0_6 │ partition_v2 │ default │
│ 201912 │ 201912_5_5_0_6 │ partition_v2 │ default │
└──────────┴──────────┴─────────┴──────┘
```

​	如上所示，目前partition_v2共拥有4个分区，其中partition_id或者name等同于分区的主键，可以基于它们的取值确定一个具体的分区。

### 4.4.2 删除指定分区

​	合理地设计分区键并利用分区的删除功能，就能够达到数据更新的目的。删除一个指定分区的语法如下所示：

```sql
ALTER TABLE tb_name DROP PARTITION partition_expr
```

​	假如现在需要更新partition_v2数据表整个7月份的数据，则可以先将7月份的分区删除：

```sql
ALTER TABLE partition_v2 DROP PARTITION 201907
```

​	然后将整个7月份的新数据重新写入，就可以达到更新的目的：

```sql
INSERT INTO partition_v2 VALUES ('A004-update','www.bruce.com', '2019-07-02'),…
```

​	查验数据表，可以看到7月份的数据已然更新：

```sql
SELECT * from partition_v2 ORDER BY EventTime
┌─ID───────┬─URL──────┬ EventTime ┐
│ A001 │ www.nauu.com │ 2019-05-02 │
│ A002 │ www.nauu1.com │ 2019-06-02 │
│ A004-update │ www.bruce.com │ 2019-07-02 │
└─────────┴─────────┴───────┘
```

### * 4.4.3 复制分区数据

​	<u>**ClickHouse支持将A表的分区数据复制到B表**，这项特性可以用于快速数据写入、多表间数据同步和备份等场景</u>，它的完整语法如下：

```sql
ALTER TABLE B REPLACE PARTITION partition_expr FROM A
```

​	不过需要注意的是，并不是任意数据表之间都能够相互复制，它们还需要满足两个前提条件：

+ **两张表需要拥有相同的分区键**；

+ **它们的表结构完全相同**。

​	假设数据表partition_v2与先前的partition_v1分区键和表结构完全相同，那么应先在partition_v1中写入一批8月份的新数据：

```sql
INSERT INTO partition_v1 VALUES ('A006-v1','www.v1.com', '2019-08-05'),('A007-
v1','www.v1.com', '2019-08-20')
```

​	再执行下面的语句：

```sql
ALTER TABLE partition_v2 REPLACE PARTITION 201908 FROM partition_v1
```

​	即能够将partition_v1的整个201908分区中的数据复制到partition_v2：

```sql
SELECT * from partition_v2 ORDER BY EventTime
┌─ID───────┬─URL──────┬─EventTime─┐
│ A000 │ www.nauu.com │ 2019-05-01 │
│ A001 │ www.nauu.com │ 2019-05-02 │
省略…
│ │ │ │
│ A004-update │ www.bruce.com │ 2019-07-02 │
│ A006-v1 │ www.v1.com │ 2019-08-05 │
│ A007-v1 │ www.v1.com │ 2019-08-20 │
└─────────┴─────────┴───────┘
```

### 4.4.4 重置分区数据

​	如果数据表某一列的数据有误，需要将其重置为初始值，此时可以使用下面的语句实现：

```sql
ALTER TABLE tb_name CLEAR COLUMN column_name IN PARTITION partition_expr
```

​	对于默认值的含义，笔者遵循如下原则：

+ **如果声明了默认值表达式，则以表达式为准**；
+ **否则以相应数据类型的默认值为准**。

​	例如，执行下面的语句会重置partition_v2表内201908分区的URL数据重置。

```sql
ALTER TABLE partition_v2 CLEAR COLUMN URL in PARTITION 201908
```

​	查验数据后会发现，URL字段已成功被全部重置为空字符串了（String类型的默认值）。

```sql
SELECT * from partition_v2
┌─ID────┬─URL─┬──EventTime┐
│ A006-v1 │ │ 2019-08-05 │
│ A007-v1 │ │ 2019-08-20 │
└──────┴────┴────────┘
```

### * 4.4.5 卸载与装载分区

​	<u>表分区可以通过DETACH语句卸载，分区被卸载后，它的物理数据并没有删除，而是被转移到了当前数据表目录的detached子目录下。而装载分区则是反向操作，它能够将detached子目录下的某个分区重新装载回去</u>。卸载与装载这一对伴生的操作，常用于分区数据的**迁移**和**备份**场景。卸载某个分区的语法如下所示：

```sql
ALTER TABLE tb_name DETACH PARTITION partition_expr
```

​	例如，执行下面的语句能够将partition_v2表内整个8月份的分区卸载：

```sql
ALTER TABLE partition_v2 DETACH PARTITION 201908
```

​	此时再次查询这张表，会发现其中2019年8月份的数据已经没有了。而进入partition_v2的磁盘目录，则可以看到被卸载的分区目录已经被移动到了detached目录中：

```shell
# pwd
/chbase/data/data/default/partition_v2/detached
# ll
total 4
drwxr-x---. 2 clickhouse clickhouse 4096 Aug 31 23:16 201908_4_4_0
```

​	记住，**一旦分区被移动到了detached子目录，就代表它已经脱离了ClickHouse的管理，ClickHouse并不会主动清理这些文件**。这些分区文件会一直存在，除非我们主动删除或者使用ATTACH语句重新装载它们。装载某个分区的完整语法如下所示：

```sql
ALTER TABLE tb_name ATTACH PARTITION partition_expr
```

​	再次执行下面的语句，就可以将刚才已被卸载的201908分区重新装载回去：

```sql
ALTER TABLE partition_v2 ATTACH PARTITION 201908
```

### 4.4.6 备份与还原分区

​	关于分区数据的备份，可以通过**FREEZE**与**FETCH**实现，由于目前还缺少相关的背景知识，所以笔者把它留到第11章专门介绍。

## * 4.5 分布式DDL执行

​	ClickHouse支持集群模式，一个集群拥有1到多个节点。<u>CREATE、ALTER、DROP、RENMAE及TRUNCATE这些DDL语句，都**支持分布式执行**</u>。<u>这意味着，如果在集群中任意一个节点上执行DDL语句，那么集群中的每个节点都会以相同的顺序执行相同的语句</u>。这项特性意义非凡，它就如同批处理命令一样，省去了需要依次去单个节点执行DDL的烦恼。

​	将一条普通的DDL语句转换成分布式执行十分简单，只需加上`ON CLUSTER cluster_name`声明即可。例如，执行下面的语句后将会对ch_cluster集群内的所有节点广播这条DDL语句：

```sql
CREATE TABLE partition_v3 ON CLUSTER ch_cluster(
  ID String,
  URL String,
  EventTime Date
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(EventTime)
ORDER BY ID
```

​	当然，如果现在执行这条语句是不会成功的。因为到目前为止还没有配置过ClickHouse的集群模式，目前还不存在一个名为ch_cluster的集群，这部内容会放到第10章展开说明。

## * 4.6 数据的写入

​	INSERT语句支持三种语法范式，三种范式各有不同，可以根据写入的需求灵活运用。其中，第一种是使用VALUES格式的常规语法：

```sql
INSERT INTO [db.]table [(c1, c2, c3…)] VALUES (v11, v12, v13…), (v21, v22, v23…),
...
```

​	其中，c1、c2、c3是列字段声明，可省略。VALUES后紧跟的是由元组组成的待写入数据，通过下标位与列字段声明一一对应。数据支持批量声明写入，多行数据之间使用逗号分隔。例如执行下面的语句，将批量写入多条数据：

```sql
INSERT INTO partition_v2 VALUES ('A0011','www.nauu.com', '2019-10-01'),
('A0012','www.nauu.com', '2019-11-20'),('A0013','www.nauu.com', '2019-12-20')
```

​	在使用VALUES格式的语法写入数据时，**支持加入表达式或函数**，例如：

```sql
INSERT INTO partition_v2 VALUES ('A0014',toString(1+2), now())
```

​	第二种是使用指定格式的语法：

```sql
INSERT INTO [db.]table [(c1, c2, c3…)] FORMAT format_name data_set
```

​	ClickHouse支持多种数据格式（更多格式可参见官方手册），以常用的CSV格式写入为例：

```sql
INSERT INTO partition_v2 FORMAT CSV \
'A0017','www.nauu.com', '2019-10-01' \
'A0018','www.nauu.com', '2019-10-01'
```

​	第三种是使用SELECT子句形式的语法：

```sql
INSERT INTO [db.]table [(c1, c2, c3…)] SELECT ...
```

​	通过SELECT子句可将查询结果写入数据表，假设需要将partition_v1的数据写入partition_v2，则可以使用下面的语句：

```sql
INSERT INTO partition_v2 SELECT * FROM partition_v1
```

​	**在通过SELECT子句写入数据的时候，同样也支持加入表达式或函数**，例如：

```sql
INSERT INTO partition_v2 SELECT 'A0020', 'www.jack.com', now()
```

​	<u>虽然VALUES和SELECT子句的形式都支持声明表达式或函数，但是表达式和函数会带来额外的性能开销，从而导致写入性能的下降。所以如果追求极致的写入性能，就应该尽可能避免使用它们</u>。

​	在第2章曾介绍过，**ClickHouse内部所有的数据操作都是面向Block数据块的，所以INSERT查询最终会将数据转换为Block数据块**。也正因如此，INSERT语句在单个数据块的写入过程中是具有原子性的。在默认的情况下，每个数据块最多可以写入1048576行数据（由max_insert_block_size参数控制）。也就是说，**如果一条INSERT语句写入的数据少于max_insert_block_size行，那么这批数据的写入是具有<u>原子性</u>的，即要么全部成功，要么全部失败**。需要注意的是，**<u>只有在ClickHouse服务端处理数据的时候才具有这种原子写入的特性</u>**，例如使用JDBC或者HTTP接口时。因为**max_insert_block_size参数在使用CLI命令行或者INSERT SELECT子句写入时是不生效的**。

## * 4.7 数据的删除与修改

> [【ClickHouse】Clickhouse中update/delete的使用_半塘少年的博客-CSDN博客_clickhouse update](https://blog.csdn.net/qq_41893274/article/details/117093168)

​	ClickHouse提供了DELETE和UPDATE的能力，这类操作被称为Mutation查询，它可以看作ALTER语句的变种。虽然Mutation能最终实现修改和删除，但不能完全以通常意义上的UPDATE和DELETE来理解，我们必须清醒地认识到它的不同：

+ 首先，**Mutation语句是一种“很重”的操作，更适用于批量数据的修改和删除**；
+ 其次，**它不支持事务**，一旦语句被提交执行，就会立刻对现有数据产生影响，**无法回滚**；
+ 最后，**Mutation语句的执行是一个异步的后台过程**，语句被提交之后就会立即返回。所以这并不代表具体逻辑已经执行完毕，它的**具体执行进度需要通过system.mutations系统表查询**。

​	DELETE语句的完整语法如下所示：

```sql
ALTER TABLE [db_name.]table_name DELETE WHERE filter_expr
```

​	数据删除的范围由WHERE查询子句决定。例如，执行下面语句可以删除partition_v2表内所有ID等于A003的数据：

```sql
ALTER TABLE partition_v2 DELETE WHERE ID = 'A003'
```

​	由于演示的数据很少，DELETE操作给人的感觉和常用的OLTP数据库无异。但是我们心中应该要明白这是一个异步的后台执行动作。

​	再次进入数据目录，让我们看看删除操作是如何实现的：

```shell
# pwd
/chbase/data/data/default/partition_v2
# ll
total 52
drwxr-x---. 2 clickhouse clickhouse 4096 Jul 6 15:03 201905_1_1_0
drwxr-x---. 2 clickhouse clickhouse 4096 Jul 6 15:03 201905_1_1_0_6
省略…
drwxr-x---. 2 clickhouse clickhouse 4096 Jul 6 15:03 201909_5_5_0
drwxr-x---. 2 clickhouse clickhouse 4096 Jul 6 15:03 201909_5_5_0_6
drwxr-x---. 2 clickhouse clickhouse 4096 Jul 6 15:02 detached
-rw-r-----. 1 clickhouse clickhouse 1 Jul 6 15:02 format_version.txt
-rw-r-----. 1 clickhouse clickhouse 89 Jul 6 15:03 mutation_6.txt
```

​	可以发现，在执行了DELETE操作之后数据目录发生了一些变化。每一个原有的数据目录都额外增加了一个同名目录，并且在末尾处增加了_6的后缀。此外，目录下还多了一个名为mutation_6.txt的文件，mutation_6.txt文件的内容如下所示：

```shell
# cat mutation_6.txt
format version: 1
create time: 2019-07-06 15:03:27
commands: DELETE WHERE ID = \'A003\'
```

​	原来mutation_6.txt是一个日志文件，它完整地记录了这次DELETE操作的执行语句和时间，而文件名的后缀_6与新增目录的后缀对应。那么后缀的数字从何而来呢？继续查询system.mutations系统表，一探究竟：

```sql
SELECT database, table ,mutation_id, block_numbers.number as num ,is_done FROM
system.mutations
┌─database─┬─table─────┬─mutation_id───┬─num──┬─is_done─┐
│ default │ partition_v2 │ mutation_6.txt │ [6] │ 1 │
└───────┴────────┴──────────┴─────┴──────┘
```

​	至此，整个Mutation操作的逻辑就比较清晰了。每执行一条ALTER DELETE语句，都会在mutations系统表中生成一条对应的执行计划，当is_done等于1时表示执行完毕。与此同时，在数据表的根目录下，会以mutation_id为名生成与之对应的日志文件用于记录相关信息。而**数据删除的过程是以数据表的每个分区目录为单位，将所有目录重写为新的目录**，新目录的命名规则是在原有名称上加上`system.mutations.block_numbers.number`。数据在重写的过程中会将需要删除的数据去掉。旧的数据目录并不会立即删除，而是会被标记成非激活状态（active为0）。等到MergeTree引擎的下一次合并动作触发时，这些非激活目录才会被真正从物理意义上删除。

​	数据修改除了需要指定具体的列字段之外，整个逻辑与数据删除别无二致，它的完整语法如下所示：

```sql
ALTER TABLE [db_name.]table_name UPDATE column1 = expr1 [, ...] WHERE filter_expr
```

​	**UPDATE支持在一条语句中同时定义多个修改字段，<u>分区键和主键不能作为修改字段</u>**。例如，执行下面的语句即能够根据WHERE条件同时修改partition_v2内的URL和OS字段：

```sql
ALTER TABLE partition_v2 UPDATE URL = 'www.wayne.com',OS = 'mac' WHERE ID IN
(SELECT ID FROM partition_v2 WHERE EventTime = '2019-06-01')
```

## 4.8 本章小结

​	通过对本章的学习，我们知道了ClickHouse的数据类型是由**基础类型**、**复合类型**和**特殊类型**组成的。基础类型相比常规数据库显得精简干练；复合类型很实用，常规数据库通常不具备这些类型；而特殊类型的使用场景较少。同时我们也掌握了数据库、数据表、临时表、分区表和视图的基本操作以及对元数据和分区的基本操作。最后我们还了解到在ClickHouse中如何写入、修改和删除数据。本章的内容为介绍后续知识点打下了坚实的基础。下一章我们将介绍数据字典。

# 5. 数据字典

​	数据字典是ClickHouse提供的一种非常简单、实用的存储媒介，它以键值和属性映射的形式定义数据。字典中的数据会主动或者被动（数据是在ClickHouse启动时主动加载还是在首次查询时惰性加载由参数设置决定）加载到内存，并**支持动态更新**。<u>由于字典数据**常驻内存**的特性，所以它非常适合保存常量或经常使用的维度表数据，以避免不必要的JOIN查询</u>。

​	数据字典分为**内置**与**扩展**两种形式，顾名思义，内置字典是ClickHouse默认自带的字典，而外部扩展字典是用户通过自定义配置实现的字典。<u>在正常情况下，字典中的数据只能通过字典函数访问（ClickHouse特别设置了一类字典函数，专门用于字典数据的取用）</u>。但是也有一种例外，那就是使用特殊的字典表引擎。**在字典表引擎的帮助下，可以将数据字典挂载到一张代理的数据表下，从而实现数据表与字典数据的JOIN查询**。关于字典表引擎的更多细节与使用方法将会在后续章节着重介绍。

## 5.1 内置字典

​	ClickHouse目前只有一种内置字典——Yandex.Metrica字典。从名称上可以看出，这是用在ClickHouse自家产品上的字典，而它的设计意图是快速存取geo地理数据。但较为遗憾的是，由于版权原因Yandex并没有将geo地理数据开放出来。这意味着ClickHouse目前的内置字典，只是提供了字典的定义机制和取数函数，而没有内置任何现成的数据。所以内置字典的现状较为尴尬，需要遵照它的字典规范自行导入数据。

### 5.1.1 内置字典配置说明

​	内置字典在默认的情况下是禁用状态，需要开启后才能使用。开启它的方式也十分简单，只需将config.xml文件中path_to_regions_hierarchy_file和path_to_regions_names_files两项配置打开。

```xml
<path_to_regions_hierarchy_file>
  /opt/geo/regions_hierarchy.txt
</path_to_regions_hierarchy_file>
<path_to_regions_names_files>
  /opt/geo/
</path_to_regions_names_files>
```

​	这两项配置是惰性加载的，只有当字典首次被查询的时候才会触发加载动作。填充Yandex.Metrica字典的geo地理数据由两组模型组成，可以分别理解为地区数据的主表及维度表。这两组模型的数据分别由上述两项配置指定，现在依次介绍它们的具体用法。

1. path_to_regions_hierarchy_file

   ​	path_to_regions_hierarchy_file等同于区域数据的主表，由1个regions_hierarchy.txt和多个regions_hierarchy_[name].txt区域层次的数据文件共同组成，缺一不可。其中[name]表示区域标识符，与i18n类似。这些TXT文件内的数据需要使用TabSeparated格式定义，其数据模型的格式如表5-1所示。

   表5-1 regions_hierarchy数据模型说明

   | 名称             | 类型   | 是否必填 | 说明                                                         |
   | ---------------- | ------ | -------- | ------------------------------------------------------------ |
   | Region ID        | UInt32 | 是       | 区域ID                                                       |
   | Parent Region ID | UInt32 | 是       | 上级区域ID                                                   |
   | Region Type      | UInt8  | 是       | 区域类型：<br/>1：continent<br/>3：country<br/>4：federal district<br/>5：region<br/>6：city |
   | Population       | UInt32 | 否       | 人口                                                         |

2. path_to_regions_names_files

   ​	path_to_regions_names_files等同于区域数据的维度表，记录了与区域ID对应的区域名称。维度数据使用6个regions_names_[name].txt文件保存，其中[name]表示区域标识符与regions_hierarchy_[name].txt对应，目前包括ru、en、ua、by、kz和tr。上述这些区域的数据文件必须全部定义，这是因为内置字典在初次加载时，会一次性加载上述6个区域标识的数据文件。如果缺少任何一个文件就会抛出异常并导致初始化失败。

   ​	这些TXT文件内的数据同样需要使用TabSeparated格式定义，其数据模型的格式如表5-2所示。

   表5-2 regions_names数据模型说明

   | 名称        | 类型   | 是否必填 | 说明     |
   | ----------- | ------ | -------- | -------- |
   | Region ID   | UInt32 | 是       | 区域ID   |
   | Parent Name | String | 是       | 区域名称 |

### 5.1.2 使用内置字典

​	在知晓了内置字典的开启方式和Yandex.Metrica字典的数据模型之后，就可以配置字典的数据并使用它们了。首先，在/opt路径下新建geo目录：

```shell
# mkdir /opt/geo
```

​	接着，进入本书附带的演示代码，找到数据字典目录。为了便于读者测试，事先已经准备好了一份测试数据，将下列用于测试的数据文件复制到刚才已经建好的/opt/geo目录下：

```shell
/opt/geo
# ll
total 36
-rw-r--r--. 1 root root 3096 Jul 7 20:38 regions_hierarchy_ru.txt
-rw-r--r--. 1 root root 3096 Jul 7 20:38 regions_hierarchy.txt
-rw-r--r--. 1 root root 3957 Jul 7 19:44 regions_names_ar.txt
-rw-r--r--. 1 root root 3957 Jul 7 19:44 regions_names_by.txt
-rw-r--r--. 1 root root 3957 Jul 7 19:44 regions_names_en.txt
-rw-r--r--. 1 root root 3957 Jul 7 19:44 regions_names_kz.txt
-rw-r--r--. 1 root root 3957 Jul 7 19:44 regions_names_ru.txt
-rw-r--r--. 1 root root 3957 Jul 7 19:44 regions_names_tr.txt
-rw-r--r--. 1 root root 3957 Jul 7 19:44 regions_names_ua.txt
```

​	最后，找到config.xml并按照5.1.1节介绍的方法开启内置字典。

​	至此，内置字典就已经全部设置好了，执行下面的语句就能够访问字典中的数据：

```sql
SELECT regionToName(toUInt32(20009))
┌─regionToName(toUInt32(20009))─┐
│ Buenos Aires Province │
└────────────────────┘
```

​	可以看到，对于Yandex.Metrica字典数据的访问，这里用到了regionToName函数。类似这样的函数还有很多，在ClickHouse中它们被称为Yandex.Metrica函数。关于这套函数的更多用法，请参阅官方手册。

## 5.2 外部扩展字典

​	外部扩展字典是以插件形式注册到ClickHouse中的，由用户自行定义数据模式及数据来源。目前扩展字典支持7种类型的内存布局和4类数据来源。相比内容十分有限的内置字典，扩展字典才是更加常用的功能。

### 5.2.1 准备字典数据

​	在接下来的篇幅中，会逐个介绍每种扩展字典的使用方法，包括它们的配置形式、数据结构及创建方法，但是在此之前还需要进行一些准备工作。为了便于演示，此处事先准备了三份测试数据，它们均使用CSV格式。其中，第一份是企业组织数据，它将用于flat、hashed、cache、complex_key_hashed和complex_key_cache字典的演示场景。这份数据拥有id、code和name三个字段，数据格式如下所示：

```shell
1,"a0001","研发部"
2,"a0002","产品部"
3,"a0003","数据部"
4,"a0004","测试部"
5,"a0005","运维部"
6,"a0006","规划部"
7,"a0007","市场部"
```

​	第二份是销售数据，它将用于range_hashed字典的演示场景。这份数据拥有id、start、end和price四个字段，数据格式如下所示：

```shell
1,2016-01-01,2017-01-10,100
2,2016-05-01,2017-07-01,200
3,2014-03-05,2018-01-20,300
4,2018-08-01,2019-10-01,400
5,2017-03-01,2017-06-01,500
6,2017-04-09,2018-05-30,600
7,2018-06-01,2019-01-25,700
8,2019-08-01,2019-12-12,800
```

​	最后一份是asn数据，它将用于演示ip_trie字典的场景。这份数据拥有ip、asn和country三个字段，数据格式如下所示：

```shell
"82.118.230.0/24","AS42831","GB"
"148.163.0.0/17","AS53755","US"
"178.93.0.0/18","AS6849","UA"
"200.69.95.0/24","AS262186","CO"
"154.9.160.0/20","AS174","US"
```

> 你可以从下面的地址获取到上述三份数据：
>
> https://github.com/nauu/clickhousebook/dict/plugin/testdata/organization.csv
>
> https://github.com/nauu/clickhousebook/dict/plugin/testdata/sales.csv
>
> https://github.com/nauu/clickhousebook/dict/plugin/testdata/asn.csv
>
> 下载后，将数据文件上传到ClickHouse节点所在的服务器即可。

### 5.2.2 扩展字典配置文件的元素组成

​	扩展字典的配置文件由config.xml文件中的dictionaries_config配置项指定：

```xml
<!-- Configuration of external dictionaries. See:
https://clickhouse.yandex/docs/en/dicts/external_dicts/
-->
<dictionaries_config>*_dictionary.xml</dictionaries_config>
```

​	在默认的情况下，ClickHouse会自动识别并加载/etc/clickhouse-server目录下所有以_dictionary.xml结尾的配置文件。同时ClickHouse也能够动态感知到此目录下配置文件的各种变化，并**支持不停机在线更新配置文件**。

​	在单个字典配置文件内可以定义多个字典，其中每一个字典由一组dictionary元素定义。在dictionary元素之下又分为5个子元素，均为必填项，它们完整的配置结构如下所示：

```xml
<?xml version="1.0"?>
<dictionaries>
  <dictionary>
    <name>dict_name</name>
    <structure>
      <!—字典的数据结构 -->
    </structure>
    <layout>
      <!—在内存中的数据格式类型 -->
    </layout>
    <source>
      <!—数据源配置 -->
    </source>
    <lifetime>
      <!—字典的自动更新频率 -->
    </lifetime>
  </dictionary>
</dictionaries>
```

​	在上述结构中，主要配置的含义如下。

+ name：字典的名称，用于确定字典的唯一标识，必须全局唯一，多个字典之间不允许重复。

+ structure：字典的数据结构，5.2.3节会详细介绍。

+ layout：字典的类型，它决定了数据在内存中以何种结构组织和存储。目前扩展字典共拥有7种类型，5.2.4节会详细介绍。

+ source：字典的数据源，它决定了字典中数据从何处加载。目前扩展字典共拥有文件、数据库和其他三类数据来源，5.2.5节会详细介绍。

+ lifetime：字典的更新时间，扩展字典支持数据在线更新，5.2.6节会详细介绍。

### 5.2.3 扩展字典的数据结构

​	扩展字典的数据结构由structure元素定义，由键值key和属性attribute两部分组成，它们分别描述字典的数据标识和字段属性。structure的完整形式如下所示（在后面的查询过程中都会通过这些字段来访问字典中的数据）：

```xml
<dictionary>
  <structure>
    <!—- <id> 或 <key> -->
    <id>
      <!—Key属性-->
    </id>
    <attribute>
      <!—- 字段属性 -->
    </attribute>
    ...
  </structure>
</dictionary>
```

​	接下来具体介绍key和attribute的含义。

1. key

   ​	key用于定义字典的键值，每个字典必须包含1个键值key字段，用于定位数据，类似数据库的表主键。键值key分为数值型和复合型两类。

   1. 数值型：数值型key由UInt64整型定义，支持flat、hashed、range_hashed和cache类型的字典（扩展字典类型会在后面介绍），它的定义方法如下所示。

      ```xml
      <structure>
        <id>
          <!—名称自定义-->
          <name>Id</name>
        </id>
        省略…
      ```

   2. 复合型：复合型key使用Tuple元组定义，可以由1到多个字段组成，类似数据库中的复合主键。它仅支持complex_key_hashed、complex_key_cache和ip_trie类型的字典。其定义方法如下所示。

      ```xml
      <structure>
        <key>
          <attribute>
            <name>field1</name>
            <type>String</type>
          </attribute>
          <attribute>
            <name>field2</name>
            <type>UInt64</type>
          </attribute>
          省略…
        </key>
        省略…
      ```

2. attribute

   ​	attribute用于定义字典的属性字段，字典可以拥有1到多个属性字段。它的完整定义方法如下所示：

   ```xml
   <structure>
     省略…
     <attribute>
       <name>Name</name>
       <type>DataType</type>
       <!—空字符串-->
       <null_value></null_value>
       <expression>generateUUIDv4()</expression>
       <hierarchical>true</hierarchical>
       <injective>true</injective>
       <is_object_id>true</is_object_id>
     </attribute>
     省略…
   </structure>
   ```

   ​	在attribute元素下共有7个配置项，其中name、type和null_value为必填项。这些配置项的详细说明如表5-3所示。

   表5-3 attribute的配置项说明

   | 配置名称     | 是否必填 | 默认值   | 说明                                                         |
   | ------------ | -------- | -------- | ------------------------------------------------------------ |
   | name         | 是       | -        | 字段名称                                                     |
   | type         | 是       | -        | 字段类型，参见第4章的数据类型部分                            |
   | null_value   | 是       | -        | 在查询时，条件key没有对应元素时的默认值                      |
   | expression   | 否       | 无表达式 | 表达式，可以调用函数或者使用运算符                           |
   | hierarchical | 否       | false    | 是否支持层次结构                                             |
   | injective    | 否       | false    | 是否支持集合单射优化。开启后，在后续的GROUP BY查询中，如果调用dictGet函数通过key获得value，则该value直接从GROUP BY数据返回 |
   | is_object_id | 否       | false    | 是否开启MongoDB优化，通过ObjectID对MongoDB文档进行查询       |

   > 注意，假设有两个集合A和B。如果集合A中的每个元素x，在集合B中都有一个唯一预知对应的元素y，那么集合A到B的映射关系就是单射映射。

   > [单射_百度百科 (baidu.com)](https://baike.baidu.com/item/单射/9274884?fr=aladdin)

### 5.2.4 扩展字典的类型

​	扩展字典的类型使用layout元素定义，目前共有7种类型。一个字典的类型，既决定了其数据在内存中的存储结构，也决定了该字典支持的key键类型。根据key键类型的不同，可以将它们划分为两类：

+ 一类是以flat、hashed、range_hashed和cache组成的单数值key类型，因为它们均使用单个数值型的id；
+ 另一类则是由complex_key_hashed、complex_key_cache和ip_trie组成的复合key类型。

​	complex_key_hashed与complex_key_cache字典在功能方面与hashed和cache并无二致，只是单纯地将数值型key替换成了复合型key而已。

​	接下来会结合5.2.1节中已准备好的测试数据，逐一介绍7种字典的完整配置方法。通过这个过程，可以领略到不同类型字典的特点以及它们的使用方法。

1. flat

   flat字典是所有类型中性能最高的字典类型，它只能使用UInt64数值型key。顾名思义，flat字典的数据在内存中使用数组结构保存，数组的初始大小为1024，上限为500 000，这意味着它最多只能保存500 000行数据。如果在创建字典时数据量超出其上限，那么字典会创建失败。代码清单5-1所示是通过手动创建的flat字典配置文件。

   代码清单5-1 flat类型字典的配置文件

   test_flat_dictionary.xml

   ```xml
   <?xml version="1.0"?>
   <dictionaries>
     <dictionary>
       <name>test_flat_dict</name>
       <source>
         <!—准备好的测试数据-->
         <file>
           <path>/chbase/data/dictionaries/organization.csv</path>
           <format>CSV</format>
         </file>
       </source>
       <layout>
         <flat/>
       </layout>
       <!—与测试数据的结构对应-->
       <structure>
         <id>
           <name>id</name>
         </id>
         <attribute>
           <name>code</name>
           <type>String</type>
           <null_value></null_value>
         </attribute>
         <attribute>
           <name>name</name>
           <type>String</type>
           <null_value></null_value>
         </attribute>
       </structure>
       <lifetime>
         <min>300</min>
         <max>360</max>
       </lifetime>
     </dictionary>
   </dictionaries>
   ```

   ​	在上述的配置中，source数据源是CSV格式的文件，structure数据结构与其对应。将配置文件复制到ClickHouse服务节点的/etc/clickhouse-server目录后，即完成了对该字典的创建过程。查验system.dictionaries系统表后，能够看到flat字典已经创建成功。

   ```sql
   SELECT name, type, key, attribute.names, attribute.types FROM system.dictionaries
   ┌─name──────────┬─type─┬─key────┬─attribute.names─┐
   │ test_flat_dict │ Flat │ UInt64 │ ['code','name'] │
   └──────────────┴────┴───────┴───────────┘
   ```

2. hashed

   ​	hashed字典同样只能够使用UInt64数值型key，但与flat字典不同的是，hashed字典的数据在内存中通过散列结构保存，且没有存储上限的制约。代码清单5-2所示是仿照flat创建的hashed字典配置文件。

   代码清单5-2 hashed类型字典的配置文件

   test_hashed_dictionary.xml

   ```xml
   <?xml version="1.0"?>
   <dictionaries>
     <dictionary>
       <name>test_hashed_dict</name>
       与flat字典配置相同,省略…
       <layout>
         <hashed/>
       </layout>
       省略…
     </dictionary>
   </dictionaries>
   ```

   ​	同样将配置文件复制到ClickHouse服务节点的/etc/clickhouse-server目录后，即完成了对该字典的创建过程。

3. range_hashed

   ​	range_hashed字典可以看作hashed字典的变种，它在原有功能的基础上增加了指定时间区间的特性，数据会以散列结构存储并按照时间排序。时间区间通过range_min和range_max元素指定，所指定的字段必须是Date或者DateTime类型。

   ​	现在仿照hashed字典的配置，创建一个名为test_range_hashed_dictionary.xml的配置文件，将layout改为range_hashed并增加range_min和range_max元素。它的完整配置如代码清单5-3所示。

   代码清单5-3 range_hashed类型字典的配置文件

   test_range_hashed_dictionary.xml

   ```xml
   <?xml version="1.0"?>
   <dictionaries>
     <dictionary>
       <name>test_range_hashed_dict</name>
       <source>
         <file>
           <path>/chbase/data/dictionaries/sales.csv</path>
           <format>CSV</format>
         </file>
       </source>
       <layout>
         <range_hashed/>
       </layout>
       <structure>
         <id>
           <name>id</name>
         </id>
         <range_min>
           <name>start</name>
         </range_min>
         <range_max>
           <name>end</name>
         </range_max>
         <attribute>
           <name>price</name>
           <type>Float32</type>
           <null_value></null_value>
         </attribute>
       </structure>
       <lifetime>
         <min>300</min>
         <max>360</max>
       </lifetime>
     </dictionary>
   </dictionaries>
   ```

   ​	在上述的配置中，使用了一份销售数据，数据中的start和end字段分别与range_min和range_max对应。将配置文件复制到ClickHouse服务节点的/etc/clickhouse-server目录后，即完成了对该字典的创建过程。查验system.dictionaries系统表后，能够看到range_hashed字典已经创建成功：

   ```sql
   SELECT name, type, key, attribute.names, attribute.types FROM system.dictionaries
   ┌─name───────────┬─type─────┬─key───┬─attribute.names─┐
   │ test_range_hashed_dict │ RangeHashed │ UInt64 │ ['price'] │
   └──────────────┴────────┴─────┴───────────┘
   ```

4. cache

   ​	cache字典只能够使用UInt64数值型key，它的字典数据在内存中会通过固定长度的向量数组保存。定长的向量数组又称cells，它的数组长度由size_in_cells指定。而size_in_cells的取值大小必须是2的整数倍，如若不是，则会自动向上取为2的倍数的整数。

   ​	cache字典的取数逻辑与其他字典有所不同，它并不会一次性将所有数据载入内存。当从cache字典中获取数据的时候，它首先会在cells数组中检查该数据是否已被缓存。如果数据没有被缓存，它才会从源头加载数据并缓存到cells中。所以**cache字典是性能最不稳定的字典，因为它的性能优劣完全取决于缓存的命中率（缓存命中率=命中次数/查询次数），如果无法做到99%或者更高的缓存命中率，则最好不要使用此类型**。代码清单5-4所示是仿照hashed创建的cache字典配置文件。

   代码清单5-4 cache类型字典的配置文件

   test_cache_dictionary.xml

   ```xml
   <?xml version="1.0"?>
   <dictionaries>
     <dictionary>
       <name>test_cache_dict</name>
       <source>
         <!—- 本地文件需要通过 executable形式 -->
         <executable>
           <command>cat /chbase/data/dictionaries/organization.csv</command>
           <format>CSV</format>
         </executable>
       </source>
       <layout>
         <cache>
           <!—- 缓存大小 -->
           <size_in_cells>10000</size_in_cells>
         </cache>
       </layout>
       省略…
     </dictionary>
   </dictionaries>
   ```

   ​	在上述配置中，layout被声明为cache并将缓存大小size_in_cells设置为10000。关于cells的取值可以根据实际情况考虑，在内存宽裕的情况下设置成1000000000也是可行的。还有一点需要注意，如果cache字典使用本地文件作为数据源，则必须使用executable的形式设置。

5. complex_key_hashed

   ​	complex_key_hashed字典在功能方面与hashed字典完全相同，只是将单个数值型key替换成了复合型。代码清单5-5所示是仿照hashed字典进行配置后，将layout改为complex_key_hashed并替换key类型的示例。

   代码清单5-5 complex_key_hashed类型字典的配置文件

   test_complex_key_hashed_dictionary.xml

   ```xml
   <?xml version="1.0"?>
   <dictionaries>
     <dictionary>
       <name>test_complex_key_hashed_dict</name>
       <!—- 与hashed字典配置相同,省略…-->
       <layout>
         <complex_key_hashed/>
       </layout>
       <structure>
         <!—- 复合型key -->
         <key>
           <attribute>
             <name>id</name>
             <type>UInt64</type>
           </attribute>
           <attribute>
             <name>code</name>
             <type>String</type>
           </attribute>
         </key>
         省略…
       </structure>
       省略…
   ```

   ​	将配置文件复制到ClickHouse服务节点的/etc/clickhouseserver目录后，即完成了对该字典的创建过程。

6. complex_key_cache

   ​	complex_key_cache字典同样与cache字典的特性完全相同，只是将单个数值型key替换成了复合型。现在仿照cache字典进行配置，将layout改为complex_key_cache并替换key类型，如代码清单5-6所示。

   代码清单5-6 complex_key_cache类型字典的配置文件

   test_complex_key_cache_dictionary.xml

   ```xml
   <?xml version="1.0"?>
   <dictionaries>
     <dictionary>
       <name>test_complex_key_cache_dict</name>
       <!—-与cache字典配置相同,省略…-->
       <layout>
         <complex_key_cache>
           <size_in_cells>10000</size_in_cells>
         </complex_key_cache>
       </layout>
       <structure>
         <!—- 复合型Key -->
         <key>
           <attribute>
             <name>id</name>
             <type>UInt64</type>
           </attribute>
           <attribute>
             <name>code</name>
             <type>String</type>
           </attribute>
         </key>
         省略…
       </structure>
       省略…
   ```

   ​	将配置文件复制到ClickHouse服务节点的/etc/clickhouse-server目录后，即完成了对该字典的创建过程。

7. ip_trie

   ​	虽然同为复合型key的字典，但ip_trie字典却较为特殊，因为它只能指定单个String类型的字段，用于指代IP前缀。ip_trie字典的数据在内存中使用trie树结构保存，且专门用于IP前缀查询的场景，例如通过IP前缀查询对应的ASN信息。它的完整配置如代码清单5-7所示。

   代码清单5-7 ip_trie类型字典的配置文件

   test_ip_trie_dictionary.xml

   ```xml
   <?xml version="1.0"?>
   <dictionaries>
     <dictionary>
       <name>test_ip_trie_dict</name>
       <source>
         <file>
           <path>/chbase/data/dictionaries/asn.csv</path>
           <format>CSV</format>
         </file>
       </source>
       <layout>
         <ip_trie/>
       </layout>
       <structure>
         <!—虽然是复合类型,但是只能设置单个String类型的字段 -->
         <key>
           <attribute>
             <name>prefix</name>
             <type>String</type>
           </attribute>
         </key>
         <attribute>
           <name>asn</name>
           <type>String</type>
           <null_value></null_value>
         </attribute>
         <attribute>
           <name>country</name>
           <type>String</type>
           <null_value></null_value>
         </attribute>
       </structure>
       省略…
     </dictionary>
   </dictionaries>
   ```

   ​	通过上述介绍，读者已经知道了7种类型字典的创建方法。**在这些字典中，flat、hashed和range_hashed依次拥有最高的性能，而cache性能最不稳定**。最后再总结一下这些字典各自的特点，如表5-4所示。

表5-4 7种类型字典的特点总结

| 名称               | 存储结构         | 字典键类型            | 支持的数据来源                                   |
| ------------------ | ---------------- | --------------------- | ------------------------------------------------ |
| flat               | 数组             | UInt64                | Local file<br/>Executable file<br/>HTTP<br/>DBMS |
| hashed             | 散列             | UInt64                | Local file<br/>Executable file<br/>HTTP<br/>DBMS |
| range_hashed       | 散列并按时间排序 | UInt64和时间          | Local file<br/>Executable file<br/>HTTP<br/>DBMS |
| complex_key_hashed | 散列             | 复合型key             | Local file<br/>Executable file<br/>HTTP<br/>DBMS |
| ip_trie            | 层次结构         | 复合型key(单个String) | Local file<br/>Executable file<br/>HTTP<br/>DBMS |
| cache              | 固定大小数组     | UInt64                | Executable file<br/>HTTP<br/>ClickHouse、MySQL   |
| complex_key_cache  | 固定大小数组     | 复合型key             | Executable file<br/>HTTP<br/>ClickHouse、MySQL   |

### 5.2.5 扩展字典的数据源

​	数据源使用source元素定义，它指定了字典的数据从何而来。通过5.2.4节其实大家已经领略过本地文件与可执行文件这两种数据源了，但扩展字典支持的数据源远不止这些。现阶段，扩展字典支持3大类共计9种数据源，接下来会以更加体系化的方式逐一介绍它们。

1. 文件类型

   文件可以细分为本地文件、可执行文件和远程文件三类，它们是最易使用且最为直接的数据源，非常适合在静态数据这类场合中使用。

   1. 本地文件

      本地文件使用file元素定义。其中，path表示数据文件的绝对路径，而format表示数据格式，例如CSV或者TabSeparated等。它的完整配置如下所示。

      ```xml
      <source>
        <file>
          <path>/data/dictionaries/organization.csv</path>
          <format>CSV</format>
        </file>
      </source>
      ```

   2. 可执行文件

      可执行文件数据源属于本地文件的变种，它需要通过cat命令访问数据文件。对于cache和complex_key_cache类型的字典，必须使用此类型的文件数据源。可执行文件使用executable元素定义。其中，command表示数据文件的绝对路径，format表示数据格式，例如CSV或者TabSeparated等。它的完整配置如下所示。

      ```xml
      <source>
        <executable>
          <command>cat /data/dictionaries/organization.csv</ command>
          <format>CSV</format>
        </executable>
      </source>
      ```

   3. 远程文件

      远程文件与可执行文件类似，只是它将cat命令替换成了post请求，支持HTTP与HTTPS协议。远程文件使用http元素定义。其中，url表示远程数据的访问地址，format表示数据格式，例如CSV或者TabSeparated。它的完整配置如下所示。

      ```xml
      <source>
        <http>
          <url>http://10.37.129.6/organization.csv</url>
          <format>CSV</format>
        </http>
      </source>
      ```

2. 数据库类型

   ​	相比文件类型，数据库类型的数据源更适合在正式的生产环境中使用。目前扩展字典支持MySQL、ClickHouse本身及MongoDB三种数据库。接下来会分别介绍它们的创建方法。对于MySQL和MongoDB数据库环境的安装，由于篇幅原因此处不再赘述，而相关的SQL脚本可以在本书附带的源码站点中下载。

   1. MySQL

      MySQL数据源支持从指定的数据库中提取数据，作为其字典的数据来源。首先，需要准备源头数据，执行下面的语句在MySQL中创建测试表：

      ```sql
      CREATE TABLE 't_organization' (
        `id` int(11) NOT NULL AUTO_INCREMENT,
        `code` varchar(40) DEFAULT NULL,
        `name` varchar(60) DEFAULT NULL,
        `updatetime` datetime DEFAULT NULL,
        PRIMARY KEY (`id`)
      ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8;
      ```

      接着，写入测试数据：

      ```sql
      INSERT INTO t_organization (code, name,updatetime) VALUES('a0001','研发部',NOW())
      INSERT INTO t_organization (code, name,updatetime) VALUES('a0002','产品部',NOW())
      …
      ```

      完成上述准备之后，就可以配置MySQL数据源的字典了。现在仿照flat字典进行配置，创建一个名为test_mysql_dictionary.xml的配置文件，将source替换成MySQL数据源：

      ```xml
      <dictionaries>
        <dictionary>
          <name>test_mysql_dict</name>
          <source>
            <mysql>
              <port>3306</port>
              <user>root</user>
              <password></password>
              <replica>
                <host>10.37.129.2</host>
                <priority>1</priority>
              </replica>
              <db>test</db>
              <table>t_organization</table>
              <!--
      				<where>id=1</where>
      				<invalidate_query>SQL_QUERY</invalidate_query>
      				-->
            </mysql>
          </source>
          省略…
      ```

      其中，各配置项的含义分别如下。

      + port：数据库端口。

      + user：数据库用户名。

      + password：数据库密码。
      + replica：数据库host地址，支持MySQL集群。
      + db：database数据库。
      + table：字典对应的数据表。
      + where：查询table时的过滤条件，非必填项。
      + invalidate_query：指定一条SQL语句，用于在数据更新时判断是否需要更新，非必填项。5.2.6节会详细说明。

      将配置文件复制到ClickHouse服务节点的/etc/clickhouseserver目录后，即完成了对该字典的创建过程。

   2. ClickHouse

      扩展字典支持将ClickHouse数据表作为数据来源，这是一种比较有意思的设计。在配置之前同样需要准备数据源的测试数据，执行下面的语句在ClickHouse中创建测试表并写入测试数据：

      ```sql
      CREATE TABLE t_organization (
        ID UInt64,
        Code String,
        Name String,
        UpdateTime DateTime
      ) ENGINE = TinyLog;
      -- 写入测试数据
      INSERT INTO t_organization VALUES(1,'a0001','研发部',NOW()),(2,'a0002','产品部'
      ,NOW()),(3,'a0003','数据部',NOW()),(4,'a0004','测试部',NOW()),(5,'a0005','运维部'
      ,NOW()),(6,'a0006','规划部',NOW()),(7,'a0007','市场部',NOW())
      ```

      ClickHouse数据源的配置与MySQL数据源极为相似，所以我们可以仿照MySQL数据源的字典配置，创建一个名为test_ch_dictionary.xml的配置文件，将source替换成ClickHouse数据源：

      ```xml
      <?xml version="1.0"?>
      <dictionaries>
        <dictionary>
          <name>test_ch_dict</name>
          <source>
            <clickhouse>
              <host>10.37.129.6</host>
              <port>9000</port>
              <user>default</user>
              <password></password>
              <db>default</db>
              <table>t_organization</table>
              <!--
              <where>id=1</where>
              <invalidate_query>SQL_QUERY</invalidate_query>
              -->
            </clickhouse>
          </source>
          省略…
      ```

      其中，各配置项的含义分别如下。

      + host：数据库host地址。

      + port：数据库端口。

      + user：数据库用户名。

      + password：数据库密码。

      + db：database数据库。

      + table：字典对应的数据表。

      + where：查询table时的过滤条件，非必填项。

      + invalidate_query：指定一条SQL语句，用于在数据更新时判断是否需要更新，非必填项。在5.2.6节会详细说明。

   3. MongoDB

      最后是MongoDB数据源，执行下面的语句，MongoDB会自动创建相应的schema并写入数据：

      ```sql
      db.t_organization.insertMany(
        [{
         id: 1,
         code: 'a0001',
         name: '研发部'
         },
         {
         id: 2,
         code: 'a0002',
         name: '产品部'
         },
         {
         id: 3,
         code: 'a0003',
         name: '数据部'
         },
         {
         id: 4,
         code: 'a0004',
         name: '测试部'
         }]
      )
      ```

      完成上述准备之后就可以配置MongoDB数据源的字典了，同样仿照MySQL字典配置，创建一个名为test_mongodb_dictionary.xml的配置文件，将source替换成mongodb数据源：

      ```xml
      <dictionaries>
        <dictionary>
          <name>test_mongodb_dict</name>
          <source>
            <source>
              <mongodb>
                <host>10.37.129.2</host>
                <port>27017</port>
                <user></user>
                <password></password>
                <db>test</db>
                <collection>t_organization</collection>
              </mongodb>
            </source>
            省略…
      ```

      其中，各配置项的含义分别如下。

      + host：数据库host地址。

      + port：数据库端口。

      + user：数据库用户名。

      + password：数据库密码。

      + db：database数据库。

      + collection：与字典对应的collection的名称。

3. 其他类型

   除了上述已经介绍过的两类数据源之外，扩展字典还支持通过ODBC的方式连接PostgreSQL和MS SQL Server数据库作为数据源。它们的配置方式与数据库类型数据源大同小异，此处不再赘述，如有需要请参见官方手册。

### * 5.2.6 扩展字典的数据更新策略

​	扩展字典支持数据的在线更新，更新后无须重启服务。字典数据的更新频率由配置文件中的lifetime元素指定，单位为秒：

```xml
<lifetime>
  <min>300</min>
  <max>360</max>
</lifetime>
```

​	其中，min与max分别指定了更新间隔的上下限。ClickHouse会在这个时间区间内随机触发更新动作，这样能够有效错开更新时间，避免所有字典在同一时间内爆发性的更新。<u>当min和max都是0的时候，将禁用字典更新。对于cache字典而言，lifetime还代表了它的缓存失效时间</u>。

​	**字典内部拥有版本的概念，在数据更新的过程中，旧版本的字典将持续提供服务，只有当更新完全成功之后，新版本的字典才会替代旧版本。所以更新操作或者更新时发生的异常，并不会对字典的使用产生任何影响**。

​	不同类型的字典数据源，更新机制也稍有差异。总体来说，**扩展字典目前并不支持增量更新**。<u>但部分数据源能够依照标识判断，只有在源数据发生实质变化后才实施更新动作。这个判断源数据是否被修改的标识，在字典内部称为previous，它保存了一个用于比对的值。ClickHouse的后台进程每隔5秒便会启动一次数据刷新的判断，依次对比每个数据字典中前后两次previous的值是否相同。若相同，则代表无须更新数据；若不同且满足更新频率，则代表需要更新数据</u>。而对于previous值的获取方式，不同的数据源有不同的实现逻辑，下面详细介绍。

1. 文件数据源

   对于文件类型的数据源，它的previous值来自系统文件的修改时间，这和Linux系统中的stat查询命令类似：

   ```shell
   #stat ./test_flat_dictionary.xml
   File: `./test_flat_dictionary.xml'
   Size: 926 Blocks: 8 IO Block: 4096 regular file
   Access: 2019-07-18 01:15:43.509000359 +0800
   Modify: 2019-07-18 01:15:32.000000000 +0800
   Change: 2019-07-18 01:15:38.865999868 +0800
   ```

   当前后两次previous的值不相同时，才会触发数据更新。

2. MySQL(InnoDB)、ClickHouse和ODBC

   对于MySQL（InnoDB引擎）、ClickHouse和ODBC数据源，它们的previous值来源于invalidate_query中定义的SQL语句。例如在下面的示例中，如果前后两次的updatetime值不相同，则会判定源数据发生了变化，字典需要更新。

   ```xml
   <source>
     <mysql>
       省略…
       <invalidate_query>select updatetime from t_organization where id =
         8</invalidate_query>
     </mysql>
   </source>
   ```

   这对源表有一定的要求，它必须拥有一个支持判断数据是否更新的字段。

3. MySQL(MyISAM)

   如果数据源是MySQL的MyISAM表引擎，则它的previous值要简单得多。因为在MySQL中，使用MyISAM表引擎的数据表支持通过`SHOW TABLE STATUS`命令查询修改时间。例如在MySQL中执行下面的语句，就能够查询到数据表的Update_time值：

   ```sql
   SHOW TABLE STATUS WHERE Name = 't_organization'
   ```

   所以，如果前后两次Update_time的值不相同，则会判定源数据发生了变化，字典需要更新。

4. 其他数据源

   <u>除了上面描述的数据源之外，其他数据源目前无法依照标识判断是否跳过更新。所以无论数据是否发生实质性更改，只要满足当前lifetime的时间要求，它们都会执行更新动作</u>。**相比之前介绍的更新方式，其他类型的更新效率更低**。

   除了按照lifetime定义的时间频率被动更新之外，数据字典也能够主动触发更新。执行下面的语句后，将会触发所有数据字典的更新：

   ```sql
   SYSTEM RELOAD DICTIONARIES
   ```

   也支持指定某个具体字典的更新：

   ```sql
   SYSTEM RELOAD DICTIONARY [dict_name]
   ```

### 5.2.7 扩展字典的基本操作

​	至此，我们已经在ClickHouse中创建了10种不同类型的扩展字典。接下来将目光聚焦到字典的基本操作上，包括对字典元数据和数据的查询，以及借助字典表引擎访问数据。

1. 元数据查询

   通过system.dictionaries系统表，可以查询扩展字典的元数据信息。例如执行下面的语句就可以看到目前所有已经创建的扩展字典的名称、类型和字段等信息：

   ```sql
   SELECT name, type, key, attribute.names, attribute.types, source FROM
   system.dictionaries
   ```

   在system.dictionaries系统表内，其主要字段的含义分别如下。

   + name：字典的名称，在使用字典函数时需要通过字典名称访问数据。

   + type：字典所属类型。

   + key：字典的key值，数据通过key值定位。
   + attribute.names：属性名称，以数组形式保存。
   + attribute.types：属性类型，以数组形式保存，其顺序与attribute.names相同。
   + bytes_allocated：已载入数据在内存中占用的字节数。
   + query_count：字典被查询的次数。
   + hit_rate：字典数据查询的命中率。
   + element_count：已载入数据的行数
   + load_factor：数据的加载率。
   + source：数据源信息。
   + last_exception：异常信息，需要重点关注。如果字典在加载过程中产生异常，那么异常信息会写入此字段。last_exception是获取字典调试信息的主要方式。

2. 数据查询

   在正常情况下，字典数据只能通过字典函数获取，例如下面的语句就使用到了`dictGet('dict_name','attr_name',key)`函数：

   ```sql
   SELECT dictGet('test_flat_dict','name',toUInt64(1))
   ```

   如果字典使用了复合型key，则需要使用元组作为参数传入：

   ```sql
   SELECT dictGet('test_ip_trie_dict', 'asn', tuple(IPv4StringToNum('82.118.230.0')))
   ```

   除了dictGet函数之外，ClickHouse还提供了一系列以dictGet为前缀的字典函数，具体如下所示。

   + 获取整型数据的函数：dictGetUInt8、dictGetUInt16、dictGetUInt32、dictGetUInt64、dictGetInt8、dictGetInt16、dictGetInt32、dictGetInt64。
   + 获取浮点数据的函数：dictGetFloat32、dictGetFloat64。
   + 获取日期数据的函数：dictGetDate、dictGetDateTime。
   + 获取字符串数据的函数：dictGetString、dictGetUUID。

   这些函数的使用方法与dictGet大同小异，此处不再赘述。

3. 字典表

   除了通过字典函数读取数据之外，ClickHouse还提供了另外一种借助字典表的形式来读取数据。字典表是使用Dictionary表引擎的数据表，比如下面的例子：

   ```sql
   CREATE TABLE tb_test_flat_dict (
     id UInt64,
     code String,
     name String
   ) ENGINE = Dictionary(test_flat_dict);
   ```

   通过这张表，就能查询到字典中的数据。更多关于字典引擎的信息详见第8章。

4. 使用DDL查询创建字典

   从19.17.4.11版本开始，ClickHouse开始支持使用DDL查询创建字典，例如：

   ```sql
   CREATE DICTIONARY test_dict(
     id UInt64,
     value String
   )
   PRIMARY KEY id
   LAYOUT(FLAT())
   SOURCE(FILE(PATH '/usr/bin/cat' FORMAT TabSeparated))
   LIFETIME(1)
   ```

   可以看到，其配置参数与之前并无差异，只是转成了DDL的形式。

## 5.3 本章小结

​	通过对本章的学习，我们知道了ClickHouse拥有内置与扩展两类数据字典，同时也掌握了数据字典的配置、更新和查询的基本操作。在内置字典方面，目前只有一种YM字典且需要自行准备数据，而扩展字典是更加常用的字典类型。在扩展字典方面，目前拥有7种类型，其中flat、hashed和range_hashed依次拥有最高的性能。数据字典能够有效地帮助我们消除不必要的JOIN操作（例如根据ID转名称），优化SQL查询，为查询性能带来质的提升。下一章将开始介绍MergeTree表引擎的核心原理。

# 6. MergeTree原理解析

​	表引擎是ClickHouse设计实现中的一大特色。可以说，是表引擎决定了一张数据表最终的“性格”，比如数据表拥有何种特性、数据以何种形式被存储以及如何被加载。ClickHouse拥有非常庞大的表引擎体系，截至本书完成时，其共拥有合并树、外部存储、内存、文件、接口和其他6大类20多种表引擎。而在这众多的表引擎中，又属合并树（MergeTree）表引擎及其家族系列（*MergeTree）最为强大，在生产环境的绝大部分场景中，都会使用此系列的表引擎。因为**只有合并树系列的表引擎才支持主键索引、数据分区、数据副本和数据采样这些特性，同时也只有此系列的表引擎支持ALTER相关操作**。

​	合并树家族自身也拥有多种表引擎的变种。其中MergeTree作为家族中最基础的表引擎，提供了主键索引、数据分区、数据副本和数据采样等基本能力，而家族中其他的表引擎则在MergeTree的基础之上各有所长。例如ReplacingMergeTree表引擎具有删除重复数据的特性，而SummingMergeTree表引擎则会按照排序键自动聚合数据。如果给合并树系列的表引擎加上Replicated前缀，又会得到一组支持数据副本的表引擎，例如ReplicatedMergeTree、ReplicatedReplacingMergeTree、ReplicatedSummingMergeTree等。合并树表引擎家族如图6-1所示。

![img](https://pic2.zhimg.com/80/v2-db132f45192b5415ed031369e7908cf5_1440w.jpg)

​	虽然合并树的变种很多，但MergeTree表引擎才是根基。作为合并树家族系列中最基础的表引擎，MergeTree具备了该系列其他表引擎共有的基本特征，所以吃透了MergeTree表引擎的原理，就能够掌握该系列引擎的精髓。本章就针对MergeTree的一些基本原理进行解读。

## 6.1 MergeTree的创建方式与存储结构

​	MergeTree在写入一批数据时，数据总会以数据片段的形式写入磁盘，且数据片段不可修改。为了避免片段过多，ClickHouse会通过后台线程，定期合并这些数据片段，属于相同分区的数据片段会被合成一个新的片段。这种数据片段往复合并的特点，也正是合并树名称的由来。

### 6.1.1 MergeTree的创建方式

> [《手摸手带你学ClickHouse》之MergeTree系列表引擎_程序员大佬超的博客-CSDN博客_mergetree 引擎](https://blog.csdn.net/xch_yang/article/details/122855242)

​	创建MergeTree数据表的方法，与我们第4章介绍的定义数据表的方法大致相同，但需要将ENGINE参数声明为MergeTree()，其完整的语法如下所示：

```sql
CREATE TABLE [IF NOT EXISTS] [db_name.]table_name (
name1 [type] [DEFAULT|MATERIALIZED|ALIAS expr],
name2 [type] [DEFAULT|MATERIALIZED|ALIAS expr],
省略...
) ENGINE = MergeTree()
[PARTITION BY expr]
[ORDER BY expr]
[PRIMARY KEY expr]
[SAMPLE BY expr]
[SETTINGS name=value, 省略...]
```

​	MergeTree表引擎除了常规参数之外，还拥有一些独有的配置选项。接下来会着重介绍其中几个重要的参数，包括它们的使用方法和工作原理。但是在此之前，还是先介绍一遍它们的作用。

1. PARTITION BY [选填]：分区键，用于指定表数据以何种标准进行分区。分区键既可以是单个列字段，也可以通过元组的形式使用多个列字段，同时它也支持使用列表达式。**<u>如果不声明分区键，则ClickHouse会生成一个名为all的分区</u>**。合理使用数据分区，可以有效减少查询时数据文件的扫描范围，更多关于数据分区的细节会在6.2节介绍。

2. ORDER BY [必填]：排序键，用于指定在一个数据片段内，数据以何种标准排序。**默认情况下主键（PRIMARY KEY）与排序键相同**。排序键既可以是单个列字段，例如`ORDER BY CounterID`，也可以通过元组的形式使用多个列字段，例如`ORDER BY（CounterID,EventDate）`。当使用多个列字段排序时，以`ORDER BY（CounterID,EventDate）`为例，在单个数据片段内，数据首先会以CounterID排序，相同CounterID的数据再按EventDate排序。

3. PRIMARY KEY [选填]：主键，顾名思义，声明后会依照主键字段生成一级索引，用于加速表查询。<u>默认情况下，主键与排序键(ORDER BY)相同，所以通常直接使用ORDER BY代为指定主键，无须刻意通过PRIMARY KEY声明。所以在一般情况下，在单个数据片段内，数据与一级索引以相同的规则升序排列</u>。**与其他数据库不同，MergeTree主键允许存在重复数据（ReplacingMergeTree可以去重）**。

4. SAMPLE BY [选填]：抽样表达式，用于声明数据以何种标准进行采样。如果使用了此配置项，那么在主键的配置中也需要声明同样的表达式，例如：

   ```sql
   省略...
   ) ENGINE = MergeTree()
   ORDER BY (CounterID, EventDate, intHash32(UserID)
   SAMPLE BY intHash32(UserID)
   ```

   抽样表达式需要配合SAMPLE子查询使用，这项功能对于选取抽样数据十分有用，更多关于抽样查询的使用方法会在第9章介绍。

5. SETTINGS：index_granularity [选填]：index_granularity对于MergeTree而言是一项非常重要的参数，它表示索引的粒度，默认值为8192。<u>也就是说，MergeTree的索引在默认情况下，每间隔8192行数据才生成一条索引</u>，其具体声明方式如下所示：

   ```sql
   省略...
   ) ENGINE = MergeTree()
   省略...
   SETTINGS index_granularity = 8192;
   ```

   8192是一个神奇的数字，在ClickHouse中大量数值参数都有它的影子，可以被其整除（例如最小压缩块大小min_compress_block_size:65536）。通常情况下并不需要修改此参数，但理解它的工作原理有助于我们更好地使用MergeTree。关于索引详细的工作原理会在后续阐述。

6. SETTINGS：index_granularity_bytes [选填]：在19.11版本之前，ClickHouse只支持固定大小的索引间隔，由index_granularity控制，默认为8192。在新版本中，它增加了自适应间隔大小的特性，即根据每一批次写入数据的体量大小，动态划分间隔大小。而数据的体量大小，正是由index_granularity_bytes参数控制的，默认为10M(10×1024×1024)，设置为0表示不启动自适应功能。
7. SETTINGS：enable_mixed_granularity_parts [选填]：设置是否开启自适应索引间隔的功能，默认开启。
8. SETTINGS：merge_with_ttl_timeout [选填]：从19.6版本开始，MergeTree提供了数据TTL的功能，关于这部分的详细介绍，将留到第7章介绍。
9. SETTINGS：storage_policy [选填]：从19.15版本开始，MergeTree提供了多路径的存储策略，关于这部分的详细介绍，同样留到第7章介绍。

### 6.1.2 MergeTree的存储结构

> [ClickHouse核心引擎MergeTree解读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/361622782)

​	MergeTree表引擎中的数据是拥有物理存储的，数据会按照分区目录的形式保存到磁盘之上，其完整的存储结构如图6-2所示。

![img](https://pic1.zhimg.com/80/v2-bccb6c9e3bbae2f9f679a82fd8fb0e98_1440w.jpg)

1. partition：分区目录，余下各类数据文件（primary.idx、[Column].mrk、[Column].bin等）都是以分区目录的形式被组织存放的，属于相同分区的数据，最终会被合并到同一个分区目录，而不同分区的数据，永远不会被合并在一起。更多关于数据分区的细节会在6.2节阐述。
2. checksums.txt：校验文件，使用二进制格式存储。它保存了余下各类文件(primary.idx、count.txt等)的size大小及size的哈希值，用于快速校验文件的完整性和正确性。

3. columns.txt：列信息文件，使用明文格式存储。用于保存此数据分区下的列字段信息，例如：

   ```shell
   $ cat columns.txt
   columns format version: 1
   4 columns:
   'ID' String
   'URL' String
   'Code' String
   'EventTime' Date
   ```

4. count.txt：计数文件，使用明文格式存储。用于记录当前数据分区目录下数据的总行数，例如：

   ```shell
   $ cat count.txt
   8
   ```

5. primary.idx：一级索引文件，使用二进制格式存储。用于存放**稀疏索引**，**一张MergeTree表只能声明一次一级索引（通过ORDER BY或者PRIMARY KEY）**。借助稀疏索引，在数据查询的时能够排除主键条件范围之外的数据文件，从而有效减少数据扫描范围，加速查询速度。更多关于稀疏索引的细节与工作原理会在6.3节阐述。

6. [Column].bin：数据文件，**使用压缩格式存储**，<u>默认为LZ4压缩格式</u>，用于存储某一列的数据。由于MergeTree采用列式存储，所以每一个列字段都拥有独立的.bin数据文件，并以列字段名称命名（例如CounterID.bin、EventDate.bin等）。更多关于数据存储的细节会在6.5节阐述。

7. [Column].mrk：列字段标记文件，使用二进制格式存储。**标记文件中保存了`.bin`文件中数据的偏移量信息**。标记文件与稀疏索引对齐，又与`.bin`文件一一对应，所以MergeTree通过标记文件建立了`primary.idx`稀疏索引与`.bin`数据文件之间的映射关系。即首先通过稀疏索引（primary.idx）找到对应数据的偏移量信息（`.mrk`），再通过偏移量直接从`.bin`文件中读取数据。由于`.mrk`标记文件与`.bin`文件一一对应，所以MergeTree中的每个列字段都会拥有与其对应的`.mrk`标记文件（例如CounterID.mrk、EventDate.mrk等）。更多关于数据标记的细节会在6.6节阐述。

8. [Column].mrk2：<u>如果使用了自适应大小的索引间隔，则标记文件会以.mrk2命名。它的工作原理和作用与.mrk标记文件相同</u>。

9. **partition.dat与minmax_[Column].idx**：如果使用了分区键，例如PARTITION BY EventTime，则会额外生成partition.dat与minmax索引文件，它们均使用二进制格式存储。

   + **partition.dat用于保存当前分区下分区表达式最终生成的值**；
   + **而minmax索引用于记录当前分区下分区字段对应原始数据的最小和最大值**。

   例如EventTime字段对应的原始数据为2019-05-01、2019-05-05，分区表达式为`PARTITION BY toYYYYMM(EventTime)`。partition.dat中保存的值将会是2019-05，而minmax索引中保存的值将会是2019-05-012019-05-05。

   **在这些分区索引的作用下，进行数据查询时能够快速跳过不必要的数据分区目录，从而减少最终需要扫描的数据范围**。

10. **skp\_idx\_[Column].idx与skp\_idx\_[Column].mrk**：如果在建表语句中声明了**二级索引**，则会额外生成相应的二级索引与标记文件，它们同样也使用二进制存储。<u>二级索引在ClickHouse中又称**跳数索引**，目前拥有minmax、set、ngrambf_v1和tokenbf_v1四种类型。这些索引的最终目标与一级稀疏索引相同，都是为了进一步减少所需扫描的数据范围，以加速整个查询过程</u>。更多关于二级索引的细节会在6.4节阐述。

## * 6.2 数据分区

​	通过先前的介绍已经知晓在MergeTree中，数据是以分区目录的形式进行组织的，每个分区独立分开存储。借助这种形式，在对MergeTree进行数据查询时，可以有效跳过无用的数据文件，只使用最小的分区目录子集。这里有一点需要明确，<u>在ClickHouse中，数据分区（partition）和数据分片（shard）是完全不同的概念</u>。

+ **数据分区是针对本地数据而言的，是对数据的一种纵向切分。MergeTree并不能依靠分区的特性，将一张表的数据分布到多个ClickHouse服务节点**。
+ **而横向切分是数据分片（shard）的能力**，关于这一点将在后续章节介绍。本节将针对“数据分区目录具体是如何运作的”这一问题进行分析。

### * 6.2.1 数据的分区规则

​	<u>MergeTree数据分区的规则由分区ID决定，而具体到每个数据分区所对应的ID，则是由分区键的取值决定的</u>。**分区键支持使用任何一个或一组字段表达式声明，其业务语义可以是年、月、日或者组织单位等任何一种规则**。针对取值数据类型的不同，分区ID的生成逻辑目前拥有四种规则：

1. <u>不指定分区键：如果不使用分区键，即不使用PARTITIONBY声明任何分区表达式，则分区ID默认取名为all，所有的数据都会被写入这个all分区</u>。

2. 使用整型：如果分区键取值属于整型（兼容UInt64，包括有符号整型和无符号整型），且无法转换为日期类型YYYYMMDD格式，则直接按照该整型的字符形式输出，作为分区ID的取值。

3. 使用日期类型：如果分区键取值属于日期类型，或者是能够转换为YYYYMMDD格式的整型，则使用按照YYYYMMDD进行格式化后的字符形式输出，并作为分区ID的取值。

4. 使用其他类型：**如果分区键取值既不属于整型，也不属于日期类型，例如String、Float等，则通过128位Hash算法取其Hash值作为分区ID的取值**。

​	数据在写入时，会对照分区ID落入相应的数据分区，表6-1列举了分区ID在不同规则下的一些示例。

<table>
  <tr>
  	<th>类型</th>
    <th>样例数据</th>
    <th>分区表达式</th>
    <th>分区ID</th>
  </tr>
  <tr>
  	<td>无分区键</td>
    <td></td>
    <td>无</td>
    <td>all</td>
  </tr>
  <tr>
  	<td rowspan="2">整型</td>
    <td>18, 19, 20</td>
    <td>PARTITION BY Age</td>
    <td>分区1: 18<br/>分区2: 19<br/>分区3: 20</td>
  </tr>
  <tr>
    <td>'A0', 'A1', 'A2'</td>
    <td>PARTITION BY length(Code)</td>
    <td>分区1: 2</td>
  </tr>
  <tr>
  	<td rowspan="2">日期</td>
    <td>2019-05-01, 2019-06-11</td>
    <td>PARTITION BY EventTime</td>
    <td>分区1: 20190501<br/>分区2: 20190611</td>
  </tr>
  <tr>
    <td>2019-05-01, 2019-06-11</td>
    <td>PARTITION BY toYYYYMM(EventTime)</td>
    <td>分区1: 201905<br/>分区2: 201906</td>
  </tr>
  <tr>
  	<td>其他</td>
    <td>'wwww.nauu.com'</td>
    <td>PARTITION BY URL</td>
    <td>分区1: 15b31467bc77falc24ac9380cd8b4033</td>
  </tr>
</table>

​	**如果通过元组的方式使用多个分区字段，则分区ID依旧是根据上述规则生成的，只是多个ID之间通过“-”符号依次拼接**。例如按照上述表格中的例子，使用两个字段分区：

```sql
PARTITION BY (length(Code),EventTime)
```

​	则最终的分区ID会是下面的模样：

```sql
2-20190501
2-20190611
```

### * 6.2.2 分区目录的命名规则

​	通过上一小节的介绍，我们已经知道了分区ID的生成规则。但是如果进入数据表所在的磁盘目录后，会发现MergeTree分区目录的完整物理名称并不是只有ID而已，在ID之后还跟着一串奇怪的数字，例如201905_1_1_0。那么这些数字又代表着什么呢？

​	众所周知，对于MergeTree而言，它最核心的特点是其分区目录的合并动作。但是我们可曾想过，从分区目录的命名中便能够解读出它的合并逻辑。在这一小节，我们会着重对命名公式中各分项进行解读，而关于具体的目录合并过程将会留在后面小节讲解。一个完整分区目录的命名公式如下所示：

```shell
PartitionID_MinBlockNum_MaxBlockNum_Level
```

​	如果对照着示例数据，那么数据与公式的对照关系会如同图6-3所示一般。

​	`201905_1_1_0`

上图中，201905表示分区目录的ID（PartitionID）；\_1\_1分别表示最小的数据块编号（MinBlockNum）与最大的数据块编号（MaxBlockNum）；而最后的_0则表示目前合并的层级（Level）。接下来开始分别解释它们的含义：

1. PartitionID：分区ID，无须多说，关于分区ID的规则在上一小节中已经做过详细阐述了。
2. MinBlockNum和MaxBlockNum：顾名思义，**最小数据块编号与最大数据块编号**。<u>ClickHouse在这里的命名似乎有些歧义，很容易让人与稍后会介绍到的数据压缩块混淆。但是本质上它们毫无关系，这里的BlockNum是一个整型的**自增长编号**</u>。如果将其设为n的话，那么计数n在单张MergeTree数据表内全局累加，n从1开始，每当新创建一个分区目录时，计数n就会累积加1。对于一个新的分区目录而言，MinBlockNum与MaxBlockNum取值一样，同等于n，例如201905_1_1_0、201906_2_2_0以此类推。但是也有例外，当分区目录发生合并时，对于新产生的合并目录MinBlockNum与MaxBlockNum有着另外的取值规则。对于合并规则，我们留到下一小节再详细讲解。
3. Level：合并的层级，可以理解为某个分区被合并过的次数，或者这个分区的年龄。数值越高表示年龄越大。Level计数与BlockNum有所不同，它并不是全局累加的。对于每一个新创建的分区目录而言，其初始值均为0。之后，以分区为单位，如果相同分区发生合并动作，则在相应分区内计数累积加1。

### * 6.2.3 分区目录的合并过程

​	MergeTree的分区目录和传统意义上其他数据库有所不同。

+ 首先，**MergeTree的分区目录并不是在数据表被创建之后就存在的，而是在数据写入过程中被创建的**。也就是说如果一张数据表没有任何数据，那么也不会有任何分区目录存在。
+ 其次，它的分区目录在建立之后也并不是一成不变的。在其他某些数据库的设计中，追加数据后目录自身不会发生变化，只是在相同分区目录中追加新的数据文件。<u>而MergeTree完全不同，伴随着每一批数据的写入（一次INSERT语句），MergeTree都会生成一批新的分区目录。即便不同批次写入的数据属于相同分区，也会生成不同的分区目录。也就是说，对于同一个分区而言，也会存在多个分区目录的情况。在之后的某个时刻（写入后的10～15分钟，也可以手动执行optimize查询语句），ClickHouse会通过后台任务再将属于相同分区的多个目录合并成一个新的目录</u>。**已经存在的旧分区目录并不会立即被删除，而是在之后的某个时刻通过后台任务被删除（默认8分钟）**。

​	属于同一个分区的多个目录，在合并之后会生成一个全新的目录，目录中的索引和数据文件也会相应地进行合并。新目录名称的合并方式遵循以下规则，其中：

+ MinBlockNum：取同一分区内所有目录中最小的MinBlockNum值。
+ MaxBlockNum：取同一分区内所有目录中最大的MaxBlockNum值。
+ Level：取同一分区内最大Level值并加1。

​	在图6-4中，partition_v5测试表按日期字段格式分区，即PARTITION BY toYYYYMM（EventTime），T表示时间。假设在T0时刻，首先分3批（3次INSERT语句）写入3条数据：

```sql
INSERT INTO partition_v5 VALUES (A, c1, '2019-05-01')
INSERT INTO partition_v5 VALUES (B, c1, '2019-05-02')
INSERT INTO partition_v5 VALUES (C, c1, '2019-06-01')
```

​	按照目录规，上述代码会创建3个分区目录。分区目录的名称由PartitionID、MinBlockNum、MaxBlockNum和Level组成，其中PartitionID根据6.2.1节介绍的生成规则，3个分区目录的ID依次为201905、201905和201906。而对于每个新建的分区目录而言，它们的MinBlockNum与MaxBlockNum取值相同，均来源于表内全局自增的BlockNum。BlockNum初始为1，每次新建目录后累计加1。所以，3个分区目录的MinBlockNum与MaxBlockNum依次为0_0、1_1和2_2。最后是Level层级，每个新建的分区目录初始Level都是0。所以3个分区目录的最终名称分别是201905_1_1_0、201905_2_2_0和201906_3_3_0。

​	假设在T1时刻，MergeTree的合并动作开始了，那么属于同一分区的201905_1_1_0与201905_2_2_0目录将发生合并。从图6-4所示过程中可以发现，合并动作完成后，生成了一个新的分区201905_1_2_1。根据本节所述的合并规则，其中，MinBlockNum取同一分区内所有目录中最小的MinBlockNum值，所以是1；MaxBlockNum取同一分区内所有目录中最大的MaxBlockNum值，所以是2；而Level则取同一分区内，最大Level值加1，所以是1。而后续T2时刻的合并规则，只是在重复刚才所述的过程而已。

​	至此，大家已经知道了分区ID、目录命名和目录合并的相关规则。最后，再用一张完整的示例图作为总结，描述MergeTree分区目录从创建、合并到删除的整个过程，如图6-5所示。

​	从图6-5中应当能够发现，**分区目录在发生合并之后，旧的分区目录并没有被立即删除，而是会存留一段时间。但是旧的分区目录已不再是激活状态（active=0），所以在数据查询时，它们会被自动过滤掉**。

## 6.3 一级索引

​	MergeTree的主键使用PRIMARY KEY定义，待主键定义之后，MergeTree会依据index_granularity间隔（默认8192行），为数据表生成一级索引并保存至primary.idx文件内，索引数据按照PRIMARY KEY排序。<u>相比使用PRIMARY KEY定义，更为常见的简化形式是通过ORDER BY指代主键</u>。<u>在此种情形下，PRIMARY KEY与ORDER BY定义相同，所以索引（primary.idx）和数据（.bin）会按照完全相同的规则排序</u>。对于PRIMARY KEY与ORDER BY定义有差异的应用场景在SummingMergeTree引擎章节部分会所有介绍，而关于数据文件的更多细节，则留在稍后的6.5节介绍，本节重点讲解一级索引部分。

### 6.3.1 稀疏索引

​	primary.idx文件内的一级索引采用稀疏索引实现。此时有人可能会问，既然提到了稀疏索引，那么是不是也有稠密索引呢？还真有！稀疏索引和稠密索引的区别如图6-6所示。

![image.png](https://ewr1.vultrobjects.com/imgur2/000/005/226/177_993_97d.jpg)

​	**简单来说，在稠密索引中每一行索引标记都会对应到一行具体的数据记录。而在稀疏索引中，每一行索引标记对应的是一段数据，而不是一行**。用一个形象的例子来说明：如果把MergeTree比作一本书，那么稀疏索引就好比是这本书的一级章节目录。一级章节目录不会具体对应到每个字的位置，只会记录每个章节的起始页码。

​	稀疏索引的优势是显而易见的，它仅需使用少量的索引标记就能够记录大量数据的区间位置信息，且数据量越大优势越为明显。以默认的索引粒度（8192）为例，MergeTree只需要12208行索引标记就能为1亿行数据记录提供索引。**由于稀疏索引占用空间小，所以primary.idx内的索引数据常驻内存，取用速度自然极快**。

### 6.3.2 索引粒度

​	在先前的篇幅中已经数次出现过index_granularity这个参数了，它表示索引的粒度。虽然在新版本中，ClickHouse提供了自适应粒度大小的特性，但是为了便于理解，仍然会使用固定的索引粒度（默认8192）进行讲解。索引粒度对MergeTree而言是一个非常重要的概念，因此很有必要对它做一番深入解读。索引粒度就如同标尺一般，会丈量整个数据的长度，并依照刻度对数据进行标注，最终将数据标记成多个间隔的小段，如图6-7所示。

![img](https://ask.qcloudimg.com/http-save/yehe-6510437/843fb04c503002fb78c5ae2c796e6c5d.png?imageView2/2/w/1620)

​	数据以index_granularity的粒度（默认8192）被标记成多个小的区间，其中每个区间最多8192行数据。MergeTree使用MarkRange表示一个具体的区间，并通过start和end表示其具体的范围。<u>index_granularity的命名虽然取了索引二字，但它不单只作用于一级索引（.idx），同时也会影响数据标记（.mrk）和数据文件（.bin）。因为仅有一级索引自身是无法完成查询工作的，它需要借助数据标记才能定位数据，所以**一级索引和数据标记的间隔粒度相同（同为index_granularity行），彼此对齐**。而数据文件也会依照index_granularity的间隔粒度生成压缩数据块</u>。关于数据文件和数据标记的细节会在后面说明。

### * 6.3.3 索引数据的生成规则

​	**由于是稀疏索引，所以MergeTree需要间隔index_granularity行数据才会生成一条索引记录，其索引值会依据声明的主键字段获取**。图6-8所示是对照测试表hits_v1中的真实数据具象化后的效果。hits_v1使用年月分区（PARTITION BY toYYYYMM(EventDate)），所以2014年3月份的数据最终会被划分到同一个分区目录内。如果使用CounterID作为主键（ORDER BY CounterID），则每间隔8192行数据就会取一次CounterID的值作为索引值，索引数据最终会被写入primary.idx文件进行保存。

![preload](https://ask.qcloudimg.com/http-save/yehe-6510437/552a4f5bf75226772a36ade497fb372f.png)

​	图6-8 测试表hits_v1具象化后的效果

​	例如第0(8192\*0)行CounterID取值57，第8192(8192\*1)行CounterID取值1635，而第16384(8192\*2)行CounterID取值3266，最终索引数据将会是5716353266。

​	从图6-8中也能够看出，MergeTree对于稀疏索引的存储是非常紧凑的，索引值前后相连，按照主键字段顺序紧密地排列在一起。不仅此处，<u>ClickHouse中很多数据结构都被设计得非常紧凑，比如其使用位读取替代专门的标志位或状态码，可以不浪费哪怕一个字节的空间。以小见大，这也是ClickHouse为何性能如此出众的深层原因之一</u>。

​	如果使用多个主键，例如ORDER BY(CounterID,EventDate)，则每间隔8192行可以同时取CounterID与EventDate两列的值作为索引值，具体如图6-9所示。

![preload](https://ask.qcloudimg.com/http-save/yehe-6510437/732f1a1965108554cb6a4fa0a8fdabe2.png)

​	图6-9 使用CounterID和EventDate作为主键

### * 6.3.4 索引的查询过程

​	在介绍了上述关于索引的一些概念之后，接下来说明索引具体是如何工作的。首先，我们需要了解什么是MarkRange。<u>MarkRange在ClickHouse中是用于定义标记区间的对象。通过先前的介绍已知，MergeTree按照index_granularity的间隔粒度，将一段完整的数据划分成了多个小的间隔数据段，一个具体的数据段即是一个MarkRange</u>。MarkRange与索引编号对应，使用start和end两个属性表示其区间范围。通过与start及end对应的索引编号的取值，即能够得到它所对应的数值区间。而数值区间表示了此MarkRange包含的数据范围。

​	如果只是这么干巴巴地介绍，大家可能会觉得比较抽象，下面用一份示例数据来进一步说明。假如现在有一份测试数据，共192行记录。其中，主键ID为String类型，ID的取值从A000开始，后面依次为A001、A002……直至A192为止。MergeTree的索引粒度index_granularity=3，根据索引的生成规则，primary.idx文件内的索引数据会如图6-10所示。

![preload](https://ask.qcloudimg.com/http-save/yehe-6510437/50a9b43913d45604556558514b036f94.png)

​	图6-10 192行ID索引的物理存储示意

​	根据索引数据，MergeTree会将此数据片段划分成192/3=64个小的MarkRange，两个相邻MarkRange相距的步长为1。其中，所有MarkRange（整个数据片段）的最大数值区间为[A000,+inf)，其完整的示意如图6-11所示。

![preload](https://ask.qcloudimg.com/http-save/yehe-6510437/adc6fdf1d99c04086c03552db9c1ea30.png)

​	图6-11 64个MarkRange与其数值区间范围的示意图

​	在引出了数值区间的概念之后，对于索引的查询过程就很好解释了。索引查询其实就是两个数值区间的交集判断。其中，一个区间是由基于主键的查询条件转换而来的条件区间；而另一个区间是刚才所讲述的与MarkRange对应的数值区间。整个索引查询过程可以大致分为3个步骤。

1. **生成查询条件区间**：首先，将查询条件转换为条件区间。即便是单个值的查询条件，也会被转换成区间的形式，例如下面的例子。

   ```sql
   WHERE ID = 'A003'
   ['A003', 'A003']
   WHERE ID > 'A000'
   ('A000', +inf)
   WHERE ID < 'A188'
   (-inf, 'A188')
   WHERE ID LIKE 'A006%'
   ['A006', 'A007')
   ```

2. **递归交集判断**：以递归的形式，依次对MarkRange的数值区间与条件区间做交集判断。从最大的区间[A000,+inf)开始：

   + 如果不存在交集，则直接通过剪枝算法优化此整段MarkRange。
   + 如果存在交集，且MarkRange步长大于8(end-start)，则将此区间进一步拆分成8个子区间（由merge_tree_coarse_index_granularity指定，默认值为8），并重复此规则，继续做递归交集判断。
   + 如果存在交集，且MarkRange不可再分解（步长小于8），则记录MarkRange并返回。

3. **合并MarkRange区间**：将最终匹配的MarkRange聚在一起，合并它们的范围。

​	完整逻辑的示意如图6-12所示。

![在这里插入图片描述](https://img-blog.csdnimg.cn/ffba91545f274ce9978a2db70abe76e9.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6YKL6YGi55qE5rWB5rWq5YmR5a6i,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

​	图6-12 索引查询完整过程的逻辑示意图

​	MergeTree通过递归的形式持续向下拆分区间，最终将MarkRange定位到最细的粒度，以帮助在后续读取数据的时候，能够最小化扫描数据的范围。以图6-12所示为例，当查询条件`WHERE ID='A003'`的时候，最终只需要读取[A000,A003]和[A003,A006]两个区间的数据,它们对应MarkRange(start:0,end:2)范围，而其他无用的区间都被裁剪掉了。因为MarkRange转换的数值区间是闭区间，所以会额外匹配到临近的一个区间。

## 6.4 二级索引

​	除了一级索引之外，MergeTree同样支持二级索引。<u>二级索引又称跳数索引，由数据的聚合信息构建而成</u>。根据索引类型的不同，其聚合信息的内容也不同。跳数索引的目的与一级索引一样，也是帮助查询时减少数据扫描的范围。

​	跳数索引在默认情况下是关闭的，需要设置allow_experimental_data_skipping_indices（该参数在新版本中已被取消）才能使用：

```sql
SET allow_experimental_data_skipping_indices = 1
```

​	跳数索引需要在CREATE语句内定义，它支持使用元组和表达式的形式声明，其完整的定义语法如下所示：

```sql
INDEX index_name expr TYPE index_type(...) GRANULARITY granularity
```

​	**与一级索引一样，如果在建表语句中声明了跳数索引，则会额外生成相应的索引与标记文件（skp_idx\_[Column].idx与skp\_idx\_[Column].mrk）**。

### * 6.4.1 granularity与index_granularity的关系

​	不同的跳数索引之间，除了它们自身独有的参数之外，还都共同拥有granularity参数。初次接触时，很容易将granularity与index_granularity的概念弄混淆。**对于跳数索引而言，index_granularity定义了数据的粒度，而granularity定义了聚合信息汇总的粒度。换言之，granularity定义了一行跳数索引能够跳过多少个index_granularity区间的数据**。

​	要解释清楚granularity的作用，就要从跳数索引的数据生成规则说起，其规则大致是这样的：首先，按照index_granularity粒度间隔将数据划分成n段，总共有[0,n-1]个区间（n=total_rows/index_granularity，向上取整）。接着，根据索引定义时声明的表达式，从0区间开始，依次按index_granularity粒度从数据中获取聚合信息，每次向前移动1步(n+1)，聚合信息逐步累加。最后，**当移动granularity次区间时，则汇总并生成一行跳数索引数据**。

​	以minmax索引为例，它的聚合信息是在一个index_granularity区间内数据的最小和最大极值。以下图为例，假设index_granularity=8192且granularity=3，则数据会按照index_granularity划分为n等份，MergeTree从第0段分区开始，依次获取聚合信息。当获取到第3个分区时（granularity=3），则汇总并会生成第一行minmax索引（前3段minmax极值汇总后取值为[1,9]），如图6-13所示。

![在这里插入图片描述](https://img-blog.csdnimg.cn/8e6de5f6ad54415fa28276023c57a764.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6YKL6YGi55qE5rWB5rWq5YmR5a6i,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

​	图6-13 跳数索引granularity与index_granularity的关系

### * 6.4.2 跳数索引的类型

​	目前，MergeTree共支持4种跳数索引，分别是minmax、set、ngrambf_v1和tokenbf_v1。**一张数据表支持同时声明多个跳数索引**，例如：

```sql
CREATE TABLE skip_test (
  ID String,
  URL String,
  Code String,
  EventTime Date,
  INDEX a ID TYPE minmax GRANULARITY 5,
  INDEX b（length(ID) * 8） TYPE set(2) GRANULARITY 5,
  INDEX c（ID，Code） TYPE ngrambf_v1(3, 256, 2, 0) GRANULARITY 5,
  INDEX d ID TYPE tokenbf_v1(256, 2, 0) GRANULARITY 5
) ENGINE = MergeTree()
省略...
```

​	接下来，就借助上面的例子逐个介绍这几种跳数索引的用法：

1. minmax：minmax索引记录了一段数据内的最小和最大极值，其索引的作用类似分区目录的minmax索引，能够快速跳过无用的数据区间，示例如下所示：

   ```sql
   INDEX a ID TYPE minmax GRANULARITY 5
   ```

   上述示例中minmax索引会记录这段数据区间内ID字段的极值。极值的计算涉及每5个index_granularity区间中的数据。

2. set：set索引直接记录了声明字段或表达式的取值（唯一值，无重复），其完整形式为set(max_rows)，其中max_rows是一个阈值，表示在一个index_granularity内，索引最多记录的数据行数。如果max_rows=0，则表示无限制，例如：

   ```sql
   INDEX b（length(ID) * 8） TYPE set(100) GRANULARITY 5
   ```

   上述示例中set索引会记录数据中ID的长度*8后的取值。其中，每个index_granularity内最多记录100条。

3. ngrambf_v1：<u>ngrambf_v1索引记录的是数据短语的布隆表过滤器，只支持String和FixedString数据类型</u>。**ngrambf_v1只能够提升in、notIn、like、equals和notEquals查询的性能**，其完整形式为`ngrambf_v1(n,size_of_bloom_filter_in_bytes,number_of_hash_functions,random_seed)`。这些参数是一个布隆过滤器的标准输入，如果你接触过布隆过滤器，应该会对此十分熟悉。它们具体的含义如下：

   + n：token长度，依据n的长度将数据切割为token短语。
   + size_of_bloom_filter_in_bytes：布隆过滤器的大小。
   + number_of_hash_functions：布隆过滤器中使用Hash函数的个数。
   + random_seed：Hash函数的随机种子。

   例如在下面的例子中，ngrambf_v1索引会依照3的粒度将数据切割成短语token，token会经过2个Hash函数映射后再被写入，布隆过滤器大小为256字节。

   ```sql
   INDEX c（ID，Code） TYPE ngrambf_v1(3, 256, 2, 0) GRANULARITY 5
   ```

4. tokenbf_v1：tokenbf_v1索引是ngrambf_v1的变种，同样也是一种布隆过滤器索引。tokenbf_v1除了短语token的处理方法外，其他与ngrambf_v1是完全一样的。tokenbf_v1会自动按照非字符的、数字的字符串分割token，具体用法如下所示：

   ```sql
   INDEX d ID TYPE tokenbf_v1(256, 2, 0) GRANULARITY 5
   ```

## 6.5 数据存储

​	此前已经多次提过，在MergeTree中数据是按列存储的。但是前面的介绍都较为抽象，具体到存储的细节、MergeTree是如何工作的，读者心中难免会有疑问。数据存储，就好比一本书中的文字，在排版时，绝不会密密麻麻地把文字堆满，这样会导致难以阅读。更为优雅的做法是，将文字按段落的形式精心组织，使其错落有致。本节将进一步介绍MergeTree在数据存储方面的细节，尤其是其中关于压缩数据块的概念。

### * 6.5.1 各列独立存储

​	**在MergeTree中，数据按列存储**。而具体到每个列字段，数据也是独立存储的，每个列字段都拥有一个与之对应的.bin数据文件。也正是这些.bin文件，最终承载着数据的物理存储。数据文件以分区目录的形式被组织存放，所以在.bin文件中只会保存当前分区片段内的这一部分数据，其具体组织形式已经在图6-2中展示过。<u>按列独立存储的设计优势显而易见：一是可以更好地进行数据压缩（相同类型的数据放在一起，对压缩更加友好），二是能够最小化数据扫描的范围</u>。

​	而对应到存储的具体实现方面，MergeTree也并不是一股脑地将数据直接写入.bin文件，而是经过了一番精心设计：

+ 首先，**数据是经过压缩的**，目前支持LZ4、ZSTD、Multiple和Delta几种算法，默认使用LZ4算法；
+ 其次，**数据会事先依照ORDER BY的声明排序**；
+ 最后，**数据是以压缩数据块的形式被组织并写入.bin文件中的**。

​	压缩数据块就好比一本书的文字段落，是组织文字的基本单元。这个概念十分重要，值得多花些篇幅进一步展开说明。

### * 6.5.2 压缩数据块

​	一个压缩数据块由**头信息**和**压缩数据**两部分组成。<u>头信息固定使用9位字节表示，具体由1个UInt8（1字节）整型和2个UInt32（4字节）整型组成，分别代表使用的压缩算法类型、压缩后的数据大小和压缩前的数据大小</u>，具体如图6-14所示。

<img src="https://img-blog.csdnimg.cn/29fde480235f41569de8863b569e9075.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6YKL6YGi55qE5rWB5rWq5YmR5a6i,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述" style="zoom:50%;" />

​	图6-14 压缩数据块示意图

​	从图6-14所示中能够看到，.bin压缩文件是由多个压缩数据块组成的，而每个压缩数据块的头信息则是基于**CompressionMethod_CompressedSize_UncompressedSize**公式生成的。

​	通过ClickHouse提供的clickhouse-compressor工具，能够查询某个.bin文件中压缩数据的统计信息。以测试数据集hits_v1为例，执行下面的命令：

```shell
clickhouse-compressor --stat < /chbase/
/data/default/hits_v1/201403_1_34_3/JavaEnable.bin
```

执行后，会看到如下信息：

```shell
65536 12000
65536 14661
65536 4936
65536 7506
省略…
```

​	其中每一行数据代表着一个压缩数据块的头信息，其分别表示该压缩块中未压缩数据大小和压缩后数据大小（打印信息与物理存储的顺序刚好相反）。

​	每个压缩数据块的体积，按照其压缩前的数据字节大小，都被严格控制在64KB～1MB，其上下限分别由min_compress_block_size（默认65536）与max_compress_block_size（默认1048576）参数指定。而**一个压缩数据块最终的大小，则和一个间隔（index_granularity）内数据的实际大小相关（是的，没错，又见到索引粒度这个老朋友了）**。

​	MergeTree在数据具体的写入过程中，会依照索引粒度（默认情况下，每次取8192行），按批次获取数据并进行处理。如果把一批数据的未压缩大小设为size，则整个写入过程遵循以下规则：

1. **单个批次数据size<64KB** ：如果单个批次数据小于64KB，则继续获取下一批数据，直至累积到size>=64KB时，生成下一个压缩数据块。
2. **单个批次数据64KB<=size<=1MB** ：如果单个批次数据大小恰好在64KB与1MB之间，则直接生成下一个压缩数据块。
3. **单个批次数据size>1MB** ：如果单个批次数据直接超过1MB，则首先按照1MB大小截断并生成下一个压缩数据块。剩余数据继续依照上述规则执行。此时，会出现一个批次数据生成多个压缩数据块的情况。

​	整个过程逻辑如图6-15所示。

![在这里插入图片描述](https://img-blog.csdnimg.cn/78880d6564cc472e82cd6dc4721470fb.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6YKL6YGi55qE5rWB5rWq5YmR5a6i,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

​	图6-15 切割压缩数据块的逻辑示意图

​	经过上述的介绍后我们知道，**一个.bin文件是由1至多个压缩数据块组成的，每个压缩块大小在64KB～1MB之间**。多个压缩数据块之间，按照写入顺序首尾相接，紧密地排列在一起，如图6-16所示。

​	在.bin文件中引入压缩数据块的目的至少有以下两个：

+ 其一，虽然数据被压缩后能够有效减少数据大小，降低存储空间并加速数据传输效率，但<u>数据的压缩和解压动作，其本身也会带来额外的性能损耗</u>。所以需要控制被压缩数据的大小，以求在性能损耗和压缩率之间寻求一种平衡。
+ 其二，在具体读取某一列数据时（.bin文件），首先需要将压缩数据加载到内存并解压，这样才能进行后续的数据处理。**通过压缩数据块，可以在不读取整个.bin文件的情况下将读取粒度降低到压缩数据块级别，从而进一步缩小数据读取的范围**。

![4bd90f52fb3d995d7ed72b76057640af.png](https://img-blog.csdnimg.cn/img_convert/4bd90f52fb3d995d7ed72b76057640af.png)

​	图6-16 读取粒度精确到压缩数据块

## * 6.6 数据标记

​	如果把MergeTree比作一本书，primary.idx一级索引好比这本书的一级章节目录，.bin文件中的数据好比这本书中的文字，那么数据标记(.mrk)会为一级章节目录和具体的文字之间建立关联。对于数据标记而言，它记录了两点重要信息：其一，是一级章节对应的页码信息；其二，是一段文字在某一页中的起始位置信息。这样一来，通过数据标记就能够很快地从一本书中立即翻到关注内容所在的那一页，并知道从第几行开始阅读。

### 6.6.1 数据标记的生成规则

​	数据标记作为衔接一级索引和数据的桥梁，其像极了做过标记小抄的书签，而且书本中每个一级章节都拥有各自的书签。它们之间的关系如图6-17所示。

![在这里插入图片描述](https://img-blog.csdnimg.cn/470393e7f5a845faa20fdf0fba6719f3.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6YKL6YGi55qE5rWB5rWq5YmR5a6i,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

​	图6-17 通过索引下标编号找到对应的数据标记

​	从图6-17中一眼就能发现数据标记的首个特征，即**数据标记和索引区间是对齐的，均按照index_granularity的粒度间隔**。如此一来，只需简单通过索引区间的下标编号就可以直接找到对应的数据标记。

​	**为了能够与数据衔接，数据标记文件也与.bin文件一一对应**。即每一个列字段`[Column].bin`文件都有一个与之对应的`[Column].mrk`数据标记文件，用于记录数据在.bin文件中的偏移量信息。

​	<u>一行标记数据使用一个元组表示，元组内包含两个整型数值的偏移量信息。它们分别表示在此段数据区间内，在对应的.bin压缩文件中，压缩数据块的起始偏移量；以及将该数据压缩块解压后，其未压缩数据的起始偏移量</u>。图6-18所示是.mrk文件内标记数据的示意。

![在这里插入图片描述](https://img-blog.csdnimg.cn/b5da8c5c13f1464a95b4cee5b3a32cab.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6YKL6YGi55qE5rWB5rWq5YmR5a6i,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

​	图6-18 标记数据示意图

​	如图6-18所示，每一行标记数据都表示了一个片段的数据（默认8192行）在.bin压缩文件中的读取位置信息。**标记数据与一级索引数据不同，<u>它并不能常驻内存</u>，而是使用LRU（最近最少使用）缓存策略加快其取用速度**。

### 6.6.2 数据标记的工作方式

​	**MergeTree在读取数据时，必须通过标记数据的位置信息才能够找到所需要的数据**。整个查找过程大致可以分为读取压缩数据块和读取数据两个步骤。为了便于解释，这里继续使用测试表hits_v1中的真实数据进行说明。图6-19所示为hits_v1测试表的JavaEnable字段及其标记数据与压缩数据的对应关系。

![在这里插入图片描述](https://img-blog.csdnimg.cn/991099c67dd04e1ba2307a8bb558fcb2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6YKL6YGi55qE5rWB5rWq5YmR5a6i,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

​	图6-19 JavaEnable字段的标记文件和压缩数据文件的对应关系

​	首先，对图6-19所示左侧的标记数据做一番解释说明。JavaEnable字段的数据类型为UInt8，所以每行数值占用1字节。而hits_v1数据表的index_granularity粒度为8192，所以一个索引片段的数据大小恰好是8192B。按照6.5.2节介绍的压缩数据块的生成规则，如果单个批次数据小于64KB，则继续获取下一批数据，直至累积到size>=64KB时，生成下一个压缩数据块。因此在JavaEnable的标记文件中，每8行标记数据对应1个压缩数据块（1B*8192=8192B,64KB=65536B,65536/8192=8）。所以，从图6-19所示中能够看到，其左侧的标记数据中，8行数据的压缩文件偏移量都是相同的，因为这8行标记都指向了同一个压缩数据块。而在这8行的标记数据中，它们的解压缩数据块中的偏移量，则依次按照8192B（每行数据1B，每一个批次8192行数据）累加，当累加达到65536(64KB)时则置0。因为根据规则，此时会生成下一个压缩数据块。

​	理解了上述标记数据之后，接下来就开始介绍MergeTree具体是如何定位压缩数据块并读取数据的。

1. 读取压缩数据块： **在查询某一列数据时，MergeTree无须一次性加载整个.bin文件，而是可以根据需要，只加载特定的压缩数据块。而这项特性需要借助标记文件中所保存的压缩文件中的偏移量**。

   ​	在图6-19所示的标记数据中，上下相邻的两个压缩文件中的起始偏移量，构成了与获取当前标记对应的压缩数据块的偏移量区间。<u>由当前标记数据开始，向下寻找，直到找到不同的压缩文件偏移量为止。此时得到的一组偏移量区间即是压缩数据块在.bin文件中的偏移量</u>。例如在图6-19所示中，读取右侧.bin文件中[0，12016]字节数据，就能获取第0个压缩数据块。

   ​	细心的读者可能会发现，在.mrk文件中，第0个压缩数据块的截止偏移量是12016。而在.bin数据文件中，第0个压缩数据块的压缩大小是12000。为什么两个数值不同呢？其实原因很简单，12000只是数据压缩后的字节数，并没有包含头信息部分。而一个完整的压缩数据块是由头信息加上压缩数据组成的，它的头信息固定由9个字节组成，压缩后大小为8个字节。所以，12016=8+12000+8，其定位方法如图6-19右上角所示。压缩数据块被整个加载到内存之后，会进行解压，在这之后就进入具体数据的读取环节了。

2. 读取数据： **在读取解压后的数据时，MergeTree并不需要一次性扫描整段解压数据，它可以根据需要，以index_granularity的粒度加载特定的一小段**。为了实现这项特性，需要借助标记文件中保存的解压数据块中的偏移量。

   ​	同样的，在图6-19所示的标记数据中，上下相邻两个解压缩数据块中的起始偏移量，构成了与获取当前标记对应的数据的偏移量区间。通过这个区间，能够在它的压缩块被解压之后，依照偏移量按需读取数据。例如在图6-19所示中，通过[0，8192]能够读取压缩数据块0中的第一个数据片段。

## * 6.7 对于分区、索引、标记和压缩数据的协同总结

​	分区、索引、标记和压缩数据，就好比是MergeTree给出的一套组合拳，使用恰当时威力无穷。那么，在依次介绍了各自的特点之后，现在将它们聚在一块进行一番总结。接下来，就分别从写入过程、查询过程，以及数据标记与压缩数据块的三种对应关系的角度展开介绍。

### 6.7.1 写入过程

​	数据写入的第一步是生成分区目录，伴随着每一批数据的写入，都会生成一个新的分区目录。在后续的某一时刻，属于相同分区的目录会依照规则合并到一起；接着，按照index_granularity索引粒度，会分别生成primary.idx一级索引（如果声明了二级索引，还会创建二级索引文件）、每一个列字段的.mrk数据标记和.bin压缩数据文件。图6-20所示是一张MergeTree表在写入数据时，它的分区目录、索引、标记和压缩数据的生成过程。

![在这里插入图片描述](https://img-blog.csdnimg.cn/b05bc915c63f498f9381e92c572ac955.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6YKL6YGi55qE5rWB5rWq5YmR5a6i,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

​	图6-20 分区目录、索引、标记和压缩数据的生成过程示意

​	从分区目录201403_1_34_3能够得知，该分区数据共分34批写入，期间发生过3次合并。在数据写入的过程中，依据index_granularity的粒度，依次为每个区间的数据生成索引、标记和压缩数据块。其中，**索引和标记区间是对齐的，而标记与压缩块则根据区间数据大小的不同，会生成多对一、一对一和一对多三种关系**。

### 6.7.2 查询过程

​	数据查询的本质，可以看作一个不断减小数据范围的过程。在最理想的情况下，MergeTree首先可以依次借助分区索引、一级索引和二级索引，将数据扫描范围缩至最小。然后再借助数据标记，将需要解压与计算的数据范围缩至最小。以图6-21所示为例，它示意了在最优的情况下，经过层层过滤，最终获取最小范围数据的过程。

![在这里插入图片描述](https://img-blog.csdnimg.cn/1d525ef8b4ed4286b1e831cb7059afea.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6YKL6YGi55qE5rWB5rWq5YmR5a6i,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

​	图6-21 将扫描数据范围最小化的过程

​	<u>如果一条查询语句没有指定任何WHERE条件，或是指定了WHERE条件，但条件没有匹配到任何索引（分区索引、一级索引和二级索引），那么MergeTree就不能预先减小数据范围。在后续进行数据查询时，它会扫描所有分区目录，以及目录内索引段的最大区间。虽然不能减少数据范围，但是MergeTree仍然能够借助数据标记，以**多线程**的形式同时读取多个压缩数据块，以提升性能</u>。

### 6.7.3 数据标记与压缩数据块的对应关系

​	由于压缩数据块的划分，与一个间隔（index_granularity）内的数据大小相关，每个压缩数据块的体积都被严格控制在64KB～1MB。而一个间隔（index_granularity）的数据，又只会产生一行数据标记。那么根据一个间隔内数据的实际字节大小，数据标记和压缩数据块之间会产生三种不同的对应关系。接下来使用具体示例做进一步说明，对于示例数据，仍然是测试表hits_v1，其中index_granularity粒度为8192，数据总量为8873898行。

1. 多对一

   ​	多个数据标记对应一个压缩数据块，当一个间隔（index_granularity）内的数据未压缩大小size小于64KB时，会出现这种对应关系。

   ​	以hits_v1测试表的JavaEnable字段为例。JavaEnable数据类型为UInt8，大小为1B，则一个间隔内数据大小为8192B。所以在此种情形下，每8个数据标记会对应同一个压缩数据块，如图6-22所示

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/aa8dd00b89b04aa6b1c470cb38bcbc77.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6YKL6YGi55qE5rWB5rWq5YmR5a6i,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

   图6-22 多个数据标记对应同一个压缩数据块的示意

2. 一对一

   ​	一个数据标记对应一个压缩数据块，当一个间隔（index_granularity）内的数据未压缩大小size大于等于64KB且小于等于1MB时，会出现这种对应关系。

   ​	以hits_v1测试表的URLHash字段为例。URLHash数据类型为UInt64，大小为8B，则一个间隔内数据大小为65536B，恰好等于64KB。所以在此种情形下，数据标记与压缩数据块是一对一的关系，如图6-23所示。

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/74879439082d49c79fa5178a41d00e7e.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6YKL6YGi55qE5rWB5rWq5YmR5a6i,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

   图6-23 一个数据标记对应一个压缩数据块的示意

3. 一对多

   ​	一个数据标记对应多个压缩数据块，当一个间隔（index_granularity）内的数据未压缩大小size直接大于1MB时，会出现这种对应关系。

   ​	以hits_v1测试表的URL字段为例。URL数据类型为String，大小根据实际内容而定。如图6-24所示，编号45的标记对应了2个压缩数据块。

   <img src="https://img-blog.csdnimg.cn/77b05fff3723465990a1ad515c97e702.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6YKL6YGi55qE5rWB5rWq5YmR5a6i,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述" style="zoom:50%;" />

   图6-24 一个数据标记对应多个压缩数据块的示意

## 6.8 本章小结

​	本章全方面、立体地解读了MergeTree表引擎的工作原理：首先，解释了MergeTree的基础属性和物理存储结构；接着，依次介绍了数据分区、一级索引、二级索引、数据存储和数据标记的重要特性；最后，结合实际样例数据，进一步总结了MergeTree上述特性在一起协同时的工作过程。掌握本章的内容，即掌握了合并树系列表引擎的精髓。下一章将进一步介绍MergeTree家族中其他常见表引擎的具体使用方法。

# 7. MergeTree系列表引擎

​	目前在ClickHouse中，按照特点可以将表引擎大致分成6个系列，分别是合并树、外部存储、内存、文件、接口和其他，每一个系列的表引擎都有着独自的特点与使用场景。在它们之中，最为核心的当属MergeTree系列，因为它们拥有最为强大的性能和最广泛的使用场合。

​	经过上一章的介绍，大家应该已经知道了MergeTree有两层含义：其一，表示合并树表引擎家族；其二，表示合并树家族中最基础的MergeTree表引擎。而在整个家族中，除了基础表引擎MergeTree之外，常用的表引擎还有ReplacingMergeTree、SummingMergeTree、AggregatingMergeTree、CollapsingMergeTree和VersionedCollapsingMergeTree。每一种合并树的变种，在继承了基础MergeTree的能力之后，又增加了独有的特性。<u>其名称中的“合并”二字奠定了所有类型MergeTree的基因，**它们的所有特殊逻辑，都是在触发合并的过程中被激活的**</u>。在本章后续的内容中，会逐一介绍它们的特点以及使用方法。

## 7.1 MergeTree

​	MergeTree作为家族系列最基础的表引擎，提供了数据分区、一级索引和二级索引等功能。对于它们的运行机理，在上一章中已经进行了详细介绍。本节将进一步介绍MergeTree家族独有的另外两项能力——数据TTL与存储策略。

### 7.1.1 数据TTL

​	TTL即Time To Live，顾名思义，它表示数据的存活时间。在MergeTree中，可以为某个**列字段**或整张**表**设置TTL。

+ 当时间到达时，如果是列字段级别的TTL，则会删除这一列的数据；
+ 如果是表级别的TTL，则会删除整张表的数据；
+ **如果同时设置了列级别和表级别的TTL，则会以先到期的那个为主**。

​	**无论是列级别还是表级别的TTL，都需要依托某个DateTime或Date类型的字段**，通过对这个时间字段的INTERVAL操作，来表述TTL的过期时间，例如：

```sql
TTL time_col + INTERVAL 3 DAY
```

​	上述语句表示数据的存活时间是time_col时间的3天之后。又例如：

```sql
TTL time_col + INTERVAL 1 MONTH
```

​	上述语句表示数据的存活时间是time_col时间的1月之后。INTERVAL完整的操作包括SECOND、MINUTE、HOUR、DAY、WEEK、MONTH、QUARTER和YEAR。

1. 列级别TTL

   ​	<u>如果想要设置列级别的TTL，则需要在定义表字段的时候，为它们声明TTL表达式</u>，**<u>主键字段不能被声明TTL</u>**。以下面的语句为例：

   ```sql
   CREATE TABLE ttl_table_v1(
     id String,
     create_time DateTime,
     code String TTL create_time + INTERVAL 10 SECOND,
     type UInt8 TTL create_time + INTERVAL 10 SECOND
   )
   ENGINE = MergeTree
   PARTITION BY toYYYYMM(create_time)
   ORDER BY id
   ```

   ​	其中，create_time是日期类型，列字段code与type均被设置了TTL，它们的存活时间是在create_time的取值基础之上向后延续10秒。

   ​	现在写入测试数据，其中第一行数据create_time取当前的系统时间，而第二行数据的时间比第一行增加10分钟：

   ```sql
   INSERT INTO TABLE ttl_table_v1 VALUES('A000',now(),'C1',1),
   ('A000',now() + INTERVAL 10 MINUTE,'C1',1)
   SELECT * FROM ttl_table_v1
   ┌─id───┬─────create_time──┬─code─┬─type─┐
   │ A000 │ 2019-06-12 22:49:00 │ C1 │ 1 │
   │ A000 │ 2019-06-12 22:59:00 │ C1 │ 1 │
   └────┴───────────────┴────┴─────┘
   ```

   ​	接着心中默数10秒，然后执行optimize命令强制触发TTL清理：

   ```sql
   optimize TABLE ttl_table_v1 FINAL
   ```

   ​	再次查询ttl_table_v1则能够看到，由于第一行数据满足TTL过期条件（当前系统时间>=create_time+10秒），它们的code和type列会**被还原为数据类型的默认值**：

   ```sql
   ┌─id───┬───────create_time─┬─code─┬─type─┐
   │ A000 │ 2019-06-12 22:49:00 │ │ 0 │
   │ A000 │ 2019-06-12 22:59:00 │ C1 │ 1 │
   └─────┴───────────────┴─────┴─────┘
   ```

   ​	如果想要修改列字段的TTL，或是为已有字段添加TTL，则可以使用ALTER语句，示例如下：

   ```sql
   ALTER TABLE ttl_table_v1 MODIFY COLUMN code String TTL create_time + INTERVAL 1
   DAY
   ```

   ​	**目前ClickHouse没有提供取消列级别TTL的方法**。

2. 表级别TTL

   ​	如果想要为整张数据表设置TTL，需要在MergeTree的表参数中增加TTL表达式，例如下面的语句：

   ```sql
   CREATE TABLE ttl_table_v2(
     id String,
     create_time DateTime,
     code String TTL create_time + INTERVAL 1 MINUTE,
     type UInt8
   )ENGINE = MergeTree
   PARTITION BY toYYYYMM(create_time)
   ORDER BY create_time
   TTL create_time + INTERVAL 1 DAY
   ```

   ​	ttl_table_v2整张表被设置了TTL，当触发TTL清理时，那些满足过期时间的数据行将会被整行删除。同样，表级别的TTL也支持修改，修改的方法如下：

   ```sql
   ALTER TABLE ttl_table_v2 MODIFY TTL create_time + INTERVAL 3 DAY
   ```

   ​	**表级别TTL目前也没有取消的方法**。

3. TTL的运行机理

   ​	在知道了列级别与表级别TTL的使用方法之后，现在简单聊一聊TTL的运行机理。如果一张MergeTree表被设置了TTL表达式，那么在写入数据时，会**以数据分区为单位**，在每个分区目录内生成一个名为ttl.txt的文件。以刚才示例中的ttl_table_v2为例，它被设置了列级别TTL：

   ```sql
   code String TTL create_time + INTERVAL 1 MINUTE
   ```

   ​	同时被设置了表级别的TTL：

   ```sql
   TTL create_time + INTERVAL 1 DAY
   ```

   ​	那么，在写入数据之后，它的每个分区目录内都会生成ttl.txt文件：

   ```shell
   # pwd
   /chbase/data/data/default/ttl_table_v2/201905_1_1_0
   # ll
   total 60
   …省略
   -rw-r-----. 1 clickhouse clickhouse 38 May 13 14:30 create_time.bin
   -rw-r-----. 1 clickhouse clickhouse 48 May 13 14:30 create_time.mrk2
   -rw-r-----. 1 clickhouse clickhouse 8 May 13 14:30 primary.idx
   -rw-r-----. 1 clickhouse clickhouse 67 May 13 14:30 ttl.txt
   …省略
   ```

   ​	进一步查看ttl.txt的内容：

   ```shell
   cat ./ttl.txt
   ttl format version: 1
   {"columns":[{"name":"code","min":1557478860,"max":1557651660}],"table":
   {"min":1557565200,"max":1557738000}}
   ```

   ​	通过上述操作会发现，原来MergeTree是通过一串JSON配置保存了TTL的相关信息，其中：

   + **columns用于保存列级别TTL信息；**

   + **table用于保存表级别TTL信息；**

   + **min和max则保存了当前数据分区内，TTL指定日期字段的最小值、最大值分别与INTERVAL表达式计算后的时间戳。**

   ​	如果将table属性中的min和max时间戳格式化，并分别与create_time最小与最大取值对比：

   ```sql
   SELECT
   	toDateTime('1557565200') AS ttl_min,
   	toDateTime('1557738000') AS ttl_max,
   	ttl_min - MIN(create_time) AS expire_min,
   	ttl_max - MAX(create_time) AS expire_max
   FROM ttl_table_v2
   ┌─────ttl_min────┬────ttl_max────┬─expire_min┬─expire_max─┐
   │ 2019-05-11 17:00:00 │ 2019-05-13 17:00:00 │ 86400 │ 86400 │
   └─────────────┴─────────────┴────────┴────────┘
   ```

   ​	则能够印证，ttl.txt中记录的极值区间恰好等于当前数据分区内create_time最小与最大值增加1天（1天=86400秒）所表示的区间，与TTL表达式`create_time+INTERVAL 1 DAY`的预期相符。

   ​	在知道了TTL信息的记录方式之后，现在看看它的大致处理逻辑。

   （1）MergeTree以分区目录为单位，通过ttl.txt文件记录过期时间，并将其作为后续的判断依据。

   （2）每当写入一批数据时，都会基于INTERVAL表达式的计算结果为这个分区生成ttl.txt文件。

   （3）**只有在MergeTree合并分区时，才会触发删除TTL过期数据的逻辑**。

   （4）**在选择删除的分区时，会使用贪婪算法，它的算法规则是尽可能找到会最早过期的，同时年纪又是最老的分区（合并次数更多，MaxBlockNum更大的）**。

   （5）<u>如果一个分区内某一列数据因为TTL到期全部被删除了，那么在合并之后生成的新分区目录中，将不会包含这个列字段的数据文件（.bin和.mrk）</u>。

   ​	这里还有几条TTL使用的小贴士。

   （1）<u>TTL默认的合并频率由MergeTree的merge_with_ttl_timeout参数控制，默认86400秒，即1天。它维护的是一个专有的TTL任务队列。有别于MergeTree的常规合并任务，如果这个值被设置的过小，可能会带来性能损耗</u>。

   （2）除了被动触发TTL合并外，也可以使用optimize命令强制触发合并。例如，触发一个分区合并：

   ```sql
   optimize TABLE table_name
   ```

   触发所有分区合并：

   ```sql
   optimize TABLE table_name FINAL
   ```

   （3）**ClickHouse目前虽然没有提供删除TTL声明的方法，但是提供了控制全局TTL合并任务的启停方法**：

   ```sql
   SYSTEM STOP/START TTL MERGES
   ```

   ​	虽然还不能做到按每张MergeTree数据表启停，但聊胜于无吧。

### 7.1.2 多路径存储策略

​	**在ClickHouse 19.15版本之前，MergeTree只支持单路径存储，所有的数据都会被写入config.xml配置中path指定的路径下，即使服务器挂载了多块磁盘，也无法有效利用这些存储空间。为了解决这个痛点，从19.15版本开始，MergeTree实现了自定义存储策略的功能，支持以数据分区为最小移动单元，将分区目录写入多块磁盘目录**。

​	根据配置策略的不同，目前大致有三类存储策略。

+ 默认策略：MergeTree原本的存储策略，无须任何配置，所有分区会自动保存到config.xml配置中path指定的路径下。

+ JBOD策略：这种策略适合服务器挂载了多块磁盘，但没有做RAID的场景。<u>JBOD的全称是Just a Bunch of Disks，它是一种轮询策略，每执行一次INSERT或者MERGE，所产生的新分区会轮询写入各个磁盘。这种策略的效果类似RAID 0，可以降低单块磁盘的负载，在一定条件下能够增加数据并行读写的性能</u>。**如果单块磁盘发生故障，则会丢掉应用JBOD策略写入的这部分数据**。（数据的可靠性需要利用副本机制保障，这部分内容将会在后面介绍副本与分片时介绍。）

+ HOT/COLD策略：这种策略适合服务器挂载了不同类型磁盘的场景。将存储磁盘分为HOT与COLD两类区域。<u>HOT区域使用SSD这类高性能存储媒介，注重存取性能；COLD区域则使用HDD这类高容量存储媒介，注重存取经济性</u>。**数据在写入MergeTree之初，首先会在HOT区域创建分区目录用于保存数据，当分区数据大小累积到阈值时，数据会自行移动到COLD区域。而在每个区域的内部，也支持定义多个磁盘，所以在单个区域的写入过程中，也能应用JBOD策略**。

  存储配置需要预先定义在config.xml配置文件中，由storage_configuration标签表示。在storage_configuration之下又分为disks和policies两组标签，分别表示磁盘与存储策略。

  disks的配置示例如下所示，支持定义多块磁盘：

  ```xml
  <storage_configuration>
    <disks>
      <disk_name_a> <!--自定义磁盘名称 -->
        <path>/chbase/data</path><!—磁盘路径 -->
        <keep_free_space_bytes>1073741824</keep_free_space_bytes>
      </disk_name_a>
      <disk_name_b>
        <path>… </path>
      </disk_name_b>
    </disks>
  ```

  其中：

  + \<disk_name_*>，必填项，必须全局唯一，表示磁盘的自定义名称；

  + \<path>，必填项，用于指定磁盘路径；

  + \<keep_free_space_bytes>，选填项，以字节为单位，用于定义磁盘的预留空间。

  在policies的配置中，需要引用先前定义的disks磁盘。policies同样支持定义多个策略，它的示例如下：

  ```xml
  <policies>
    <policie_name_a> <!--自定义策略名称 -->
      <volumes>
        <volume_name_a> <!--自定义卷名称 -->
          <disk>disk_name_a</disk>
          <disk>disk_name_b</disk>
          <max_data_part_size_bytes>1073741824</max_data_part_size_bytes>
          </volume_name_b>
      </volumes>
      <move_factor>0.2</move_factor>
    </policie_name_a>
    <policie_name_b>
    </policie_name_b>
  </policies>
  </storage_configuration>
  ```

  其中：

  + \<policie_name_*>，必填项，必须全局唯一，表示策略的自定义名称；

  + \<volume_name_*>，必填项，必须全局唯一，表示卷的自定义名称；

  + \<disk>，必填项，用于关联配置内的磁盘，可以声明多个disk，MergeTree会按定义的顺序选择disk；

  + \<max_data_part_size_bytes>，选填项，以字节为单位，表示在这个卷的单个disk磁盘中，一个数据分区的最大存储阈值，如果当前分区的数据大小超过阈值，则之后的分区会写入下一个disk磁盘；

  + \<move_factor>，选填项，默认为0.1；如果当前卷的可用空间小于factor因子，并且定义了多个卷，则数据会自动向下一个卷移动。

  ​	在知道了配置格式之后，现在用一组示例说明它们的使用方法。

  1. JBOD策略

     首先，在config.xml配置文件中增加storage_configuration元素，并配置3块磁盘：

     ```xml
     <storage_configuration>
       <!--自定义磁盘配置 -->
       <disks>
         <disk_hot1> <!--自定义磁盘名称 -->
           <path>/chbase/data</path>
         </disk_hot1>
         <disk_hot2>
           <path>/chbase/hotdata1</path>
         </disk_hot2>
         <disk_cold>
           <path>/chbase/cloddata</path>
           <keep_free_space_bytes>1073741824</keep_free_space_bytes>
         </disk_cold>
       </disks>
       …省略
     ```

     接着，配置一个存储策略，在volumes卷下引用两块磁盘，组成一个磁盘组：

     ```xml
     <!-- 实现JDOB效果 -->
     <policies>
       <default_jbod> <!--自定义策略名称 -->
         <volumes>
           <jbod> <!—自定义名称 磁盘组 -->
             <disk>disk_hot1</disk>
             <disk>disk_hot2</disk>
           </jbod>
         </volumes>
       </default_jbod>
     </policies>
     </storage_configuration>
     ```

     至此，一个支持JBOD策略的存储策略就配置好了。在正式应用之前，还需要做一些准备工作。首先，需要给磁盘路径授权，使ClickHouse用户拥有路径的读写权限：

     ```xml
     sudo chown clickhouse:clickhouse -R /chbase/cloddata /chbase/hotdata1
     ```

     **由于存储配置不支持动态更新，为了使配置生效，还需要重启clickhouse-server服务**：

     ```shell
     sudo service clickhouse-server restart
     ```

     服务重启好之后，可以查询系统表以验证配置是否已经生效：

     ```sql
     SELECT
     name,
     path,formatReadableSize(free_space) AS free,
     formatReadableSize(total_space) AS total,
     formatReadableSize(keep_free_space) AS reserved
     FROM system.disks
     ┌─name─────┬─path────────┬─free────┬─total────┬─reserved─┐
     │ default │ /chbase/data/ │ 38.26 GiB │ 49.09 GiB │ 0.00 B │
     │ disk_cold │ /chbase/cloddata/ │ 37.26 GiB │ 48.09 GiB │ 1.00 GiB │
     │ disk_hot1 │ /chbase/data/ │ 38.26 GiB │ 49.09 GiB │ 0.00 B │
     │ disk_hot2 │ /chbase/hotdata1/ │ 38.26 GiB │ 49.09 GiB │ 0.00 B │
     └────────┴────────────┴────────┴────────┴───────┘
     ```

     ​	通过system.disks系统表可以看到刚才声明的3块磁盘配置已经生效。接着验证策略配置：

     ```sql
     SELECT policy_name,
     volume_name,
     volume_priority,
     disks,
     formatReadableSize(max_data_part_size) max_data_part_size ,
     move_factor FROM
     system.storage_policies
     ┌─policy_name─┬─volume_name─┬─disks──────────┬─max_data_part_size─┬─move_factor─┐
     │ default │ default │ ['default'] │ 0.00 B │ 0
     │
     │ default_jbod │ jbod │ ['disk_hot1','disk_hot2']│ 0.00 B │ 0.1
     │
     └────────┴────────┴─────────────┴──────────┴─────────┘
     ```

     通过system.storage_policies系统表可以看到刚才配置的存储策略也已经生效了。

     现在创建一张MergeTree表，用于测试default_jbod存储策略的效果：

     ```sql
     CREATE TABLE jbod_table(
       id UInt64
     )ENGINE = MergeTree()
     ORDER BY id
     SETTINGS storage_policy = 'default_jbod'
     ```

     在定义MergeTree时，使用storage_policy配置项指定刚才定义的default_jbod存储策略。**存储策略一旦设置，就不能修改了**。现在开始测试它的效果。首先写入第一批数据，创建一个分区目录：

     ```sql
     INSERT INTO TABLE jbod_table SELECT rand() FROM numbers(10)
     ```

     查询分区系统表，可以看到第一个分区写入了第一块磁盘disk_hot1：

     ```sql
     SELECT name, disk_name FROM system.parts WHERE table = 'jbod_table'
     ┌─name─────┬─disk_name─┐
     │ all_1_1_0 │ disk_hot1 │
     └────────┴───────┘
     ```

     接着写入第二批数据，创建一个新的分区目录：

     ```sql
     INSERT INTO TABLE jbod_table SELECT rand() FROM numbers(10)
     ```

     再次查询分区系统表，可以看到第二个分区写入了第二块磁盘disk_hot2：

     ```sql
     SELECT name, disk_name FROM system.parts WHERE table = 'jbod_table'
     ┌─name─────┬─disk_name─┐
     │ all_1_1_0 │ disk_hot1 │
     │ all_2_2_0 │ disk_hot2 │
     └────────┴───────┘
     ```

     最后触发一次分区合并动作，生成一个合并后的新分区目录：

     ```sql
     optimize TABLE jbod_table
     ```

     还是查询分区系统表，可以看到合并后生成的all_1_2_1分区，再一次写入了第一块磁盘disk_hot1：

     ```sql
     ┌─name─────┬─disk_name─┐
     │ all_1_1_0 │ disk_hot1 │
     │ all_1_2_1 │ disk_hot1 │
     │ all_2_2_0 │ disk_hot2 │
     └────────┴───────┘
     ```

     至此，大家应该已经明白JBOD策略的工作方式了。**在这个策略中，由多个磁盘组成了一个磁盘组，即volume卷。每当生成一个新数据分区的时候，分区目录会依照volume卷中磁盘定义的顺序，依次轮询并写入各个磁盘**。

  2. HOT/COLD策略

     现在介绍HOT/COLD策略的使用方法。首先在上一小节介绍的配置文件中添加一个新的策略：

     ```xml
     <policies>
       …省略
       <moving_from_hot_to_cold><!--自定义策略名称 -->
         <volumes>
           <hot><!--自定义名称 ,hot区域磁盘 -->
             <disk>disk_hot1</disk>
             <max_data_part_size_bytes>1073741824</max_data_part_size_bytes>
           </hot>
           <cold><!--自定义名称 ,cold区域磁盘 -->
             <disk>disk_cold</disk>
           </cold>
         </volumes>
         <move_factor>0.2</move_factor>
       </moving_from_hot_to_cold>
     </policies>
     ```

     **存储配置不支持动态更新，所以为了使配置生效，需要重启clickhouse-server服务**：

     ```shell
     sudo service clickhouse-server restart
     ```

     通过system.storage_policies系统表可以看到，刚才配置的存储策略已经生效了。

     ```sql
     ┌─policy_name────────┬─volume_name┬─disks────┬max_data_part_size─┬─move_factor─┐
     │ moving_from_hot_to_cold │ hot │ ['disk_hot1']│ 1.00 MiB │ 0.2
     │
     │ moving_from_hot_to_cold │ cold │ ['disk_cold']│ 0.00 B │ 0.2
     │
     └───────────────┴───────┴────────┴──────────┴────────┘
     ```

     moving_from_hot_to_cold存储策略拥有hot和cold两个磁盘卷，在每个卷下各拥有1块磁盘。注意，hot磁盘卷的max_data_part_size列显示的值是1M，这个值的含义表示，在这个磁盘卷下，如果一个分区的大小超过1MB，则它需要被移动到紧邻的下一个磁盘卷。

     与先前一样，现在创建一张MergeTree表，用于测试moving_from_hot_to_cold存储策略的效果：

     ```sql
     CREATE TABLE hot_cold_table(
       id UInt64
     )ENGINE = MergeTree()
     ORDER BY id
     SETTINGS storage_policy = 'moving_from_hot_to_cold'
     ```

     在定义MergeTree时，使用storage_policy配置项指定刚才定义的moving_from_hot_to_cold存储策略。存储策略一旦设置就不能再修改。

     现在开始测试它的效果，首先写入第一批数据，创建一个分区目录，数据大小500KB：

     ```sql
     -- 写入500K大小,分区会写入hot
     INSERT INTO TABLE hot_cold_table SELECT rand()FROM numbers(100000)
     ```

     查询分区系统表，可以看到第一个分区写入了hot卷：

     ```sql
     SELECT name, disk_name FROM system.parts WHERE table = 'hot_cold_table'
     ┌─name─────┬─disk_name─┐
     │ all_1_1_0 │ disk_hot1 │
     └────────┴───────┘
     ```

     接着写入第二批数据，创建一个新的分区目录，数据大小还是500KB：

     ```sql
     INSERT INTO TABLE hot_cold_table SELECT rand()FROM numbers(100000)
     ```

     再次查询分区系统表，可以看到第二个分区，仍然写入了hot卷：

     ```sql
     SELECT name, disk_name FROM system.parts WHERE table = 'hot_cold_table'
     ┌─name─────┬─disk_name─┐
     │ all_1_1_0 │ disk_hot1 │
     │ all_2_2_0 │ disk_hot1 │
     └────────┴───────┘
     ```

     这是由于hot磁盘卷的max_data_part_size是1MB，而前两次数据写入所创建的分区，单个分区大小是500KB，自然分区目录都被保存到了hot磁盘卷下的disk_hot1磁盘。现在触发一次分区合并动作，生成一个新的分区目录：

     ```sql
     optimize TABLE hot_cold_table
     ```

     查询分区系统表，可以看到合并后生成的all_1_2_1分区写入了cold卷：

     ```sql
     ┌─name─────┬─disk_name─┐
     │ all_1_1_0 │ disk_hot1 │
     │ all_1_2_1 │ disk_cold │
     │ all_2_2_0 │ disk_hot1 │
     └────────┴───────┘
     ```

     这是因为两个分区合并之后，所创建的新分区的大小超过了1MB，所以它被写入了cold卷，相关查询代码如下：

     ```sql
     SELECT
     disk_name,
     formatReadableSize(bytes_on_disk) AS size
     FROM system.parts
     WHERE (table = 'hot_cold_table') AND active = 1
     ┌─disk_name─┬─size────┐
     │ disk_cold │ 1.01 MiB │
     └────────┴───────┘
     ```

     注意，如果一次性写入大于1MB的数据，分区也会被写入cold卷。

     至此，大家应该明白HOT/COLD策略的工作方式了。**在这个策略中，由多个磁盘卷（volume卷）组成了一个volume组。每当生成一个新数据分区的时候，按照阈值大小（max_data_part_size），分区目录会依照volume组中磁盘卷定义的顺序，依次轮询并写入各个卷下的磁盘**。

     **<u>虽然MergeTree的存储策略目前不能修改，但是分区目录却支持移动</u>**。例如，将某个分区移动至当前存储策略中当前volume卷下的其他disk磁盘：

     ```sql
     ALTER TABLE hot_cold_table MOVE PART 'all_1_2_1' TO DISK 'disk_hot1'
     ```

     或是将某个分区移动至当前存储策略中其他的volume卷：

     ```sql
     ALTER TABLE hot_cold_table MOVE PART 'all_1_2_1' TO VOLUME 'cold'
     ```

## 7.2 ReplacingMergeTree

​	**虽然MergeTree拥有主键，但是它的主键却没有唯一键的约束**。这意味着即便多行数据的主键相同，它们还是能够被正常写入。在某些使用场合，用户并不希望数据表中含有重复的数据。**ReplacingMergeTree就是在这种背景下为了数据去重而设计的，它能够在合并分区时删除重复的数据**。它的出现，确实也在一定程度上解决了重复数据的问题。为什么说是“一定程度”？此处先按下不表。

​	创建一张ReplacingMergeTree表的方法与创建普通MergeTree表无异，只需要替换Engine：

```sql
ENGINE = ReplacingMergeTree(ver)
```

​	其中，ver是选填参数，会指定一个UInt*、Date或者DateTime类型的字段作为<u>版本号</u>。**这个参数决定了数据去重时所使用的算法**。接下来，用一个具体的示例说明它的用法。首先执行下面的语句创建数据表：

```sql
CREATE TABLE replace_table(
  id String,
  code String,
  create_time DateTime
)ENGINE = ReplacingMergeTree()
PARTITION BY toYYYYMM(create_time)
ORDER BY (id,code)
PRIMARY KEY id
```

​	**注意这里的ORDER BY是去除重复数据的关键，排序键ORDERBY所声明的表达式是后续作为判断数据是否重复的依据**。在这个例子中，数据会基于id和code两个字段去重。假设此时表内的测试数据如下：

```sql
┌─id───┬─code─┬───────create_time─┐
│ A001 │ C1 │ 2019-05-10 17:00:00 │
│ A001 │ C1 │ 2019-05-11 17:00:00 │
│ A001 │ C100 │ 2019-05-12 17:00:00 │
│ A001 │ C200 │ 2019-05-13 17:00:00 │
│ A002 │ C2 │ 2019-05-14 17:00:00 │
│ A003 │ C3 │ 2019-05-15 17:00:00 │
└─────┴─────┴───────────────┘
```

​	那么在执行optimize强制触发合并后，会按照id和code分组，**保留分组内的最后一条**（观察create_time日期字段）：

```sql
optimize TABLE replace_table FINAL
```

​	将其余重复的数据删除：

```sql
┌─id───┬─code─┬──────create_time─┐
│ A001 │ C1 │ 2019-05-11 17:00:00 │
│ A001 │ C100 │ 2019-05-12 17:00:00 │
│ A001 │ C200 │ 2019-05-13 17:00:00 │
│ A002 │ C2 │ 2019-05-14 17:00:00 │
│ A003 │ C3 │ 2019-05-15 17:00:00 │
└────┴────┴──────────────┘
```

​	从执行的结果来看，ReplacingMergeTree在去除重复数据时，确实是以ORDER BY排序键为基准的，而不是PRIMARY KEY。因为在上面的例子中，ORDER BY是(id,code)，而PRIMARY KEY是id，如果按照id值去除重复数据，则最终结果应该只剩下A001、A002和A003三行数据。

​	到目前为止，ReplacingMergeTree看起来完美地解决了重复数据的问题。事实果真如此吗？现在尝试写入一批新数据：

```sql
INSERT INTO TABLE replace_table VALUES('A001','C1','2019-08-10 17:00:00')
```

​	写入之后，执行optimize强制分区合并，并查询数据：

```sql
┌─id───┬─code─┬─────────create_time─┐
│ A001 │ C1 │ 2019-08-22 17:00:00 │
└─────┴────┴─────────────────┘
┌─id──┬─code─┬─────────create_time─┐
│ A001 │ C1 │ 2019-05-11 17:00:00 │
│ A001 │ C100 │ 2019-05-12 17:00:00 │
│ A001 │ C200 │ 2019-05-13 17:00:00 │
│ A002 │ C2 │ 2019-05-14 17:00:00 │
│ A003 │ C3 │ 2019-05-15 17:00:00 │
└────┴──────┴─────────────────┘
```

​	再次观察返回的数据，可以看到A001:C1依然出现了重复。这是怎么回事呢？这是因为**<u>ReplacingMergeTree是以分区为单位删除重复数据的</u>**。**只有在相同的数据分区内重复的数据才可以被删除，而不同数据分区之间的重复数据依然不能被剔除。这就是上面说ReplacingMergeTree只是在一定程度上解决了重复数据问题的原因**。

​	现在接着说明ReplacingMergeTree版本号的用法。以下面的语句为例：

```sql
CREATE TABLE replace_table_v(
  id String,
  code String,
  create_time DateTime
)ENGINE = ReplacingMergeTree(create_time)
PARTITION BY toYYYYMM(create_time)
ORDER BY id
```

​	replace_table_v基于id字段去重，并且使用create_time字段作为版本号，假设表内的数据如下所示：

```sql
┌─id──┬─code──┬───────────create_time─┐
│ A001 │ C1 │ 2019-05-10 17:00:00 │
│ A001 │ C1 │ 2019-05-25 17:00:00 │
│ A001 │ C1 │ 2019-05-13 17:00:00 │
└────┴─────┴───────────────────┘
```

​	那么在删除重复数据的时候，会保留同一组数据内create_time时间最长的那一行：

```sql
┌─id────┬─code─┬──────────create_time─┐
│ A001 │ C1 │ 2019-05-25 17:00:00 │
└──────┴────┴──────────────────┘
```

​	在知道了ReplacingMergeTree的使用方法后，现在简单梳理一下它的处理逻辑。

（1）**使用ORBER BY排序键作为判断重复数据的唯一键**。

（2）**只有在合并分区的时候才会触发删除重复数据的逻辑**。

（3）**<u>以数据分区为单位删除重复数据</u>**。**当分区合并时，同一分区内的重复数据会被删除；不同分区之间的重复数据不会被删除**。

（4）在进行数据去重时，因为分区内的数据已经基于ORBER BY进行了排序，所以能够找到那些相邻的重复数据。

（5）数据去重策略有两种：

+ **如果没有设置ver版本号，则保留同一组重复数据中的最后一行。**
+ **如果设置了ver版本号，则保留同一组重复数据中ver字段取值最大的那一行**。

## 7.3 SummingMergeTree

​	假设有这样一种查询需求：终端用户只需要查询数据的汇总结果，不关心明细数据，并且数据的汇总条件是预先明确的（GROUP BY条件明确，且不会随意改变）。

​	对于这样的查询场景，在ClickHouse中如何解决呢？最直接的方案就是使用MergeTree存储数据，然后通过GROUP BY聚合查询，并利用SUM聚合函数汇总结果。这种方案存在两个问题。

+ **存在额外的存储开销：终端用户不会查询任何明细数据，只关心汇总结果，所以不应该一直保存所有的明细数据**。
+ **存在额外的查询开销：终端用户只关心汇总结果，虽然MergeTree性能强大，但是每次查询都进行实时聚合计算也是一种性能消耗**。

​	SummingMergeTree就是为了应对这类查询场景而生的。顾名思义，<u>它能够**在合并分区的时候**按照预先定义的条件聚合汇总数据，将同一分组下的多行数据汇总合并成一行，这样既减少了数据行，又降低了后续汇总查询的开销</u>。

​	<u>在先前介绍MergeTree原理时曾提及，在MergeTree的每个数据分区内，数据会按照ORDER BY表达式排序。主键索引也会按照PRIMARY KEY表达式取值并排序。而ORDER BY可以指代主键，所以在一般情形下，只单独声明ORDER BY即可。此时，ORDER BY与PRIMARY KEY定义相同，数据排序与主键索引相同</u>。

​	如果需要同时定义ORDER BY与PRIMARY KEY，通常只有一种可能，那便是明确希望ORDER BY与PRIMARY KEY不同。这种情况通常只会在使用SummingMergeTree或AggregatingMergeTree时才会出现。这是为何呢？这是因为**<u>SummingMergeTree与AggregatingMergeTree的聚合都是根据ORDER BY进行的</u>**。由此可以引出两点原因：<u>**主键与聚合的条件定义分离，为修改聚合条件留下空间**</u>。

​	现在用一个示例说明。假设一张SummingMergeTree数据表有A、B、C、D、E、F六个字段，如果需要按照A、B、C、D汇总，则有：

```sql
ORDER BY (A，B，C，D)
```

​	但是如此一来，此表的主键也被定义成了A、B、C、D。而在业务层面，其实只需要对字段A进行查询过滤，应该只使用A字段创建主键。所以，一种更加优雅的定义形式应该是：

```sql
ORDER BY (A、B、C、D)
PRIMARY KEY A
```

​	**<u>如果同时声明了ORDER BY与PRIMARY KEY，MergeTree会强制要求PRIMARY KEY列字段必须是ORDER BY的前缀</u>**。例如下面的定义是错误的：

```sql
ORDER BY (B、C)
PRIMARY KEY A
```

​	PRIMARY KEY必须是ORDER BY的前缀：

```sql
ORDER BY (B、C)
PRIMARY KEY B
```

​	**<u>这种强制约束保障了即便在两者定义不同的情况下，主键仍然是排序键的前缀，不会出现索引与数据顺序混乱的问题</u>**。

​	假设现在业务发生了细微的变化，需要减少字段，将先前的A、B、C、D改为按照A、B聚合汇总，则可以按如下方式修改排序键：

```sql
ALTER TABLE table_name MODIFY ORDER BY (A,B)
```

​	<u>在修改ORDER BY时会有一些限制，只能在现有的基础上减少字段。如果是新增排序字段，则只能添加通过ALTER ADD COLUMN新增的字段</u>。但是ALTER是一种元数据的操作，修改成本很低，相比不能被修改的主键，这已经非常便利了。

​	现在开始正式介绍SummingMergeTree的使用方法。表引擎的声明方式如下所示：

```sql
ENGINE = SummingMergeTree((col1,col2,…))
```

​	其中，col1、col2为columns参数值，这是一个选填参数，**用于设置除主键外的其他数值类型字段**，以指定被SUM汇总的列字段。**如若不填写此参数，则会将所有非主键的数值类型字段进行SUM汇总**。接来下用一组示例说明它的使用方法：

```sql
CREATE TABLE summing_table(
  id String,
  city String,
  v1 UInt32,
  v2 Float64,
  create_time DateTime
)ENGINE = SummingMergeTree()
PARTITION BY toYYYYMM(create_time)
ORDER BY (id, city)
PRIMARY KEY id
```

​	注意，**<u>这里的ORDER BY是一项关键配置，SummingMergeTree在进行数据汇总时，会根据ORDER BY表达式的取值进行聚合操作</u>**。假设此时表内的数据如下所示：

```sql
┌─id──┬─city───┬─v1─┬─v2─┬────────create_time─┐
│ A001 │ wuhan │ 10 │ 20 │ 2019-08-10 17:00:00 │
│ A001 │ wuhan │ 20 │ 30 │ 2019-08-20 17:00:00 │
│ A001 │ zhuhai │ 20 │ 30 │ 2019-08-10 17:00:00 │
└─────┴───────┴───┴───┴────────────────┘
┌─id──┬─city───┬─v1─┬─v2─┬────────create_time─┐
│ A001 │ wuhan │ 10 │ 20 │ 2019-02-10 09:00:00 │
└─────┴───────┴───┴───┴───────────────┘
┌─id──┬─city───┬─v1─┬─v2─┬────────create_time─┐
│ A002 │ wuhan │ 60 │ 50 │ 2019-10-10 17:00:00 │
└────┴──────┴───┴───┴───────────────┘
```

执行optimize强制进行触发和合并操作：

```sql
optimize TABLE summing_table FINAL
```

再次查询，表内数据会变成下面的样子：

```sql
┌─id──┬─city───┬─v1─┬─v2─┬─────────create_time─┐
│ A001 │ wuhan │ 30 │ 50 │ 2019-08-10 17:00:00 │
│ A001 │ zhuhai │ 20 │ 30 │ 2019-08-10 17:00:00 │
└─────┴──────┴────┴────┴─────────────────┘
┌─id──┬─city───┬─v1─┬─v2─┬─────────create_time─┐
│ A001 │ wuhan │ 10 │ 20 │ 2019-02-10 09:00:00 │
└────┴──────┴────┴────┴─────────────────┘
┌─id──┬─city───┬─v1─┬─v2─┬─────────create_time─┐
│ A002 │ wuhan │ 60 │ 50 │ 2019-10-10 17:00:00 │
└────┴──────┴────┴────┴─────────────────┘
```

​	至此能够看到，在第一个分区内，同为A001:wuhan的两条数据汇总成了一行。<u>其中，v1和v2被SUM汇总，不在汇总字段之列的create_time则选取了同组内第一行数据的取值。而不同分区之间，数据没有被汇总合并</u>。

​	<u>**SummingMergeTree也支持嵌套类型的字段**，**在使用嵌套类型字段时，需要被SUM汇总的字段名称必须以Map后缀结尾**</u>，例如：

```sql
CREATE TABLE summing_table_nested(
  id String,
  nestMap Nested(
    id UInt32,
    key UInt32,
    val UInt64
  ),
  create_time DateTime
)ENGINE = SummingMergeTree()
PARTITION BY toYYYYMM(create_time)
ORDER BY id
```

​	**在使用嵌套数据类型的时候，默认情况下，会以<u>嵌套类型中第一个字段</u>作为聚合条件Key**。假设表内的数据如下所示：

```sql
┌─id──┬─nestMap.id─┬─nestMap.key─┬─nestMap.val─┬─────create_time─┐
│ A001 │ [1,1,2] │ [10,20,30] │ [40,50,60] │ 2019-08-10 17:00:00 │
└─────┴────────┴─────────┴─────────┴─────────────┘
```

​	上述示例中数据会按照第一个字段id聚合，汇总后的数据会变成下面的样子：

```sql
┌─id──┬─nestMap.id─┬─nestMap.key─┬─nestMap.val─┬─────create_time─┐
│ A001 │ [1,2] │ [30,30] │ [90,60] │ 2019-08-10 17:00:00 │
└────┴────────┴─────────┴─────────┴─────────────┘
```

​	数据汇总的逻辑示意如下所示：

```shell
[(1, 10, 40)] + [(1, 20, 50)] -> [(1, 30, 90)]
[(2, 30, 60)] -> [(2, 30, 60)]
```

​	**在使用嵌套数据类型的时候，也支持使用复合Key作为数据聚合的条件。为了使用复合Key，在嵌套类型的字段中，<u>除第一个字段以外，任何名称是以Key、Id或Type为后缀结尾的字段，都将和第一个字段一起组成复合Key</u>**。例如将上面的例子中小写key改为Key：

```sql
nestMap Nested(
  id UInt32,
  Key UInt32,
  val UInt64
),
```

​	上述例子中数据会以id和Key作为聚合条件。

​	在知道了SummingMergeTree的使用方法后，现在简单梳理一下它的处理逻辑。

（1）**用ORBER BY排序键作为聚合数据的条件Key**。

（2）只有在合并分区的时候才会触发汇总的逻辑。

（3）以数据分区为单位来聚合数据。当分区合并时，**同一数据分区内聚合Key相同的数据会被合并汇总，而不同分区之间的数据则不会被汇总**。

（4）如果在定义引擎时指定了columns汇总列（非主键的数值类型字段），则SUM汇总这些列字段；**如果未指定，则聚合所有非主键的数值类型字段**。

（5）<u>在进行数据汇总时，因为分区内的数据已经基于ORBER BY排序，所以能够找到相邻且拥有相同聚合Key的数据</u>。

（6）<u>在汇总数据时，同一分区内，相同聚合Key的多行数据会合并成一行。其中，汇总字段会进行SUM计算；**对于那些非汇总字段，则会使用第一行数据的取值**</u>。

（7）**<u>支持嵌套结构，但列字段名称必须以Map后缀结尾。嵌套类型中，默认以第一个字段作为聚合Key。除第一个字段以外，任何名称以Key、Id或Type为后缀结尾的字段，都将和第一个字段一起组成复合Key</u>**。

## 7.4 AggregatingMergeTree

> [数据立方_百度百科 (baidu.com)](https://baike.baidu.com/item/数据立方/13237096?fromtitle=数据立方体&fromid=9851963&fr=aladdin)

​	有过数据仓库建设经验的读者一定知道“数据立方体”的概念，这是一个在数据仓库领域十分常见的模型。<u>它通过以空间换时间的方法提升查询性能，将需要聚合的数据预先计算出来，并将结果保存起来。在后续进行聚合查询的时候，直接使用结果数据</u>。

​	<u>AggregatingMergeTree就有些许数据立方体的意思，它能够**在合并分区的时候**，按照预先定义的条件聚合数据。同时，根据预先定义的聚合函数计算数据并通过二进制的格式存入表内</u>。将同一分组下的多行数据聚合成一行，既减少了数据行，又降低了后续聚合查询的开销。**可以说，AggregatingMergeTree是SummingMergeTree的升级版，它们的许多设计思路是一致的**，例如同时定义ORDER BY与PRIMARY KEY的原因和目的。但是在使用方法上，两者存在明显差异，应该说AggregatingMergeTree的定义方式是MergeTree家族中最为特殊的一个。

​	声明使用AggregatingMergeTree的方式如下：

```sql
ENGINE = AggregatingMergeTree()
```

​	AggregatingMergeTree没有任何额外的设置参数，**在分区合并时，在每个数据分区内，会按照ORDER BY聚合**。而<u>使用何种聚合函数，以及针对哪些列字段计算，则是通过定义AggregateFunction数据类型实现的</u>。以下面的语句为例：

```sql
CREATE TABLE agg_table(
  id String,
  city String,
  code AggregateFunction(uniq,String),
  value AggregateFunction(sum,UInt32),
  create_time DateTime
)ENGINE = AggregatingMergeTree()
PARTITION BY toYYYYMM(create_time)
ORDER BY (id,city)
PRIMARY KEY id
```

​	上例中列字段id和city是聚合条件，等同于下面的语义：

```sql
GROUP BY id，city
```

​	而code和value是聚合字段，其语义等同于：

```sql
UNIQ(code), SUM(value)
```

​	**AggregateFunction是ClickHouse提供的一种特殊的数据类型，它能够以二进制的形式存储中间状态结果**。其使用方法也十分特殊，对于AggregateFunction类型的列字段，数据的写入和查询都与寻常不同。

+ **在写入数据时，需要调用\*State函数；**
+ **而在查询数据时，则需要调用相应的\*Merge函数**。

​	其中，\*表示定义时使用的聚合函数。

例如示例中定义的code和value，使用了uniq和sum函数：

```sql
code AggregateFunction(uniq,String),
value AggregateFunction(sum,UInt32),
```

​	那么，在写入数据时需要调用与uniq、sum对应的uniqState和sumState函数，并使用INSERT SELECT语法：

```sql
INSERT INTO TABLE agg_table
SELECT 'A000','wuhan',
uniqState('code1'),
sumState(toUInt32(100)),
'2019-08-10 17:00:00'
```

​	<u>在查询数据时，如果直接使用列名访问code和value，将会是无法显示的二进制形式。此时，需要调用与uniq、sum对应的uniqMerge、sumMerge函数</u>：

```sql
SELECT id,city,uniqMerge(code),sumMerge(value) FROM agg_table
GROUP BY id,city
```

​	讲到这里，你是否会认为AggregatingMergeTree使用起来过于烦琐了？连正常进行数据写入都需要借助INSERT…SELECT的句式并调用特殊函数。如果直接像刚才示例中那样使用AggregatingMergeTree，确实会非常麻烦。不过各位读者并不需要忧虑，因为目前介绍的这种使用方法，并不是它的主流用法。

​	**AggregatingMergeTree更为常见的应用方式是结合物化视图使用，将它作为物化视图的表引擎**。<u>而这里的物化视图是作为其他数据表上层的一种查询视图</u>，如图7-1所示。

​	现在用一组示例说明。首先，建立明细数据表，也就是俗称的底表：

```sql
CREATE TABLE agg_table_basic(
  id String,
  city String,
  code String,
  value UInt32
)ENGINE = MergeTree()
PARTITION BY city
ORDER BY (id,city)
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/70830a77b24b4082a59b6ddd61e0e449.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAJ-WXr-WTvOOAgg==,size_20,color_FFFFFF,t_70,g_se,x_16)

​	<u>通常会使用MergeTree作为底表，用于存储全量的明细数据，并以此对外提供实时查询。接着，新建一张物化视图</u>：

```sql
CREATE MATERIALIZED VIEW agg_view
ENGINE = AggregatingMergeTree()
PARTITION BY city
ORDER BY (id,city)
AS SELECT
id,
city,
uniqState(code) AS code,
sumState(value) AS value
FROM agg_table_basic
GROUP BY id, city
```

​	**物化视图使用AggregatingMergeTree表引擎，用于特定场景的数据查询，相比MergeTree，它拥有更高的性能**。

​	在新增数据时，面向的对象是底表MergeTree：

```sql
INSERT INTO TABLE agg_table_basic
VALUES('A000','wuhan','code1',100),('A000','wuhan','code2',200),('A000','zhuhai',
                                                                 'code1',200)
```

​	**<u>数据会自动同步到物化视图，并按照AggregatingMergeTree引擎的规则处理</u>**。

​	在查询数据时，面向的对象是物化视图AggregatingMergeTree：

```sql
SELECT id, sumMerge(value), uniqMerge(code) FROM agg_view GROUP BY id, city
┌─id──┬─sumMerge(value)──┬──uniqMerge(code)─┐
│ A000 │ 200 │ 1 │
│ A000 │ 300 │ 2 │
└─────┴────────────┴────────────┘
```

接下来，简单梳理一下AggregatingMergeTree的处理逻辑。

（1）用ORBER BY排序键作为聚合数据的条件Key。

（2）使用AggregateFunction字段类型定义聚合函数的类型以及聚合的字段。

（3）**只有在合并分区的时候才会触发聚合计算的逻辑**。

（4）**以数据分区为单位来聚合数据**。当分区合并时，同一数据分区内聚合Key相同的数据会被合并计算，而不同分区之间的数据则不会被计算。

（5）在进行数据计算时，因为分区内的数据已经基于ORBER BY排序，所以能够找到那些相邻且拥有相同聚合Key的数据。

（6）在聚合数据时，同一分区内，相同聚合Key的多行数据会合并成一行。对于那些非主键、非AggregateFunction类型字段，则会使用第一行数据的取值。

（7）**AggregateFunction类型的字段使用二进制存储，在写入数据时，需要调用\*State函数；而在查询数据时，则需要调用相应的\*Merge函数。其中，\*表示定义时使用的聚合函数**。

（8）<u>**AggregatingMergeTree通常作为物化视图的表引擎，与普通MergeTree搭配使用**</u>。

## 7.5 CollapsingMergeTree

​	假设现在需要设计一款数据库，该数据库支持对已经存在的数据实现行级粒度的修改或删除，你会怎么设计？一种最符合常理的思维可能是：首先找到保存数据的文件，接着修改这个文件，删除或者修改那些需要变化的数据行。<u>然而在大数据领域，对于ClickHouse这类高性能分析型数据库而言，对数据源文件修改是一件非常奢侈且代价高昂的操作。**相较于直接修改源文件，它们会将修改和删除操作转换成新增操作，即以增代删**</u>。

​	CollapsingMergeTree就是一种通过以增代删的思路，**支持行级数据修改和删除的表引擎**。<u>**它通过定义一个sign标记位字段，记录数据行的状态**</u>。

+ 如果sign标记为1，则表示这是一行有效的数据；
+ 如果sign标记为-1，则表示这行数据需要被删除。

​	**当CollapsingMergeTree分区合并时，同一数据分区内，sign标记为1和-1的一组数据会被抵消删除**。这种1和-1相互抵消的操作，犹如将一张瓦楞纸折叠了一般。这种直观的比喻，想必也正是折叠合并树（CollapsingMergeTree）名称的由来，其折叠的过程如图7-2所示。

![img](https://upload-images.jianshu.io/upload_images/1233356-15a9203fea815d89.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

​	图7-2 CollapsingMergeTree折叠数据的示意图

声明CollapsingMergeTree的方式如下：

```sql
ENGINE = CollapsingMergeTree(sign)
```

​	其中，sign用于指定一个Int8类型的标志位字段。一个完整的使用示例如下所示：

```sql
CREATE TABLE collpase_table(
  id String,
  code Int32,
  create_time DateTime,
  sign Int8
)ENGINE = CollapsingMergeTree(sign)
PARTITION BY toYYYYMM(create_time)
ORDER BY id
```

​	与其他的MergeTree变种引擎一样，**CollapsingMergeTree同样是以ORDER BY排序键作为后续判断数据唯一性的依据**。按照之前的介绍，对于上述collpase_table数据表而言，除了常规的新增数据操作之外，还能够支持两种操作。

​	其一，修改一行数据：

```sql
--修改前的源数据, 它需要被修改
INSERT INTO TABLE collpase_table VALUES('A000',100,'2019-02-20 00:00:00',1)
--镜像数据, ORDER BY字段与源数据相同(其他字段可以不同),sign取反为-1,它会和源数据折叠
INSERT INTO TABLE collpase_table VALUES('A000',100,'2019-02-20 00:00:00',-1)
--修改后的数据 ,sign为1
INSERT INTO TABLE collpase_table VALUES('A000',120,'2019-02-20 00:00:00',1)
```

​	其二，删除一行数据：

```sql
--修改前的源数据, 它需要被删除
INSERT INTO TABLE collpase_table VALUES('A000',100,'2019-02-20 00:00:00',1)
--镜像数据, ORDER BY字段与源数据相同, sign取反为-1, 它会和源数据折叠
INSERT INTO TABLE collpase_table VALUES('A000',100,'2019-02-20 00:00:00',-1)
```

​	CollapsingMergeTree在折叠数据时，遵循以下规则。

+ **如果sign=1比sign=-1的数据多一行，则保留最后一行sign=1的数据**。

+ **如果sign=-1比sign=1的数据多一行，则保留第一行sign=-1的数据**。

+ **如果sign=1和sign=-1的数据行一样多，并且最后一行是sign=1，则保留第一行sign=-1和最后一行sign=1的数据**。

+ **如果sign=1和sign=-1的数据行一样多，并且最后一行是sign=-1，则什么也不保留**。

+ **<u>其余情况，ClickHouse会打印警告日志，但不会报错，在这种情形下，查询结果不可预知</u>**。

​	在使用CollapsingMergeTree的时候，还有几点需要注意。

（1）<u>折叠数据并不是实时触发的，和所有其他的MergeTree变种表引擎一样，这项特性也**只有在分区合并的时候**才会体现。所以在分区合并之前，用户还是会看到旧的数据</u>。解决这个问题的方式有两种。

+ 在查询数据之前，使用`optimize TABLE table_name FINAL`命令强制分区合并，但是这种方法效率极低，在实际生产环境中慎用。

+ 需要改变我们的查询方式。以collpase_table举例，如果原始的SQL如下所示：

  ```sql
  SELECT id,SUM(code),COUNT(code),AVG(code),uniq(code)
  FROM collpase_table
  GROUP BY id
  ```

  则需要改写成如下形式：

  ```sql
  SELECT id,SUM(code * sign),COUNT(code * sign),AVG(code * sign),uniq(code * sign)
  FROM collpase_table
  GROUP BY id
  HAVING SUM(sign) > 0
  ```

（2）**只有相同分区内的数据才有可能被折叠**。不过这项限制对于CollapsingMergeTree来说通常不是问题，因为修改或者删除数据的时候，这些数据的分区规则通常都是一致的，并不会改变。

（3）**<u>最后这项限制可能是CollapsingMergeTree最大的命门所在。CollapsingMergeTree对于写入数据的顺序有着严格要求</u>**。现在用一个示例说明。如果按照正常顺序写入，先写入sign=1，再写入sign=-1，则能够正常折叠：

```sql
--先写入sign=1
INSERT INTO TABLE collpase_table VALUES('A000',102,'2019-02-20 00:00:00',1)
--再写入sign=-1
INSERT INTO TABLE collpase_table VALUES('A000',101,'2019-02-20 00:00:00',-1)
```

现在将写入的顺序置换，先写入sign=-1，再写入sign=1，则不能够折叠：

```sql
--先写入sign=-1
INSERT INTO TABLE collpase_table VALUES('A000',101,'2019-02-20 00:00:00',-1)
--再写入sign=1
INSERT INTO TABLE collpase_table VALUES('A000',102,'2019-02-20 00:00:00',1)
```

​	**这种现象是CollapsingMergeTree的处理机制引起的，因为它要求sign=1和sign=-1的数据相邻。而分区内的数据基于ORBER BY排序，要实现sign=1和sign=-1的数据相邻，则只能依靠严格按照顺序写入**。

​	如果数据的写入程序是单线程执行的，则能够较好地控制写入顺序；如果需要处理的数据量很大，数据的写入程序通常是多线程执行的，那么此时就不能保障数据的写入顺序了。在这种情况下，CollapsingMergeTree的工作机制就会出现问题。为了解决这个问题，ClickHouse另外提供了一个名为VersionedCollapsingMergeTree的表引擎，7.6节会介绍它。

## 7.6 VersionedCollapsingMergeTree

​	VersionedCollapsingMergeTree表引擎的作用与CollapsingMergeTree完全相同，它们的不同之处在于，VersionedCollapsingMergeTree对数据的写入顺序没有要求，在同一个分区内，任意顺序的数据都能够完成折叠操作。VersionedCollapsingMergeTree是如何做到这一点的呢？其实从它的命名各位就应该能够猜出来，是版本号。

​	**在定义VersionedCollapsingMergeTree的时候，除了需要指定sign标记字段以外，还需要指定一个UInt8类型的ver版本号字段**：

```sql
ENGINE = VersionedCollapsingMergeTree(sign,ver)
```

​	一个完整的例子如下：

```sql
CREATE TABLE ver_collpase_table(
  id String,
  code Int32,
  create_time DateTime,
  sign Int8,
  ver UInt8
)ENGINE = VersionedCollapsingMergeTree(sign,ver)
PARTITION BY toYYYYMM(create_time)
ORDER BY id
```

​	VersionedCollapsingMergeTree是如何使用版本号字段的呢？其实很简单，**<u>在定义ver字段之后，VersionedCollapsingMergeTree会自动将ver作为排序条件并增加到ORDER BY的末端</u>**。以上面的ver_collpase_table表为例，在每个数据分区内，数据会按照ORDER BY id，ver DESC排序。所以无论写入时数据的顺序如何，在折叠处理时，都能回到正确的顺序。

​	可以用一组示例证明，首先是删除数据：

```sql
--删除
INSERT INTO TABLE ver_collpase_table VALUES('A000',101,'2019-02-20
00:00:00',-1,1)
INSERT INTO TABLE ver_collpase_table VALUES('A000',102,'2019-02-20 00:00:00',1,1)
```

​	接着是修改数据：

```sql
--修改
INSERT INTO TABLE ver_collpase_table VALUES('A000',101,'2019-02-20
00:00:00',-1,1)
INSERT INTO TABLE ver_collpase_table VALUES('A000',102,'2019-02-20 00:00:00',1,1)
INSERT INTO TABLE ver_collpase_table VALUES('A000',103,'2019-02-20 00:00:00',1,2)
```

​	上述操作中，数据均能够按照正常预期被折叠。

## 7.7 各种MergeTree之间的关系总结

​	经过上述介绍之后是不是觉得MergeTree功能非常丰富？但凡事都有两面性，功能丰富的同时很多朋友也会被这么多表引擎弄晕。其实我们可以使用继承和组合这两种关系来理解整个MergeTree。	

### 7.7.1 继承关系

​	首先，为了便于理解，可以使用继承关系来理解MergeTree。MergeTree表引擎向下派生出6个变种表引擎，如图7-3所示。

![img](https://img2020.cnblogs.com/blog/1287132/202104/1287132-20210416220411550-948812111.png)

​	图7-3 MergeTree家族的继承关系示意图

​	在ClickHouse底层的实现方法中，上述7种表引擎的区别主要体现在**Merge合并**的逻辑部分。图7-4所示是简化后的对象关系。

​	可以看到，在具体的实现逻辑部分，7种MergeTree共用一个主体，在触发Merge动作时，它们调用了各自独有的合并逻辑。

![img](https://cdn.bianchengquan.com/da8ce53cf0240070ce6c69c48cd588ee/blog/5ffc3961b3698.jpeg)

​	图7-4 MergeTree各种表引擎的逻辑部分

​	除MergeTree之外的其他6个变种表引擎的Merge合并逻辑，全部是建立在MergeTree基础之上的，且均继承于MergeTree的MergingSortedBlockInputStream，如图7-5所示。

![img](https://cdn.bianchengquan.com/da8ce53cf0240070ce6c69c48cd588ee/blog/5ffc39615b109.png)

​	图7-5 合并树变种表引擎的Merge逻辑

​	**MergingSortedBlockInputStream的主要作用是按照ORDER BY的规则保持新分区数据的有序性**。而其他6种变种MergeTree的合并逻辑，则是在有序的基础之上“各有所长”，要么是将排序后相邻的重复数据消除、要么是将重复数据累加汇总……

​	所以，从继承关系的角度来看，7种MergeTree的主要区别在于Merge逻辑部分，所以特殊功能只会在Merge合并时才会触发。

### 7.7.2 组合关系

​	上一节已经介绍了7种MergeTree关系，本节介绍ReplicatedMergeTree系列。

​	ReplicatedMergeTree与普通的MergeTree有什么区别呢？我们看图7-6所示。

![img](https://cdn.bianchengquan.com/da8ce53cf0240070ce6c69c48cd588ee/blog/5ffc396128bb1.jpeg)

​	图7-6 ReplicatedMergeTree系列

​	上图中的虚线框部分是MergeTree的能力边界，而**ReplicatedMergeTree在MergeTree能力的基础之上增加了分布式协同的能力，其借助ZooKeeper的消息日志广播功能，实现了副本实例之间的数据同步功能**。

​	ReplicatedMergeTree系列可以用组合关系来理解，如图7-7所示。

![img](https://cdn.bianchengquan.com/da8ce53cf0240070ce6c69c48cd588ee/blog/5ffc3960ed104.png)

​	图7-7 ReplicatedMergeTree组合关系示意图

​	当我们为7种MergeTree加上Replicated前缀后，又能组合出7种新的表引擎，这些ReplicatedMergeTree拥有副本协同的能力。关于ReplicatedMergeTree表引擎的详细说明见第10章。

## 7.8 本章小结

​	本章全面介绍了MergeTree表引擎系列，通过本章我们知道了，合并树家族除了基础表引擎MergeTree之外，还有另外5种常用的变种来引擎。对于MergeTree而言，继上一章介绍了它的核心工作原理之后，本章又进一步介绍了它的TTL机制和多数据块存储。除此之外，我们还知道了MergeTree各个变种表引擎的特点和使用方法，包括支持数据去重的ReplacingMergeTree、支持预先聚合计算的SummingMergeTree与AggregatingMergeTree，以及支持数据更新且能够折叠数据的CollapsingMergeTree与VersionedCollapsingMergeTree。这些MergeTree系列的表引擎，都用ORDER BY作为条件Key，在分区合并时触发各自的处理逻辑。下一章将进一步介绍其他常见表引擎的具体使用方法。
